---
title: "Testing Agents in DevSynth"
date: "2025-07-07"
version: "0.1.0"
tags:
  - "developer-guide"

status: "published"
author: "DevSynth Team"
last_reviewed: "2025-07-10"
---

# Testing Agents in DevSynth

This guide explains how to test agent implementations in the DevSynth project, focusing on both unit testing and behavior-driven testing (BDD) approaches.

## Unit Testing Agents

### BaseAgent Testing

The `BaseAgent` class is the foundation for all agent implementations in DevSynth. It provides common functionality for agent initialization, text generation, and WSDE (Worker-Supervisor-Designer-Evaluator) operations.

The unit tests for `BaseAgent` are located in `tests/unit/application/agents/test_base_agent.py` and cover:

1. **Agent initialization and configuration**
   - Verifying that agent properties are correctly set during initialization
   - Testing the `get_capabilities` method

2. **Text generation**
   - Testing text generation with and without an LLM port
   - Testing text generation with conversation context
   - Verifying error handling in text generation methods

3. **WSDE operations**
   - Testing WSDE creation and updating
   - Verifying that metadata is correctly handled

4. **Role-specific functionality**
   - Testing role-specific prompts for Worker, Supervisor, Designer, Evaluator, and Primus roles


### Testing Concrete Agent Implementations

When testing concrete agent implementations that extend `BaseAgent`, follow these guidelines:

1. **Create a test fixture** that initializes the agent with appropriate configuration
2. **Mock external dependencies** like LLM ports to isolate the agent's behavior
3. **Test the `process` method** with various input scenarios
4. **Verify that the agent's output** matches the expected format and content


Example:

```python
@pytest.fixture
def my_agent(mock_llm_port):
    """Create a MyAgent instance for testing."""
    agent = MyAgent()
    config = AgentConfig(
        name="TestMyAgent",
        agent_type=AgentType.ORCHESTRATOR,
        description="Test My Agent",
        capabilities=["test", "example"]
    )
    agent.initialize(config)
    agent.set_llm_port(mock_llm_port)
    return agent

def test_process_specific_task(my_agent):
    """Test processing a specific task."""
    inputs = {
        "task_type": "specific_task",
        "data": "test data"
    }
    
    result = my_agent.process(inputs)
    
    assert "output" in result
    assert result["agent"] == my_agent.name
    # Add more assertions specific to the expected behavior
```

## Behavior-Driven Testing (BDD) for Agents

DevSynth uses pytest-bdd for behavior-driven testing. BDD tests for agents are organized into:

1. **Feature files** in `tests/behavior/features/` that describe agent behavior in Gherkin syntax
2. **Step definition files** in `tests/behavior/steps/` that implement the steps


### Example: WSDE Agent Model

The WSDE agent model is tested using BDD in:

- Feature file: `tests/behavior/features/wsde_agent_model.feature`
- Step definitions: `tests/behavior/steps/test_wsde_agent_model_steps.py`


These tests verify the non-hierarchical, context-driven agent collaboration model, including:

- Peer-based collaboration
- Context-driven leadership
- Autonomous collaboration
- Consensus-based decision making
- Dialectical review process


### Writing New BDD Tests for Agents

When writing new BDD tests for agents:

1. **Create a feature file** that describes the agent's behavior from a user perspective
2. **Implement step definitions** that create and configure agents
3. **Use a context object** to share state between steps
4. **Mock external dependencies** to isolate the agent's behavior
5. **Make assertions** that verify the agent's behavior matches the expected outcomes


## Best Practices

1. **Follow TDD principles**:
   - Write failing tests first
   - Implement code to make tests pass
   - Refactor as needed

2. **Isolate tests** from external dependencies using mocks

3. **Test edge cases** and error handling

4. **Keep tests focused** on specific behaviors

5. **Use descriptive test names** that explain what is being tested

6. **Document test fixtures** to explain their purpose and configuration

7. **Run tests regularly** to catch regressions early
## Implementation Status

This feature is **planned** and not yet implemented.
