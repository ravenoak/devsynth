2025-10-28 10:27:29,778 - devsynth.testing.run_tests - INFO - test collection cache miss for target=all-tests (fast) — decomposing all-tests into dependent targets
2025-10-28 10:27:29,782 - devsynth.testing.run_tests - INFO - test collection cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:27:35,120 - devsynth.testing.run_tests - INFO - test collection cache miss for target=integration-tests (fast) — collecting via pytest
2025-10-28 10:27:45,152 - devsynth.testing.run_tests - INFO - test collection cache miss for target=behavior-tests (fast) — collecting via pytest
2025-10-28 10:28:34,901 - devsynth.testing.run_tests - INFO - test collection cache hit for target=all-tests (medium)
2025-10-28 10:35:33,630 - devsynth.testing.run_tests - INFO - Coverage data file detected at .coverage (196608 bytes)
2025-10-28 10:35:46,623 - devsynth.testing.run_tests - INFO - Coverage HTML report generated
2025-10-28 10:35:51,207 - devsynth.testing.run_tests - INFO - Coverage JSON report generated
2025-10-28 10:35:51,354 - devsynth.application.cli.commands.run_tests_cmd - WARNING - Rich markup parsing failed, retrying with markup disabled: closing tag '[/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/                                                                               T/pytest-of-caitlyn/pytest-1440/test_init_cmd_creates_config_s0]' at position 2083534 doesn't match any open tag
============================= test session starts ==============================
platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False
min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10
warmup=False warmup_iterations=100000)
rootdir: /Users/caitlyn/Projects/github.com/ravenoak/devsynth
configfile: pytest.ini
plugins: mock-3.15.1, asyncio-1.2.0, anyio-4.11.0, html-4.1.1, xdist-3.8.0,
langsmith-0.4.37, metadata-3.1.1, Faker-37.11.0, benchmark-5.1.0,
hypothesis-6.142.3, bdd-8.1.0, rerunfailures-16.1, cov-7.0.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None,
asyncio_default_test_loop_scope=function
collected 4620 items / 9 deselected / 4611 selected

tests/unit/adapters/cli/test_typer_adapter.py ....                       [  0%]
tests/unit/adapters/issues/test_github_adapter.py .                      [  0%]
tests/unit/adapters/issues/test_jira_adapter.py .                        [  0%]
tests/unit/adapters/llm/test_llm_adapter.py .........                    [  0%]
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py ....          [  0%]
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py ........           [  0%]
tests/unit/adapters/test_agent_adapter.py ...................            [  0%]
tests/unit/adapters/test_backend_resource_gates.py sss                   [  1%]
tests/unit/adapters/test_chromadb_memory_store_unit.py ......            [  1%]
tests/unit/adapters/test_fake_memory_store.py ..                         [  1%]
tests/unit/adapters/test_github_project_adapter.py ........              [  1%]
tests/unit/adapters/test_jira_adapter.py ...                             [  1%]
tests/unit/adapters/test_onnx_runtime_adapter.py sssssss                 [  1%]
tests/unit/adapters/test_provider_safe_defaults.py .....                 [  1%]
tests/unit/adapters/test_provider_stub.py ...                            [  1%]
tests/unit/adapters/test_provider_system.py ............................ [  2%]
..........                                                               [  2%]
tests/unit/adapters/test_provider_system_additional.py .FFF.FFFF........ [  2%]
...........                                                              [  3%]
tests/unit/adapters/test_provider_system_fallbacks_fast.py ...           [  3%]
tests/unit/adapters/test_provider_system_resilience.py ...               [  3%]
tests/unit/adapters/test_resource_gating_seams.py s.                     [  3%]
tests/unit/adapters/test_storage_adapter_protocol.py .                   [  3%]
tests/unit/agents/test_alignment_metrics_tool.py ..                      [  3%]
tests/unit/agents/test_doctor_tool.py ..                                 [  3%]
tests/unit/agents/test_multi_agent_coordinator.py .                      [  3%]
tests/unit/agents/test_run_tests_tool.py ..                              [  3%]
tests/unit/agents/test_security_audit_tool.py ..                         [  3%]
tests/unit/agents/test_test_generator.py ..........                      [  3%]
tests/unit/agents/test_tool_sandbox.py ....                              [  3%]
tests/unit/agents/test_tools.py ...                                      [  3%]
tests/unit/agents/test_wsde_team_coordinator_strict.py ..                [  4%]
tests/unit/api/test_fastapi_testclient_import.py .                       [  4%]
tests/unit/api/test_public_api_contract.py ..                            [  4%]
tests/unit/application/agents/test_base_agent.py ...........             [  4%]
tests/unit/application/agents/test_test_agent_integration.py .           [  4%]
tests/unit/application/agents/test_validation_agent.py .....             [  4%]
tests/unit/application/agents/test_validation_agent_decision.py .....    [  4%]
tests/unit/application/agents/test_wsde_memory_integration_fast.py .     [  4%]
tests/unit/application/cli/commands/test_config_cmd.py ....              [  4%]
tests/unit/application/cli/commands/test_doctor_cmd_typed.py .           [  4%]
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py .       [  4%]
tests/unit/application/cli/commands/test_ingest_cli_command.py .         [  4%]
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py . [
4%]
                                                                         [  4%]
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y . [  4%]
...                                                                      [  4%]
tests/unit/application/cli/commands/test_module_imports.py ............. [  5%]
.F...................................                                    [  5%]
tests/unit/application/cli/commands/test_parse_feature_options_unit.py . [  5%]
...                                                                      [  6%]
tests/unit/application/cli/commands/test_run_pipeline_cmd.py ...         [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd.py ....F......FFF [  6%]
                                                                         [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py EEEE [  6%]
E                                                                        [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py . [  6%]
.FFFF                                                                    [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py . [
6%]
FFFFFF                                                                   [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py
F [  6%]
FF                                                                       [  6%]
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py F [
6%]
FFFFFF                                                                   [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py EE   [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_features.py EE    [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py E   [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py FF   [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py E [  7%]
EEE                                                                      [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py EE     [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_more.py EEE       [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py E [
7%]
E                                                                        [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py F [
7%]
F                                                                        [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py EE [  7%]
EE                                                                       [  7%]
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py F [  7%]
FF                                                                       [  7%]
tests/unit/application/cli/commands/test_run_tests_dummy.py .            [  7%]
tests/unit/application/cli/commands/test_run_tests_features.py F         [  7%]
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py F [  7%]
                                                                         [  7%]
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py E [  7%]
E                                                                        [  7%]
tests/unit/application/cli/commands/test_run_tests_subprocess.py F       [  7%]
tests/unit/application/cli/commands/test_run_tests_validation.py FF      [  7%]
tests/unit/application/cli/commands/test_security_audit_cmd.py ....      [  7%]
tests/unit/application/cli/commands/test_testing_cmd.py ..........       [  8%]
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py ..      [  8%]
tests/unit/application/cli/test_command_output_formatter.py .........    [  8%]
tests/unit/application/cli/test_ingest_cmd.py ..                         [  8%]
tests/unit/application/cli/test_long_running_progress.py .............   [  8%]
tests/unit/application/cli/test_long_running_progress_deterministic.py . [  8%]
....                                                                     [  8%]
tests/unit/application/cli/test_output.py .........                      [  8%]
tests/unit/application/cli/test_progress.py F                            [  8%]
tests/unit/application/cli/test_progress_aliasing.py .........           [  9%]
tests/unit/application/cli/test_requirements_commands.py ......          [  9%]
tests/unit/application/cli/test_requirements_gathering.py .              [  9%]
tests/unit/application/cli/test_run_tests_cmd.py FFFFF                   [  9%]
tests/unit/application/cli/test_run_tests_cmd_options.py FFF             [  9%]
tests/unit/application/cli/test_run_tests_cmd_smoke.py EEEE              [  9%]
tests/unit/application/cli/test_setup_wizard.py EEEEEE                   [  9%]
tests/unit/application/cli/test_setup_wizard_textual.py ..               [  9%]
tests/unit/application/cli/test_sprint_cmd_types.py ...                  [  9%]
tests/unit/application/code_analysis/test_analyzer.py .                  [  9%]
tests/unit/application/code_analysis/test_ast_transformer.py ..........  [ 10%]
tests/unit/application/code_analysis/test_ast_workflow_integration.py .. [ 10%]
...                                                                      [ 10%]
tests/unit/application/code_analysis/test_project_state_analyzer.py .... [ 10%]
.....                                                                    [ 10%]
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py
F [ 10%]
                                                                         [ 10%]
tests/unit/application/code_analysis/test_repo_analyzer.py ..            [ 10%]
tests/unit/application/code_analysis/test_self_analyzer.py ...........   [ 10%]
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py . [ 10%]
                                                                         [ 10%]
tests/unit/application/code_analysis/test_transformer.py ...........     [ 10%]
tests/unit/application/code_analysis/test_transformer_basic.py .         [ 10%]
tests/unit/application/code_analysis/test_transformer_helpers.py ...     [ 10%]
tests/unit/application/collaboration/test_agent_collaboration_system.py . [ 11%]
..                                                                       [ 11%]
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py . [ 11%]
.                                                                        [ 11%]
tests/unit/application/collaboration/test_memory_utils_conversion.py .   [ 11%]
tests/unit/application/collaboration/test_message_protocol.py ...        [ 11%]
tests/unit/application/collaboration/test_peer_review_store.py ....      [ 11%]
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py F.   [ 11%]
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py . [ 11%]
                                                                         [ 11%]
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py . [
11%]
..F                                                                      [ 11%]
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py . [ 11%]
.                                                                        [ 11%]
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py . [
11%]
                                                                         [ 11%]
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py . [
11%]
                                                                         [ 11%]
tests/unit/application/documentation/test_documentation_fetcher_parsing.py . [
11%]
...                                                                      [ 11%]
tests/unit/application/documentation/test_ingestion_search_variance.py . [ 11%]
.                                                                        [ 11%]
tests/unit/application/edrr/coordinator/test_core.py ...FFFFFFFFFFFFFFFF [ 12%]
FFFF                                                                     [ 12%]
tests/unit/application/edrr/test_coordinator.py .                        [ 12%]
tests/unit/application/edrr/test_coordinator_core.py F                   [ 12%]
tests/unit/application/edrr/test_coordinator_reasoning.py FF             [ 12%]
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py .          [ 12%]
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py .        [ 12%]
tests/unit/application/edrr/test_persistence_module.py ..........        [ 12%]
tests/unit/application/edrr/test_phase_management_module.py ..FF....     [ 12%]
tests/unit/application/edrr/test_reasoning_loop_retries.py F             [ 12%]
tests/unit/application/edrr/test_recursion_termination.py ss             [ 12%]
tests/unit/application/edrr/test_sprint_planning.py ....                 [ 12%]
tests/unit/application/edrr/test_sprint_retrospective.py .....           [ 12%]
tests/unit/application/edrr/test_threshold_helpers.py .....              [ 13%]
tests/unit/application/ingestion/test_ingestion_pure.py ...              [ 13%]
tests/unit/application/ingestion/test_phases.py ..                       [ 13%]
tests/unit/application/llm/test_import_without_openai.py .F              [ 13%]
tests/unit/application/llm/test_lmstudio_health_check.py FF              [ 13%]
tests/unit/application/llm/test_lmstudio_offline_resilience.py ..        [ 13%]
tests/unit/application/llm/test_lmstudio_provider.py ...........FFFFFFFF [ 13%]
FF......                                                                 [ 13%]
tests/unit/application/llm/test_offline_provider.py ...                  [ 13%]
tests/unit/application/llm/test_openai_env_key_mock.py .                 [ 13%]
tests/unit/application/llm/test_openai_offline_resilience.py ....        [ 14%]
tests/unit/application/llm/test_provider_factory.py FF                   [ 14%]
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py ...  [ 14%]
tests/unit/application/llm/test_provider_selection.py FF                 [ 14%]
tests/unit/application/memory/test_chromadb_store.py s                   [ 14%]
tests/unit/application/memory/test_chromadb_store_typed.py ss            [ 14%]
tests/unit/application/memory/test_circuit_breaker.py ..                 [ 14%]
tests/unit/application/memory/test_duckdb_store_schema_flags.py ss       [ 14%]
tests/unit/application/memory/test_error_logger.py ....                  [ 14%]
tests/unit/application/memory/test_execution_learning_integration.py .FF [ 14%]
..FFEEEEEEF.FF                                                           [ 14%]
tests/unit/application/memory/test_faiss_store.py ssss                   [ 14%]
tests/unit/application/memory/test_fast_in_memory_components.py ........ [ 15%]
                                                                         [ 15%]
tests/unit/application/memory/test_graph_memory_adapter.py .             [ 15%]
tests/unit/application/memory/test_lmdb_store.py ....                    [ 15%]
tests/unit/application/memory/test_memory_manager.py ...                 [ 15%]
tests/unit/application/memory/test_memory_system_adapter_unit.py ....... [ 15%]
.                                                                        [ 15%]
tests/unit/application/memory/test_metadata_serialization_helpers.py ..F [ 15%]
.                                                                        [ 15%]
tests/unit/application/memory/test_phase3_integration_system.py .F...F.. [ 15%]
..F.....EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE                              [ 16%]
tests/unit/application/memory/test_query_router.py ..F.                  [ 16%]
tests/unit/application/memory/test_rdflib_store_transactions.py ...      [ 16%]
tests/unit/application/memory/test_search_memory_fallback.py .           [ 16%]
tests/unit/application/memory/test_sync_manager_transactions.py F.       [ 16%]
tests/unit/application/memory/test_tiered_cache_termination.py ..        [ 16%]
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py F       [ 16%]
tests/unit/application/memory/test_vector_memory_adapter_extra.py ss     [ 16%]
tests/unit/application/orchestration/test_dialectical_reasoner.py ...    [ 16%]
tests/unit/application/promises/test_agent_create_promise.py .           [ 17%]
tests/unit/application/promises/test_interface_not_implemented.py .      [ 17%]
tests/unit/application/promises/test_interface_pure.py ...               [ 17%]
tests/unit/application/prompts/test_auto_tuning_pure.py ...              [ 17%]
tests/unit/application/requirements/test_dialectical_reasoner.py ..F.... [ 17%]
..FFFF                                                                   [ 17%]
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y . [ 17%]
.                                                                        [ 17%]
tests/unit/application/requirements/test_dialectical_reasoner_pure.py .. [ 17%]
..                                                                       [ 17%]
tests/unit/application/requirements/test_interactions.py ...             [ 17%]
tests/unit/application/requirements/test_requirement_service_dtos.py ..  [ 17%]
tests/unit/application/requirements/test_wizard.py ...                   [ 17%]
tests/unit/application/sprint/test_planning.py .                         [ 17%]
tests/unit/application/test_documentation_fetcher.py ..                  [ 17%]
tests/unit/application/testing/test_enhanced_test_collector.py ..EEE.EEE [ 18%]
E..EEEEE..EF.....F....                                                   [ 18%]
tests/unit/application/utils/test_extras_helper.py ...                   [ 18%]
tests/unit/behavior/test_alignment_metrics_steps_unit.py F               [ 18%]
tests/unit/behavior/test_analyze_commands_steps_unit.py FF               [ 18%]
tests/unit/cli/test_cli_entry.py .                                       [ 18%]
tests/unit/cli/test_cli_error_handling.py F                              [ 18%]
tests/unit/cli/test_cli_help.py .                                        [ 18%]
tests/unit/cli/test_command_module_loading.py .                          [ 18%]
tests/unit/cli/test_command_registry.py F.                               [ 18%]
tests/unit/cli/test_completion_progress.py .                             [ 18%]
tests/unit/cli/test_entry_points_help.py ...                             [ 18%]
tests/unit/cli/test_help_examples.py ..                                  [ 18%]
tests/unit/cli/test_import_gating.py ..                                  [ 18%]
tests/unit/cli/test_init_features_option.py ..                           [ 18%]
tests/unit/cli/test_key_commands_help.py .......                         [ 19%]
tests/unit/cli/test_logging_flags.py FFF                                 [ 19%]
tests/unit/cli/test_mvu_commands.py ..                                   [ 19%]
tests/unit/cli/test_mvuu_command_registration.py .                       [ 19%]
tests/unit/cli/test_mvuu_dashboard_smoke.py F                            [ 19%]
tests/unit/cli/test_mvuu_dashboard_telemetry.py ....                     [ 19%]
tests/unit/cli/test_run_tests_regression.py F                            [ 19%]
tests/unit/cli/test_version.py .                                         [ 19%]
tests/unit/config/test_config_llm_env.py F                               [ 19%]
tests/unit/config/test_exception_handling.py .....                       [ 19%]
tests/unit/config/test_feature_flag_defaults.py ..                       [ 19%]
tests/unit/config/test_provider_env.py ....                              [ 19%]
tests/unit/config/test_provider_env_apply_and_parse.py ...               [ 19%]
tests/unit/config/test_provider_env_behavior.py ....                     [ 19%]
tests/unit/config/test_provider_env_bool_parsing_edges.py ...            [ 19%]
tests/unit/config/test_provider_env_with_test_defaults.py ..             [ 19%]
tests/unit/config/test_unified_loader.py .                               [ 19%]
tests/unit/core/mvu/test_api.py ..                                       [ 19%]
tests/unit/core/mvu/test_atomic_rewrite.py .                             [ 19%]
tests/unit/core/mvu/test_linter.py ......                                [ 20%]
tests/unit/core/mvu/test_mvuu_schema_validation.py .                     [ 20%]
tests/unit/core/mvu/test_report.py ..                                    [ 20%]
tests/unit/core/mvu/test_storage.py ..                                   [ 20%]
tests/unit/core/mvu/test_validator.py ...                                [ 20%]
tests/unit/core/test_config_loader.py .                                  [ 20%]
tests/unit/core/test_config_loader_json_types.py ...                     [ 20%]
tests/unit/core/test_config_loader_mvu.py F                              [ 20%]
tests/unit/core/test_config_loader_optional_deps.py ...                  [ 20%]
tests/unit/core/test_config_loader_validation.py ....................... [ 20%]
....                                                                     [ 21%]
tests/unit/core/test_deterministic_fixtures.py ...                       [ 21%]
tests/unit/core/test_mvu.py ..                                           [ 21%]
tests/unit/deployment/test_bootstrap_script.py FFF                       [ 21%]
tests/unit/deployment/test_deployment_scripts.py ..                      [ 21%]
tests/unit/deployment/test_enforcement.py ..                             [ 21%]
tests/unit/deployment/test_health_check_smoke.py FFFFFF                  [ 21%]
tests/unit/deployment/test_scripts_dir.py ..                             [ 21%]
tests/unit/deployment/test_security_hardening.py ......                  [ 21%]
tests/unit/devsynth/test_consensus.py .....                              [ 21%]
tests/unit/devsynth/test_fallback_reliability.py ..                      [ 21%]
tests/unit/devsynth/test_logger.py ...                                   [ 21%]
tests/unit/devsynth/test_metrics.py ....                                 [ 21%]
tests/unit/devsynth/test_simple_addition.py ..                           [ 21%]
tests/unit/docs/test_dialectical_audit.py ..                             [ 22%]
tests/unit/domain/interfaces/test_interfaces.py ...                      [ 22%]
tests/unit/domain/models/test_agent_coverage.py ..                       [ 22%]
tests/unit/domain/models/test_memetic_unit.py ...........                [ 22%]
tests/unit/domain/models/test_project.py ...                             [ 22%]
tests/unit/domain/models/test_project_model.py ............              [ 22%]
tests/unit/domain/models/test_wsde.py ...F...FFFF                        [ 22%]
tests/unit/domain/models/test_wsde_base_methods.py ..                    [ 22%]
tests/unit/domain/models/test_wsde_code_improvements.py ...              [ 23%]
tests/unit/domain/models/test_wsde_decision_making.py ......             [ 23%]
tests/unit/domain/models/test_wsde_dialectical_helpers.py ...            [ 23%]
tests/unit/domain/models/test_wsde_dialectical_typing.py .               [ 23%]
tests/unit/domain/models/test_wsde_dialectical_workflow.py F.            [ 23%]
tests/unit/domain/models/test_wsde_dynamic_workflows.py .F               [ 23%]
tests/unit/domain/models/test_wsde_enhanced_dialectical.py ......        [ 23%]
tests/unit/domain/models/test_wsde_knowledge.py .....                    [ 23%]
tests/unit/domain/models/test_wsde_roles_personas.py ......              [ 23%]
tests/unit/domain/models/test_wsde_security_checks.py FFF                [ 23%]
tests/unit/domain/models/test_wsde_solution_analysis.py ....             [ 23%]
tests/unit/domain/models/test_wsde_strategies.py .F.                     [ 23%]
tests/unit/domain/models/test_wsde_team.py ..F..F                        [ 24%]
tests/unit/domain/models/test_wsde_utils.py ......F.                     [ 24%]
tests/unit/domain/models/test_wsde_voting_logic.py ....                  [ 24%]
tests/unit/domain/test_code_analysis_interfaces.py ...                   [ 24%]
tests/unit/domain/test_wsde_expertise_score.py F                         [ 24%]
tests/unit/domain/test_wsde_facade.py .F                                 [ 24%]
tests/unit/domain/test_wsde_facade_roles.py FF                           [ 24%]
tests/unit/domain/test_wsde_peer_review_workflow.py ..                   [ 24%]
tests/unit/domain/test_wsde_phase_role_rotation.py .F.                   [ 24%]
tests/unit/domain/test_wsde_primus_selection.py ..FFF.F                  [ 24%]
tests/unit/domain/test_wsde_team.py .FFFF....F.FF                        [ 25%]
tests/unit/domain/test_wsde_voting_logic.py FFFFFFF....                  [ 25%]
tests/unit/fallback/test_retry_counts.py ..                              [ 25%]
tests/unit/fallback/test_retry_predicates.py ..                          [ 25%]
tests/unit/general/test_agent_coordinator.py .......                     [ 25%]
tests/unit/general/test_agent_models.py ....                             [ 25%]
tests/unit/general/test_agent_system.py ............                     [ 25%]
tests/unit/general/test_anthropic_provider_unit.py .....                 [ 25%]
tests/unit/general/test_api.py ..                                        [ 26%]
tests/unit/general/test_api_health.py FF                                 [ 26%]
tests/unit/general/test_atomic_rewrite_cli.py ...                        [ 26%]
tests/unit/general/test_backend_resource_flags.py ...F                   [ 26%]
tests/unit/general/test_base.py .                                        [ 26%]
tests/unit/general/test_chroma_db_adapter.py .FFFFFF                     [ 26%]
tests/unit/general/test_chromadb_store.py ssssss                         [ 26%]
tests/unit/general/test_cli_commands.py ..                               [ 26%]
tests/unit/general/test_code_analysis_interface.py ...                   [ 26%]
tests/unit/general/test_code_analysis_models.py ..                       [ 26%]
tests/unit/general/test_code_analyzer.py ....                            [ 26%]
tests/unit/general/test_config_loader.py .....                           [ 26%]
tests/unit/general/test_config_settings.py .................             [ 27%]
tests/unit/general/test_core_config_loader.py ...                        [ 27%]
tests/unit/general/test_core_values.py ...                               [ 27%]
tests/unit/general/test_core_workflows.py ...........                    [ 27%]
tests/unit/general/test_delegate_task_disabled.py .                      [ 27%]
tests/unit/general/test_dialectical_reasoner.py F.FF.                    [ 27%]
tests/unit/general/test_documentation_fetcher.py .                       [ 27%]
tests/unit/general/test_dpg_flag.py FFs                                  [ 27%]
tests/unit/general/test_edrr_cycle_cmd.py .......                        [ 27%]
tests/unit/general/test_edrr_manifest_string.py .                        [ 27%]
tests/unit/general/test_exception_logging.py .                           [ 27%]
tests/unit/general/test_exceptions.py .....................              [ 28%]
tests/unit/general/test_fallback_utils.py .                              [ 28%]
tests/unit/general/test_ingest_cmd.py .....................F             [ 28%]
tests/unit/general/test_ingestion_edrr_integration.py .                  [ 28%]
tests/unit/general/test_ingestion_type_hints.py s                        [ 28%]
tests/unit/general/test_inspect_config_cmd.py ......                     [ 29%]
tests/unit/general/test_isolation.py ......                              [ 29%]
tests/unit/general/test_isolation_auto_marking.py ..                     [ 29%]
tests/unit/general/test_kuzu_adapter.py ssss                             [ 29%]
tests/unit/general/test_kuzu_embedded_missing.py .                       [ 29%]
tests/unit/general/test_langgraph_adapter.py ............                [ 29%]
tests/unit/general/test_llm_provider_selection.py FF                     [ 29%]
tests/unit/general/test_lmstudio_integration_regression.py ..F.F..       [ 29%]
tests/unit/general/test_lmstudio_service.py E                            [ 29%]
tests/unit/general/test_logger.py ..                                     [ 29%]
tests/unit/general/test_logging_setup.py ....                            [ 30%]
tests/unit/general/test_logging_setup_idempotent.py ...                  [ 30%]
tests/unit/general/test_memory_models.py .....                           [ 30%]
tests/unit/general/test_memory_store.py .                                [ 30%]
tests/unit/general/test_memory_system.py ....................            [ 30%]
tests/unit/general/test_memory_system_with_chromadb.py ssss              [ 30%]
tests/unit/general/test_methodology_logging.py .                         [ 30%]
tests/unit/general/test_multi_agent_adapter_workflow.py F.               [ 30%]
tests/unit/general/test_mvu_exec_cli.py ..                               [ 30%]
tests/unit/general/test_mvu_exec_cmd.py ..                               [ 30%]
tests/unit/general/test_mvu_init_cmd.py .                                [ 30%]
tests/unit/general/test_mvu_lint_cli.py FF                               [ 30%]
tests/unit/general/test_mvuu_dashboard_cli.py .                          [ 30%]
tests/unit/general/test_mypy_config.py ..                                [ 31%]
tests/unit/general/test_no_devsynth_dir_creation.py ..                   [ 31%]
tests/unit/general/test_onnx_port.py .                                   [ 31%]
tests/unit/general/test_path_restrictions.py F.                          [ 31%]
tests/unit/general/test_ports_with_fixtures.py E                         [ 31%]
tests/unit/general/test_primus_selection.py ..FF.F                       [ 31%]
tests/unit/general/test_project_yaml.py .....                            [ 31%]
tests/unit/general/test_promise_agent.py ...........                     [ 31%]
tests/unit/general/test_promise_system.py ..............                 [ 31%]
tests/unit/general/test_provider_logging.py EE                           [ 31%]
tests/unit/general/test_requirement_models.py .....                      [ 32%]
tests/unit/general/test_requirement_repository_interface.py .            [ 32%]
tests/unit/general/test_requirement_repository_port_interface.py ..      [ 32%]
tests/unit/general/test_requirement_service.py .....                     [ 32%]
tests/unit/general/test_resource_markers.py ..F.sF                       [ 32%]
tests/unit/general/test_retry_failure_scenarios.py ..                    [ 32%]
tests/unit/general/test_speed_option.py F                                [ 32%]
tests/unit/general/test_sync_manager_persistence.py .                    [ 32%]
tests/unit/general/test_template_location.py ..                          [ 32%]
tests/unit/general/test_test_first_metrics.py .....                      [ 32%]
tests/unit/general/test_token_tracker.py ......                          [ 32%]
tests/unit/general/test_unified_agent_code_prompt.py .                   [ 32%]
tests/unit/general/test_unified_config_loader.py .......                 [ 32%]
tests/unit/general/test_unit_cli_commands.py .                           [ 32%]
tests/unit/general/test_ux_bridge.py FF                                  [ 32%]
tests/unit/general/test_workflow.py F.F...                               [ 33%]
tests/unit/general/test_workflow_models.py ....                          [ 33%]
tests/unit/general/test_wsde_dynamic_roles.py .                          [ 33%]
tests/unit/general/test_wsde_model.py ..                                 [ 33%]
tests/unit/general/test_wsde_role_mapping.py F                           [ 33%]
tests/unit/general/test_wsde_team_extended.py .....F.F.FFFFFFFFFFFFFF    [ 33%]
tests/unit/general/test_wsde_team_voting_invalid.py FF                   [ 33%]
tests/unit/general/test_wsde_voting.py FFFF                              [ 33%]
tests/unit/general/test_wsde_voting_mechanisms.py FFFFFF                 [ 34%]
tests/unit/infrastructure/test_test_infrastructure_sanity.py .           [ 34%]
tests/unit/integrations/test_autoresearch_client.py ...                  [ 34%]
tests/unit/interface/test_agent_api_fastapi_guard.py .                   [ 34%]
tests/unit/interface/test_agentapi_enhanced.py ...................       [ 34%]
tests/unit/interface/test_agentapi_enhanced_bridge.py ssss               [ 34%]
tests/unit/interface/test_agentapi_rate_limit_progress.py ssss           [ 34%]
tests/unit/interface/test_api_endpoints.py ..F                           [ 34%]
tests/unit/interface/test_cli_components.py .                            [ 34%]
tests/unit/interface/test_cli_progress_indicator.py ...                  [ 34%]
tests/unit/interface/test_cli_prompt_toolkit_bridge.py ...               [ 34%]
tests/unit/interface/test_cli_uxbridge_noninteractive.py .F              [ 35%]
tests/unit/interface/test_command_output.py ....                         [ 35%]
tests/unit/interface/test_dpg_ui.py ...                                  [ 35%]
tests/unit/interface/test_enhanced_error_handler.py ..                   [ 35%]
tests/unit/interface/test_mvuu_dashboard.py ..........                   [ 35%]
tests/unit/interface/test_nicegui_bridge.py ....                         [ 35%]
tests/unit/interface/test_nicegui_webui.py ......                        [ 35%]
tests/unit/interface/test_output_formatter_command_options_fast.py ...   [ 35%]
tests/unit/interface/test_output_formatter_core_behaviors.py ........... [ 35%]
                                                                         [ 35%]
tests/unit/interface/test_output_formatter_error_rendering_fast.py ...   [ 36%]
tests/unit/interface/test_output_formatter_fallbacks.py ....             [ 36%]
tests/unit/interface/test_output_formatter_structured_fast.py .......... [ 36%]
.................                                                        [ 36%]
tests/unit/interface/test_output_sanitization.py F                       [ 36%]
tests/unit/interface/test_progress_helpers.py ..                         [ 36%]
tests/unit/interface/test_progress_utils.py ......                       [ 36%]
tests/unit/interface/test_prompt_toolkit_adapter.py ...                  [ 36%]
tests/unit/interface/test_research_telemetry.py ......                   [ 37%]
tests/unit/interface/test_textual_ux_bridge.py ....s                     [ 37%]
tests/unit/interface/test_ux_bridge_coverage.py .......                  [ 37%]
tests/unit/interface/test_uxbridge_aliases.py .F                         [ 37%]
tests/unit/interface/test_webui_behavior_checklist_fast.py FFFFFFFFFFFFF [ 37%]
FFFFFFFFFFFFF                                                            [ 37%]
tests/unit/interface/test_webui_bootstrap_fast.py FFF                    [ 37%]
tests/unit/interface/test_webui_bridge_aa_coverage.py F.                 [ 38%]
tests/unit/interface/test_webui_bridge_cli_parity.py .                   [ 38%]
tests/unit/interface/test_webui_bridge_fast_suite.py FF.......           [ 38%]
tests/unit/interface/test_webui_bridge_handshake.py ....FF...            [ 38%]
tests/unit/interface/test_webui_bridge_normalize.py ....                 [ 38%]
tests/unit/interface/test_webui_bridge_progress.py .F..F....             [ 38%]
tests/unit/interface/test_webui_bridge_require_streamlit.py ..           [ 38%]
tests/unit/interface/test_webui_bridge_routing.py .....                  [ 38%]
tests/unit/interface/test_webui_bridge_spec_alignment.py .F....          [ 39%]
tests/unit/interface/test_webui_bridge_state_fast.py ...                 [ 39%]
tests/unit/interface/test_webui_bridge_targeted.py ........              [ 39%]
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py ....... [ 39%]
                                                                         [ 39%]
tests/unit/interface/test_webui_commands.py F..........                  [ 39%]
tests/unit/interface/test_webui_dashboard_toggles_fast.py EE             [ 39%]
tests/unit/interface/test_webui_display_and_layout.py FEEEEEEEEEEEEEEEEE [ 40%]
EEEEEEEEEEEEEEEEEEEE                                                     [ 40%]
tests/unit/interface/test_webui_display_guidance.py EEEE                 [ 40%]
tests/unit/interface/test_webui_enhanced.py FFFFFFFFF                    [ 40%]
tests/unit/interface/test_webui_handle_command_errors.py EEEEEEE         [ 40%]
tests/unit/interface/test_webui_layout_and_display_branching.py FFFFFFFF [ 41%]
FFFFFF                                                                   [ 41%]
tests/unit/interface/test_webui_layout_and_messaging.py EEEEEEEEEEEEEEEE [ 41%]
EEE                                                                      [ 41%]
tests/unit/interface/test_webui_lazy_loader_fast.py FFF                  [ 41%]
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py EFEE   [ 41%]
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py FE..        [ 41%]
tests/unit/interface/test_webui_progress.py FFFFF                        [ 42%]
tests/unit/interface/test_webui_progress_cascade_fast.py EEEEEE          [ 42%]
tests/unit/interface/test_webui_progress_time.py s                       [ 42%]
tests/unit/interface/test_webui_rendering.py ....F..F..F..F.......F..... [ 42%]
.                                                                        [ 42%]
tests/unit/interface/test_webui_rendering_module.py sss                  [ 42%]
tests/unit/interface/test_webui_rendering_progress.py ..                 [ 42%]
tests/unit/interface/test_webui_require_streamlit.py FF                  [ 42%]
tests/unit/interface/test_webui_requirements_wizard.py FFFFFFFF          [ 43%]
tests/unit/interface/test_webui_routing.py .......                       [ 43%]
tests/unit/interface/test_webui_run_edge_cases.py FFFFFFF.               [ 43%]
tests/unit/interface/test_webui_run_fast.py E                            [ 43%]
tests/unit/interface/test_webui_simulations_fast.py FFFF.F.              [ 43%]
tests/unit/interface/test_webui_state_errors.py .                        [ 43%]
tests/unit/interface/test_webui_streamlit_free_progress_fast.py EEE      [ 43%]
tests/unit/interface/test_webui_streamlit_free_regressions.py F.FFFF.... [ 43%]
........F.                                                               [ 44%]
tests/unit/interface/test_webui_streamlit_stub.py EFEEEE                 [ 44%]
tests/unit/interface/test_webui_targeted_branches.py EEEEEEEEE           [ 44%]
tests/unit/interface/webui/test_rendering.py ....F..F..F..F.......F..... [ 45%]
.                                                                        [ 45%]
tests/unit/llm/test_lmstudio_provider.py .....FFF.F.....FFFFF            [ 45%]
tests/unit/llm/test_openai_provider.py ..sF..FF.........FFFFF            [ 45%]
tests/unit/llm/test_openrouter_provider.py ..F.F.FF.F........FFFF        [ 46%]
tests/unit/logging/test_logging_setup.py .......................         [ 46%]
tests/unit/logging/test_logging_setup_additional_paths.py .......        [ 47%]
tests/unit/logging/test_logging_setup_branches.py .....                  [ 47%]
tests/unit/logging/test_logging_setup_configuration.py ...               [ 47%]
tests/unit/logging/test_logging_setup_configure_logging.py .....F...F    [ 47%]
tests/unit/logging/test_logging_setup_contexts.py ....                   [ 47%]
tests/unit/logging/test_logging_setup_invariants.py ......               [ 47%]
tests/unit/logging/test_logging_setup_levels.py ...                      [ 47%]
tests/unit/logging/test_logging_setup_retention.py F...FF                [ 47%]
tests/unit/memory/test_issue3_regression_guard.py .                      [ 47%]
tests/unit/memory/test_layered_cache.py ...                              [ 47%]
tests/unit/memory/test_layered_cache_runtime_protocol.py ...             [ 48%]
tests/unit/memory/test_sync_manager_protocol.py F.FFFFFF.F               [ 48%]
tests/unit/memory/test_sync_manager_protocol_runtime.py ...........      [ 48%]
tests/unit/memory/test_sync_manager_transaction_failure.py .             [ 48%]
tests/unit/memory/test_transaction_lifecycle_failures.py ...             [ 48%]
tests/unit/methodology/edrr/test_reasoning_loop.py ..                    [ 48%]
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py . [ 48%]
......                                                                   [ 48%]
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py . [ 48%]
.............                                                            [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py ........ [ 49%]
                                                                         [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py ..    [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py .........  [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py ...       [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_retry.py .........       [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py ....       [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py ..     [ 49%]
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py ....... [ 50%]
                                                                         [ 50%]
tests/unit/methodology/test_adhoc_adapter.py ..                          [ 50%]
tests/unit/methodology/test_dialectical_reasoner_termination.py ..       [ 50%]
tests/unit/methodology/test_dialectical_reasoning.py FFF                 [ 50%]
tests/unit/methodology/test_dialectical_reasoning_loop.py FFF            [ 50%]
tests/unit/methodology/test_edrr_coordinator.py ..                       [ 50%]
tests/unit/methodology/test_kanban_adapter.py ..                         [ 50%]
tests/unit/methodology/test_milestone_adapter.py ..                      [ 50%]
tests/unit/methodology/test_reasoning_loop_time_budget.py F              [ 50%]
tests/unit/methodology/test_sprint_adapter.py ...F...                    [ 50%]
tests/unit/methodology/test_sprint_hooks.py FF                           [ 50%]
tests/unit/orchestration/test_graph_transitions_and_controls.py F.FFF    [ 50%]
tests/unit/policies/test_verify_security_policy.py ..                    [ 50%]
tests/unit/providers/test_provider_contract.py .                         [ 50%]
tests/unit/providers/test_provider_stub_offline.py F                     [ 50%]
tests/unit/providers/test_provider_system_additional.py .........        [ 50%]
tests/unit/providers/test_provider_system_branches.py .................. [ 51%]
....................                                                     [ 51%]
tests/unit/providers/test_resource_gating_meta.py ..                     [ 51%]
tests/unit/requirements/test_dialectical_reasoner_determinism.py ..F..   [ 51%]
tests/unit/retrieval/test_backend_gating_smoke.py sss.                   [ 52%]
tests/unit/scripts/test_analyze_test_dependencies.py ........F...        [ 52%]
tests/unit/scripts/test_audit_testing_scripts.py .............           [ 52%]
tests/unit/scripts/test_auto_issue_comment.py ...                        [ 52%]
tests/unit/scripts/test_benchmark_test_execution.py .F.F......           [ 52%]
tests/unit/scripts/test_check_internal_links.py ..                       [ 52%]
tests/unit/scripts/test_enhanced_test_parser.py ...                      [ 52%]
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py F          [ 53%]
tests/unit/scripts/test_examples_smoke_script.py ..                      [ 53%]
tests/unit/scripts/test_find_syntax_errors.py FF                         [ 53%]
tests/unit/scripts/test_gen_ref_pages.py .                               [ 53%]
tests/unit/scripts/test_generate_quality_report.py .....FF.              [ 53%]
tests/unit/scripts/test_run_all_tests_wrapper.py ...                     [ 53%]
tests/unit/scripts/test_security_ops.py ....                             [ 53%]
tests/unit/scripts/test_security_scan_script.py .                        [ 53%]
tests/unit/scripts/test_verify_coverage_threshold.py ..                  [ 53%]
tests/unit/scripts/test_verify_mvuu_references.py ....                   [ 53%]
tests/unit/scripts/test_verify_release_state.py ...........              [ 53%]
tests/unit/scripts/test_verify_test_markers.py .F....                    [ 53%]
tests/unit/scripts/test_verify_test_markers_cli.py ..                    [ 54%]
tests/unit/scripts/test_verify_test_markers_cross_check.py .             [ 54%]
tests/unit/scripts/test_wsde_edrr_simulation.py .                        [ 54%]
tests/unit/security/test_api_authentication.py .....                     [ 54%]
tests/unit/security/test_auth_and_encryption_defaults.py .....           [ 54%]
tests/unit/security/test_authentication_optional_dependency.py .         [ 54%]
tests/unit/security/test_authorization_checks.py ..                      [ 54%]
tests/unit/security/test_deployment_coverage.py ......                   [ 54%]
tests/unit/security/test_encryption.py ..........                        [ 54%]
tests/unit/security/test_logging_redaction.py ..                         [ 54%]
tests/unit/security/test_memory_encryption.py ...                        [ 54%]
tests/unit/security/test_policy_audit.py FF                              [ 54%]
tests/unit/security/test_review.py ...                                   [ 54%]
tests/unit/security/test_sanitization.py ...........                     [ 55%]
tests/unit/security/test_security_audit.py ....F                         [ 55%]
tests/unit/security/test_security_audit_cmd.py ...                       [ 55%]
tests/unit/security/test_security_flags_env.py ......                    [ 55%]
tests/unit/security/test_tls_config.py ..............                    [ 55%]
tests/unit/security/test_validation.py ................................. [ 56%]
.....                                                                    [ 56%]
tests/unit/specifications/test_mvuu_config_schema_validation.py F        [ 56%]
tests/unit/test_cli.py ..s.ss.                                           [ 56%]
tests/unit/test_sentinel_speed_markers.py .                              [ 56%]
tests/unit/test_simple_addition.py ...                                   [ 56%]
tests/unit/test_verify_test_organization_sentinel.py .                   [ 56%]
tests/unit/testing/test_collect_behavior_fallback.py F                   [ 56%]
tests/unit/testing/test_collect_cache_sanitize.py .F                     [ 56%]
tests/unit/testing/test_collect_synthesize_on_empty.py F                 [ 56%]
tests/unit/testing/test_collect_tests_cache_bad_json.py F                [ 56%]
tests/unit/testing/test_collect_tests_cache_invalidation.py FFF          [ 57%]
tests/unit/testing/test_collect_tests_cache_ttl.py FF                    [ 57%]
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py FFF [ 57%]
F                                                                        [ 57%]
tests/unit/testing/test_collect_tests_with_cache_fallback.py FF          [ 57%]
tests/unit/testing/test_coverage_segmentation_simulation.py ..           [ 57%]
tests/unit/testing/test_deterministic_seed_fixture.py .                  [ 57%]
tests/unit/testing/test_env_ttl_and_sanitize.py ..                       [ 57%]
tests/unit/testing/test_failure_tips.py .                                [ 57%]
tests/unit/testing/test_html_report_artifacts.py F                       [ 57%]
tests/unit/testing/test_mutation_testing.py ....................F        [ 57%]
tests/unit/testing/test_run_tests.py ..F.F                               [ 57%]
tests/unit/testing/test_run_tests_additional_coverage.py .......F        [ 58%]
tests/unit/testing/test_run_tests_additional_error_paths.py FFF          [ 58%]
tests/unit/testing/test_run_tests_artifacts.py .......F.F                [ 58%]
tests/unit/testing/test_run_tests_benchmark_warning.py F                 [ 58%]
tests/unit/testing/test_run_tests_cache_prune_and_tips.py .F             [ 58%]
tests/unit/testing/test_run_tests_cache_pruning.py F                     [ 58%]
tests/unit/testing/test_run_tests_cli_helpers_focus.py .FFFF.            [ 58%]
tests/unit/testing/test_run_tests_cli_invocation.py FF.F.FF...FFF        [ 58%]
tests/unit/testing/test_run_tests_collection_cache.py .                  [ 58%]
tests/unit/testing/test_run_tests_coverage_artifacts.py ....F..          [ 59%]
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py .      [ 59%]
tests/unit/testing/test_run_tests_coverage_short_circuit.py .            [ 59%]
tests/unit/testing/test_run_tests_coverage_status.py ......              [ 59%]
tests/unit/testing/test_run_tests_coverage_uplift.py ...                 [ 59%]
tests/unit/testing/test_run_tests_extra.py FF                            [ 59%]
tests/unit/testing/test_run_tests_extra_marker.py FF                     [ 59%]
tests/unit/testing/test_run_tests_extra_marker_passthrough.py F          [ 59%]
tests/unit/testing/test_run_tests_extra_paths.py FFF                     [ 59%]
tests/unit/testing/test_run_tests_failure_tips.py F                      [ 59%]
tests/unit/testing/test_run_tests_keyword_exec.py .                      [ 59%]
tests/unit/testing/test_run_tests_keyword_filter.py FF                   [ 59%]
tests/unit/testing/test_run_tests_keyword_filter_empty.py F              [ 59%]
tests/unit/testing/test_run_tests_logic.py ..FFFFF                       [ 59%]
tests/unit/testing/test_run_tests_main_function.py ...........           [ 59%]
tests/unit/testing/test_run_tests_main_logic.py ....FF..FF.FFFFF.FF      [ 60%]
tests/unit/testing/test_run_tests_marker_fallback.py ..                  [ 60%]
tests/unit/testing/test_run_tests_marker_merge.py ..                     [ 60%]
tests/unit/testing/test_run_tests_module.py .FFFFFF.F..                  [ 60%]
tests/unit/testing/test_run_tests_no_xdist_assertions.py F               [ 60%]
tests/unit/testing/test_run_tests_option_parsing.py ...                  [ 60%]
tests/unit/testing/test_run_tests_orchestration.py .FFFF..               [ 60%]
tests/unit/testing/test_run_tests_parallel_flags.py F                    [ 60%]
tests/unit/testing/test_run_tests_parallel_no_cov.py F                   [ 60%]
tests/unit/testing/test_run_tests_plugin_env.py ....                     [ 61%]
tests/unit/testing/test_run_tests_plugin_timeouts.py F.                  [ 61%]
tests/unit/testing/test_run_tests_pytest_cov_plugin.py ........          [ 61%]
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py ......F          [ 61%]
tests/unit/testing/test_run_tests_report.py F                            [ 61%]
tests/unit/testing/test_run_tests_returncode5_success.py F               [ 61%]
tests/unit/testing/test_run_tests_sanitize_node_ids.py ...               [ 61%]
tests/unit/testing/test_run_tests_segmentation.py F                      [ 61%]
tests/unit/testing/test_run_tests_segmentation_helpers.py ............   [ 61%]
tests/unit/testing/test_run_tests_segmented.py ...                       [ 61%]
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py F [ 61%]
                                                                         [ 61%]
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py F       [ 61%]
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py F          [ 61%]
tests/unit/testing/test_run_tests_segmented_failure_paths.py .F          [ 61%]
tests/unit/testing/test_run_tests_segmented_failures.py ....             [ 62%]
tests/unit/testing/test_run_tests_segmented_orchestration.py ...         [ 62%]
tests/unit/testing/test_run_tests_segmented_report_flag.py .             [ 62%]
tests/unit/testing/test_run_tests_speed_keyword_loop.py .                [ 62%]
tests/unit/testing/test_run_tests_speed_selection.py ....                [ 62%]
tests/unit/testing/test_sanitize_node_ids.py ..                          [ 62%]
tests/unit/testing/test_sanitize_node_ids_minimal.py .                   [ 62%]
tests/unit/utils/test_logging_coverage.py ......                         [ 62%]
tests/unit/utils/test_logging_final_coverage.py .......                  [ 62%]
tests/unit/utils/test_logging_utils.py ...                               [ 62%]
tests/unit/utils/test_serialization.py ...                               [ 62%]
tests/unit/utils/test_serialization_coverage.py ..........               [ 62%]
tests/unit/utils/test_serialization_edges.py ..                          [ 63%]
tests/unit/utils/test_serialization_extra.py ...                         [ 63%]
tests/unit/utils/test_serialization_final_coverage.py ..........         [ 63%]
tests/integration/agents/test_generation/test_run_generated_tests.py ..  [ 63%]
tests/integration/agents/test_generation/test_scaffold_generation.py ..  [ 63%]
tests/integration/api/test_api_startup.py F.                             [ 63%]
tests/integration/deployment/test_compose_workflow.py ...                [ 63%]
tests/integration/deployment/test_deployment_scripts.py ..F..            [ 63%]
tests/integration/general/test_complex_workflow.py .                     [ 63%]
tests/integration/general/test_end_to_end_workflow.py .                  [ 63%]
tests/integration/general/test_lmstudio_integration_regression.py ssssss [ 63%]
s                                                                        [ 63%]
tests/integration/generated/test_generated_module.py s                   [ 63%]
tests/integration/generated/test_run_generated_tests.py ..               [ 63%]
tests/integration/llm/test_lmstudio_timing_baseline.py .                 [ 63%]
tests/integration/mvu/test_command_execution.py ..                       [ 63%]
tests/integration/utils/test_logging_integration.py ..                   [ 63%]
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py ..FF     [ 64%]
tests/behavior/steps/test_webui_synthesis_steps.py EE                    [ 64%]
tests/behavior/test_documentation_generation.py F                        [ 64%]
tests/behavior/test_marker_auto_injection_guardrail.py .                 [ 64%]
tests/behavior/test_progress_failover_and_recursion.py ....              [ 64%]
tests/unit/adapters/cli/test_typer_adapter.py .F..FFFF.F                 [ 64%]
tests/unit/adapters/issues/test_mvu_enrichment.py .                      [ 64%]
tests/unit/adapters/memory/test_kuzu_adapter.py .                        [ 64%]
tests/unit/adapters/providers/test_embeddings.py ..ss.ss..               [ 64%]
tests/unit/adapters/providers/test_fallback.py ......                    [ 64%]
tests/unit/adapters/test_kuzu_memory_store.py FFFFF                      [ 64%]
tests/unit/adapters/test_provider_factory.py FF                          [ 64%]
tests/unit/adapters/test_provider_factory_env_vars.py FF                 [ 64%]
tests/unit/adapters/test_sync_manager.py .......                         [ 65%]
tests/unit/agents/test_critique_agent.py ....                            [ 65%]
tests/unit/application/agents/test_agent_memory_integration.py ......... [ 65%]
......                                                                   [ 65%]
tests/unit/application/agents/test_code_agent.py ......                  [ 65%]
tests/unit/application/agents/test_critic_agent.py ........              [ 65%]
tests/unit/application/agents/test_diagram_agent.py ......               [ 65%]
tests/unit/application/agents/test_documentation_agent.py ......         [ 66%]
tests/unit/application/agents/test_multi_language_code.py .........      [ 66%]
tests/unit/application/agents/test_planner_agent.py ......               [ 66%]
tests/unit/application/agents/test_refactor_agent.py .......             [ 66%]
tests/unit/application/agents/test_specification_agent.py ..             [ 66%]
tests/unit/application/agents/test_test_agent.py ....                    [ 66%]
tests/unit/application/agents/test_unified_agent_generic.py .            [ 66%]
tests/unit/application/agents/test_wsde_memory_integration.py .......... [ 66%]
.....                                                                    [ 67%]
tests/unit/application/cli/commands/test_completion_cmd_errors.py xxxx   [ 67%]
tests/unit/application/cli/commands/test_help_rendering.py ...F          [ 67%]
tests/unit/application/cli/test_autocomplete.py ...........              [ 67%]
tests/unit/application/cli/test_completion_cmd.py FF.                    [ 67%]
tests/unit/application/cli/test_config_validation.py FF                  [ 67%]
tests/unit/application/cli/test_help.py ............                     [ 67%]
tests/unit/application/cli/test_ingest_cmd.py ..                         [ 67%]
tests/unit/application/cli/test_ingest_phases.py ...F                    [ 67%]
tests/unit/application/cli/test_init_cmd.py F........                    [ 68%]
tests/unit/application/cli/test_metrics_commands.py ....                 [ 68%]
tests/unit/application/cli/test_run_tests_cmd_options.py FFF             [ 68%]
tests/unit/application/cli/test_serve_cmd.py .                           [ 68%]
tests/unit/application/collaboration/test_collaborative_wsde_team.py FF. [ 68%]
F.FF                                                                     [ 68%]
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py . [ 68%]
...F...                                                                  [ 68%]
tests/unit/application/collaboration/test_coordinator.py F.....          [ 68%]
tests/unit/application/collaboration/test_delegate_task.py .....         [ 68%]
tests/unit/application/collaboration/test_delegate_workflows.py ....     [ 69%]
tests/unit/application/collaboration/test_memory_utils_edge_cases.py ... [ 69%]
                                                                         [ 69%]
tests/unit/application/collaboration/test_memory_utils_fallback.py .     [ 69%]
tests/unit/application/collaboration/test_message_protocol.py .....      [ 69%]
tests/unit/application/collaboration/test_wsde_phase_transition_and_memory_flush
.py . [ 69%]
F                                                                        [ 69%]
tests/unit/application/collaboration/test_wsde_team_extended_consensus.py . [
69%]
.                                                                        [ 69%]
tests/unit/application/config/test_unified_config_loader.py .....        [ 69%]
tests/unit/application/documentation/test_documentation_ingestion_manager.py . [
69%]
..............                                                           [ 69%]
tests/unit/application/documentation/test_documentation_manager_utils.py . [
69%]
......                                                                   [ 69%]
tests/unit/application/edrr/coordinator/test_core.py FFFF                [ 69%]
tests/unit/application/edrr/test_auto_progress.py ..                     [ 70%]
tests/unit/application/edrr/test_coordinator.py ...                      [ 70%]
tests/unit/application/edrr/test_coordinator_core.py ................... [ 70%]
                                                                         [ 70%]
tests/unit/application/edrr/test_coordinator_macro_micro_simulation.py . [ 70%]
..                                                                       [ 70%]
tests/unit/application/edrr/test_coordinator_phases_simple.py FF...      [ 70%]
tests/unit/application/edrr/test_edrr_coordinator.py F.F.....F.F........ [ 71%]
.                                                                        [ 71%]
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py .F........ [ 71%]
                                                                         [ 71%]
tests/unit/application/edrr/test_edrr_phase_transitions.py ............. [ 71%]
.............                                                            [ 71%]
tests/unit/application/edrr/test_enhanced_recursion_termination.py ..... [ 71%]
.                                                                        [ 72%]
tests/unit/application/edrr/test_execute_single_agent_task.py F          [ 72%]
tests/unit/application/edrr/test_manifest_parser.py .................... [ 72%]
..............F.......                                                   [ 72%]
tests/unit/application/edrr/test_micro_cycle.py EEEEEE                   [ 73%]
tests/unit/application/edrr/test_micro_cycle_execution.py ....           [ 73%]
tests/unit/application/edrr/test_micro_cycle_hooks.py .                  [ 73%]
tests/unit/application/edrr/test_phase_progression.py F.F                [ 73%]
tests/unit/application/edrr/test_phase_recovery_helpers.py ..            [ 73%]
tests/unit/application/edrr/test_progress_recursion.py F.........        [ 73%]
tests/unit/application/edrr/test_recovery_hooks.py ...                   [ 73%]
tests/unit/application/edrr/test_recursion_features.py ................. [ 73%]
.........                                                                [ 74%]
tests/unit/application/edrr/test_recursive_edrr_coordinator.py ....FFFF. [ 74%]
.........FFF....F.                                                       [ 74%]
tests/unit/application/edrr/test_result_analysis.py .......              [ 74%]
tests/unit/application/edrr/test_templates.py .......                    [ 75%]
tests/unit/application/edrr/test_threshold_helpers.py ...                [ 75%]
tests/unit/application/llm/test_import_without_lmstudio.py FF            [ 75%]
tests/unit/application/llm/test_offline_provider_deps.py ...             [ 75%]
tests/unit/application/memory/test_basic_crud_adapters.py ..ss           [ 75%]
tests/unit/application/memory/test_chromadb_store_delete_versions.py s   [ 75%]
tests/unit/application/memory/test_duckdb_store.py sssssssssss           [ 75%]
tests/unit/application/memory/test_duckdb_store_hnsw.py ssssss           [ 75%]
tests/unit/application/memory/test_fallback.py ......................... [ 76%]
...                                                                      [ 76%]
tests/unit/application/memory/test_graph_memory_adapter.py ....F........ [ 76%]
.....FFF                                                                 [ 76%]
tests/unit/application/memory/test_import_without_duckdb.py .            [ 76%]
tests/unit/application/memory/test_import_without_tinydb.py .            [ 76%]
tests/unit/application/memory/test_knowledge_graph_utils.py ..........   [ 76%]
tests/unit/application/memory/test_kuzu_store.py ssss                    [ 77%]
tests/unit/application/memory/test_lmdb_store.py ..........              [ 77%]
tests/unit/application/memory/test_memory_adapters_regression.py .....F  [ 77%]
tests/unit/application/memory/test_memory_manager.py F...F.F..           [ 77%]
tests/unit/application/memory/test_memory_manager_search.py ..           [ 77%]
tests/unit/application/memory/test_mixed_backend_transactions.py ..      [ 77%]
tests/unit/application/memory/test_multi_layered_memory.py ............. [ 77%]
...                                                                      [ 78%]
tests/unit/application/memory/test_rdflib_store.py ........F..F          [ 78%]
tests/unit/application/memory/test_recovery.py ..........FF........F.F.F [ 78%]
.                                                                        [ 78%]
tests/unit/application/memory/test_retry.py ..................           [ 79%]
tests/unit/application/memory/test_retry_logic.py ...                    [ 79%]
tests/unit/application/memory/test_s3_memory_adapter.py ...              [ 79%]
tests/unit/application/memory/test_sync_wrappers.py F                    [ 79%]
tests/unit/application/memory/test_tiered_cache_validation.py ss         [ 79%]
tests/unit/application/memory/test_tinydb_store.py .......               [ 79%]
tests/unit/application/memory/test_vector_memory_adapter_extra.py ssss   [ 79%]
tests/unit/application/orchestration/test_workflow_manager.py .          [ 79%]
tests/unit/application/promises/test_basic_promise.py ....               [ 79%]
tests/unit/application/promises/test_timeout.py ..                       [ 79%]
tests/unit/application/prompts/test_auto_tuning.py .....                 [ 79%]
tests/unit/application/prompts/test_persistence.py ...                   [ 80%]
tests/unit/application/test_documentation_fetcher.py ......              [ 80%]
tests/unit/application/test_offline_provider_cli.py F                    [ 80%]
tests/unit/application/test_offline_provider_unit.py .                   [ 80%]
tests/unit/application/test_prompt_auto_tuning.py ......F.........       [ 80%]
tests/unit/application/wsde/test_wsde_utils.py .F.                       [ 80%]
tests/unit/behavior/test_wsde_team_extended.py F                         [ 80%]
tests/unit/cli/test_exit_codes.py ....                                   [ 80%]
tests/unit/config/test_config_validation_extended.py ...........         [ 80%]
tests/unit/config/test_feature_flags_mvuu_gui.py .                       [ 80%]
tests/unit/config/test_project_config_validation.py ..                   [ 81%]
tests/unit/config/test_unified_loader.py ...                             [ 81%]
tests/unit/core/test_config_loader.py .....F                             [ 81%]
tests/unit/core/test_core_values.py ...                                  [ 81%]
tests/unit/core/test_unified_config_loader.py ........                   [ 81%]
tests/unit/docs/test_feature_matrix.py .                                 [ 81%]
tests/unit/domain/models/test_wsde_base_methods.py .......               [ 81%]
tests/unit/domain/models/test_wsde_context_driven_leadership.py FFFFFFFF [ 81%]
FFFFFFF                                                                  [ 81%]
tests/unit/domain/models/test_wsde_dialectical_reasoning.py .FFFFFFFFFFF [ 82%]
F                                                                        [ 82%]
tests/unit/domain/test_memory_type.py .................................. [ 82%]
.                                                                        [ 82%]
tests/unit/domain/test_primus_selection_edge_cases.py .F.F               [ 83%]
tests/unit/domain/test_requirement_interfaces.py .......                 [ 83%]
tests/unit/domain/test_wsde_core_methods.py ..F                          [ 83%]
tests/unit/fallback/test_circuit_breaker_metrics.py .                    [ 83%]
tests/unit/fallback/test_condition_callbacks_prometheus.py ..F.FF        [ 83%]
tests/unit/fallback/test_retry.py ..                                     [ 83%]
tests/unit/fallback/test_retry_condition_metrics.py ...                  [ 83%]
tests/unit/fallback/test_retry_conditions.py ...........                 [ 83%]
tests/unit/fallback/test_retry_logic.py ......                           [ 83%]
tests/unit/fallback/test_retry_metrics.py ..........                     [ 84%]
tests/unit/fallback/test_retry_metrics_prometheus_integration.py .       [ 84%]
tests/unit/fallback/test_retry_with_conditions.py ...                    [ 84%]
tests/unit/general/test_agent_adapter.py .                               [ 84%]
tests/unit/general/test_agent_adapter_delegate.py .....                  [ 84%]
tests/unit/general/test_agent_collaboration.py ............              [ 84%]
tests/unit/general/test_cli.py .FFFFFF..FF...                            [ 84%]
tests/unit/general/test_fallback_utils.py .                              [ 84%]
tests/unit/general/test_knowledge_graph_utils.py ..........              [ 85%]
tests/unit/general/test_kuzu_project_startup.py .                        [ 85%]
tests/unit/general/test_kuzu_store.py sss                                [ 85%]
tests/unit/general/test_kuzu_store_fallback.py s                         [ 85%]
tests/unit/general/test_lmstudio_provider_unit.py sssss                  [ 85%]
tests/unit/general/test_memory_system_with_kuzu.py ss                    [ 85%]
tests/unit/general/test_metrics_dashboard_hook.py .                      [ 85%]
tests/unit/general/test_promise_agent.py ..                              [ 85%]
tests/unit/general/test_rdflib_store.py ........F..F                     [ 85%]
tests/unit/general/test_unified_agent.py ........                        [ 85%]
tests/unit/general/test_unit_cli_commands.py ..xFFFFFFFFF....FFFF.F..FFF [ 86%]
FFFF.FFFF.FFFFFFFFFFFFFFFFFFF                                            [ 87%]
tests/unit/general/test_wsde_team_coordinator.py ...F.F.......           [ 87%]
tests/unit/interface/test_agentapi_class.py .                            [ 87%]
tests/unit/interface/test_agentapi_enhanced.py .                         [ 87%]
tests/unit/interface/test_api_endpoints.py FF...........                 [ 87%]
tests/unit/interface/test_cli_components.py ..FFFFF                      [ 87%]
tests/unit/interface/test_cliuxbridge.py ..FFFFFF.F..                    [ 88%]
tests/unit/interface/test_dpg_bridge.py sssssssssss                      [ 88%]
tests/unit/interface/test_output_formatter.py .........                  [ 88%]
tests/unit/interface/test_output_sanitization.py F.F.F.                  [ 88%]
tests/unit/interface/test_state_access.py ......F...FFF.                 [ 89%]
tests/unit/interface/test_ux_bridge.py F                                 [ 89%]
tests/unit/interface/test_uxbridge.py F                                  [ 89%]
tests/unit/interface/test_uxbridge_config.py .FFFFF                      [ 89%]
tests/unit/interface/test_uxbridge_question_result.py F...               [ 89%]
tests/unit/interface/test_uxbridge_sanitization.py F.F                   [ 89%]
tests/unit/interface/test_webui.py EEEEEEEEEEEEEEEEEEEEEEE               [ 89%]
tests/unit/interface/test_webui_bridge_progress.py ...                   [ 89%]
tests/unit/interface/test_webui_cli_imports.py F                         [ 89%]
tests/unit/interface/test_webui_error_handling.py FFFFFFFFF              [ 90%]
tests/unit/interface/test_webui_gather_wizard.py FFFF                    [ 90%]
tests/unit/interface/test_webui_gather_wizard_with_state.py FFFsFsFF     [ 90%]
tests/unit/interface/test_webui_navigation_and_validation.py FF          [ 90%]
tests/unit/interface/test_webui_onboarding.py FF                         [ 90%]
tests/unit/interface/test_webui_progress.py FFFFFFF                      [ 90%]
tests/unit/interface/test_webui_requirements.py FF                       [ 90%]
tests/unit/interface/test_webui_setup.py s                               [ 90%]
tests/unit/interface/test_webui_wizard_state.py ..........F              [ 90%]
tests/unit/interface/test_wizard_state_manager.py .FF.FFFFFFFFFFFFFFFFFF [ 91%]
                                                                         [ 91%]
tests/unit/llm/test_lmstudio_provider.py FFFFFFFFF.                      [ 91%]
tests/unit/llm/test_openai_provider.py F.FFFFFFFF.F                      [ 91%]
tests/unit/llm/test_openrouter_provider.py FFFFFFFFFF.F                  [ 92%]
tests/unit/methodology/test_dialectical_reasoner.py F..                  [ 92%]
tests/unit/methodology/test_dialectical_reasoner_hooks.py ...            [ 92%]
tests/unit/methodology/test_wsde_edrr_coordinator.py F                   [ 92%]
tests/unit/providers/test_provider_contract.py ..                        [ 92%]
tests/unit/security/test_argon2_hash.py .                                [ 92%]
tests/unit/security/test_authentication.py ...                           [ 92%]
tests/unit/security/test_authorization.py .............                  [ 92%]
tests/unit/test_sentinel_speed_markers.py .                              [ 92%]
tests/integration/adapters/test_github_project_adapter.py .              [ 92%]
tests/integration/adapters/test_jira_adapter.py .                        [ 92%]
tests/integration/api/test_api_smoke_import.py .                         [ 92%]
tests/integration/collaboration/test_voting_summary_edrr.py FFFF         [ 92%]
tests/integration/config/test_unified_loader.py ..                       [ 92%]
tests/integration/edrr/test_wsde_edrr_integration.py .....               [ 93%]
tests/integration/general/test_agent_api.py FFFFFFFF...                  [ 93%]
tests/integration/general/test_agent_api_security.py FF..F.              [ 93%]
tests/integration/general/test_agent_collaboration_integration.py .F.... [ 93%]
.F.                                                                      [ 93%]
tests/integration/general/test_agent_system_integration.py ....          [ 93%]
tests/integration/general/test_agentapi.py FFF                           [ 93%]
tests/integration/general/test_agentapi_routes.py FFF                    [ 93%]
tests/integration/general/test_anthropic_provider.py sss                 [ 93%]
tests/integration/general/test_chromadb_client_connection.py s           [ 93%]
tests/integration/general/test_cli_webui_agentapi_pipeline.py EEFF.FFF   [ 94%]
tests/integration/general/test_collaborative_decision_making.py FF       [ 94%]
tests/integration/general/test_collaborative_voting.py FFF               [ 94%]
tests/integration/general/test_complex_workflow.py F                     [ 94%]
tests/integration/general/test_comprehensive_workflow.py .F.F            [ 94%]
tests/integration/general/test_config_loader.py FF..                     [ 94%]
tests/integration/general/test_config_loader_integration.py ...          [ 94%]
tests/integration/general/test_config_loader_workflow.py ...             [ 94%]
tests/integration/general/test_critique_workflow.py ..                   [ 94%]
tests/integration/general/test_delegate_task.py F..                      [ 94%]
tests/integration/general/test_delegate_task_consensus.py F.F            [ 94%]
tests/integration/general/test_delegate_task_primus_selection.py .F      [ 94%]
tests/integration/general/test_delegate_task_workflow.py F               [ 94%]
tests/integration/general/test_end_to_end_workflow.py FF                 [ 94%]
tests/integration/general/test_error_handling_at_integration_points.py F [ 94%]
F.FF.                                                                    [ 94%]
tests/integration/general/test_feature_flag_integration.py .....         [ 95%]
tests/integration/general/test_graph_memory_edrr_integration.py ......   [ 95%]
tests/integration/general/test_graph_memory_error_handling.py .........  [ 95%]
tests/integration/general/test_kuzu_adapter_transactions.py s            [ 95%]
tests/integration/general/test_kuzu_memory_fallback.py s                 [ 95%]
tests/integration/general/test_kuzu_memory_integration.py sssssssssssss  [ 95%]
tests/integration/general/test_lmstudio_integration_basic.py .....       [ 95%]
tests/integration/general/test_lmstudio_integration_comprehensive.py ..  [ 95%]
tests/integration/general/test_lmstudio_integration_real.py FF           [ 95%]
tests/integration/general/test_lmstudio_integration_standalone.py ..     [ 95%]
tests/integration/general/test_lmstudio_provider.py sssssssss            [ 96%]
tests/integration/general/test_memory_agent_integration.py .......s.     [ 96%]
tests/integration/general/test_multi_store_sync_manager.py ss            [ 96%]
tests/integration/general/test_non_hierarchical_decision.py .            [ 96%]
tests/integration/general/test_openai_provider.py FFF...FFF              [ 96%]
tests/integration/general/test_project_state_analyzer.py FFF             [ 96%]
tests/integration/general/test_prompt_auto_tuning.py ..                  [ 96%]
tests/integration/general/test_provider_system_async.py .....            [ 96%]
tests/integration/general/test_provider_system_configurations.py FFFFFFF [ 96%]
                                                                         [ 96%]
tests/integration/general/test_query_router_integration.py FFFF          [ 97%]
tests/integration/general/test_recursive_recovery_flow.py .              [ 97%]
tests/integration/general/test_refactor_workflow.py .....                [ 97%]
tests/integration/general/test_requirements_gathering.py EEEE            [ 97%]
tests/integration/general/test_self_analyzer.py F..                      [ 97%]
tests/integration/general/test_webui_pages.py E                          [ 97%]
tests/integration/general/test_webui_setup.py sss                        [ 97%]
tests/integration/general/test_wsde_edrr_component_interactions.py ..... [ 97%]
.F.                                                                      [ 97%]
tests/integration/general/test_wsde_peer_review_memory_integration.py .. [ 97%]
F.F                                                                      [ 97%]
tests/integration/generated_tests/test_scaffold_output.py F              [ 97%]
tests/integration/interface/test_bridge_consistency.py F.                [ 97%]
tests/integration/interface/test_multi_agent_collaboration.py FF.        [ 97%]
tests/integration/interface/test_mvuu_dashboard_report.py F              [ 97%]
tests/integration/interface/test_small_workflow_bridge.py F              [ 97%]
tests/integration/interface/test_webui_cli_lookup.py F                   [ 97%]
tests/integration/interface/test_webui_flow.py sss                       [ 97%]
tests/integration/interface/test_webui_run_navigation.py EE              [ 97%]
tests/integration/live/test_openai_live_smoke.py FF                      [ 98%]
tests/integration/llm/test_lmstudio_streaming.py ss                      [ 98%]
tests/integration/llm/test_provider_consistency.py FFsFFFsFFFsFFFsFFFsFF [ 98%]
FsFFFsFFFsFFFsFFFsFFFsF                                                  [ 99%]
tests/integration/llm/test_provider_selection_integration.py FFFF        [ 99%]
tests/integration/memory/test_backend_persistence.py s                   [ 99%]
tests/integration/memory/test_chromadb_adapter_transactions.py s         [ 99%]
tests/integration/memory/test_chromadb_fallback.py s                     [ 99%]
tests/integration/memory/test_circuit_breaker_integration.py .           [ 99%]
tests/integration/memory/test_cross_store_query.py s                     [ 99%]
tests/integration/memory/test_cross_store_sync.py .....                  [ 99%]
tests/integration/memory/test_cross_store_transactions.py sss            [ 99%]
tests/integration/memory/test_faiss_transactions.py s                    [ 99%]
tests/integration/memory/test_multistore_transaction_wrapper.py s        [ 99%]
tests/integration/memory/test_reverse_sync_retrieval.py s                [ 99%]
tests/integration/memory/test_sync_manager_core_transactions.py ss       [ 99%]
tests/integration/memory/test_transaction_failure_recovery.py s          [ 99%]
tests/integration/memory/test_transactional_sync.py s                    [ 99%]
tests/integration/mvu/test_atomic_rewrite_round_trip.py .                [ 99%]
tests/integration/mvu/test_report_generation.py F                        [ 99%]
tests/integration/sprint_edrr/test_ceremony_phase_alignment.py .         [ 99%]
tests/integration/templates/test_generated_module.py s                   [ 99%]
tests/integration/test_real_world_scenarios.py ......                    [ 99%]
tests/integration/wsde/test_wsde_edrr_integration.py F                   [ 99%]
tests/behavior/test_agentapi.py F                                        [ 99%]
tests/behavior/test_alignment_metrics_command.py ..                      [ 99%]
tests/behavior/test_cli_help_and_completion.py .F.                       [ 99%]
tests/behavior/test_dialectical_reasoner_termination_behavior.py .       [ 99%]
tests/behavior/test_interactive_workflow.py ss                           [ 99%]
tests/behavior/test_sprint_edrr_cycle.py .                               [100%]

==================================== ERRORS ====================================
________________ ERROR at setup of test_cli_marker_passthrough _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc24380>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage
helpers."""

        app, module = build_minimal_cli_app(monkeypatch)

        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
___________ ERROR at setup of test_cli_feature_flags_set_environment ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25df0>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage
helpers."""

        app, module = build_minimal_cli_app(monkeypatch)

        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
_________ ERROR at setup of test_cli_segmentation_arguments_forwarded __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc26c90>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage
helpers."""

        app, module = build_minimal_cli_app(monkeypatch)

        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
____________ ERROR at setup of test_cli_inventory_mode_exports_json ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27ec0>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage
helpers."""

        app, module = build_minimal_cli_app(monkeypatch)

        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
___________ ERROR at setup of test_cli_failure_propagates_exit_code ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc26690>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage
helpers."""

        app, module = build_minimal_cli_app(monkeypatch)

        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
_____ ERROR at setup of test_inner_test_env_tightening_forces_no_parallel ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc1ac30>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        # Ensure a clean slate for env vars we mutate
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_env_paths.py:31: AttributeError
_ ERROR at setup of
test_unit_tests_sets_allow_requests_by_default_and_respects_existing _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc1b950>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        # Ensure a clean slate for env vars we mutate
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_env_paths.py:31: AttributeError
_______ ERROR at setup of test_feature_flags_set_env_and_success_message _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc1aea0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_features.py:11:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_marker_option_is_passed_as_extra_marker ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc24ef0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_features.py:11:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____ ERROR at setup of test_inner_test_mode_disables_plugins_and_parallel _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27860>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inner_test.py:21: AttributeError
_______ ERROR at setup of test_inventory_mode_exports_json_and_skips_run _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27560>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
______ ERROR at setup of test_inventory_mode_handles_collection_failures _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27530>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
__________ ERROR at setup of test_invalid_target_exits_with_help_text __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27590>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
_________ ERROR at setup of test_marker_option_is_forwarded_to_runner __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc27a70>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
_______ ERROR at setup of test_marker_anding_passthrough_multiple_speeds _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc275c0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_markers.py:13:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_invalid_marker_expression_exits_cleanly ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc32930>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_markers.py:13:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ ERROR at setup of test_speed_and_marker_forwarding ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc327e0>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_________ ERROR at setup of test_report_true_prints_output_and_success _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc30200>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_____________ ERROR at setup of test_observability_and_error_path ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc30230>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_______ ERROR at setup of test_provider_defaults_are_applied_when_unset ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc322d0>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_provider_defaults.py:26: AttributeError
______ ERROR at setup of test_provider_defaults_do_not_override_existing _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc321e0>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k:
100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_provider_defaults.py:26: AttributeError
___ ERROR at setup of test_report_flag_with_missing_directory_prints_warning ___

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc316d0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_smoke_mode_sets_env_and_disables_parallel _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc31e80>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________________ ERROR at setup of test_no_parallel_maps_to_n0 _________________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc36030>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_emit_coverage_messages_reports_artifacts ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc366f0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_run_tests_cli_report_option_forwards_true _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc365d0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_reporting_and_env.py:21: AttributeError
_____ ERROR at setup of test_run_tests_cmd_respects_explicit_provider_env ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f6cfdd0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_reporting_and_env.py:21: AttributeError
__ ERROR at setup of test_smoke_mode_sets_pytest_disable_plugin_autoload_env ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc20410>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:17: AttributeError
___ ERROR at setup of test_smoke_mode_skips_coverage_gate_when_cov_disabled ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc20d70>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:17: AttributeError
_______ ERROR at setup of test_smoke_mode_cli_imports_fastapi_testclient _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc20ef0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:17: AttributeError
___ ERROR at setup of test_smoke_mode_skips_coverage_gate_when_instrumented ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc21a60>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a,
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10ac6ad40> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:17: AttributeError
__________ ERROR at setup of test_setup_wizard_instantiation_succeeds __________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc22480>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_wizard_prompts_via_cli_bridge_succeeds _________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc23bc0>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ ERROR at setup of test_setup_wizard_run_succeeds _______________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f22f8f0>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ ERROR at setup of test_setup_wizard_abort_succeeds ______________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f1fc950>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____ ERROR at setup of test_prompt_features_uses_prompt_toolkit_multiselect ____

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f262300>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________ ERROR at setup of test_setup_wizard_accepts_typed_inputs ___________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f1dbbf0>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""

>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter",
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____ ERROR at setup of TestExecutionTrajectoryCollector.test_initialization ____

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x1192711c0>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
_ ERROR at setup of TestExecutionTrajectoryCollector.test_analyze_code_structure
_

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x119271670>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
_ ERROR at setup of
TestExecutionTrajectoryCollector.test_extract_execution_patterns _

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x119270140>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
_ ERROR at setup of
TestExecutionTrajectoryCollector.test_create_memetic_units_from_trajectories _

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x119271c70>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
_ ERROR at setup of TestExecutionTrajectoryCollector.test_get_execution_insights
_

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x1192721b0>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
_ ERROR at setup of
TestExecutionTrajectoryCollector.test_validate_trajectory_quality _

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x1192726f0>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: NameError
____ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_initialization _____

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193db920>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,744 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,744 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_process_complex_query _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193dbdd0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,767 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,767 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
__ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_parse_query_intent ___

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e81d0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,786 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,786 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
___ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_extract_entities ____

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e8710>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,803 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,803 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_extract_relationships _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e8c50>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,819 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,819 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_calculate_required_hops
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e9190>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,838 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,838 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
___ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_resolve_entities ____

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e96d0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,861 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,861 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
_ ERROR at setup of
TestEnhancedGraphRAGQueryEngine.test_plan_multi_hop_traversal _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193e9c10>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,875 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,875 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
_ ERROR at setup of
TestEnhancedGraphRAGQueryEngine.test_execute_semantic_traversal _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1193ea150>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph,
self.execution_learning)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:329: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,889 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,890 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
______ ERROR at setup of TestAutomataSynthesisEngine.test_initialization _______

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193ea360>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_ ERROR at setup of
TestAutomataSynthesisEngine.test_synthesize_automata_from_exploration _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193ea810>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_ ERROR at setup of TestAutomataSynthesisEngine.test_generate_task_segmentation
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193ead20>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_ ERROR at setup of TestAutomataSynthesisEngine.test_validate_automata_quality _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193eb260>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_ ERROR at setup of
TestAutomataSynthesisEngine.test_create_memetic_units_from_automata _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193eb7a0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_ ERROR at setup of
TestAutomataSynthesisEngine.test_get_task_segmentation_for_query _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x1193fc260>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:475: NameError
_______ ERROR at setup of TestHybridLLMArchitecture.test_initialization ________

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193ebe00>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
_ ERROR at setup of
TestHybridLLMArchitecture.test_process_complex_reasoning_task _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193fc710>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
_ ERROR at setup of TestHybridLLMArchitecture.test_get_optimal_provider_for_task
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193fcc50>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
_ ERROR at setup of
TestHybridLLMArchitecture.test_benchmark_hybrid_vs_individual _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193fd190>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
________ ERROR at setup of TestHybridLLMArchitecture.test_add_provider _________

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193fd6d0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
_ ERROR at setup of TestHybridLLMArchitecture.test_get_architecture_statistics _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x1193fdc10>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:579: NameError
____ ERROR at setup of TestMetacognitiveTrainingSystem.test_initialization _____

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193fde50>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of
TestMetacognitiveTrainingSystem.test_start_think_aloud_session _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193fe300>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of TestMetacognitiveTrainingSystem.test_record_verbalization __

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193fe840>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of TestMetacognitiveTrainingSystem.test_end_think_aloud_session
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193fed80>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of
TestMetacognitiveTrainingSystem.test_get_metacognitive_insights _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193ff2c0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of
TestMetacognitiveTrainingSystem.test_apply_metacognitive_improvements _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1193ff800>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_ ERROR at setup of
TestMetacognitiveTrainingSystem.test_generate_self_monitoring_report _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x11940c2c0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system =
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:689: NameError
_____ ERROR at setup of TestContextualPromptingSystem.test_initialization ______

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1193fd760>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of TestContextualPromptingSystem.test_create_contextual_prompt
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940c710>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of
TestContextualPromptingSystem.test_engineer_contextual_prompt _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940cc50>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of TestContextualPromptingSystem.test_add_behavioral_directive
_

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940d190>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of
TestContextualPromptingSystem.test_add_environmental_constraint _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940d6d0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of
TestContextualPromptingSystem.test_get_prompt_performance_analytics _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940dc10>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of
TestContextualPromptingSystem.test_create_agent_specific_prompt _

self =
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11940e150>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system =
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:806: NameError
_ ERROR at setup of
TestEnhancedTestCollector.test_collect_tests_by_category_unit _

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category0/test_project/tests/behavior/fe
atures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_by_category0/test_project/tests/behavior/features/exa
mple.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of
TestEnhancedTestCollector.test_collect_tests_by_category_integration _

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category1')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category1/test_project/tests/behavior/fe
atures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_by_category1/test_project/tests/behavior/features/exa
mple.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of
TestEnhancedTestCollector.test_collect_tests_by_category_behavior _

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category2')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_by_category2/test_project/tests/behavior/fe
atures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_by_category2/test_project/tests/behavior/features/exa
mple.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of TestEnhancedTestCollector.test_collect_tests_all_categories
_

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_all_categor0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_all_categor0/test_project/tests/behavior/fe
atures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_all_categor0/test_project/tests/behavior/features/exa
mple.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
___ ERROR at setup of TestEnhancedTestCollector.test_get_tests_with_markers ____

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_get_tests_with_markers0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_get_tests_with_markers0/test_project/tests/behavior/featu
res/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_tests_with_markers0/test_project/tests/behavior/features/exampl
e.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
____ ERROR at setup of TestEnhancedTestCollector.test_caching_functionality ____

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_caching_functionality0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_caching_functionality0/test_project/tests/behavior/featur
es/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_caching_functionality0/test_project/tests/behavior/features/example
.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_force_refresh_cache _____

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_force_refresh_cache0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_force_refresh_cache0/test_project/tests/behavior/features
/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_force_refresh_cache0/test_project/tests/behavior/features/example.f
eature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_memory_integration ______

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_memory_integration0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_memory_integration0/test_project/tests/behavior/features/
example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_memory_integration0/test_project/tests/behavior/features/example.fe
ature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_is_valid_test_file ______

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_is_valid_test_file0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_is_valid_test_file0/test_project/tests/behavior/features/
example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_is_valid_test_file0/test_project/tests/behavior/features/example.fe
ature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_contains_test_code ______

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_contains_test_code0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_contains_test_code0/test_project/tests/behavior/features/
example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_contains_test_code0/test_project/tests/behavior/features/example.fe
ature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_______ ERROR at setup of TestEnhancedTestCollector.test_test_has_marker _______

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_test_has_marker0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_test_has_marker0/test_project/tests/behavior/features/exa
mple.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_test_has_marker0/test_project/tests/behavior/features/example.featu
re'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_______ ERROR at setup of TestEnhancedTestCollector.test_analyze_markers _______

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_analyze_markers0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_analyze_markers0/test_project/tests/behavior/features/exa
mple.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_analyze_markers0/test_project/tests/behavior/features/example.featu
re'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
__ ERROR at setup of TestEnhancedTestCollector.test_store_collection_results ___

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_store_collection_results0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"

        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)

        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef
test_another_example():\n    """Test another function."""\n    assert True is
True'
        )

        # Create integration tests
        (project_dir / "tests" / "integration" /
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n
"""Test integration functionality."""\n    assert "integration" in "integration
test"'
        )

        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" /
"example.feature").write_text(
            'Feature: Example Feature\n  Scenario: Example scenario\n    Given
something\n    When I do something\n    Then something should happen'
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline)
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_store_collection_results0/test_project/tests/behavior/fea
tures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_store_collection_results0/test_project/tests/behavior/features/exam
ple.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
________ ERROR at setup of test_lmstudio_mock_fixture_returns_base_url _________

fixturedef = <FixtureDef argname='lmstudio_mock' scope='function' baseid=''>
request = <SubRequest 'lmstudio_mock' for <Function
test_lmstudio_mock_fixture_returns_base_url>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict
mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:733:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f0f4230>

    @pytest.fixture
    def lmstudio_mock(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Generator[LMStudioMockServer]:
        """Provide a mocked LM Studio HTTP API with streaming responses.

        Skips unless ``DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE`` is truthy and the
        :mod:`lmstudio` package is installed.
        """

        if os.environ.get("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
"false").lower() not in (
            "1",
            "true",
            "yes",
        ):
            pytest.skip("LMStudio service not available")

        lmstudio = cast(LMStudioModule, _IMPORTORSKIP("lmstudio"))

        # Lazy import FastAPI classes to avoid MRO issues during collection
>       if FastAPI is None:
           ^^^^^^^
E       UnboundLocalError: cannot access local variable 'FastAPI' where it is
not associated with a value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/lmstudio_moc
k.py:103: UnboundLocalError
________________ ERROR at setup of test_ports_fixtures_succeeds ________________

fixturedef = <FixtureDef argname='llm_port' scope='function' baseid=''>
request = <SubRequest 'llm_port' for <Function test_ports_fixtures_succeeds>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict
mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:733:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    @pytest.fixture
    def llm_port():
        """Provide an LLMPort using the MockLLMAdapter."""
>       port = LLMPort(_MockProviderFactory())
                       ^^^^^^^^^^^^^^^^^^^^
E       NameError: name '_MockProviderFactory' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/ports.py:92:
NameError
_______________ ERROR at setup of test_provider_logging_cleanup ________________

fixturedef = <FixtureDef argname='lmstudio_service' scope='function' baseid=''>
request = <SubRequest 'lmstudio_service' for <Function
test_provider_logging_cleanup>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict
mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:733:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee2060>

    @pytest.fixture
    def lmstudio_service(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Generator[LMStudioMockServer]:
        """Provide a mocked LM Studio HTTP API with streaming responses.

        Skips unless ``DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE`` is truthy and the
        :mod:`lmstudio` package is installed.
        """

        if os.environ.get("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
"false").lower() not in (
            "1",
            "true",
            "yes",
        ):
            pytest.skip("LMStudio service not available")

        lmstudio = cast(LMStudioModule, _IMPORTORSKIP("lmstudio"))

        # Lazy import FastAPI classes to avoid MRO issues during collection
>       if FastAPI is None:
           ^^^^^^^
E       UnboundLocalError: cannot access local variable 'FastAPI' where it is
not associated with a value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/lmstudio_ser
vice.py:97: UnboundLocalError
______ ERROR at setup of test_lmstudio_retry_metrics_and_circuit_breaker _______

fixturedef = <FixtureDef argname='lmstudio_service' scope='function' baseid=''>
request = <SubRequest 'lmstudio_service' for <Function
test_lmstudio_retry_metrics_and_circuit_breaker>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict
mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:733:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ef05310>

    @pytest.fixture
    def lmstudio_service(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Generator[LMStudioMockServer]:
        """Provide a mocked LM Studio HTTP API with streaming responses.

        Skips unless ``DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE`` is truthy and the
        :mod:`lmstudio` package is installed.
        """

        if os.environ.get("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
"false").lower() not in (
            "1",
            "true",
            "yes",
        ):
            pytest.skip("LMStudio service not available")

        lmstudio = cast(LMStudioModule, _IMPORTORSKIP("lmstudio"))

        # Lazy import FastAPI classes to avoid MRO issues during collection
>       if FastAPI is None:
           ^^^^^^^
E       UnboundLocalError: cannot access local variable 'FastAPI' where it is
not associated with a value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/lmstudio_ser
vice.py:97: UnboundLocalError
_____ ERROR at setup of test_webui_layout_breakpoints_toggle_between_modes _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb6ac00>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) ->
Iterator[Tuple[object, object]]:
        """Reload ``devsynth.interface.webui`` with a deterministic Streamlit
stub."""

        st = _mock_streamlit()
        st.markdown.reset_mock()
        st.error.reset_mock()

        stub_argon2 = ModuleType("argon2")
        stub_argon2.PasswordHasher = MagicMock()
        stub_exceptions = ModuleType("argon2.exceptions")

        class _VerifyMismatchError(Exception):
            pass

        stub_exceptions.VerifyMismatchError = _VerifyMismatchError
        stub_argon2.exceptions = stub_exceptions
        monkeypatch.setitem(sys.modules, "argon2", stub_argon2)
        monkeypatch.setitem(sys.modules, "argon2.exceptions", stub_exceptions)

        stub_config = ModuleType("devsynth.config")
        stub_config.load_project_config = MagicMock(return_value={})
        stub_config.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", stub_config)

        stub_yaml = ModuleType("yaml")
        stub_yaml.safe_dump = MagicMock(return_value="{}")
        monkeypatch.setitem(sys.modules, "yaml", stub_yaml)

        stub_output_formatter =
ModuleType("devsynth.interface.output_formatter")

        class _Formatter:
            def __init__(self, *_args, **_kwargs) -> None:
                pass

            def format_message(
                self,
                message: str,
                *,
                message_type: str | None = None,
                highlight: bool = False,
            ) -> str:
                return message

        stub_output_formatter.OutputFormatter = _Formatter
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.output_formatter",
stub_output_formatter
        )

        stub_shared_bridge = ModuleType("devsynth.interface.shared_bridge")

        class _SharedBridgeMixin:
            def __init__(self, *args, **kwargs) -> None:
                super().__init__(*args, **kwargs)  # type: ignore[misc]
                self.formatter = _Formatter(None)

            def _format_for_output(
                self,
                message: str,
                *,
                highlight: bool = False,
                message_type: str | None = None,
            ) -> str:
                return message

        stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.shared_bridge", stub_shared_bridge
        )

        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_dashboard_toggles_fast.py:88:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_webui_error_guidance_surfaces_suggestions_and_docs ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edf05f0>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) ->
Iterator[Tuple[object, object]]:
        """Reload ``devsynth.interface.webui`` with a deterministic Streamlit
stub."""

        st = _mock_streamlit()
        st.markdown.reset_mock()
        st.error.reset_mock()

        stub_argon2 = ModuleType("argon2")
        stub_argon2.PasswordHasher = MagicMock()
        stub_exceptions = ModuleType("argon2.exceptions")

        class _VerifyMismatchError(Exception):
            pass

        stub_exceptions.VerifyMismatchError = _VerifyMismatchError
        stub_argon2.exceptions = stub_exceptions
        monkeypatch.setitem(sys.modules, "argon2", stub_argon2)
        monkeypatch.setitem(sys.modules, "argon2.exceptions", stub_exceptions)

        stub_config = ModuleType("devsynth.config")
        stub_config.load_project_config = MagicMock(return_value={})
        stub_config.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", stub_config)

        stub_yaml = ModuleType("yaml")
        stub_yaml.safe_dump = MagicMock(return_value="{}")
        monkeypatch.setitem(sys.modules, "yaml", stub_yaml)

        stub_output_formatter =
ModuleType("devsynth.interface.output_formatter")

        class _Formatter:
            def __init__(self, *_args, **_kwargs) -> None:
                pass

            def format_message(
                self,
                message: str,
                *,
                message_type: str | None = None,
                highlight: bool = False,
            ) -> str:
                return message

        stub_output_formatter.OutputFormatter = _Formatter
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.output_formatter",
stub_output_formatter
        )

        stub_shared_bridge = ModuleType("devsynth.interface.shared_bridge")

        class _SharedBridgeMixin:
            def __init__(self, *args, **kwargs) -> None:
                super().__init__(*args, **kwargs)  # type: ignore[misc]
                self.formatter = _Formatter(None)

            def _format_for_output(
                self,
                message: str,
                *,
                highlight: bool = False,
                message_type: str | None = None,
            ) -> str:
                return message

        stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.shared_bridge", stub_shared_bridge
        )

        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_dashboard_toggles_fast.py:88:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[500-expected0] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edf2f90>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[800-expected1] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa73500>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[1200-expected2] _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa73740>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[None-expected3] _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b3bc0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______ ERROR at setup of test_display_result_renders_markup_and_sanitizes ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b2f30>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_display_result_highlight_uses_info ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b2f60>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_display_result_routes_message_types_and_plain_write __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b3620>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______ ERROR at setup of test_display_result_error_suggestions_and_docs _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b31d0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_display_result_error_prefix_without_message_type ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b25a0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_display_result_heading_routes_to_header ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b3fe0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_display_result_additional_headings ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edf0620>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[File not found:
missing.yaml-file_not_found] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9b28d0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Permission denied when
opening-permission_denied] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6fef90>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Invalid parameter
--foo-invalid_parameter] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb6a0c0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Invalid format
provided-invalid_format] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce52570>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Missing key 'api'-key_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0ab920>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Type error while
casting-type_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0a93d0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Configuration error
detected-config_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0a85c0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Connection error
occurred-connection_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0ab260>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_get_error_type_mappings[API error status-api_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0a8bc0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Validation error
raised-validation_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0aba40>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Syntax error unexpected
token-syntax_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0aaab0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Import error for
module-import_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0abf20>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______ ERROR at setup of test_get_error_type_mappings[Unrelated message-] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0a95b0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ ERROR at setup of test_error_helper_defaults _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72de80>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_render_traceback_uses_expander _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72d2b0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ ERROR at setup of test_format_error_message __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e09c920>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ensure_router_caches_instance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2dcb00>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_run_configures_streamlit_and_router __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2dfa40>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_handles_page_config_error _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2de8a0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_handles_components_error ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2dcfb0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ui_progress_updates_emit_eta ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd9880>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ ERROR at setup of test_ui_progress_subtask_flow ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6ff6b0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_webui_ensure_router_caches_instance __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2de510>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_webui_run_configures_layout_and_router _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e09dee0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_webui_run_handles_page_config_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e09fcb0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_webui_run_handles_component_error ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2dda00>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_display_result_translates_markup_to_markdown ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72f200>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation",
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication",
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")

        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str,
object]]] = []

            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))

        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")

        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs

        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")

        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs

        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")

        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs

        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")

        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs

        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")

        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] =
[]

            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))

            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))

        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")

        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  #
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj

        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_display_result_surfaces_guidance_for_file_errors ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72e720>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation",
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication",
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")

        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str,
object]]] = []

            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))

        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")

        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs

        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")

        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs

        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")

        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs

        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")

        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs

        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")

        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] =
[]

            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))

            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))

        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")

        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  #
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj

        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ ERROR at setup of test_display_result_highlights_information _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72e690>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation",
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication",
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")

        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str,
object]]] = []

            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))

        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")

        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs

        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")

        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs

        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")

        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs

        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")

        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs

        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")

        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] =
[]

            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))

            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))

        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")

        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  #
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj

        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_ui_progress_tracks_status_and_subtasks _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f72dd90>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation",
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication",
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")

        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str,
object]]] = []

            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))

        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")

        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs

        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")

        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs

        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")

        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs

        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")

        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs

        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")

        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] =
[]

            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))

            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))

        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")

        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  #
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj

        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_handle_command_errors_passthrough ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1ff860>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR:
File not found: config.yaml-Make sure the file exists] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fd58bf0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR:
Permission denied: secrets.env-necessary permissions] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee38bc0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR:
Invalid value: bad input-Please check your input] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1ffb90>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR:
Missing key: 'api_key'-Verify that the referenced key exists] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9c87a0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR:
Type error: wrong type-Check that all inputs] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6b8dd0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_handle_command_errors_generic_exception ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fc8e270>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_get_layout_config_respects_breakpoints _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd1430>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_ask_question_and_confirm_choice_use_streamlit_controls
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd1a90>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_display_result_message_types_provide_guidance _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff02fc0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______ ERROR at setup of test_display_result_markup_and_keyword_routing _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff01820>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[File not
found-file_not_found] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff02d20>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Permission
denied-permission_denied] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ffb6420>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Invalid
parameter-invalid_parameter] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ffb5be0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Invalid
format-invalid_format] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ffbe0c0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Missing key-key_error]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ffbe900>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Type error-type_error]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ffbd400>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[TypeError-type_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d8d9dc0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Configuration
error-config_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d8d95b0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Connection
error-connection_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d8d9be0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[API error-api_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d982060>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Validation
error-validation_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d981670>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Syntax
error-syntax_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d982cc0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Import
error-import_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13da5af00>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Completely different-]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13da5a5a0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_error_suggestions_and_docs_cover_known_and_unknown ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13da5b470>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> Tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_lazy_streamlit_proxy_imports_once ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db182c0>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) ->
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the
duration."""

        original_import = importlib.import_module
        stub = _HarnessStreamlit()

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
___ ERROR at setup of test_progress_indicator_emits_eta_and_sanitized_status ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db19250>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) ->
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the
duration."""

        original_import = importlib.import_module
        stub = _HarnessStreamlit()

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
______ ERROR at setup of test_permission_denied_error_renders_suggestions ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db185c0>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) ->
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the
duration."""

        original_import = importlib.import_module
        stub = _HarnessStreamlit()

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
_______ ERROR at setup of test_display_result_translates_markup_to_html ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db28140>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch):
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""

        streamlit_stub = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_stub)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_streamlit_and_wizard.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_progress_complete_cascades_with_sanitized_fallback ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db68320>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
__________ ERROR at setup of test_webui_layout_and_display_behaviors ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db6b920>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
________ ERROR at setup of test_ui_progress_status_transitions_and_eta _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f003920>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
_____________ ERROR at setup of test_ensure_router_caches_instance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f0000b0>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
________ ERROR at setup of test_webui_run_configures_layout_and_router _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f001a60>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
__________ ERROR at setup of test_webui_run_handles_streamlit_errors ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f000560>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
_ ERROR at setup of test_webui_run_injects_resize_script_and_configures_layout _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee42840>

    @pytest.fixture
    def streamlit_router_stub(monkeypatch: pytest.MonkeyPatch):
        """Provide a stubbed Streamlit module and Router replacement."""

        from devsynth.interface import webui as webui_module

        previous_streamlit = sys.modules.get("streamlit")
        # Import Router from the webui package submodule
>       from devsynth.interface.webui import Router

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_fast.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_webui_run_configures_dashboard_and_invokes_router ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1a44d0>

    @pytest.fixture
    def streamlit_free_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[Tuple[ModuleType, ModuleType, dict[str, MagicMock]]]:
        """Reload :mod:`devsynth.interface.webui` with a deterministic Streamlit
stub."""

        st = _mock_streamlit()
        st.sidebar.radio = MagicMock(return_value="Summary")
        st.session_state.screen_width = 860
        st.session_state.screen_height = 600

        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of
test_progress_updates_emit_telemetry_and_sanitize_checkpoints _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1a4b60>

    @pytest.fixture
    def progress_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[Tuple[ModuleType, ModuleType, MagicMock, MagicMock,
MagicMock]]:
        """Provide a reloaded WebUI module with deterministic progress
containers."""

        st = _mock_streamlit()
        status_container = MagicMock(name="status_container")
        time_container = MagicMock(name="time_container")
        bar_container = MagicMock(name="bar_container")

        st.empty = MagicMock(side_effect=[status_container, time_container])
        st.progress = MagicMock(return_value=bar_container)

        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____ ERROR at setup of test_display_result_sanitizes_message_before_render _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f169220>

    @pytest.fixture
    def sanitized_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[Tuple[ModuleType, ModuleType]]:
        """Yield a reloaded WebUI module for sanitization assertions."""

        st = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", st)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:82:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_lazy_loader_imports_streamlit_stub_once ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcae8d0>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""

        stub = StreamlitStub()
        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:227: AttributeError
_________ ERROR at setup of test_display_result_sanitizes_error_output _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb726c0>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""

        stub = StreamlitStub()
        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:227: AttributeError
________ ERROR at setup of test_ui_progress_tracks_status_and_subtasks _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb73110>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""

        stub = StreamlitStub()
        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:227: AttributeError
____ ERROR at setup of test_router_run_uses_default_and_persists_selection _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcafc50>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""

        stub = StreamlitStub()
        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:227: AttributeError
________ ERROR at setup of test_webui_run_configures_router_and_layout _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcaee10>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""

        stub = StreamlitStub()
        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:227: AttributeError
________ ERROR at setup of test_ask_question_selectbox_indexes_default _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcaf980>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_ask_question_text_input_when_no_choices ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcaf680>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ ERROR at setup of test_confirm_choice_returns_checkbox_value _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcad4c0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_display_result_error_surfaces_suggestions_and_docs ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d982d80>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_render_traceback_expander_renders_code _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d9828d0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ui_progress_sanitizes_updates _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d9827e0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_ensure_router_memoizes_instance ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f20b050>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_run_handles_page_config_errors _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f20aab0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_renders_layout_and_router _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f20ade0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""

        from tests.unit.interface.test_webui_enhanced import _mock_streamlit

        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ ERROR at setup of test_generation_executed __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f12c4d0>

    @pytest.fixture
    def webui_context(monkeypatch):
        st = ModuleType("streamlit")

        class SS(dict):
            pass

        st.session_state = SS()
        st.session_state.wizard_step = 0
        st.session_state.wizard_data = {}
        st.sidebar = ModuleType("sidebar")
        st.sidebar.radio = MagicMock(return_value="Synthesis")
        st.sidebar.title = MagicMock()
        st.sidebar.markdown = MagicMock()
        st.set_page_config = MagicMock()
        st.header = MagicMock()
        st.error = MagicMock()
        st.success = MagicMock()
        st.warning = MagicMock()
        st.info = MagicMock()
        st.expander = lambda *_a, **_k: DummyForm(True)
        st.form = lambda *_a, **_k: DummyForm(True)
        st.form_submit_button = MagicMock(return_value=True)
        st.text_input = MagicMock(return_value="text")
        st.text_area = MagicMock(return_value="desc")
        st.selectbox = MagicMock(return_value="choice")
        st.checkbox = MagicMock(return_value=True)
        st.button = MagicMock(return_value=True)
        st.toggle = MagicMock(return_value=True)
        st.number_input = MagicMock(return_value=1)
        st.spinner = DummyForm
        st.divider = MagicMock()

        class _CompV1:
            @staticmethod
            def html(_html, **_kwargs):
                return None

        class _Components:
            v1 = _CompV1()

        st.components = _Components()
        # Create MagicMock objects for columns that can be configured during
tests
        col1_mock = MagicMock()
        col1_mock.button = MagicMock(return_value=False)
        col2_mock = MagicMock()
        col2_mock.button = MagicMock(return_value=False)
        col3_mock = MagicMock()
        col3_mock.button = MagicMock(return_value=False)
        st.columns = MagicMock(return_value=(col1_mock, col2_mock, col3_mock))
        st.progress = MagicMock()
        st.write = MagicMock()
        st.markdown = MagicMock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

        # Stub optional dependencies used during WebUI import
        modules = [
            "langgraph",
            "langgraph.checkpoint",
            "langgraph.checkpoint.base",
            "langgraph.graph",
            "langchain",
            "langchain_openai",
            "langchain_community",
            "tiktoken",
            "tinydb",
            "tinydb.storages",
            "tinydb.middlewares",
            "duckdb",
            "lmdb",
            "faiss",
            "httpx",
            "lmstudio",
            "openai",
            "openai.types",
            "openai.types.chat",
            "torch",
            "transformers",
            "astor",
        ]

        for name in modules:
            module = ModuleType(name)
            if name == "langgraph.checkpoint.base":
                module.BaseCheckpointSaver = object
                module.empty_checkpoint = object()
            if name == "langgraph.graph":
                module.END = None
                module.StateGraph = object
            if name == "tinydb":
                module.TinyDB = object
                module.Query = object
            if name == "tinydb.storages":
                module.JSONStorage = object
                module.MemoryStorage = object
            if name == "tinydb.middlewares":
                module.CachingMiddleware = object
            if name == "openai":
                module.OpenAI = object
                module.AsyncOpenAI = object
            if name == "openai.types.chat":
                module.ChatCompletion = object
                module.ChatCompletionChunk = object
            if name == "transformers":
                module.AutoModelForCausalLM = object
                module.AutoTokenizer = object
            if name == "httpx":
                module.RequestError = Exception
                module.HTTPStatusError = Exception
            monkeypatch.setitem(sys.modules, name, module)

        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)

        from pathlib import Path

>       from devsynth.config import ProjectUnifiedConfig
E       ImportError: cannot import name 'ProjectUnifiedConfig' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_w
ebui_synthesis_steps.py:142: ImportError
______________ ERROR at setup of test_generation_executed_custom _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0a98e0>

    @pytest.fixture
    def webui_context(monkeypatch):
        st = ModuleType("streamlit")

        class SS(dict):
            pass

        st.session_state = SS()
        st.session_state.wizard_step = 0
        st.session_state.wizard_data = {}
        st.sidebar = ModuleType("sidebar")
        st.sidebar.radio = MagicMock(return_value="Synthesis")
        st.sidebar.title = MagicMock()
        st.sidebar.markdown = MagicMock()
        st.set_page_config = MagicMock()
        st.header = MagicMock()
        st.error = MagicMock()
        st.success = MagicMock()
        st.warning = MagicMock()
        st.info = MagicMock()
        st.expander = lambda *_a, **_k: DummyForm(True)
        st.form = lambda *_a, **_k: DummyForm(True)
        st.form_submit_button = MagicMock(return_value=True)
        st.text_input = MagicMock(return_value="text")
        st.text_area = MagicMock(return_value="desc")
        st.selectbox = MagicMock(return_value="choice")
        st.checkbox = MagicMock(return_value=True)
        st.button = MagicMock(return_value=True)
        st.toggle = MagicMock(return_value=True)
        st.number_input = MagicMock(return_value=1)
        st.spinner = DummyForm
        st.divider = MagicMock()

        class _CompV1:
            @staticmethod
            def html(_html, **_kwargs):
                return None

        class _Components:
            v1 = _CompV1()

        st.components = _Components()
        # Create MagicMock objects for columns that can be configured during
tests
        col1_mock = MagicMock()
        col1_mock.button = MagicMock(return_value=False)
        col2_mock = MagicMock()
        col2_mock.button = MagicMock(return_value=False)
        col3_mock = MagicMock()
        col3_mock.button = MagicMock(return_value=False)
        st.columns = MagicMock(return_value=(col1_mock, col2_mock, col3_mock))
        st.progress = MagicMock()
        st.write = MagicMock()
        st.markdown = MagicMock()
        monkeypatch.setitem(sys.modules, "streamlit", st)

        # Stub optional dependencies used during WebUI import
        modules = [
            "langgraph",
            "langgraph.checkpoint",
            "langgraph.checkpoint.base",
            "langgraph.graph",
            "langchain",
            "langchain_openai",
            "langchain_community",
            "tiktoken",
            "tinydb",
            "tinydb.storages",
            "tinydb.middlewares",
            "duckdb",
            "lmdb",
            "faiss",
            "httpx",
            "lmstudio",
            "openai",
            "openai.types",
            "openai.types.chat",
            "torch",
            "transformers",
            "astor",
        ]

        for name in modules:
            module = ModuleType(name)
            if name == "langgraph.checkpoint.base":
                module.BaseCheckpointSaver = object
                module.empty_checkpoint = object()
            if name == "langgraph.graph":
                module.END = None
                module.StateGraph = object
            if name == "tinydb":
                module.TinyDB = object
                module.Query = object
            if name == "tinydb.storages":
                module.JSONStorage = object
                module.MemoryStorage = object
            if name == "tinydb.middlewares":
                module.CachingMiddleware = object
            if name == "openai":
                module.OpenAI = object
                module.AsyncOpenAI = object
            if name == "openai.types.chat":
                module.ChatCompletion = object
                module.ChatCompletionChunk = object
            if name == "transformers":
                module.AutoModelForCausalLM = object
                module.AutoTokenizer = object
            if name == "httpx":
                module.RequestError = Exception
                module.HTTPStatusError = Exception
            monkeypatch.setitem(sys.modules, name, module)

        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)

        from pathlib import Path

>       from devsynth.config import ProjectUnifiedConfig
E       ImportError: cannot import name 'ProjectUnifiedConfig' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_w
ebui_synthesis_steps.py:142: ImportError
___________ ERROR at setup of test_recursion_depth_exceeded_succeeds ___________

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5h0uizof'
memory_manager = <MagicMock spec='MemoryManager' id='6108346848'>
wsde_team = <MagicMock spec='WSDETeam' id='5329157104'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='5090009280'>
ast_transformer = <MagicMock spec='AstTransformer' id='6123100480'>
prompt_manager = <MagicMock spec='PromptManager' id='6123103264'>
documentation_manager = <MagicMock spec='DocumentationManager' id='6123097792'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
__________ ERROR at setup of test_recursion_depth_increments_succeeds __________

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp9y3frtf8'
memory_manager = <MagicMock spec='MemoryManager' id='6108187104'>
wsde_team = <MagicMock spec='WSDETeam' id='6123103936'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='5327602800'>
ast_transformer = <MagicMock spec='AstTransformer' id='6108632160'>
prompt_manager = <MagicMock spec='PromptManager' id='5329146976'>
documentation_manager = <MagicMock spec='DocumentationManager' id='5329801760'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
_________ ERROR at setup of test_abort_when_should_terminate_succeeds __________

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpvkm5c93i'
memory_manager = <MagicMock spec='MemoryManager' id='6123692176'>
wsde_team = <MagicMock spec='WSDETeam' id='5328382304'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='6107334848'>
ast_transformer = <MagicMock spec='AstTransformer' id='6107337248'>
prompt_manager = <MagicMock spec='PromptManager' id='5095494560'>
documentation_manager = <MagicMock spec='DocumentationManager' id='6107345744'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
__________ ERROR at setup of test_store_metadata_and_results_succeeds __________

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp9j4ft65c'
memory_manager = <MagicMock spec='MemoryManager' id='6108567488'>
wsde_team = <MagicMock spec='WSDETeam' id='6123638560'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='5330759568'>
ast_transformer = <MagicMock spec='AstTransformer' id='5330338160'>
prompt_manager = <MagicMock spec='PromptManager' id='5330345504'>
documentation_manager = <MagicMock spec='DocumentationManager' id='5330341280'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
_____ ERROR at setup of test_parent_aggregates_after_micro_phase_succeeds ______

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5y0slg2m'
memory_manager = <MagicMock spec='MemoryManager' id='5329263536'>
wsde_team = <MagicMock spec='WSDETeam' id='5099963072'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='5330292560'>
ast_transformer = <MagicMock spec='AstTransformer' id='5330289776'>
prompt_manager = <MagicMock spec='PromptManager' id='5330288816'>
documentation_manager = <MagicMock spec='DocumentationManager' id='5329142320'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
____ ERROR at setup of test_create_micro_cycle_from_manifest_dict_succeeds _____

temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpcmrl3r4a'
memory_manager = <MagicMock spec='MemoryManager' id='5330344784'>
wsde_team = <MagicMock spec='WSDETeam' id='5330360160'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='5329801184'>
ast_transformer = <MagicMock spec='AstTransformer' id='6107347664'>
prompt_manager = <MagicMock spec='PromptManager' id='6107335664'>
documentation_manager = <MagicMock spec='DocumentationManager' id='6107344688'>

    @pytest.fixture
    def coordinator(
        temp_dir,
        memory_manager,
        wsde_team,
        code_analyzer,
        ast_transformer,
        prompt_manager,
        documentation_manager,
    ) -> Generator[EDRRCoordinator, None, None]:
        """Return an EDRR coordinator with mock dependencies.

        This fixture uses a generator pattern to provide teardown functionality.
        It also uses the temp_dir fixture to ensure any files created by the
coordinator
        are isolated and cleaned up after the test.
        """
        # Setup: Create the coordinator
>       coordinator = EDRRCoordinator(
            memory_manager=memory_manager,
            wsde_team=wsde_team,
            code_analyzer=code_analyzer,
            ast_transformer=ast_transformer,
            prompt_manager=prompt_manager,
            documentation_manager=documentation_manager,
            enable_enhanced_logging=True,
            workspace_dir=temp_dir,  # Use temporary directory for workspace
        )
E       TypeError: EDRRCoordinator.__init__() got an unexpected keyword argument
'workspace_dir'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_micro_cycle.py:180: TypeError
____________ ERROR at setup of test_onboarding_calls_init_succeeds _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd24b0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________ ERROR at setup of test_requirements_calls_spec_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd2ea0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________ ERROR at setup of test_analysis_calls_analyze_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd0110>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
______________ ERROR at setup of test_synthesis_buttons_succeeds _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd1190>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________ ERROR at setup of test_config_update_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd3680>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________ ERROR at setup of test_diagnostics_runs_doctor_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f86a5a0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_______________ ERROR at setup of test_edrr_cycle_page_succeeds ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc810d0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________ ERROR at setup of test_alignment_page_succeeds ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5c050>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________ ERROR at setup of test_alignment_metrics_page_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5e3c0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_____________ ERROR at setup of test_inspect_config_page_succeeds ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edc7a10>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________ ERROR at setup of test_validate_manifest_page_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc839b0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________ ERROR at setup of test_validate_metadata_page_succeeds ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x109dd46b0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
______________ ERROR at setup of test_test_metrics_page_succeeds _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74e2d0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_____________ ERROR at setup of test_docs_generation_page_succeeds _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d552330>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________ ERROR at setup of test_ingestion_page_succeeds ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d551880>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ ERROR at setup of test_apispec_page_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5527b0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________ ERROR at setup of test_refactor_page_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5533e0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ ERROR at setup of test_webapp_page_succeeds __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d552b40>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
__________________ ERROR at setup of test_serve_page_succeeds __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5529f0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________ ERROR at setup of test_dbschema_page_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5528d0>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ ERROR at setup of test_doctor_page_succeeds __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d555850>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________ ERROR at setup of test_run_method_renders_pages_succeeds ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d557c50>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________ ERROR at setup of test_wizard_navigation_helper_clamps_steps _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d554710>

    @pytest.fixture(autouse=True)
    def stub_streamlit(monkeypatch):
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        cli_stub = ModuleType("devsynth.application.cli")
        for name in [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "inspect_cmd",
            "doctor_cmd",
        ]:
            setattr(cli_stub, name, MagicMock())
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_stub)
        utils_stub = ModuleType("devsynth.application.cli.utils")
        utils_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.utils",
utils_stub)
        ingest_stub = ModuleType("devsynth.application.cli.ingest_cmd")
        ingest_stub.ingest_cmd = MagicMock()
        ingest_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.ingest_cmd",
ingest_stub)
        cli_stub.ingest_cmd = ingest_stub.ingest_cmd
        commands_stub = ModuleType("devsynth.application.cli.commands")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.commands",
commands_stub)
        command_modules = {
            "inspect_code_cmd": "inspect_code_cmd",
            "doctor_cmd": "doctor_cmd",
            "edrr_cycle_cmd": "edrr_cycle_cmd",
            "align_cmd": "align_cmd",
            "alignment_metrics_cmd": "alignment_metrics_cmd",
            "inspect_config_cmd": "inspect_config_cmd",
            "validate_manifest_cmd": "validate_manifest_cmd",
            "validate_metadata_cmd": "validate_metadata_cmd",
            "test_metrics_cmd": "test_metrics_cmd",
            "generate_docs_cmd": "generate_docs_cmd",
        }
        for module_name, cmd_name in command_modules.items():
            module_path = f"devsynth.application.cli.commands.{module_name}"
            module_stub = ModuleType(module_path)
            setattr(module_stub, cmd_name, MagicMock())
            module_stub.bridge = MagicMock()
            monkeypatch.setitem(sys.modules, module_path, module_stub)
            if module_name == "doctor_cmd":
                cli_stub.doctor_cmd = module_stub.doctor_cmd
        apispec_stub = ModuleType("devsynth.application.cli.apispec")
        apispec_stub.apispec_cmd = MagicMock()
        apispec_stub.bridge = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.application.cli.apispec",
apispec_stub)
        setup_wizard_stub = ModuleType("devsynth.application.cli.setup_wizard")
        setup_wizard_stub.SetupWizard = MagicMock()
        monkeypatch.setitem(
            sys.modules, "devsynth.application.cli.setup_wizard",
setup_wizard_stub
        )
        # Ensure command mixins see the stubbed CLI module
        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_ ERROR at setup of
TestCLIWebUIAgentAPIPipeline.test_init_command_pipeline_succeeds _

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284bc350>
mock_streamlit = <module 'streamlit'>

    @pytest.fixture
    def webui_bridge(self, mock_streamlit):
        """Create a WebUI bridge for testing."""
>       return WebUI()
               ^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:109: TypeError
_ ERROR at setup of
TestCLIWebUIAgentAPIPipeline.test_spec_command_pipeline_succeeds _

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284bc890>
mock_streamlit = <module 'streamlit'>

    @pytest.fixture
    def webui_bridge(self, mock_streamlit):
        """Create a WebUI bridge for testing."""
>       return WebUI()
               ^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:109: TypeError
____________ ERROR at setup of test_gather_updates_config_succeeds _____________
file
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py, line 25
  @pytest.mark.medium
  def test_gather_updates_config_succeeds(tmp_path, monkeypatch):
E       fixture 'stub_optional_deps' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example,
_restore_env_and_cwd_between_tests, _session_faker, _session_scoped_runner,
_stub_external_services, _sys_snapshot, anyio_backend, anyio_backend_name,
anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary,
caplog, capsys, capsysbinary, capteesys, chromadb_client, chromadb_temp_path,
class_mocker, cov, coverage_stub_factory, deterministic_seed, disable_network,
doctest_namespace, duckdb_connection, duckdb_path, enforce_test_timeout,
ephemeral_kuzu_store, event_loop_policy, extra, extras, faiss_index, faker,
free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory,
gather_wizard_state, gather_wizard_state_manager, global_test_isolation,
include_metadata_in_junit_xml, kuzu_db_path, linecomp, llm_port, lmdb_env,
lmstudio_mock, lmstudio_service, memory_port, metadata, mock_datetime,
mock_lm_studio_provider, mock_openai_provider, mock_session_state,
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui,
mock_wizard_state, mocker, module_mocker, monkeypatch, no_cover,
normalize_subsystem_stubs, onnx_port, package_mocker, patch_settings_paths,
pytestbdd_stepdef_given_trace, pytestbdd_stepdef_then_trace,
pytestbdd_stepdef_when_trace, pytestconfig, pytester, rdflib_graph,
record_property, record_testsuite_property, record_xml_attribute, recwarn,
require_modules, reset_coverage, reset_global_state, session_mocker,
streamlit_bridge_stub, stub_devsynth_config, stub_optional_dependencies,
temp_log_dir, temp_memory_path, temporary_kuzu_config, test_environment,
testdir, testrun_uid, tinydb_path, tmp_path, tmp_path_factory, tmp_project_dir,
tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory,
unused_udp_port, unused_udp_port_factory, webui_context, wizard_state,
wizard_state_manager, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py:25
____ ERROR at setup of test_requirements_wizard_persists_priority_succeeds _____
file
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py, line 74
  @pytest.mark.medium
  def test_requirements_wizard_persists_priority_succeeds(tmp_path,
monkeypatch):
E       fixture 'stub_optional_deps' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example,
_restore_env_and_cwd_between_tests, _session_faker, _session_scoped_runner,
_stub_external_services, _sys_snapshot, anyio_backend, anyio_backend_name,
anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary,
caplog, capsys, capsysbinary, capteesys, chromadb_client, chromadb_temp_path,
class_mocker, cov, coverage_stub_factory, deterministic_seed, disable_network,
doctest_namespace, duckdb_connection, duckdb_path, enforce_test_timeout,
ephemeral_kuzu_store, event_loop_policy, extra, extras, faiss_index, faker,
free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory,
gather_wizard_state, gather_wizard_state_manager, global_test_isolation,
include_metadata_in_junit_xml, kuzu_db_path, linecomp, llm_port, lmdb_env,
lmstudio_mock, lmstudio_service, memory_port, metadata, mock_datetime,
mock_lm_studio_provider, mock_openai_provider, mock_session_state,
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui,
mock_wizard_state, mocker, module_mocker, monkeypatch, no_cover,
normalize_subsystem_stubs, onnx_port, package_mocker, patch_settings_paths,
pytestbdd_stepdef_given_trace, pytestbdd_stepdef_then_trace,
pytestbdd_stepdef_when_trace, pytestconfig, pytester, rdflib_graph,
record_property, record_testsuite_property, record_xml_attribute, recwarn,
require_modules, reset_coverage, reset_global_state, session_mocker,
streamlit_bridge_stub, stub_devsynth_config, stub_optional_dependencies,
temp_log_dir, temp_memory_path, temporary_kuzu_config, test_environment,
testdir, testrun_uid, tinydb_path, tmp_path, tmp_path_factory, tmp_project_dir,
tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory,
unused_udp_port, unused_udp_port_factory, webui_context, wizard_state,
wizard_state_manager, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py:74
___ ERROR at setup of test_requirements_wizard_backtracks_priority_succeeds ____
file
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py, line 113
  @pytest.mark.medium
  def test_requirements_wizard_backtracks_priority_succeeds(tmp_path,
monkeypatch):
E       fixture 'stub_optional_deps' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example,
_restore_env_and_cwd_between_tests, _session_faker, _session_scoped_runner,
_stub_external_services, _sys_snapshot, anyio_backend, anyio_backend_name,
anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary,
caplog, capsys, capsysbinary, capteesys, chromadb_client, chromadb_temp_path,
class_mocker, cov, coverage_stub_factory, deterministic_seed, disable_network,
doctest_namespace, duckdb_connection, duckdb_path, enforce_test_timeout,
ephemeral_kuzu_store, event_loop_policy, extra, extras, faiss_index, faker,
free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory,
gather_wizard_state, gather_wizard_state_manager, global_test_isolation,
include_metadata_in_junit_xml, kuzu_db_path, linecomp, llm_port, lmdb_env,
lmstudio_mock, lmstudio_service, memory_port, metadata, mock_datetime,
mock_lm_studio_provider, mock_openai_provider, mock_session_state,
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui,
mock_wizard_state, mocker, module_mocker, monkeypatch, no_cover,
normalize_subsystem_stubs, onnx_port, package_mocker, patch_settings_paths,
pytestbdd_stepdef_given_trace, pytestbdd_stepdef_then_trace,
pytestbdd_stepdef_when_trace, pytestconfig, pytester, rdflib_graph,
record_property, record_testsuite_property, record_xml_attribute, recwarn,
require_modules, reset_coverage, reset_global_state, session_mocker,
streamlit_bridge_stub, stub_devsynth_config, stub_optional_dependencies,
temp_log_dir, temp_memory_path, temporary_kuzu_config, test_environment,
testdir, testrun_uid, tinydb_path, tmp_path, tmp_path_factory, tmp_project_dir,
tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory,
unused_udp_port, unused_udp_port_factory, webui_context, wizard_state,
wizard_state_manager, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py:113
_________ ERROR at setup of test_gather_cmd_logging_exc_info_succeeds __________
file
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py, line 151
  @pytest.mark.medium
  def test_gather_cmd_logging_exc_info_succeeds(tmp_path, monkeypatch):
E       fixture 'stub_optional_deps' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example,
_restore_env_and_cwd_between_tests, _session_faker, _session_scoped_runner,
_stub_external_services, _sys_snapshot, anyio_backend, anyio_backend_name,
anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary,
caplog, capsys, capsysbinary, capteesys, chromadb_client, chromadb_temp_path,
class_mocker, cov, coverage_stub_factory, deterministic_seed, disable_network,
doctest_namespace, duckdb_connection, duckdb_path, enforce_test_timeout,
ephemeral_kuzu_store, event_loop_policy, extra, extras, faiss_index, faker,
free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory,
gather_wizard_state, gather_wizard_state_manager, global_test_isolation,
include_metadata_in_junit_xml, kuzu_db_path, linecomp, llm_port, lmdb_env,
lmstudio_mock, lmstudio_service, memory_port, metadata, mock_datetime,
mock_lm_studio_provider, mock_openai_provider, mock_session_state,
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui,
mock_wizard_state, mocker, module_mocker, monkeypatch, no_cover,
normalize_subsystem_stubs, onnx_port, package_mocker, patch_settings_paths,
pytestbdd_stepdef_given_trace, pytestbdd_stepdef_then_trace,
pytestbdd_stepdef_when_trace, pytestconfig, pytester, rdflib_graph,
record_property, record_testsuite_property, record_xml_attribute, recwarn,
require_modules, reset_coverage, reset_global_state, session_mocker,
streamlit_bridge_stub, stub_devsynth_config, stub_optional_dependencies,
temp_log_dir, temp_memory_path, temporary_kuzu_config, test_environment,
testdir, testrun_uid, tinydb_path, tmp_path, tmp_path_factory, tmp_project_dir,
tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory,
unused_udp_port, unused_udp_port_factory, webui_context, wizard_state,
wizard_state_manager, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_requirements_gathering.py:151
_________ ERROR at setup of test_webui_pages_invoke_commands_succeeds __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x33665bf20>

    @pytest.fixture
    def webui_env(monkeypatch):
        st = _setup_streamlit(monkeypatch)
        st.form_submit_button = MagicMock(return_value=True)
        st.number_input = MagicMock(return_value=30)
        monkeypatch.setattr("pathlib.Path.exists", lambda self: True)
        for name in ["numpy", "responses", "networkx", "requests"]:
            monkeypatch.setitem(sys.modules, name, ModuleType(name))
        commands = {
            "edrr_cycle_page": (
                "devsynth.application.cli.commands.edrr_cycle_cmd",
                "edrr_cycle_cmd",
            ),
            "alignment_page": ("devsynth.application.cli.commands.align_cmd",
"align_cmd"),
            "alignment_metrics_page": (
                "devsynth.application.cli.commands.alignment_metrics_cmd",
                "alignment_metrics_cmd",
            ),
            "inspect_config_page": (
                "devsynth.application.cli.commands.inspect_config_cmd",
                "inspect_config_cmd",
            ),
            "validate_manifest_page": (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            "validate_metadata_page": (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
            "test_metrics_page": (
                "devsynth.application.cli.commands.test_metrics_cmd",
                "test_metrics_cmd",
            ),
            "docs_generation_page": (
                "devsynth.application.cli.commands.generate_docs_cmd",
                "generate_docs_cmd",
            ),
            "ingestion_page": ("devsynth.application.cli.ingest_cmd",
"ingest_cmd"),
            "apispec_page": ("devsynth.application.cli.apispec", "apispec_cmd"),
            "refactor_page": ("devsynth.application.cli.cli_commands",
"refactor_cmd"),
            "webapp_page": ("devsynth.application.cli.cli_commands",
"webapp_cmd"),
            "serve_page": ("devsynth.application.cli.cli_commands",
"serve_cmd"),
            "dbschema_page": ("devsynth.application.cli.cli_commands",
"dbschema_cmd"),
        }
        all_cmds = [
            "init_cmd",
            "spec_cmd",
            "test_cmd",
            "code_cmd",
            "run_pipeline_cmd",
            "config_cmd",
            "enable_feature_cmd",
            "gather_cmd",
            "config_app",
            "inspect_cmd",
            "webapp_cmd",
            "webui_cmd",
            "dbschema_cmd",
            "check_cmd",
            "refactor_cmd",
            "serve_cmd",
        ]
        mocks = {}
        for _, (module_path, func_name) in commands.items():
            module = ModuleType(module_path)
            module.bridge = None
            func = MagicMock()
            setattr(module, func_name, func)
            monkeypatch.setitem(sys.modules, module_path, module)
            mocks[func_name] = func
        for name in all_cmds:
            mocks.setdefault(name, MagicMock())
        cli_cmds = ModuleType("devsynth.application.cli.cli_commands")
        for name, func in mocks.items():
            setattr(cli_cmds, name, func)
        monkeypatch.setitem(sys.modules,
"devsynth.application.cli.cli_commands", cli_cmds)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_webui_pages.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______ ERROR at setup of test_run_injects_assets_and_resets_navigation ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336a1b950>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a controlled Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_webui_run_navigation.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_run_handles_page_config_errors _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x3369d5e20>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a controlled Streamlit substitute."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_webui_run_navigation.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
=================================== FAILURES ===================================
_____________ test_provider_factory_offline_uses_stub_safe_default _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce514f0>

    @pytest.mark.fast
    def test_provider_factory_offline_uses_stub_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return the stub provider when offline guard is enabled.

        ReqID: N/A
        """

        config = _make_provider_config(openai_key="token")
        _install_factory_config(monkeypatch, config)

        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "stub")
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
raising=False)

        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, StubProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x10cd305f0>, StubProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:251: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,058 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 250, in
test_provider_factory_offline_uses_stub_safe_default
    provider = ProviderFactory.create_provider("openai")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 311, in create_provider
    return _safe_provider("DEVSYNTH_OFFLINE active; using safe provider")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 300, in _safe_provider
    logger.info("Falling back to Stub provider: %s", reason)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Stub provider: %s'
Arguments: ('DEVSYNTH_OFFLINE active; using safe provider',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_____________ test_provider_factory_offline_uses_null_safe_default _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce50320>

    @pytest.mark.fast
    def test_provider_factory_offline_uses_null_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return the null provider when offline guard favors null safe
defaults.

        ReqID: N/A
        """

        config = _make_provider_config(openai_key="token")
        _install_factory_config(monkeypatch, config)

        monkeypatch.setenv("DEVSYNTH_OFFLINE", "1")
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
raising=False)

        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.NullProvider object at
0x10cd30b00>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:272: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,384 - devsynth.adapters.provider_system - INFO - Falling
back to Null provider: DEVSYNTH_OFFLINE active; using safe provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 271, in
test_provider_factory_offline_uses_null_safe_default
    provider = ProviderFactory.create_provider("openai")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 311, in create_provider
    return _safe_provider("DEVSYNTH_OFFLINE active; using safe provider")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('DEVSYNTH_OFFLINE active; using safe provider',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Null provider: DEVSYNTH_OFFLINE active; using safe provider
_
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce53380>

    @pytest.mark.fast
    def
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Use the safe default provider when LM Studio fallback is not
permitted.

        ReqID: N/A
        """

        config = _make_provider_config(openai_key=None)
        _install_factory_config(monkeypatch, config)

        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "false")
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)

        provider = ProviderFactory.create_provider()
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.NullProvider object at
0x10cc254c0>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:294: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,407 - devsynth.adapters.provider_system - INFO - Falling
back to Null provider: No OPENAI_API_KEY; LM Studio not marked available. Hint:
export OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set
DEVSYNTH_PROVIDER=stub for offline runs.
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 293, in
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable
    provider = ProviderFactory.create_provider()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 349, in create_provider
    return _safe_provider(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('No OPENAI_API_KEY; LM Studio not marked available. Hint: export
OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set
DEVSYNTH_PROVIDER=stub for offline runs.',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Null provider: No OPENAI_API_KEY; LM Studio not marked available. Hint: export
OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set
DEVSYNTH_PROVIDER=stub for offline runs.
_ test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc268a0>

    @pytest.mark.fast
    def
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return a null provider when LM Studio instantiation raises an
exception.

        ReqID: N/A
        """

        config = _make_provider_config()
        _install_factory_config(monkeypatch, config)

        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
raising=False)

        monkeypatch.setattr(
            provider_system,
            "LMStudioProvider",
            MagicMock(side_effect=RuntimeError("kaboom")),
        )

        provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.NullProvider object at
0x10ccbda90>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:351: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,436 - devsynth.adapters.provider_system - WARNING - Unknown
provider type 'ProviderType.LMSTUDIO', falling back to safe default
2025-10-28 10:29:42,437 - devsynth.adapters.provider_system - INFO - Falling
back to Null provider: Unknown provider type
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 350, in
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default
    provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 432, in create_provider
    logger.warning(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 627, in warning
    self._log(logging.WARNING, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: "Unknown provider type '%s', falling back to safe default"
Arguments: (<ProviderType.LMSTUDIO: 'lmstudio'>,)
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 350, in
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default
    provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 436, in create_provider
    return _safe_provider("Unknown provider type")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('Unknown provider type',)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider
type 'ProviderType.LMSTUDIO', falling back to safe default
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Null provider: Unknown provider type
_______ test_provider_factory_openai_explicit_missing_key_surfaces_error _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25d60>

    @pytest.mark.fast
    def test_provider_factory_openai_explicit_missing_key_surfaces_error(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Surface explicit OpenAI credential errors via null provider.

        ReqID: N/A
        """

        config = _make_provider_config(openai_key=None)
        _install_factory_config(monkeypatch, config)

        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", raising=False)

        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.NullProvider object at
0x10ccaafc0>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:372: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,459 - devsynth.adapters.provider_system - ERROR - OpenAI API
key is missing for explicitly requested OpenAI provider. Set OPENAI_API_KEY or
choose a safe provider via DEVSYNTH_PROVIDER=stub.
2025-10-28 10:29:42,460 - devsynth.adapters.provider_system - ERROR - Failed to
create provider openai: Missing OPENAI_API_KEY for OpenAI provider. Set
OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 371, in
test_provider_factory_openai_explicit_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("openai")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 318, in create_provider
    logger.error(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'OpenAI API key is missing for explicitly requested OpenAI provider.
Set OPENAI_API_KEY or choose a safe provider via DEVSYNTH_PROVIDER=stub.'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 324, in create_provider
    raise ProviderError(
devsynth.exceptions.ProviderError: Missing OPENAI_API_KEY for OpenAI provider.
Set OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 371, in
test_provider_factory_openai_explicit_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("openai")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 438, in create_provider
    logger.error(f"Failed to create provider {provider_type}: {e}")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Failed to create provider openai: Missing OPENAI_API_KEY for OpenAI
provider. Set OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.'
Arguments: ()
------------------------------ Captured log call -------------------------------
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 OpenAI API key
is missing for explicitly requested OpenAI provider. Set OPENAI_API_KEY or
choose a safe provider via DEVSYNTH_PROVIDER=stub.
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Failed to create
provider openai: Missing OPENAI_API_KEY for OpenAI provider. Set OPENAI_API_KEY
or use DEVSYNTH_PROVIDER=stub.
__________ test_provider_factory_anthropic_missing_key_surfaces_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25a30>

    @pytest.mark.fast
    def test_provider_factory_anthropic_missing_key_surfaces_error(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Surface explicit Anthropic credential errors via null provider.

        ReqID: N/A
        """

        config = _make_provider_config()
        _install_factory_config(monkeypatch, config)

        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)

        provider = ProviderFactory.create_provider("anthropic")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.NullProvider object at
0x10cc23c20>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:393: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,473 - devsynth.adapters.provider_system - ERROR - Anthropic
API key is missing for explicitly requested Anthropic provider
2025-10-28 10:29:42,474 - devsynth.adapters.provider_system - ERROR - Failed to
create provider anthropic: Anthropic API key is required for Anthropic provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 392, in
test_provider_factory_anthropic_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("anthropic")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 387, in create_provider
    logger.error(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Anthropic API key is missing for explicitly requested Anthropic
provider'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 391, in create_provider
    raise ProviderError(
devsynth.exceptions.ProviderError: Anthropic API key is required for Anthropic
provider

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 392, in
test_provider_factory_anthropic_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("anthropic")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 438, in create_provider
    logger.error(f"Failed to create provider {provider_type}: {e}")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Failed to create provider anthropic: Anthropic API key is required for
Anthropic provider'
Arguments: ()
------------------------------ Captured log call -------------------------------
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Anthropic API
key is missing for explicitly requested Anthropic provider
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Failed to create
provider anthropic: Anthropic API key is required for Anthropic provider
_______________ test_provider_factory_accepts_provider_type_enum _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25310>

    @pytest.mark.fast
    def test_provider_factory_accepts_provider_type_enum(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Interpret ``ProviderType`` enum values when selecting providers.

        ReqID: N/A
        """

        config = _make_provider_config(default_provider="stub")
        _install_factory_config(monkeypatch, config)

        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", raising=False)

        provider = ProviderFactory.create_provider(ProviderType.STUB)
>       assert isinstance(provider, StubProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x11d002750>, StubProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:414: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:42,496 - devsynth.adapters.provider_system - WARNING - Unknown
provider type 'ProviderType.STUB', falling back to safe default
2025-10-28 10:29:42,498 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: Unknown provider type
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 413, in
test_provider_factory_accepts_provider_type_enum
    provider = ProviderFactory.create_provider(ProviderType.STUB)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 432, in create_provider
    logger.warning(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 627, in warning
    self._log(logging.WARNING, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: "Unknown provider type '%s', falling back to safe default"
Arguments: (<ProviderType.STUB: 'stub'>,)
--- Logging error ---
Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory:
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs,
firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 413, in
test_provider_factory_accepts_provider_type_enum
    provider = ProviderFactory.create_provider(ProviderType.STUB)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 436, in create_provider
    return _safe_provider("Unknown provider type")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 300, in _safe_provider
    logger.info("Falling back to Stub provider: %s", reason)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Stub provider: %s'
Arguments: ('Unknown provider type',)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider
type 'ProviderType.STUB', falling back to safe default
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: Unknown provider type
_
test_command_module_import[devsynth.application.cli.commands.enhanced_analysis_c
md] _

module_name = 'devsynth.application.cli.commands.enhanced_analysis_cmd'

    @pytest.mark.parametrize("module_name", MODULE_NAMES)
    def test_command_module_import(module_name: str) -> None:
        """Each CLI command module should be importable."""

>       importlib.import_module(module_name)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_module_imports.py:28:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Enhanced Analysis Command

    This module provides a comprehensive command that runs all enhanced analysis
    capabilities including test infrastructure analysis, quality assurance,
    security validation, and requirements traceability.

    Key features:
    - Unified command interface for all enhanced analysis tools
    - Configurable analysis scope and depth
    - Integration with DevSynth's EDRR workflow system
    - Comprehensive reporting with multiple output formats
    - Memory system integration for persistent analysis results
    """

    import json
    import os
    import time
    from pathlib import Path
    from typing import Any, Dict, List, Optional

    import typer
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.table import Table

    from devsynth.application.testing.enhanced_test_collector import
EnhancedTestCollector
>   from devsynth.application.testing.test_isolation_analyzer import
TestIsolationAnalyzer
E   ModuleNotFoundError: No module named
'devsynth.application.testing.test_isolation_analyzer'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/enhanced_analysis_cmd.py:28: ModuleNotFoundError
_________________________ test_inventory_exports_file __________________________

obj = <function run_tests_cmd at 0x10ac6ad40>, name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cd330b0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_inventory_exports_file0')
tmp_path_factory = TempPathFactory(_given_basetemp=None,
_trace=<pluggy._tracing.TagTracerSub object at 0x10b296990>,
_basetemp=PosixPath...lders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitly
n/pytest-1440'), _retention_count=3, _retention_policy='all')

    @pytest.mark.fast
    def test_inventory_exports_file(monkeypatch, tmp_path, tmp_path_factory) ->
None:
        """ReqID: CLI-RT-08 — inventory mode exports JSON and skips run."""

        # Return deterministic collections
        def fake_collect(tgt: str, spd: str | None = None) -> list[str]:
            return ["tests/unit/test_example.py::test_ok"]

        def fake_run_tests(*args, **kwargs) -> tuple[bool, str]:
            pytest.fail("run_tests should not be called in inventory mode")

>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_
cache",
            fake_collect,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:191:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <function run_tests_cmd at 0x10ac6ad40>, name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'function' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
__________________ test_failed_run_surfaces_maxfail_guidance ___________________

obj = <function run_tests_cmd at 0x10ac6ad40>, name = 'run_tests'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc20a70>

    @pytest.mark.fast
    def test_failed_run_surfaces_maxfail_guidance(monkeypatch) -> None:
        """ReqID: CLI-RT-17 — Failed runs surface maxfail troubleshooting
tips."""

        failure_output = (
            "Pytest exited with code 1. Command: python -m pytest --maxfail=2
tests/unit\n"
            "Troubleshooting tips:\n"
            "- Segment large suites to localize failures.\n"
        )

        def fake_run_tests(*args, **kwargs) -> tuple[bool, str]:  # noqa: ANN001
            return False, failure_output

>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.run_tests",
            fake_run_tests,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:403:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <function run_tests_cmd at 0x10ac6ad40>, name = 'run_tests'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'function' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ test_run_tests_cmd_exits_when_pytest_cov_missing _______________

obj = <function run_tests_cmd at 0x10ac6ad40>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc22a20>

    @pytest.mark.fast
    def test_run_tests_cmd_exits_when_pytest_cov_missing(monkeypatch) -> None:
        """Missing pytest-cov triggers an actionable remediation banner."""

>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_
status",
            lambda env=None: (
                False,
                run_tests_cmd_module.PYTEST_COV_PLUGIN_MISSING_MESSAGE,
            ),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:437:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <function run_tests_cmd at 0x10ac6ad40>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'function' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________ test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov ___________

obj = <function run_tests_cmd at 0x10ac6ad40>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25430>

    @pytest.mark.fast
    def test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov(monkeypatch) ->
None:
        """Autoload blocking pytest-cov halts execution for standard runs."""

>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_
status",
            lambda env=None: (
                False,
                run_tests_cmd_module.PYTEST_COV_AUTOLOAD_DISABLED_MESSAGE,
            ),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:483:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <function run_tests_cmd at 0x10ac6ad40>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'function' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________ test_cli_runner_inventory_handles_collection_errors ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10b82f050>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_runner_inventory_hand0')

    @pytest.mark.fast
    def test_cli_runner_inventory_handles_collection_errors(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Inventory exports tolerate collection errors and still succeed."""

        monkeypatch.chdir(tmp_path)

        calls: list[tuple[str, str | None]] = []

        def fake_collect(target: str, speed: str | None) -> list[str]:
            calls.append((target, speed))
            if target == "integration-tests" and speed == "medium":
                raise RuntimeError("collection failed")
            suffix = speed or "all"
            return [f"{target}::{suffix}::test_case"]

        app, cli_module = _build_minimal_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache",
fake_collect)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:311: AttributeError
_____________ test_cli_runner_failed_run_surfaces_maxfail_guidance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc240b0>

    @pytest.mark.fast
    def test_cli_runner_failed_run_surfaces_maxfail_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Failed Typer invocation should surface the maxfail troubleshooting
tip."""

        app, cli_module = _build_minimal_app(monkeypatch)

        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            cmd = ["python", "-m", "pytest", "--maxfail", "2"]
            tips = run_tests_module._failure_tips(1, cmd)
            return False, "segment error\n" + tips

>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:340: AttributeError
____________ test_cli_runner_inventory_write_failure_exits_nonzero _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc8b90>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_runner_inventory_writ0')

    @pytest.mark.fast
    def test_cli_runner_inventory_write_failure_exits_nonzero(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """Disk errors while exporting inventory should exit with code 1."""

        monkeypatch.chdir(tmp_path)

        def fake_collect(target: str, speed: str | None) -> list[str]:
            suffix = speed or "all"
            return [f"{target}::{suffix}::case"]

        app, cli_module = _build_minimal_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache",
fake_collect)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:365: AttributeError
_____________ test_cli_runner_maxfail_option_propagates_to_runner ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc85c0>

    @pytest.mark.fast
    def test_cli_runner_maxfail_option_propagates_to_runner(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The Typer surface must forward --maxfail to run_tests."""

        app, cli_module = _build_minimal_app(monkeypatch)

        received: dict[str, object] = {}

        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            received["args"] = args
            received["kwargs"] = dict(kwargs)
            return True, "pytest ok"

>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:395: AttributeError
________________ test_cli_inventory_mode_exports_json_via_typer ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc97c0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_inventory_mode_export1')

    @pytest.mark.fast
    def test_cli_inventory_mode_exports_json_via_typer(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Inventory-only mode collects all targets and writes JSON via the
CLI."""

        monkeypatch.chdir(tmp_path)

        calls: list[tuple[str, str | None]] = []

        def fake_collect(target: str, speed: str | None) -> list[str]:
            calls.append((target, speed))
            # Encode target/speed in the node id so assertions are deterministic
            suffix = speed or "all"
            return [f"{target}::{suffix}::test_case"]

        app, cli_module = build_minimal_cli_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache",
fake_collect)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:112: AttributeError
____________________ test_cli_smoke_dry_run_invokes_preview ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc9160>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_smoke_dry_run_invokes0')

    @pytest.mark.fast
    def test_cli_smoke_dry_run_invokes_preview(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Smoke dry-run previews pytest invocation without executing suites."""

        monkeypatch.chdir(tmp_path)
        sys.modules.pop("devsynth.config", None)
        sys.modules.pop("devsynth.config.settings", None)
        sys.modules.pop("devsynth.config.provider_env", None)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("DEVSYNTH_TEST_TIMEOUT_SECONDS", raising=False)
        app, cli_module = build_minimal_cli_app(monkeypatch)

        calls: list[dict[str, Any]] = []

        def fake_run_tests(*args: Any, **kwargs: Any) -> tuple[bool, str]:
            calls.append({"args": args, "kwargs": kwargs})
            return (
                True,
                "Dry run: pytest invocation prepared but not executed.\n"
                "Command: python -m pytest
tests/unit/test_example.py::test_case\n",
            )

        monkeypatch.setattr(run_tests_module, "run_tests", fake_run_tests)
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:159: AttributeError
_____________ test_cli_enforces_coverage_threshold_via_cli_runner ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc9430>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_enforces_coverage_thr0')

    @pytest.mark.fast
    def test_cli_enforces_coverage_threshold_via_cli_runner(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Successful Typer invocation enforces coverage thresholds and emits
tips."""

        monkeypatch.chdir(tmp_path)

        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader",
core_stub)
        monkeypatch.setitem(sys.modules, "tinydb", ModuleType("tinydb"))

        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd",
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            ("devsynth.application.cli.commands.generate_docs_cmd",
"generate_docs_cmd"),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)

        app, cli_module = build_minimal_cli_app(monkeypatch)

        runner = CliRunner()

>       monkeypatch.setattr(cli_module, "run_tests", lambda *args, **kwargs:
(True, "ok"))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:239: AttributeError
___________ test_cli_smoke_mode_reports_coverage_skip_and_artifacts ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc89e0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_smoke_mode_reports_co0')

    @pytest.mark.fast
    def test_cli_smoke_mode_reports_coverage_skip_and_artifacts(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Smoke mode prints diagnostic skip notice while still surfacing
artifacts."""

        monkeypatch.chdir(tmp_path)

        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader",
core_stub)
        monkeypatch.setitem(sys.modules, "tinydb", ModuleType("tinydb"))

        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd",
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            ("devsynth.application.cli.commands.generate_docs_cmd",
"generate_docs_cmd"),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)

        app, cli_module = build_minimal_cli_app(monkeypatch)

        recorded_args: list[tuple[tuple[object, ...], dict[str, object]]] = []

        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            recorded_args.append((args, kwargs))
            return True, "pytest output"

>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:340: AttributeError
_______________ test_cli_exits_when_autoload_disables_pytest_cov _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc9280>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_exits_when_autoload_d0')

    @pytest.mark.fast
    def test_cli_exits_when_autoload_disables_pytest_cov(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Fail fast when plugin autoloading disables pytest-cov
instrumentation."""

        monkeypatch.chdir(tmp_path)

        app, cli_module = build_minimal_cli_app(monkeypatch)

>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True,
""))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:397: AttributeError
_____________ test_cli_exits_when_pytest_cov_disabled_via_autoload _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc88c0>

    @pytest.mark.fast
    def test_cli_exits_when_pytest_cov_disabled_via_autoload(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Typer run surfaces remediation tips when pytest-cov is disabled."""

        app, cli_module = build_minimal_cli_app(monkeypatch)

        runner = CliRunner()

>       monkeypatch.setattr(cli_module, "run_tests", lambda *args, **kwargs:
(True, ""))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:444: AttributeError
_________________ test_cli_reports_coverage_artifacts_success __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc9310>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_reports_coverage_arti0')

    @pytest.mark.fast
    def test_cli_reports_coverage_artifacts_success(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Successful CLI run emits artifact locations after enforcing
thresholds."""

        monkeypatch.chdir(tmp_path)

        app, cli_module = build_minimal_cli_app(monkeypatch)

        html_dir = tmp_path / "htmlcov"
        html_dir.mkdir()
        html_index = html_dir / "index.html"
        html_index.write_text("<html>coverage</html>")

        json_path = tmp_path / "test_reports" / "coverage.json"
        json_path.parent.mkdir()
        json_path.write_text("{}")

        monkeypatch.setattr(cli_module, "COVERAGE_HTML_DIR", html_dir)
        monkeypatch.setattr(cli_module, "COVERAGE_JSON_PATH", json_path)

>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True,
"pytest ok"))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:35: AttributeError
________________ test_cli_exits_when_coverage_artifacts_missing ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc8110>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_exits_when_coverage_a0')

    @pytest.mark.fast
    def test_cli_exits_when_coverage_artifacts_missing(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Missing coverage artifacts trigger an exit with remediation
guidance."""

        monkeypatch.chdir(tmp_path)

        app, cli_module = build_minimal_cli_app(monkeypatch)

>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True,
""))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:82: AttributeError
__________________ test_cli_surfaces_threshold_runtime_errors __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bcc9490>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_surfaces_threshold_ru0')

    @pytest.mark.fast
    def test_cli_surfaces_threshold_runtime_errors(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Runtime errors from threshold enforcement bubble up through the
CLI."""

        monkeypatch.chdir(tmp_path)

        app, cli_module = build_minimal_cli_app(monkeypatch)

>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True,
""))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:125: AttributeError
_______________ test_smoke_command_generates_coverage_artifacts ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bea75c0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_smoke_command_generates_c0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10bea4620>

    @pytest.mark.fast
    def test_smoke_command_generates_coverage_artifacts(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """Smoke profile writes coverage artifacts even with plugin autoload
disabled."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)

        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:173:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_________________ test_smoke_command_injects_pytest_bdd_plugin _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bea6d50>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_smoke_command_injects_pyt0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c7b18b0>

    @pytest.mark.fast
    def test_smoke_command_injects_pytest_bdd_plugin(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """Smoke profile explicitly loads pytest-bdd when plugin autoload is
disabled."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)

        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:209:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_ test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bea75f0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_fast_medium_command_gener0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10bc1a480>

    @pytest.mark.fast
    def
test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """Fast+medium aggregate run preserves coverage artifacts when autoload
is disabled."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)

        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:248:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_fast_medium_preserves_existing_cov_fail_under ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bea6780>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_fast_medium_preserves_exi0')

    @pytest.mark.fast
    def test_fast_medium_preserves_existing_cov_fail_under(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Custom fail-under thresholds survive coverage plugin injection."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "--cov-fail-under=95")

        popen_envs, _ = _install_pytest_stubs(monkeypatch)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:288:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_fast_medium_command_handles_empty_collection _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc1b0e0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_fast_medium_command_handl0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c7b3200>

    @pytest.mark.fast
    def test_fast_medium_command_handles_empty_collection(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """Fallback to marker execution when collection returns no node
identifiers."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)

        popen_envs, combine_calls = _install_pytest_stubs(
            monkeypatch, collect_stdout_sequence=["", ""]
        )

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:323:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_________ test_fast_profile_generates_coverage_and_exits_successfully __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc19730>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_fast_profile_generates_co0')

    @pytest.mark.fast
    def test_fast_profile_generates_coverage_and_exits_successfully(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Default fast profile produces coverage artifacts and a zero exit
code."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)

        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:362:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______ test_fast_profile_missing_coverage_artifacts_returns_exit_code_one ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bc189e0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_fast_profile_missing_cove0')

    @pytest.mark.fast
    def test_fast_profile_missing_coverage_artifacts_returns_exit_code_one(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Missing coverage artifacts in fast profile should exit with code
1."""

        monkeypatch.chdir(tmp_path)

        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:399:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """CLI command to run DevSynth tests.

    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.

    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """

    from __future__ import annotations

    import os
    import shlex
    from typing import cast

    import typer

    # Import run_tests module so monkeypatching
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module

    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env';
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_inventory_mode_writes_file_and_prints_message ______________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module
'devsynth.application.cli.commands.run_tests_cmd' has no attribute
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25df0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_inventory_mode_writes_fil0')
capsys = <_pytest.capture.CaptureFixture object at 0x10bc1a3c0>

    @pytest.mark.fast
    def test_inventory_mode_writes_file_and_prints_message(monkeypatch,
tmp_path, capsys):
        """ReqID: TR-CLI-03 — Inventory mode writes JSON and prints message.

        Validates that running with --inventory writes
test_reports/test_inventory.json
        and prints a user-facing message with the path.
        """
        # run in temporary cwd
        monkeypatch.chdir(tmp_path)

        # Return deterministic collections
        def fake_collect(target: str, speed: str):  # noqa: ARG001
            return [f"{target}::{speed}::test_x"]

        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("DEVSYNTH_INNER_TEST", "1")
        monkeypatch.setenv("DEVSYNTH_TEST_ALLOW_REQUESTS", "true")

>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_
cache",
            fake_collect,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory.py:29:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.application.cli.commands.run_tests_cmd has no attribute
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________________ test_inventory_handles_collection_errors ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc25940>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_inventory_handles_collect0')

    @pytest.mark.fast
    def test_inventory_handles_collection_errors(monkeypatch, tmp_path):
        """ReqID: TR-CLI-04 — Inventory mode handles collection errors.

        When collection raises, ensure JSON still includes empty lists for all
            targets and speeds.
        """
        monkeypatch.chdir(tmp_path)

        def flaky_collect(target: str, speed: str):  # noqa: ARG001
            raise RuntimeError("boom")

        app, cli_module = build_minimal_cli_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache",
flaky_collect)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory.py:60: AttributeError
______________ test_cli_report_flag_warns_when_directory_missing _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc31f40>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_report_flag_warns_whe0')

    @pytest.mark.fast
    def test_cli_report_flag_warns_when_directory_missing(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Successful runs with --report mention missing directories."""

        monkeypatch.chdir(tmp_path)

        # Provide minimal config loader stubs so CLI bootstraps cleanly.
        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader",
core_stub)

        tinydb_stub = ModuleType("tinydb")
        tinydb_stub.Query = object  # type: ignore[attr-defined]
        tinydb_stub.TinyDB = object  # type: ignore[attr-defined]
        monkeypatch.setitem(sys.modules, "tinydb", tinydb_stub)

        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd",
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.generate_docs_cmd",
                "generate_docs_cmd",
            ),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)

        app, cli_module = build_minimal_cli_app(monkeypatch)

        runner = CliRunner()

>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True,
"pytest ok"))
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_guidance.py:68: AttributeError
____________ test_cli_segment_option_failure_surfaces_failure_tips _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc31220>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_segment_option_failur0')

    @pytest.mark.fast
    def test_cli_segment_option_failure_surfaces_failure_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Segmentation failures bubble remediation guidance and exit
non-zero."""

        monkeypatch.chdir(tmp_path)

        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader",
core_stub)
        tinydb_stub = ModuleType("tinydb")
        tinydb_stub.Query = object  # type: ignore[attr-defined]
        tinydb_stub.TinyDB = object  # type: ignore[attr-defined]
        monkeypatch.setitem(sys.modules, "tinydb", tinydb_stub)

        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd",
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.generate_docs_cmd",
                "generate_docs_cmd",
            ),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)

        app, cli_module = build_minimal_cli_app(monkeypatch)

        captured_args: tuple[object, ...] = ()
        captured_kwargs: dict[str, object] = {}

        def failing_run_tests(*args: object, **kwargs: object) -> tuple[bool,
str]:
            nonlocal captured_args
            captured_args = args
            captured_kwargs.update(kwargs)
            return False, "pytest failure output\n" + SEGMENTATION_FAILURE_TIPS

>       monkeypatch.setattr(cli_module, "run_tests", failing_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_guidance.py:152: AttributeError
____________ test_segmented_cli_failure_emits_tips_and_reinjection _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc350d0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_cli_failure_emi0')

    @pytest.mark.fast
    def test_segmented_cli_failure_emits_tips_and_reinjection(
        monkeypatch, tmp_path
    ) -> None:
        """Segmented runs surface remediation tips and reinjection notices
once."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")

        app, cli_module = build_minimal_cli_app(monkeypatch)

        cov_calls: list[dict[str, str]] = []
        bdd_calls: list[dict[str, str]] = []

        def cov_wrapper(env: dict[str, str]) -> bool:
            cov_calls.append(env.copy())
            return run_tests_module.ensure_pytest_cov_plugin_env(env)

        def bdd_wrapper(env: dict[str, str]) -> bool:
            bdd_calls.append(env.copy())
            return run_tests_module.ensure_pytest_bdd_plugin_env(env)

>       monkeypatch.setattr(cli_module, "ensure_pytest_cov_plugin_env",
cov_wrapper)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'ensure_pytest_cov_plugin_env'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:47: AttributeError
_
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-batch]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc354f0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_cli_failure_rep0')
failed_batches = ['one']

    @pytest.mark.fast
    @pytest.mark.parametrize(
        "failed_batches",
        [
            pytest.param(["one"], id="single-batch"),
            pytest.param(["one", "two", "three"], id="multiple-batches"),
        ],
    )
    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        failed_batches: list[str],
    ) -> None:
        """Remediation banners surface once per failed segment and aggregate."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")

        app, cli_module = build_minimal_cli_app(monkeypatch)

        observed_batches: list[str] = []

        def raising_segment(label: str) -> None:
            observed_batches.append(label)
            raise RuntimeError(f"segment {label} crashed")

        monkeypatch.setattr(
            run_tests_module,
            "_segment_batches",
            raising_segment,
            raising=False,
        )

        received_kwargs: dict[str, object] = {}

        def fake_run_tests(*_: object, **kwargs: object) -> tuple[bool, str]:
            received_kwargs.update(kwargs)
            batch_outputs: list[str] = []
            for index, batch in enumerate(failed_batches, start=1):
                try:
                    run_tests_module._segment_batches(batch)  # type:
ignore[attr-defined]
                except RuntimeError as exc:  # pragma: no cover - exercised via
test logic
                    batch_outputs.append(_build_batch_output(str(index), exc))
            batch_outputs.append(
                "Aggregate segmentation failure\n" + SEGMENTATION_FAILURE_TIPS
            )
            return False, "\n".join(batch_outputs)

>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:139: AttributeError
_
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-batch
es] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc35d60>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_cli_failure_rep1')
failed_batches = ['one', 'two', 'three']

    @pytest.mark.fast
    @pytest.mark.parametrize(
        "failed_batches",
        [
            pytest.param(["one"], id="single-batch"),
            pytest.param(["one", "two", "three"], id="multiple-batches"),
        ],
    )
    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        failed_batches: list[str],
    ) -> None:
        """Remediation banners surface once per failed segment and aggregate."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")

        app, cli_module = build_minimal_cli_app(monkeypatch)

        observed_batches: list[str] = []

        def raising_segment(label: str) -> None:
            observed_batches.append(label)
            raise RuntimeError(f"segment {label} crashed")

        monkeypatch.setattr(
            run_tests_module,
            "_segment_batches",
            raising_segment,
            raising=False,
        )

        received_kwargs: dict[str, object] = {}

        def fake_run_tests(*_: object, **kwargs: object) -> tuple[bool, str]:
            received_kwargs.update(kwargs)
            batch_outputs: list[str] = []
            for index, batch in enumerate(failed_batches, start=1):
                try:
                    run_tests_module._segment_batches(batch)  # type:
ignore[attr-defined]
                except RuntimeError as exc:  # pragma: no cover - exercised via
test logic
                    batch_outputs.append(_build_batch_output(str(index), exc))
            batch_outputs.append(
                "Aggregate segmentation failure\n" + SEGMENTATION_FAILURE_TIPS
            )
            return False, "\n".join(batch_outputs)

>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:139: AttributeError
___________________ test_run_tests_cli_feature_flags_set_env ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc355e0>

    @pytest.mark.fast
    def test_run_tests_cli_feature_flags_set_env(monkeypatch):
        # Ensure a clean env for the feature vars
        monkeypatch.delenv("DEVSYNTH_FEATURE_EXPERIMENTAL", raising=False)
        monkeypatch.delenv("DEVSYNTH_FEATURE_LOGGING", raising=False)

        runner = CliRunner()
>       with patch(
            "devsynth.application.cli.commands.run_tests_cmd.run_tests",
            return_value=(True, ""),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_features.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10cc321e0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module
'devsynth.application.cli.commands.run_tests_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> does not have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________ test_run_tests_cmd_applies_stub_offline_defaults_when_unset __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cc35880>

    @pytest.mark.fast
    def test_run_tests_cmd_applies_stub_offline_defaults_when_unset(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        # Ensure variables are unset
        for key in [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]:
            monkeypatch.delenv(key, raising=False)

>       with patch.object(module, "run_tests", return_value=(True, "")):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_provider_defaults.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x118a66090>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________ test_run_tests_command_succeeds_without_optional_providers __________

    @pytest.mark.fast
    def test_run_tests_command_succeeds_without_optional_providers() -> None:
        """``devsynth run-tests`` should exit 0 without external providers.

        ReqID: FR-22
        """

        if os.environ.get("DEVSYNTH_INNER_TEST") == "1":
            pytest.skip("inner run")

        env = os.environ.copy()
        env["DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE"] = "false"
        env["DEVSYNTH_INNER_TEST"] = "1"
        env["PYTEST_ADDOPTS"] = "-k test_dummy"
>       result = subprocess.run(
            [
                "devsynth",
                "run-tests",
                "--speed",
                "fast",
            ],
            capture_output=True,
            text=True,
            env=env,
            cwd=Path(__file__).resolve().parents[5],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_subprocess.py:22:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:550: in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:1209: in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:2115: in _communicate
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/selectors.py:415: in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x10b8a6d40, file
'/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/selectors.py', line 416, code select>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
________________ test_invalid_target_exits_with_helpful_message ________________

    @pytest.mark.fast
    def test_invalid_target_exits_with_helpful_message() -> None:
        runner = CliRunner()
        app = build_app()
>       result = runner.invoke(app, ["run-tests", "--target", "weird-tests"])  #
nosec
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_validation.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke
    use_cli = _get_command(app)
              ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:352: in get_command
    click_command: click.Command = get_group(typer_instance)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:334: in get_group
    group = get_group_from_info(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:479: in get_group_from_info
    sub_group = get_group_from_info(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

group_info = <typer.models.TyperInfo object at 0x119a53710>

    def get_group_from_info(
        group_info: TyperInfo,
        *,
        pretty_exceptions_short: bool,
        rich_markup_mode: MarkupMode,
    ) -> TyperGroup:
>       assert group_info.typer_instance, (
               ^^^^^^^^^^^^^^^^^^^^^^^^^
            "A Typer instance is needed to generate a Click Group"
        )
E       AssertionError: A Typer instance is needed to generate a Click Group

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:466: AssertionError
________________ test_invalid_speed_exits_with_helpful_message _________________

    @pytest.mark.fast
    def test_invalid_speed_exits_with_helpful_message() -> None:
        runner = CliRunner()
        app = build_app()
        # Include one valid and one invalid to ensure detection
>       result = runner.invoke(
            app,
            [
                "run-tests",
                "--target",
                "unit-tests",
                "--speed",
                "fast",
                "--speed",
                "warp",
            ],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_validation.py:28:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke
    use_cli = _get_command(app)
              ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:352: in get_command
    click_command: click.Command = get_group(typer_instance)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:334: in get_group
    group = get_group_from_info(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:479: in get_group_from_info
    sub_group = get_group_from_info(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

group_info = <typer.models.TyperInfo object at 0x10cd97950>

    def get_group_from_info(
        group_info: TyperInfo,
        *,
        pretty_exceptions_short: bool,
        rich_markup_mode: MarkupMode,
    ) -> TyperGroup:
>       assert group_info.typer_instance, (
               ^^^^^^^^^^^^^^^^^^^^^^^^^
            "A Typer instance is needed to generate a Click Group"
        )
E       AssertionError: A Typer instance is needed to generate a Click Group

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:466: AssertionError
___________________ test_progress_manager_handles_lifecycle ____________________

self = <MagicMock name='mock.update' id='4509905648'>, args = ()
kwargs = {'advance': 1, 'description': 'step one'}
expected = call(advance=1, description='step one')
actual = call(advance=1, description='step one', status=None)
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x10c33df80>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: update(advance=1, description='step one')
E             Actual: update(advance=1, description='step one', status=None)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='mock.update' id='4509905648'>, args = ()
kwargs = {'advance': 1, 'description': 'step one'}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected: update(advance=1, description='step one')
E         Actual: update(advance=1, description='step one', status=None)
E
E       pytest introspection follows:
E
E       Kwargs:
E       assert {'advance': 1...status': None} == {'advance': 1...': 'step one'}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 1 more item:
E         {'status': None}
E         Use -v to get more diff

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

    def test_progress_manager_handles_lifecycle():
        bridge = DummyBridge()
        manager = ProgressManager(bridge)  # type: ignore[arg-type]

        indicator = manager.create_progress("task", "Task", total=2)
        manager.update_progress("task", description="step one")
>       bridge.indicator.update.assert_called_once_with(advance=1,
description="step one")
E       AssertionError: expected call not found.
E       Expected: update(advance=1, description='step one')
E         Actual: update(advance=1, description='step one', status=None)
E
E       pytest introspection follows:
E
E       Kwargs:
E       assert {'advance': 1...status': None} == {'advance': 1...': 'step one'}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 1 more item:
E         {'status': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_progress.py:24: AssertionError
__________________________ test_parse_feature_options __________________________

    @pytest.mark.fast
    def test_parse_feature_options() -> None:
        """`_parse_feature_options` converts option values to booleans."""

>       result = module._parse_feature_options(["a", "b=false", "c=1"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'function' object has no attribute
'_parse_feature_options'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:16: AttributeError
________________________ test_cli_accepts_feature_flags ________________________

    @pytest.mark.fast
    def test_cli_accepts_feature_flags() -> None:
        """CLI invocation with ``--feature`` delegates to `run_tests`."""

        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:25:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x11f2614c0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_cli_reports_coverage_percent _______________________

    @pytest.mark.fast
    def test_cli_reports_coverage_percent() -> None:
        """Successful runs print the measured coverage percentage."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers",
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages",
return_value=None),
            patch.object(
                module, "enforce_coverage_threshold", return_value=92.5
            ) as mock_enforce,
            patch.object(
                module, "_coverage_instrumentation_status", return_value=(True,
None)
            ),
            patch.object(module, "coverage_artifacts_status",
return_value=(True, None)),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:42:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10c7b10d0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_cli_errors_when_plugins_disabled _____________________

    @pytest.mark.fast
    def test_cli_errors_when_plugins_disabled() -> None:
        """CLI fails fast when pytest-cov was disabled by plugin autoload
settings."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers",
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages",
return_value=None),
            patch.object(module, "enforce_coverage_threshold",
return_value=95.0),
            patch.object(
                module,
                "_coverage_instrumentation_status",
                return_value=(
                    False,
                    "pytest plugin autoload disabled without -p pytest_cov",
                ),
            ),
            patch.object(
                module,
                "coverage_artifacts_status",
                return_value=(False, "Coverage JSON missing at
test_reports/coverage.json"),
            ),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:69:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12852fa70>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_cli_errors_when_artifacts_missing ____________________

    @pytest.mark.fast
    def test_cli_errors_when_artifacts_missing() -> None:
        """CLI reports actionable remediation when coverage artifacts are
absent."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers",
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages",
return_value=None),
            patch.object(
                module, "enforce_coverage_threshold", return_value=95.0
            ) as mock_enforce,
            patch.object(
                module, "_coverage_instrumentation_status", return_value=(True,
None)
            ),
            patch.object(
                module,
                "coverage_artifacts_status",
                return_value=(False, "Coverage JSON missing at
test_reports/coverage.json"),
            ),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:102:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x11fb76c00>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_feature_flags_set_environment ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f24e9f0>

    @pytest.mark.fast
    def test_feature_flags_set_environment(monkeypatch) -> None:
        """--feature name[=bool] should set DEVSYNTH_FEATURE_<NAME> env vars."""
        # Ensure clean env
        monkeypatch.delenv("DEVSYNTH_FEATURE_ALPHA", raising=False)
        monkeypatch.delenv("DEVSYNTH_FEATURE_BETA", raising=False)

        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10ccfc3e0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ test_no_parallel_flag_is_passed_to_runner ___________________

    @pytest.mark.fast
    def test_no_parallel_flag_is_passed_to_runner() -> None:
        """--no-parallel should result in parallel=False in run_tests call."""
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10bc18d10>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_____________________ test_segment_options_are_propagated ______________________

    @pytest.mark.fast
    def test_segment_options_are_propagated() -> None:
        """--segment and --segment-size should be passed through to
run_tests."""
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:59:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x11fb773b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ test_project_state_analyzer_analyze_graceful_fallback _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10cd93e60>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_project_state_analyzer_an0')

    @pytest.mark.fast
    def test_project_state_analyzer_analyze_graceful_fallback(monkeypatch,
tmp_path: Path):
        """ReqID: T-ANALYZER-ERR-002
        When any internal step raises, ProjectStateAnalyzer.analyze should not
raise
        and must return a safe dictionary shape with default values.
        It must also avoid creating files outside the provided environment.
        """

        # Force an early method to raise to cover the error path
        def boom(self, *args: Any, **kwargs: Any) -> None:  # noqa: ARG002
            raise RuntimeError("boom")

        monkeypatch.setattr(ProjectStateAnalyzer, "_index_files", boom)

        before = set(os.listdir(tmp_path))

        analyzer = ProjectStateAnalyzer(str(tmp_path))

        # Act
        result: Dict[str, Any] = analyzer.analyze()

        after = set(os.listdir(tmp_path))

        # Assert: safe shape
        assert set(result.keys()) == {
            "files",
            "languages",
            "architecture",
            "components",
            "health_report",
        }

        assert result["files"] == {}
        assert result["languages"] == {}
        assert isinstance(result["architecture"], dict)
        assert result["architecture"].get("components", []) == []
        assert result["components"] == []

        hr = result["health_report"]
        assert isinstance(hr, dict)
>       assert hr.get("status") == "unknown"
E       AssertionError: assert None == 'unknown'
E        +  where None = <built-in method get of dict object at
0x10bf61e40>('status')
E        +    where <built-in method get of dict object at 0x10bf61e40> =
{'architecture': {'components': [], 'confidence': 0.0, 'type': 'unknown'},
'code_count': 0, 'file_count': 0, 'health_score': 0.0, ...}.get

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/code
_analysis/test_project_state_analyzer_error_paths.py:52: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:50,749 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_project_state_analyzer_an0
2025-10-28 10:29:50,749 -
devsynth.application.code_analysis.project_state_analyzer - ERROR -
ProjectStateAnalyzer.analyze failed: boom
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_project_state_analyzer_an0
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
ProjectStateAnalyzer.analyze failed: boom
_______________ test_build_consensus_stores_decision_and_summary _______________

memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x12e09a0f0>

    @pytest.mark.fast
    def test_build_consensus_stores_decision_and_summary(
        memory_manager: MemoryManager,
    ) -> None:
        team = DummyTeam(memory_manager)
        task = {"id": "t1", "title": "Test"}
>       team.build_consensus(task)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_wsde_memory_sync_hooks.py:125:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:60: in build_consensus
    return self._build_consensus_inner(task, phase)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<tests.unit.application.collaboration.test_wsde_memory_sync_hooks.DummyTeam
object at 0x12e09a180>
task = {'id': 't1', 'title': 'Test'}, phase = None

    def _build_consensus_inner(
        self, task: Dict[str, Any], phase: Optional[Phase] = None
    ) -> ConsensusOutcome:
        """Internal implementation of consensus building."""
        if "id" not in task:
            task["id"] = str(uuid.uuid4())

        self.logger.info(
            f"Building consensus for task {task['id']}: {task.get('title',
'Untitled')}"
        )

        task_text = (
            (task.get("description", "") or "") + " " + (task.get("title", "")
or "")
        )
        keywords = set(re.findall(r"\b\w+\b", task_text.lower()))

        agent_opinions = self._collect_agent_opinion_records(task,
keywords=keywords)
        if not agent_opinions:
            self._generate_agent_opinions(task)
            agent_opinions = self._collect_agent_opinion_records(
                task, keywords=keywords
            )

        consensus_id = str(uuid.uuid4())
>       conflicts = self._identify_conflicts(task, agent_opinions)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: DummyTeam._identify_conflicts() takes 2 positional arguments
but 3 were given

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:86: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:29:54,170 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb, graph
2025-10-28 10:29:54,170 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb, graph
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:54,170 - dummy - INFO - Building consensus for task t1: Test
------------------------------ Captured log call -------------------------------
INFO     dummy:wsde_team_consensus.py:69 Building consensus for task t1: Test
______________ test_consensus_outcome_round_trip_orders_conflicts ______________

    @pytest.mark.fast
    def test_consensus_outcome_round_trip_orders_conflicts() -> None:
        mixin = DummyTeam()
        payload = {
            "dto_type": "ConsensusOutcome",
            "consensus_id": "c3",
            "task_id": "t1",
            "method": "conflict_resolution_synthesis",
            "agent_opinions": [
                {
                    "dto_type": "AgentOpinionRecord",
                    "agent_id": "beta",
                    "opinion": "no",
                    "timestamp": "2025-01-02T00:00:00",
                },
                {
                    "dto_type": "AgentOpinionRecord",
                    "agent_id": "alpha",
                    "opinion": "yes",
                    "timestamp": "2025-01-01T00:00:00",
                },
            ],
            "conflicts": [
                {
                    "dto_type": "ConflictRecord",
                    "conflict_id": "c2",
                    "task_id": "t1",
                    "agent_a": "beta",
                    "agent_b": "alpha",
                    "opinion_a": "no",
                    "opinion_b": "yes",
                },
                {
                    "dto_type": "ConflictRecord",
                    "conflict_id": "c1",
                    "task_id": "t1",
                    "agent_a": "alpha",
                    "agent_b": "beta",
                    "opinion_a": "yes",
                    "opinion_b": "no",
                },
            ],
            "conflicts_identified": 0,
            "synthesis": {
                "dto_type": "SynthesisArtifact",
                "text": "resolved",
                "key_points": ["compromise"],
                "expertise_weights": {"alpha": 0.6, "beta": 0.4},
                "readability_score": {"flesch_reading_ease": 65.0},
            },
            "timestamp": "2025-01-01T00:00:00",
        }

>       outcome = ConsensusOutcome.from_dict(payload)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_wsde_team_consensus_summary.py:124:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:270: in from_dict
    return cls(**kwargs)
           ^^^^^^^^^^^^^
<string>:18: in __init__
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:340: in __post_init__
    sorted(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

opinion = {'agent_id': 'beta', 'dto_type': 'AgentOpinionRecord', 'opinion':
'no', 'timestamp': '2025-01-02T00:00:00'}

>           key=lambda opinion: (opinion.agent_id or "", opinion.timestamp or
""),
                                 ^^^^^^^^^^^^^^^^
        )
    )
E   AttributeError: 'dict' object has no attribute 'agent_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:342: AttributeError
__ TestEDRRCoordinatorInitialization.test_coordinator_initialization_defaults __

self = <test_core.TestEDRRCoordinatorInitialization object at 0x118c06240>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpk64vgn76')

    @pytest.mark.fast
    def test_coordinator_initialization_defaults(self, tmp_project_dir):
        """Test coordinator initialization with default values."""
>       with patch(
            "devsynth.application.edrr.coordinator.core.get_llm_settings"
        ) as mock_get_settings:

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e0b0170>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.edrr.coordinator.core'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/e
drr/coordinator/core.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_
TestEDRRCoordinatorInitialization.test_coordinator_initialization_custom_config
_

self = <test_core.TestEDRRCoordinatorInitialization object at 0x118c06690>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnbpt7mqj')

    @pytest.mark.fast
    def test_coordinator_initialization_custom_config(self, tmp_project_dir):
        """Test coordinator initialization with custom configuration."""
        custom_config = {
            "max_recursion_depth": 5,
            "granularity_threshold": 0.3,
            "cost_benefit_ratio": 0.6,
        }

>       coordinator = EDRRCoordinator(custom_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 5 required positional
arguments: 'wsde_team', 'code_analyzer', 'ast_transformer', 'prompt_manager',
and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:89: TypeError
_ TestEDRRCoordinatorInitialization.test_coordinator_dependencies_initialization
_

self = <test_core.TestEDRRCoordinatorInitialization object at 0x118c06b40>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5s0mgouw')

    @pytest.mark.fast
    def test_coordinator_dependencies_initialization(self, tmp_project_dir):
        """Test that coordinator initializes all dependencies correctly."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:98: TypeError
_______ TestEDRRCoordinatorPhaseExecution.test_start_cycle_from_manifest _______

self = <test_core.TestEDRRCoordinatorPhaseExecution object at 0x118c079b0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwy2dp4w9')

    @pytest.mark.fast
    def test_start_cycle_from_manifest(self, tmp_project_dir):
        """Test cycle execution from manifest file."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:165: TypeError
___ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_depth_limit ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x118c1c3b0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbiin7ri0')

    @pytest.mark.fast
    def test_should_terminate_recursion_depth_limit(self, tmp_project_dir):
        """Test recursion termination based on depth limit."""
>       coordinator = EDRRCoordinator({"max_recursion_depth": 2})
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 5 required positional
arguments: 'wsde_team', 'code_analyzer', 'ast_transformer', 'prompt_manager',
and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:198: TypeError
___ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_granularity ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x118c1c890>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmps85kgcfq')

    @pytest.mark.fast
    def test_should_terminate_recursion_granularity(self, tmp_project_dir):
        """Test recursion termination based on granularity threshold."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:211: TypeError
__ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_cost_benefit ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x118c1cd70>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp8d06hl10')

    @pytest.mark.fast
    def test_should_terminate_recursion_cost_benefit(self, tmp_project_dir):
        """Test recursion termination based on cost-benefit ratio."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:227: TypeError
_ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_resource_limit __

self = <test_core.TestEDRRCoordinatorRecursion object at 0x118c1d250>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmppi383rk_')

    @pytest.mark.fast
    def test_should_terminate_recursion_resource_limit(self, tmp_project_dir):
        """Test recursion termination based on resource limits."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:243: TypeError
_ TestEDRRCoordinatorRecursion.test_should_not_terminate_recursion_good_metrics
_

self = <test_core.TestEDRRCoordinatorRecursion object at 0x118c1d730>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp2d08qn8i')

    @pytest.mark.fast
    def test_should_not_terminate_recursion_good_metrics(self, tmp_project_dir):
        """Test that recursion continues when metrics are good."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:259: TypeError
________ TestEDRRCoordinatorMicroCycles.test_register_micro_cycle_hook _________

self = <test_core.TestEDRRCoordinatorMicroCycles object at 0x118c1da60>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpejf_89bh')

    @pytest.mark.fast
    def test_register_micro_cycle_hook(self, tmp_project_dir):
        """Test micro-cycle hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:301: TypeError
_________ TestEDRRCoordinatorMicroCycles.test_invoke_micro_cycle_hooks _________

self = <test_core.TestEDRRCoordinatorMicroCycles object at 0x118c1df40>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpalufznjj')

    @pytest.mark.fast
    def test_invoke_micro_cycle_hooks(self, tmp_project_dir):
        """Test micro-cycle hook invocation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:314: TypeError
_______________ TestEDRRCoordinatorHooks.test_register_sync_hook _______________

self = <test_core.TestEDRRCoordinatorHooks object at 0x118c1e030>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp09we5zdz')

    @pytest.mark.fast
    def test_register_sync_hook(self, tmp_project_dir):
        """Test sync hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:345: TypeError
_______________ TestEDRRCoordinatorHooks.test_invoke_sync_hooks ________________

self = <test_core.TestEDRRCoordinatorHooks object at 0x118c1e480>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp_0mxfb6a')

    @pytest.mark.fast
    def test_invoke_sync_hooks(self, tmp_project_dir):
        """Test sync hook invocation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:358: TypeError
_____________ TestEDRRCoordinatorHooks.test_register_recovery_hook _____________

self = <test_core.TestEDRRCoordinatorHooks object at 0x118c1e960>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpij8xds21')

    @pytest.mark.fast
    def test_register_recovery_hook(self, tmp_project_dir):
        """Test recovery hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:380: TypeError
_____________ TestEDRRCoordinatorHooks.test_execute_recovery_hooks _____________

self = <test_core.TestEDRRCoordinatorHooks object at 0x118c1ee40>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpv0ezve_h')

    @pytest.mark.fast
    def test_execute_recovery_hooks(self, tmp_project_dir):
        """Test recovery hook execution."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:393: TypeError
______ TestEDRRCoordinatorPhaseManagement.test_set_manual_phase_override _______

self = <test_core.TestEDRRCoordinatorPhaseManagement object at 0x118c1f440>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpvmv9ut0l')

    @pytest.mark.fast
    def test_set_manual_phase_override(self, tmp_project_dir):
        """Test manual phase override setting."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:425: TypeError
_____ TestEDRRCoordinatorPhaseManagement.test_get_phase_quality_threshold ______

self = <test_core.TestEDRRCoordinatorPhaseManagement object at 0x118c1f890>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmptuv9fco1')

    @pytest.mark.fast
    def test_get_phase_quality_threshold(self, tmp_project_dir):
        """Test phase quality threshold retrieval."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:438: TypeError
_________ TestEDRRCoordinatorUtilityMethods.test_sanitize_positive_int _________

self = <test_core.TestEDRRCoordinatorUtilityMethods object at 0x118c342c0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpk9xpx161')

    @pytest.mark.fast
    def test_sanitize_positive_int(self, tmp_project_dir):
        """Test positive integer sanitization."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:456: TypeError
__________ TestEDRRCoordinatorUtilityMethods.test_sanitize_threshold ___________

self = <test_core.TestEDRRCoordinatorUtilityMethods object at 0x118c347a0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpb_psr_2j')

    @pytest.mark.fast
    def test_sanitize_threshold(self, tmp_project_dir):
        """Test threshold sanitization."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:481: TypeError
________ TestEDRRCoordinatorIntegration.test_edrr_cycle_error_recovery _________

self = <test_core.TestEDRRCoordinatorIntegration object at 0x118c35190>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp1apkzowe')

    @pytest.mark.fast
    def test_edrr_cycle_error_recovery(self, tmp_project_dir):
        """Test EDRR cycle with error recovery."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:574: TypeError
____________________ test_maybe_auto_progress_respects_flag ____________________

self = <MagicMock name='_decide_next_phase' id='5069723872'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected '_decide_next_phase' to have been called
once. Called 10 times.
E           Calls: [call(), call(), call(), call(), call(), call(), call(),
call(), call(), call()].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

    @pytest.mark.fast
    def test_maybe_auto_progress_respects_flag():
        """Auto progress runs only when enabled.

        ReqID: N/A"""
        core = EDRRCoordinatorCore(
            memory_manager=MagicMock(spec=MemoryManager),
            wsde_team=MagicMock(spec=WSDETeam),
            code_analyzer=MagicMock(spec=CodeAnalyzer),
            ast_transformer=MagicMock(spec=AstTransformer),
            prompt_manager=MagicMock(spec=PromptManager),
            documentation_manager=MagicMock(spec=DocumentationManager),
        )

        core.current_phase = Phase.EXPAND
        core.task = {"name": "task"}

        core.config = {"auto_progress": True}
        with patch.object(
            core, "_decide_next_phase", return_value=Phase.DIFFERENTIATE
        ) as decide:
            with patch.object(core, "progress_to_phase") as progress:
                core._maybe_auto_progress()
>       decide.assert_called_once()
E       AssertionError: Expected '_decide_next_phase' to have been called once.
Called 10 times.
E       Calls: [call(), call(), call(), call(), call(), call(), call(), call(),
call(), call()].

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_core.py:575: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:54,702 - devsynth.application.edrr.coordinator_core - INFO -
Initialized EDRR Coordinator (cycle_id: 972f5320-5de4-4668-846f-3571733ceed5)
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
2025-10-28 10:29:54,703 - devsynth.application.edrr.coordinator_core - INFO -
Auto-progressing to next phase
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Initialized EDRR Coordinator (cycle_id: 972f5320-5de4-4668-846f-3571733ceed5)
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615
Auto-progressing to next phase
___________________ test_apply_dialectical_reasoning_success ___________________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12e0b8800>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_success(coordinator: EDRRCoordinator)
-> None:
        """ReqID: N/A"""

        final = {"status": "completed", "synthesis": "done"}
        with patch(
            "devsynth.application.edrr.coordinator.core.reasoning_loop",
            return_value=[{"synthesis": "next"}, final],
        ) as rl:
            result = coordinator.apply_dialectical_reasoning(
                {"solution": "initial"}, MagicMock()
            )
        rl.assert_called_once()
        coordinator.memory_manager.flush_updates.assert_called_once()
>       assert isinstance(result, DialecticalSequence)
E       AssertionError: assert False
E        +  where False = isinstance({'status': 'completed', 'synthesis':
'done'}, DialecticalSequence)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_reasoning.py:51: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:29:54,753 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:29:54,754 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:29:54,754 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:29:54,754 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:29:54,754 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:29:54,754 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:54,756 - devsynth.application.edrr.coordinator.core - INFO -
EDRRCoordinator invoking dialectical reasoning
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
EDRRCoordinator invoking dialectical reasoning
______________ test_apply_dialectical_reasoning_consensus_failure ______________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12e0a9a60>
caplog = <_pytest.logging.LogCaptureFixture object at 0x12e0a5e80>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_consensus_failure(
        coordinator: EDRRCoordinator, caplog: pytest.LogCaptureFixture
    ) -> None:
        """ReqID: N/A"""

        with patch(
            "devsynth.application.edrr.coordinator.core.reasoning_loop",
            return_value=[],
        ):
            with caplog.at_level(logging.WARNING):
                result = coordinator.apply_dialectical_reasoning(
                    {"solution": "initial"}, MagicMock()
                )
>       assert isinstance(result, DialecticalSequence)
E       assert False
E        +  where False = isinstance({}, DialecticalSequence)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_reasoning.py:70: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:29:54,777 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:29:54,777 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:29:54,777 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:29:54,777 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:29:54,777 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:29:54,777 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:54,778 - devsynth.application.edrr.coordinator.core - WARNING -
Consensus failure during dialectical reasoning
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Consensus failure during dialectical reasoning
______________ test_decide_next_phase_respects_quality_threshold _______________

coordinator =
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x12e0a73b0>

    def test_decide_next_phase_respects_quality_threshold(
        coordinator: StubCoordinator,
    ) -> None:
        """ReqID: N/A"""

        coordinator.current_phase = Phase.EXPAND
        coordinator.results = {"EXPAND": {"quality_score": 0.2}}
        coordinator._quality_thresholds = {Phase.EXPAND: 0.5}
        assert coordinator._decide_next_phase() is None

        coordinator.results["EXPAND"]["quality_score"] = 0.8
        coordinator._phase_start_times[Phase.EXPAND] = datetime.now() -
timedelta(
            seconds=10
        )
        coordinator.phase_transition_timeout = 1
>       assert coordinator._decide_next_phase() == Phase.DIFFERENTIATE
E       AssertionError: assert None == <Phase.DIFFERENTIATE: 'differentiate'>
E        +  where None = _decide_next_phase()
E        +    where _decide_next_phase =
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x12e0a73b0>._decide_next_phase
E        +  and   <Phase.DIFFERENTIATE: 'differentiate'> = Phase.DIFFERENTIATE

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_management_module.py:108: AssertionError
_________________ test_maybe_auto_progress_invokes_progression _________________

self = <MagicMock id='5067358960'>, args = (<Phase.REFINE: 'refine'>,)
kwargs = {}, msg = "Expected 'mock' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'mock' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

coordinator =
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x12e241040>

    def test_maybe_auto_progress_invokes_progression(coordinator:
StubCoordinator) -> None:
        """ReqID: N/A"""

        coordinator.auto_phase_transitions = True
        coordinator.wsde_team.elaborate_details = MagicMock()
        coordinator._decide_next_phase = MagicMock(side_effect=[Phase.REFINE,
None])
        coordinator.progress_to_phase = MagicMock()

        coordinator._maybe_auto_progress()

>       coordinator.progress_to_phase.assert_called_once_with(Phase.REFINE)
E       AssertionError: Expected 'mock' to be called once. Called 0 times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_management_module.py:121: AssertionError
________________ test_reasoning_loop_retries_on_transient_error ________________

obj = <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'>
name = '_apply_dialectical_reasoning'
ann = 'devsynth.methodology.edrr.reasoning_loop'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.methodology.edrr.reasoning_loop'
has no attribute '_apply_dialectical_reasoning'. Did you mean:
'ApplyDialecticalReasoning'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2b4110>

    @pytest.mark.fast
    @pytest.mark.unit
    def test_reasoning_loop_retries_on_transient_error(monkeypatch):
        calls = {"count": 0}

        def flaky_apply(team, task, critic, memory):  # signature mirrors
underlying call
            calls["count"] += 1
            # First call fails with a transient error; second returns success
            if calls["count"] == 1:
                raise RuntimeError("transient")
            return {
                "status": "completed",
                "phase": "refine",
                "synthesis": task.get("solution"),
            }

        # Patch the internal alias used by the reasoning loop
>       monkeypatch.setattr(
            "devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoni
ng",
            flaky_apply,
            raising=True,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_reasoning_loop_retries.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'>
name = '_apply_dialectical_reasoning'
ann = 'devsynth.methodology.edrr.reasoning_loop'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at
devsynth.methodology.edrr.reasoning_loop has no attribute
'_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____________________ test_openai_provider_requires_api_key _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2d0da0>

    @pytest.mark.fast
    def test_openai_provider_requires_api_key(monkeypatch):
        """Missing API key should raise a clear error."""
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)

        # Import the module (it should handle missing OpenAI gracefully)
        module =
importlib.import_module("devsynth.application.llm.openai_provider")

        # Test that instantiation fails with a clear error when no API key is
provided
        with pytest.raises(module.OpenAIConnectionError) as exc:
>           module.OpenAIProvider({})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_import_without_openai.py:29:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12e241eb0>
config = {}

    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the OpenAI provider.

        Args:
            config: Configuration dictionary with the following keys:
                - api_key: OpenAI API key (default: from config)
                - model: Model name to use (default: from config)
                - max_tokens: Maximum tokens for responses (default: from
config)
                - temperature: Temperature for generation (default: from config)
                - api_base: Base URL for the OpenAI API (default: from config)

        Raises:
            OpenAIConnectionError: If no API key is provided or available in
environment
        """
        # Get default settings from configuration
        from ...config.settings import get_llm_settings

        default_settings = get_llm_settings()

        # Initialize with default settings, overridden by provided config
        self.config = {**default_settings, **(config or {})}

        # Set instance variables from config using standardized parameter names
        self.api_key = self.config.get("api_key") or self.config.get(
            "openai_api_key"
        )  # Support both old and new names
        self.model = (
            self.config.get("model")
            or self.config.get("openai_model")
            or "gpt-3.5-turbo"
        )
        self.max_tokens = self.config.get("max_tokens") or 1024
        self.temperature = self.config.get("temperature") or 0.7
        self.api_base = self.config.get("base_url") or self.config.get(
            "api_base"
        )  # Support both old and new names
        self.timeout = self.config.get("timeout") or 60
        self.max_retries = self.config.get("max_retries", 3)
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=self.config.get("failure_threshold", 3),
            recovery_timeout=self.config.get("recovery_timeout", 60),
        )
        # Deterministic per-call timeout (seconds)
        # Env var precedence: OPENAI_HTTP_TIMEOUT (docs/tasks.md Task 70)
        try:
            timeout_env = os.environ.get("OPENAI_HTTP_TIMEOUT")
            self.call_timeout = (
                float(timeout_env)
                if timeout_env is not None
                else float(self.config.get("call_timeout", 15))
            )
        except (TypeError, ValueError):
            self.call_timeout = 15.0

        # Check for API key in config or environment
        if not self.api_key and "OPENAI_API_KEY" in os.environ:
            self.api_key = os.environ["OPENAI_API_KEY"]

        # Initialize token tracker
        self.token_tracker = TokenTracker()

        # Require API key explicitly for this provider; tests enforce clear
error
        if not self.api_key:
>           raise OpenAIAuthenticationError(
                "OpenAI API key is required. Set OPENAI_API_KEY or provide
'api_key' in config."
            )
E           devsynth.application.llm.openai_provider.OpenAIAuthenticationError:
OpenAI API key is required. Set OPENAI_API_KEY or provide 'api_key' in config.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:131: OpenAIAuthenticationError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:55,137 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
____________ test_health_check_succeeds_when_sync_api_lists_models _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2d3470>

    @pytest.mark.fast
    def test_health_check_succeeds_when_sync_api_lists_models(monkeypatch):
        """ReqID: LMSTUDIO-HC-1
        When sync_api.list_downloaded_models returns, health_check should be
True.
        """
        # Ensure resource flag is enabled so health_check runs
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "true")

        from devsynth.application.llm.lmstudio_provider import LMStudioProvider

        provider = LMStudioProvider({"auto_select_model": False})

        # Patch list_downloaded_models to return non-empty list quickly
        with patch(
            "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.list_d
ownloaded_models",
            return_value=[type("M", (), {"model_key": "m", "display_name":
"M"})()],
        ):
>           assert provider.health_check() is True
E           assert False is True
E            +  where False = health_check()
E            +    where health_check =
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e2d2840>.health_check

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_health_check.py:25: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:55,156 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:29:55,158 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:29:55,159 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:29:55,159 - devsynth.application.llm.lmstudio_provider - INFO -
Using default model: qwen/qwen3-4b-2507
2025-10-28 10:29:55,668 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio health_check failed within budget: Network access disabled during tests
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using
default model: qwen/qwen3-4b-2507
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio health_check failed within budget: Network access disabled during tests
_________ test_health_check_bounded_retry_and_returns_false_on_failure _________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e21e000>

    def health_check(self) -> bool:
        """Lightweight health check to the LM Studio endpoint.

        Performs a GET to /api/v0/models via HTTP request. If
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE
        is true but the endpoint is unreachable, tests may skip quickly with a
clear reason.
        The total time spent in retries is bounded (<= 5 seconds).
        """
        # If resource flag is not enabled, we consider health as not
applicable/false
        resource_enabled = (
            os.environ.get("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
"false").lower()
            == "true"
        )
        if not resource_enabled:
            return False

        # Compute bounded retries/delay to stay under 10s total (conservative
for shared host)
        max_total_seconds = 10.0
        # Default to at most 3 retries with small backoff, but cap by
max_total_seconds using call_timeout as base
        attempt = 0
        delay = min(0.5, self.call_timeout / 4)
        total = 0.0
        last_exc: Exception | None = None
        while total <= max_total_seconds and attempt < max(1, self.max_retries):
            attempt += 1
            try:
                # Use HTTP request to /api/v0/models endpoint
                try:
                    self._lmstudio.sync_api.configure_default_client(self.api_ho
st)
                except Exception as cfg_err:  # noqa: BLE001
                    # Don't fail health check early on configure failure;
proceed to list models
                    logger.debug(
                        "LM Studio health_check configure_default_client failed
(ignored): %s",
                        cfg_err,
                    )
                # Use native API for health check (5s timeout for quick health
checks)
                import requests

>               response = requests.get(f"{self.api_base}/api/v0/models",
timeout=5)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:354:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:589: in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:703: in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/adapters.py:644: in send
    resp = conn.urlopen(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:787: in urlopen
    response = self._make_request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:493: in _make_request
    conn.request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:445: in request
    self.endheaders()
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1333: in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1093: in _send_output
    self.send(msg)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1037: in send
    self.connect()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:276: in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:198: in _new_conn
    sock = connection.create_connection(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/util/connection.py:73: in create_connection
    sock.connect(sa)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<socket.socket fd=36, family=2, type=1, proto=6, laddr=('0.0.0.0', 0)>,
('127.0.0.1', 1234))
kwargs = {}

    def guard_connect(*args: Any, **kwargs: Any) -> None:
>       raise RuntimeError("Network access disabled during tests")
E       RuntimeError: Network access disabled during tests

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:32: RuntimeError

During handling of the above exception, another exception occurred:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2d1cd0>

    @pytest.mark.fast
    def
test_health_check_bounded_retry_and_returns_false_on_failure(monkeypatch):
        """ReqID: LMSTUDIO-HC-2
        If sync_api.list_downloaded_models keeps failing, health_check returns
False within ~5s budget.
        """
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "true")
        # Keep retries small and timeout small to speed up
        monkeypatch.setenv("DEVSYNTH_LMSTUDIO_RETRIES", "3")
        monkeypatch.setenv("DEVSYNTH_LMSTUDIO_TIMEOUT_SECONDS", "0.4")

        from devsynth.application.llm.lmstudio_provider import LMStudioProvider

        provider = LMStudioProvider({"auto_select_model": False})

        call_count = {"n": 0}

        def _boom(kind: str):  # noqa: ARG001
            call_count["n"] += 1
            raise RuntimeError("unreachable")

        with (
            patch(
                "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.li
st_downloaded_models",
                side_effect=_boom,
            ),
            patch(
                "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.co
nfigure_default_client",
                return_value=None,
            ),
        ):
            t0 = time.perf_counter()
>           ok = provider.health_check()
                 ^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_health_check.py:59:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:369: in health_check
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x12e08fc40, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/lmstudio_provider.py', line 369, code health_check>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:29:55,683 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:29:55,686 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:29:55,686 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:29:55,686 - devsynth.application.llm.lmstudio_provider - INFO -
Using default model: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using
default model: qwen/qwen3-4b-2507
____________ TestRequireLMStudio.test_require_lmstudio_import_error ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestRequireLMStudio
object at 0x1191a1250>

    @pytest.mark.fast
    def test_require_lmstudio_import_error(self):
        """Test error handling when lmstudio module is not available."""
        with patch.dict("sys.modules", {}, clear=True):
            with patch(
                "builtins.__import__", side_effect=ImportError("Module not
found")
            ):
                with pytest.raises(DevSynthError) as exc_info:
>                   _require_lmstudio()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:214:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:42: in _require_lmstudio
    import lmstudio as _lmstudio  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/__init__.py:16: in <module>
    from .sdk_api import *
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sdk_api.py:3: in <module>
    from contextlib import AsyncContextDecorator, ContextDecorator
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:3: in <module>
    import os
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap>:1176: in exec_module
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   AttributeError: module 'sys' has no attribute 'builtin_module_names'

<frozen os>:33: AttributeError
_______ TestLMStudioProvider.test_provider_initialization_default_config _______

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a1700>

    @pytest.mark.fast
    def test_provider_initialization_default_config(self):
        """Test provider initialization with default configuration."""
>       with patch(
            "devsynth.application.llm.lmstudio_provider.get_llm_settings"
        ) as mock_get_settings:

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:226:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e22e2d0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.lmstudio_provider'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/lmstudio_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______ TestLMStudioProvider.test_provider_initialization_custom_config ________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a1bb0>

    @pytest.mark.fast
    def test_provider_initialization_custom_config(self):
        """Test provider initialization with custom configuration."""
        custom_config = {
            "api_base": "http://custom:8080",
            "model": "custom-model",
            "max_tokens": 2000,
            "temperature": 0.8,
        }

        provider = LMStudioProvider(custom_config)

        assert provider.api_base == "http://custom:8080"
>       assert provider.model == "custom-model"
E       AssertionError: assert 'qwen/qwen3-4b-2507' == 'custom-model'
E
E         - custom-model
E         + qwen/qwen3-4b-2507

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:262: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,323 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,332 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080):
Max retries exceeded with url: /api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21f740>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)"))
2025-10-28 10:30:02,332 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'custom-model': Failed to connect to LM Studio:
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url:
/api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21f740>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)")), falling back to auto-selection
2025-10-28 10:30:02,338 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080):
Max retries exceeded with url: /api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21eab0>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)"))
2025-10-28 10:30:02,338 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio:
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url:
/api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21eab0>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)")). Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): Max
retries exceeded with url: /api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21f740>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)"))
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'custom-model': Failed to connect to LM Studio:
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url:
/api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21f740>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)")), falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): Max
retries exceeded with url: /api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21eab0>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)"))
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio:
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url:
/api/v0/models (Caused by
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x12e21eab0>:
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not
known)")). Using fallback: qwen/qwen3-4b-2507
______________ TestLMStudioProvider.test_provider_complete_method ______________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a2060>

    @pytest.mark.fast
    def test_provider_complete_method(self):
        """Test the complete method functionality."""
        provider = LMStudioProvider()

>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e09c620>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError:
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e22d190> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,364 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,366 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,366 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,369 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,369 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_______________ TestLMStudioProvider.test_provider_embed_method ________________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a2510>

    @pytest.mark.fast
    def test_provider_embed_method(self):
        """Test the embed method functionality."""
        provider = LMStudioProvider()

>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:288:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x1289cee70>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError:
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e22d280> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,434 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,436 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,436 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,439 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,439 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________ TestLMStudioProvider.test_provider_health_check_success ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a29c0>

    @pytest.mark.fast
    def test_provider_health_check_success(self):
        """Test successful health check."""
        provider = LMStudioProvider()

>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:306:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e0aa7e0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError:
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e21c680> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,513 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,515 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,516 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,517 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,517 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________ TestLMStudioProvider.test_provider_health_check_failure ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a2e70>

    @pytest.mark.fast
    def test_provider_health_check_failure(self):
        """Test failed health check."""
        provider = LMStudioProvider()

>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:321:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e09bb30>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError:
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e21cfb0> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,637 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,639 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,639 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,641 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,641 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_____________ TestLMStudioProvider.test_provider_get_client_method _____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a3320>

    @pytest.mark.fast
    def test_provider_get_client_method(self):
        """Test the _get_client method."""
        provider = LMStudioProvider()

        with patch(
            "devsynth.application.llm.lmstudio_provider.lmstudio"
        ) as mock_lmstudio:
            mock_client = MagicMock()
            mock_lmstudio.llm.return_value = mock_client

>           result = provider._get_client()
                     ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'_get_client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:339: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,731 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,733 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,734 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,736 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,736 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______________ TestLMStudioProvider.test_provider_model_property _______________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191a37d0>

    @pytest.mark.fast
    def test_provider_model_property(self):
        """Test the model property getter and setter."""
        provider = LMStudioProvider()

        # Test getter
>       assert provider.model == "default-model"
E       AssertionError: assert 'qwen/qwen3-4b-2507' == 'default-model'
E
E         - default-model
E         + qwen/qwen3-4b-2507

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:350: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,766 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,768 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,769 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,771 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,771 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_________ TestLMStudioProvider.test_provider_available_models_property _________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider
object at 0x1191b4170>

    @pytest.mark.fast
    def test_provider_available_models_property(self):
        """Test the available_models property."""
        provider = LMStudioProvider()

>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:361:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10cc72300>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError:
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e21c770> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:02,791 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:02,793 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,794 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:02,796 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:02,796 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________________ test_default_selection_is_deterministic ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce52c60>

    def test_default_selection_is_deterministic(monkeypatch):
        """Factory should fall back to the next provider in order."""

        class DummyOpenAI:
            def __init__(self, config=None):
                self.config = config

        class DummyAnthropic:
            def __init__(self, config=None):
                self.config = config

        monkeypatch.setattr(
            factory, "provider_types", {"openai": DummyOpenAI, "anthropic":
DummyAnthropic}
        )
        monkeypatch.delitem(factory.provider_types, "openai", raising=False)
>       provider = factory.create_provider()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_factory.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:61: in create_provider
    return super().create_provider("offline", config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.provider_factory.ProviderFactory object at
0x1190a3c50>
provider_type = 'offline', config = None

    def create_provider(
        self, provider_type: str, config: Dict[str, Any] = None
    ) -> LLMProvider:
        """Create an LLM provider of the specified type."""
        if provider_type not in self.provider_types:
            if provider_type == "lmstudio":
                raise ValidationError(
                    "LMStudio provider is unavailable. Install the 'lmstudio'
package to enable this provider."
                )
>           raise ValidationError(f"Unknown provider type: {provider_type}")
E           devsynth.application.llm.providers.ValidationError: Unknown provider
type: offline

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:339: ValidationError
_______________________ test_case_insensitive_selection ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10ce52330>

    def test_case_insensitive_selection(monkeypatch):
        class DummyOpenAI:
            def __init__(self, config=None):
                self.config = config

        monkeypatch.setattr(factory, "provider_types", {"openai": DummyOpenAI})
>       provider = factory.create_provider("OPENAI")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_factory.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:50: in create_provider
    return super().create_provider("offline", config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.provider_factory.ProviderFactory object at
0x1190a3c50>
provider_type = 'offline', config = None

    def create_provider(
        self, provider_type: str, config: Dict[str, Any] = None
    ) -> LLMProvider:
        """Create an LLM provider of the specified type."""
        if provider_type not in self.provider_types:
            if provider_type == "lmstudio":
                raise ValidationError(
                    "LMStudio provider is unavailable. Install the 'lmstudio'
package to enable this provider."
                )
>           raise ValidationError(f"Unknown provider type: {provider_type}")
E           devsynth.application.llm.providers.ValidationError: Unknown provider
type: offline

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:339: ValidationError
________________________ test_get_llm_provider_offline _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2791f0>

    @pytest.mark.fast
    def test_get_llm_provider_offline(monkeypatch):
        """Selects offline provider when offline mode is enabled.

        ReqID: FR-85"""
        dummy = DummyFactory()
        monkeypatch.setattr(providers, "factory", dummy)
        monkeypatch.setattr(
            providers,
            "load_config",
            lambda: types.SimpleNamespace(
                as_dict=lambda: {"offline_mode": True, "offline_provider":
"local"}
            ),
        )
>       monkeypatch.setattr(providers, "get_llm_settings", lambda: {"provider":
"openai"})
E       AttributeError: <module 'devsynth.application.llm.providers' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_selection.py:31: AttributeError
________________________ test_get_llm_provider_default _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e27b4d0>

    @pytest.mark.fast
    def test_get_llm_provider_default(monkeypatch):
        """Uses configured provider when offline mode is disabled.

        ReqID: FR-85"""
        dummy = DummyFactory()
        monkeypatch.setattr(providers, "factory", dummy)
        monkeypatch.setattr(
            providers, "load_config", lambda:
types.SimpleNamespace(as_dict=lambda: {})
        )
>       monkeypatch.setattr(providers, "get_llm_settings", lambda: {"provider":
"local"})
E       AttributeError: <module 'devsynth.application.llm.providers' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_selection.py:49: AttributeError
_______ TestExecutionLearningIntegration.test_learn_from_code_execution ________

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x11924e900>

    def test_learn_from_code_execution(self):
        """Test learning from code execution."""
        code_snippets = [
            "def add(a, b): return a + b",
            "def multiply(a, b): return a * b"
        ]

        # Mock the learning process
        with patch.object(self.integration.trajectory_collector,
'collect_python_trajectories') as mock_collect:
            mock_collect.return_value = [
                ExecutionTrace(
                    code=code_snippets[0],
                    execution_steps=[
                        ExecutionStep(
                            step_number=1,
                            line_number=1,
                            code_line="def add(a, b):"
                        )
                    ],
                    execution_outcome="success"
                )
            ]

            with patch.object(self.integration.learning_algorithm,
'train_on_trajectories') as mock_train:
                mock_train.return_value = {
                    "trajectories_processed": 1,
                    "patterns_extracted": 2,
                    "validation_score": 0.85,
                    "patterns": {"pattern1": Mock(), "pattern2": Mock()},
                    "understandings": {"understanding1": Mock()}
                }

>               result =
self.integration.learn_from_code_execution(code_snippets)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:57: in learn_from_code_execution
    semantic_understandings =
self.learning_algorithm._build_semantic_understanding(trajectories,
learning_results["patterns"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_algorithm.py:248: in _build_semantic_understanding
    relevant_patterns = self._find_relevant_patterns(patterns,
semantic_components)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.execution_learning_algorithm.ExecutionLearningAlgor
ithm object at 0x12e2c3500>
patterns = {'pattern1': <Mock id='5069429440'>, 'pattern2': <Mock
id='5069429344'>}
components = {'avg_execution_time': 0.0, 'error_types': [], 'function_calls':
{}, 'success_rate': 1.0, ...}

    def _find_relevant_patterns(self, patterns: List[ExecutionPattern],
components: Dict[str, Any]) -> List[ExecutionPattern]:
        """Find patterns relevant to the given semantic components."""
        relevant_patterns = []

        # Match patterns based on function calls
        if "function_calls" in components:
            for pattern in patterns:
>               if pattern.pattern_type == "function_behavior":
                   ^^^^^^^^^^^^^^^^^^^^
E               AttributeError: 'str' object has no attribute 'pattern_type'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_algorithm.py:332: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,165 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,165 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,165 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,165 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
_______ TestExecutionLearningIntegration.test_enhance_code_understanding _______

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x11924f560>

    def test_enhance_code_understanding(self):
        """Test code understanding enhancement."""
        code = "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) +
fibonacci(n-2)"

        # Mock pattern library
        with patch.object(self.integration.pattern_library, 'find_matches') as
mock_find:
            mock_pattern = Mock()
            mock_pattern.pattern_id = "fib_pattern"
            mock_pattern.pattern_type = "recursive_algorithm"
            mock_pattern.confidence = 0.9
            mock_find.return_value = [mock_pattern]

            with patch.object(self.integration.understanding_engine,
'predict_execution_behavior') as mock_predict:
                mock_predict.return_value = {
                    "prediction": "recursive_execution",
                    "confidence": 0.85,
                    "predicted_success_rate": 0.9
                }

                with patch.object(self.integration.understanding_engine,
'analyze_behavioral_intent') as mock_intent:
                    mock_intent.return_value = Mock(
                        primary_purpose="fibonacci_calculation",
                        complexity_level="moderate",
                        intent_confidence=0.8
                    )

>                   result = self.integration.enhance_code_understanding(code)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py💯 in enhance_code_understanding
    components = self.understanding_engine.extract_semantic_components(code)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:84: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x12e0d2390>
code = 'def fibonacci(n): return n if n <= 1 else fibonacci(n-1) +
fibonacci(n-2)'

    def _analyze_ast_structure(self, code: str) -> Dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)

            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:253: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,195 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,196 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,196 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,196 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
__ TestExecutionLearningIntegration.test_validate_against_research_benchmarks __

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x119270aa0>

    def test_validate_against_research_benchmarks(self):
        """Test validation against research benchmarks."""
        validation_suite = {
            "semantic_robustness": Mock(overall_score=0.91,
benchmark_compliance={"mutation_resistance": True}),
            "execution_prediction": Mock(overall_score=0.83,
benchmark_compliance={"prediction_accuracy": True}),
            "multi_hop_reasoning": Mock(overall_score=0.87,
benchmark_compliance={"multi_hop_accuracy": True})
        }

>       result =
self.integration.validate_against_research_benchmarks(validation_suite)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:171:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.execution_learning_integration.ExecutionLearningInt
egration object at 0x12e2c1400>
test_results = {'execution_prediction': <Mock id='5069603904'>,
'multi_hop_reasoning': <Mock id='5069603184'>, 'semantic_robustness': <Mock
id='5069607504'>}

    def validate_against_research_benchmarks(self, test_results: Dict[str, Any])
-> Dict[str, Any]:
        """Validate learning results against research benchmarks."""
        benchmarks = {
            "semantic_understanding": 0.8,  # 80% semantic understanding target
            "mutation_resistance": 0.9,     # 90% resistance to semantic
mutations
            "pattern_accuracy": 0.85,       # 85% pattern prediction accuracy
            "execution_prediction": 0.8     # 80% execution outcome prediction
        }

        validation_report = {
            "benchmark_comparison": {},
            "research_alignment": True,
            "improvement_areas": [],
            "validation_method": "research_benchmark_comparison"
        }

        # Compare against benchmarks
        for metric, benchmark_value in benchmarks.items():
            if metric in test_results:
                achieved_value = test_results[metric]
>               meets_benchmark = achieved_value >= benchmark_value
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E               TypeError: '>=' not supported between instances of 'Mock' and
'float'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:354: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,239 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,239 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,239 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,239 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
______ TestExecutionLearningIntegration.test_export_import_learning_state ______

self =
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x119270fe0>

    def test_export_import_learning_state(self):
        """Test learning state export and import."""
        # Set up learning state
        self.integration.learning_history = [{"test": "session"}]
        self.integration.understanding_cache = {"test": "cache"}

        # Mock pattern library export
        with patch.object(self.integration.pattern_library, 'export_patterns')
as mock_export:
            mock_export.return_value = {"patterns": {}, "total_patterns": 0}

            # Export state
>           exported_state = self.integration.export_learning_state()
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:188:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:381: in export_learning_state
    "statistics": self.get_learning_statistics(),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:312: in get_learning_statistics
    total_patterns = sum(session["patterns_learned"] for session in
self.learning_history)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x12e0bd2a0>

>   total_patterns = sum(session["patterns_learned"] for session in
self.learning_history)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'patterns_learned'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:312: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,251 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,251 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,251 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,251 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
_______ TestSemanticUnderstandingEngine.test_extract_semantic_components _______

self =
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x119272e70>

        def test_extract_semantic_components(self):
            """Test semantic component extraction."""
            code = """
    def calculate_fibonacci(n):
        if n <= 1:
            return n
        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
    """

>           components = self.engine.extract_semantic_components(code)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:356:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:84: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x12e25ae10>
code = '\ndef calculate_fibonacci(n):\n    if n <= 1:\n        return n\n
return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n'

    def _analyze_ast_structure(self, code: str) -> Dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)

            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:253: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,318 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,318 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
_______ TestSemanticUnderstandingEngine.test_detect_semantic_equivalence _______

self =
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x119273860>

    def test_detect_semantic_equivalence(self):
        """Test semantic equivalence detection."""
        code1 = "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) +
fibonacci(n-2)"
        code2 = "def fib_calc(num): return num if num <= 1 else fib_calc(num-1)
+ fib_calc(num-2)"

>       equivalence = self.engine.detect_semantic_equivalence(code1, code2)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:380:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:167: in detect_semantic_equivalence
    components1 = self.extract_semantic_components(code1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:84: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x12e297fb0>
code = 'def fibonacci(n): return n if n <= 1 else fibonacci(n-1) +
fibonacci(n-2)'

    def _analyze_ast_structure(self, code: str) -> Dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)

            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:253: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,345 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,345 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
_______ TestSemanticUnderstandingEngine.test_predict_execution_behavior ________

self =
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x119284320>

    def test_predict_execution_behavior(self):
        """Test execution behavior prediction."""
        code = "def divide(a, b): return a / b"

        # Mock pattern library to return relevant patterns
        mock_pattern = Mock()
        mock_pattern.pattern_id = "division_pattern"
        mock_pattern.pattern_type = "mathematical_operation"
        mock_pattern.confidence = 0.8
        mock_pattern.expected_outcomes = {"success_rate": 0.7}

        with patch.object(self.engine.pattern_library, 'find_matches') as
mock_find:
            mock_find.return_value = [mock_pattern]

>           prediction = self.engine.predict_execution_behavior(code)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:401:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:202: in predict_execution_behavior
    components = self.extract_semantic_components(code)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:84: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x12e295490>
code = 'def divide(a, b): return a / b'

    def _analyze_ast_structure(self, code: str) -> Dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)

            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:253: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,368 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,368 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
_________________ test_query_results_from_rows_shapes_records __________________

    @pytest.mark.fast
    def test_query_results_from_rows_shapes_records() -> None:
        """Row helpers should produce query results with normalized metadata."""

        rows = [
            {
                "id": "a",
                "content": "alpha",
                "memory_type": "context",
                "metadata": to_serializable({"score": 1}),
            },
            {
                "id": "b",
                "content": "beta",
                "memory_type": "knowledge",
                "metadata": to_serializable({"score": 2}),
                "source": "secondary",
                "similarity": 0.33,
            },
        ]

        results = query_results_from_rows(
            "primary",
            rows,
            total="2",
            latency_ms="3.5",
            metadata=to_serializable(
                {"batch": 1, "started_at": datetime(2024, 4, 5, 6, 7)}
            ),
        )

        assert results["store"] == "primary"
        assert results["total"] == 2
        assert results["latency_ms"] == pytest.approx(3.5)
        assert results["metadata"]["batch"] == 1
        assert results["metadata"]["started_at"] == datetime(2024, 4, 5, 6, 7)

        primary_record, secondary_record = results["records"]
        assert primary_record.source == "primary"
        assert primary_record.memory_type is MemoryType.CONTEXT
        assert secondary_record.source == "secondary"
        assert secondary_record.similarity == pytest.approx(0.33)
>       assert secondary_record.memory_type is MemoryType.CONTEXT
E       AssertionError: assert <MemoryType.KNOWLEDGE: 'knowledge'> is
<MemoryType.CONTEXT: 'context'>
E        +  where <MemoryType.KNOWLEDGE: 'knowledge'> =
MemoryRecord(item=MemoryItem(id='b', content='beta',
memory_type=<MemoryType.KNOWLEDGE: 'knowledge'>,
metadata={'score...ted_at=datetime.datetime(2025, 10, 28, 10, 30, 6, 582180)),
similarity=0.33, source='secondary', metadata={'score': 2}).memory_type
E        +  and   <MemoryType.CONTEXT: 'context'> = MemoryType.CONTEXT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_metadata_serialization_helpers.py:132: AssertionError
_______ TestPhase3IntegrationSystem.test_process_advanced_reasoning_task _______

self =
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x1193ae990>

    def test_process_advanced_reasoning_task(self):
        """Test processing of complex reasoning tasks."""
        task = {
            "task_id": "test_task_123",
            "description": "Analyze user authentication system",
            "type": "analysis",
            "complexity": "medium"
        }

        # Mock all the component methods
        with patch.object(self.integration_system, '_analyze_and_segment_task')
as mock_segment:
            mock_segment.return_value = [
                {"segment_id": "seg1", "description": "Analyze requirements"},
                {"segment_id": "seg2", "description": "Review implementation"}
            ]

            with patch.object(self.integration_system,
'_execute_multi_hop_reasoning') as mock_reasoning:
                mock_reasoning.return_value = {
                    "success": True,
                    "total_hops": 3,
                    "confidence": 0.85
                }

                with patch.object(self.integration_system,
'_execute_hybrid_llm_processing') as mock_hybrid:
                    mock_hybrid.return_value = {
                        "success": True,
                        "result": {"confidence": 0.9, "execution_time": 2.5}
                    }

                    with patch.object(self.integration_system,
'_apply_metacognitive_enhancement') as mock_meta:
                        mock_meta.return_value = {
                            "success": True,
                            "insights": ["Strategy improvement", "Efficiency
gain"]
                        }

                        with patch.object(self.integration_system,
'_optimize_contextual_prompts') as mock_prompts:
                            mock_prompts.return_value = {
                                "success": True,
                                "engineered_prompts": ["Prompt 1", "Prompt 2"]
                            }

                            result =
self.integration_system.process_advanced_reasoning_task(task)

        assert result["success"] is True
        assert result["task_id"] == "test_task_123"
        assert "processing_summary" in result
        assert result["processing_summary"]["task_segments"] == 2
        assert result["processing_summary"]["reasoning_hops"] == 3
>       assert result["processing_summary"]["confidence_score"] > 0.8
E       assert 0.6125 > 0.8

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:89: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,598 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,598 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
2025-10-28 10:30:06,598 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-28 10:30:06,598 - devsynth.application.memory.automata_synthesis_engine
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-28 10:30:06,598 - devsynth.application.memory.hybrid_llm_architecture -
INFO - Hybrid LLM architecture initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-28 10:30:06,598 -
devsynth.application.memory.contextual_prompting_system - INFO - Contextual
prompting system initialized
2025-10-28 10:30:06,598 - devsynth.application.memory.phase3_integration_system
- INFO - Phase 3 Integration System initialized with all advanced reasoning
components
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
INFO
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615
Enhanced GraphRAG query engine initialized
INFO
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615
Automata synthesis engine initialized (min_samples: 10)
INFO
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid
LLM architecture initialized
INFO
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615
Metacognitive training system initialized
INFO
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615
Contextual prompting system initialized
INFO
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:06,599 - devsynth.application.memory.phase3_integration_system
- INFO - Processing advanced reasoning task: test_task_123
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.phase3_integration_system:logging_setup.py:615
Processing advanced reasoning task: test_task_123
_______ TestPhase3IntegrationSystem.test_apply_metacognitive_enhancement _______

self =
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x1193d8290>

    def test_apply_metacognitive_enhancement(self):
        """Test metacognitive enhancement application."""
        task = {"description": "Test metacognitive enhancement"}
        hybrid_results = {"result": {"confidence": 0.85}}

        # Mock metacognitive training
        with patch.object(self.integration_system.metacognitive_training,
'start_think_aloud_session') as mock_start:
            mock_start.return_value = "session_123"

            with patch.object(self.integration_system.metacognitive_training,
'record_verbalization') as mock_record:
                with
patch.object(self.integration_system.metacognitive_training,
'end_think_aloud_session') as mock_end:
                    mock_end.return_value = {
                        "session_id": "session_123",
                        "insights": ["Strategy improvement", "Error pattern"],
                        "verbalizations_count": 5
                    }

                    result =
self.integration_system._apply_metacognitive_enhancement(task, hybrid_results)

        assert result["success"] is True
>       assert len(result["insights"]) == 2
E       assert 0 == 2
E        +  where 0 = len([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:176: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,628 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,628 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,628 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,628 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,628 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,628 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,628 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,628 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,629 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
2025-10-28 10:30:06,629 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-28 10:30:06,629 - devsynth.application.memory.automata_synthesis_engine
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-28 10:30:06,629 - devsynth.application.memory.hybrid_llm_architecture -
INFO - Hybrid LLM architecture initialized
2025-10-28 10:30:06,629 -
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-28 10:30:06,629 -
devsynth.application.memory.contextual_prompting_system - INFO - Contextual
prompting system initialized
2025-10-28 10:30:06,629 - devsynth.application.memory.phase3_integration_system
- INFO - Phase 3 Integration System initialized with all advanced reasoning
components
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
INFO
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615
Enhanced GraphRAG query engine initialized
INFO
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615
Automata synthesis engine initialized (min_samples: 10)
INFO
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid
LLM architecture initialized
INFO
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615
Metacognitive training system initialized
INFO
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615
Contextual prompting system initialized
INFO
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
_________ TestPhase3IntegrationSystem.test_export_import_system_state __________

self =
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x1193d9cd0>

    def test_export_import_system_state(self):
        """Test system state export and import."""
        # Set up some state
        self.integration_system.execution_learning.learning_history = [{"test":
"session"}]
        self.integration_system.enhanced_graphrag.query_cache = {"query1":
"result1"}

        # Export state
>       exported_state = self.integration_system.export_system_state()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:267:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/phase3_integration_system.py:471: in export_system_state
    "execution_learning": self.execution_learning.export_learning_state(),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:381: in export_learning_state
    "statistics": self.get_learning_statistics(),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:312: in get_learning_statistics
    total_patterns = sum(session["patterns_learned"] for session in
self.learning_history)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x12e21d4e0>

>   total_patterns = sum(session["patterns_learned"] for session in
self.learning_history)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'patterns_learned'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:312: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:06,669 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-28 10:30:06,669 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-28 10:30:06,669 -
devsynth.application.memory.execution_trajectory_collector - INFO - Execution
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-28 10:30:06,669 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.execution_learning_algorithm - INFO - Execution
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-28 10:30:06,670 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic
understanding engine initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern
library initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.execution_learning_integration - INFO - Execution
learning integration initialized (max_trajectories: 1000)
2025-10-28 10:30:06,670 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-28 10:30:06,670 - devsynth.application.memory.automata_synthesis_engine
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-28 10:30:06,670 - devsynth.application.memory.hybrid_llm_architecture -
INFO - Hybrid LLM architecture initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-28 10:30:06,670 -
devsynth.application.memory.contextual_prompting_system - INFO - Contextual
prompting system initialized
2025-10-28 10:30:06,670 - devsynth.application.memory.phase3_integration_system
- INFO - Phase 3 Integration System initialized with all advanced reasoning
components
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615
Enhanced knowledge graph initialized
INFO
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615
Semantic understanding engine initialized
INFO
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615
Pattern library initialized
INFO
devsynth.application.memory.execution_learning_integration:logging_setup.py:615
Execution learning integration initialized (max_trajectories: 1000)
INFO
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615
Enhanced GraphRAG query engine initialized
INFO
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615
Automata synthesis engine initialized (min_samples: 10)
INFO
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid
LLM architecture initialized
INFO
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615
Metacognitive training system initialized
INFO
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615
Contextual prompting system initialized
INFO
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
_________________________ test_cascading_and_federated _________________________

router = <devsynth.application.memory.query_router.QueryRouter object at
0x12e2da540>

    @pytest.mark.fast
    def test_cascading_and_federated(router: QueryRouter) -> None:
        """Cascading and federated strategies yield MemoryRecord sequences."""

        cascading = router.cascading_query("topic")
>       assert {record.source for record in cascading} == {"vector", "graph"}
E       AssertionError: assert {'graph', 'tinydb', 'vector'} == {'graph',
'vector'}
E
E         Extra items in the left set:
E         'tinydb'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_query_router.py:128: AssertionError
___________________ test_queue_update_enqueues_memory_record ___________________

    @pytest.mark.fast
    def test_queue_update_enqueues_memory_record() -> None:
        manager = _manager()
        sync_manager: SyncManager = manager.sync_manager
        item = MemoryItem(
            id="queued-1",
            content="queue-test",
            memory_type=MemoryType.SHORT_TERM,
            metadata={"origin": "alpha"},
            created_at=datetime.now(),
        )

        sync_manager.queue_update("alpha", item)

        with sync_manager._queue_lock:  # noqa: SLF001 - internal verification
for test
>           queued_store, record = sync_manager._queue[-1]
            ^^^^^^^^^^^^^^^^^^^^
E           ValueError: too many values to unpack (expected 2)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_sync_manager_transactions.py:61: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:07,513 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: alpha, beta
2025-10-28 10:30:07,513 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:30:07,513 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: alpha, beta
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
________________ test_tinydb_adapter_serializes_bytes_and_tuple ________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_tinydb_adapter_serializes0')

    @pytest.mark.fast
    def test_tinydb_adapter_serializes_bytes_and_tuple(tmp_path):
        """Ensure TinyDBMemoryAdapter serializes non-JSON types safely.

        This guards against the TypeError observed during `task release:prep`
when
        metadata/content contain bytes or tuples.
        """
        adapter = TinyDBMemoryAdapter(db_path=str(tmp_path / "db.json"))

        item = MemoryItem(
            id="bytes_tuple",
            content={
                "payload": b"hello",
                "coords": (1, 2, 3),
                "nested": {"t": ("a", "b")},
            },
            memory_type=MemoryType.KNOWLEDGE,
            metadata={
                "tags": ("x", "y"),
                "raw": b"world",
                "timestamp": datetime(2024, 1, 1),
            },
        )

        stored_id = adapter.store(item)
        assert stored_id == item.id

        retrieved = adapter.retrieve(stored_id)
        assert retrieved is not None

        # bytes should become a string representation (utf-8 or base64); at
least be str
        assert isinstance(retrieved.content["payload"], str)
        assert isinstance(retrieved.metadata["raw"], str)

        # tuples should become lists
        assert retrieved.content["coords"] == [1, 2, 3]
        assert retrieved.content["nested"]["t"] == ["a", "b"]
        assert retrieved.metadata["tags"] == ["x", "y"]

        # datetime should round-trip as ISO string (already covered elsewhere
but asserted here for completeness)
>       assert isinstance(retrieved.metadata["timestamp"], str)
E       assert False
E        +  where False = isinstance(datetime.datetime(2024, 1, 1, 0, 0), str)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_tinydb_adapter_bytes_tuple.py:54: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:07,543 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:30:07,544 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID bytes_tuple in TinyDB Memory Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID bytes_tuple in TinyDB Memory Adapter
____________________ test_evaluate_change_stores_with_phase ____________________

    def test_evaluate_change_stores_with_phase():
        memory = DummyMemoryManager()
        service = _build_service("yes", memory_manager=memory)
        change = RequirementChange(requirement_id=uuid4(), created_by="carol")

        service.evaluate_change(change, edrr_phase=EDRRPhase.EXPAND)

        assert memory.calls
>       assert memory.calls[0][1] == MemoryType.DIALECTICAL_REASONING
E       AssertionError: assert <MemoryType.RELATIONSHIP: 'relationship'> ==
<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>
E        +  where <MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'> =
MemoryType.DIALECTICAL_REASONING

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:124: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:07,642 - devsynth.application.requirements.dialectical_reasoner
- INFO - Evaluating change
2025-10-28 10:30:07,642 - devsynth.application.requirements.dialectical_reasoner
- INFO - Consensus reached for change
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615
Evaluating change
INFO
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615
Consensus reached for change
_______________ test_generate_arguments_parses_counterarguments ________________

    def test_generate_arguments_parses_counterarguments():
        response = (
            "Argument 1:\n"
            "Position: Thesis\n"
            "Content: Improve UX\n"
            "Counterargument: Increases complexity\n\n"
            "Argument 2:\n"
            "Position: Antithesis\n"
            "Content: Maintain simplicity\n"
            "Counterargument: Misses UX gains"
        )
        service = _build_service_for_arguments(response)
        change = RequirementChange(requirement_id=uuid4(), created_by="mallory")
>       args = service._generate_arguments(change, "thesis", "antithesis")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:220:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:575: in _generate_arguments
    prompt = self._create_arguments_prompt(change, thesis, antithesis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.requirements.dialectical_reasoner.DialecticalReasonerServi
ce object at 0x12e294980>
change = RequirementChange(id=UUID('6bb9db09-169f-468c-be80-e4f8fbacec71'),
requirement_id=UUID('e0b656ab-a48a-4dff-af2b-a228f9..., 10, 30, 7, 681869),
created_by='mallory', reason='', approved=False, approved_at=None,
approved_by=None, comments=[])
thesis = 'thesis', antithesis = 'antithesis'

    def _create_arguments_prompt(
        self, change: RequirementChange, thesis: str, antithesis: str
    ) -> str:
        """
        Create a prompt for generating arguments.

        Args:
            change: The requirement change.
            thesis: The thesis statement.
            antithesis: The antithesis statement.

        Returns:
            The prompt.
        """
        prompt = (
            "You are a requirements analyst evaluating a proposed change to a
requirement. "
            "Please generate a list of arguments for and against the proposed
change. "
            "For each argument, specify whether it supports the thesis or
antithesis, provide a clear explanation, "
            "and then offer a counterargument that challenges the original
point. "
            "\n\nProposed change:\n"
        )

        if change.change_type.value == "add":
            prompt += f"Add a new requirement: {change.new_state.title}\n"
            prompt += f"Description: {change.new_state.description}\n"
        elif change.change_type.value == "remove":
            prompt += f"Remove requirement: {change.previous_state.title}\n"
            prompt += f"Description: {change.previous_state.description}\n"
        elif change.change_type.value == "modify":
            prompt += f"Modify requirement from:\n"
>           prompt += f"Title: {change.previous_state.title}\n"
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'title'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:1068: AttributeError
___________ test_generate_arguments_handles_missing_counterargument ____________

    def test_generate_arguments_handles_missing_counterargument():
        response = (
            "Argument 1:\n"
            "Position: Thesis\n"
            "Content: Example argument\n\n"
            "Argument 2:\n"
            "Position: Antithesis\n"
            "Content: Another argument"
        )
        service = _build_service_for_arguments(response)
        change = RequirementChange(requirement_id=uuid4(), created_by="nina")
>       args = service._generate_arguments(change, "thesis", "antithesis")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:247:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:575: in _generate_arguments
    prompt = self._create_arguments_prompt(change, thesis, antithesis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.requirements.dialectical_reasoner.DialecticalReasonerServi
ce object at 0x12e2cfc20>
change = RequirementChange(id=UUID('a208615d-592f-42f5-a995-9256b3a2f1cf'),
requirement_id=UUID('3698b30b-3441-4bb3-ae16-bbd01c... 28, 10, 30, 7, 704614),
created_by='nina', reason='', approved=False, approved_at=None,
approved_by=None, comments=[])
thesis = 'thesis', antithesis = 'antithesis'

    def _create_arguments_prompt(
        self, change: RequirementChange, thesis: str, antithesis: str
    ) -> str:
        """
        Create a prompt for generating arguments.

        Args:
            change: The requirement change.
            thesis: The thesis statement.
            antithesis: The antithesis statement.

        Returns:
            The prompt.
        """
        prompt = (
            "You are a requirements analyst evaluating a proposed change to a
requirement. "
            "Please generate a list of arguments for and against the proposed
change. "
            "For each argument, specify whether it supports the thesis or
antithesis, provide a clear explanation, "
            "and then offer a counterargument that challenges the original
point. "
            "\n\nProposed change:\n"
        )

        if change.change_type.value == "add":
            prompt += f"Add a new requirement: {change.new_state.title}\n"
            prompt += f"Description: {change.new_state.description}\n"
        elif change.change_type.value == "remove":
            prompt += f"Remove requirement: {change.previous_state.title}\n"
            prompt += f"Description: {change.previous_state.description}\n"
        elif change.change_type.value == "modify":
            prompt += f"Modify requirement from:\n"
>           prompt += f"Title: {change.previous_state.title}\n"
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'title'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:1068: AttributeError
______________________ test_wsde_team_hook_positive_path _______________________

    def test_wsde_team_hook_positive_path():
        team = WSDETeam("test")
        service = _build_service("yes")
>       service.register_evaluation_hook(team.requirement_evaluation_hook)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'requirement_evaluation_hook'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:266: AttributeError
______________________ test_wsde_team_hook_negative_path _______________________

    def test_wsde_team_hook_negative_path():
        team = WSDETeam("test")
        service = _build_service("no")
>       service.register_evaluation_hook(team.requirement_evaluation_hook)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'requirement_evaluation_hook'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:279: AttributeError
_____________ TestEnhancedTestCollector.test_nonexistent_directory _____________

self = <test_enhanced_test_collector.TestEnhancedTestCollector object at
0x11952eba0>

    def test_nonexistent_directory(self):
        """Test handling of nonexistent directories."""
        collector = EnhancedTestCollector()

        tests = collector.collect_tests_by_category("nonexistent")
        assert len(tests) == 0

>       isolation_report =
collector._isolation_analyzer.analyze_test_isolation("nonexistent")
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'EnhancedTestCollector' object has no attribute
'_isolation_analyzer'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:318: AttributeError
______________ TestCacheOperations.test_cache_directory_creation _______________

self = <test_enhanced_test_collector.TestCacheOperations object at 0x1195384d0>

    def test_cache_directory_creation(self):
        """Test that cache directory is created."""
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = Path(temp_dir) / "cache"
            collector = EnhancedTestCollector()
            collector.cache_dir = cache_dir

            # Cache directory should be created
>           assert cache_dir.exists()
E           AssertionError: assert False
E            +  where False = exists()
E            +    where exists =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpflo_b3t4/cache').
exists

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:416: AssertionError
_____________________ test_metrics_fail_patches_calculate ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10b265910>

    @pytest.mark.fast
    def test_metrics_fail_patches_calculate(monkeypatch):
>       metrics_fail(monkeypatch)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior/test_al
ignment_metrics_steps_unit.py:17:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

command_context = <_pytest.monkeypatch.MonkeyPatch object at 0x10b265910>

    @given("alignment metrics calculation fails")
    def metrics_fail(command_context: MutableMapping[str, object]) -> None:
        """Force the simulated CLI command to raise an error."""

>       command_context["force_error"] = True
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'MonkeyPatch' object does not support item assignment

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_a
lignment_metrics_steps.py:47: TypeError
________________________ test_run_command_inspect_code _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e296450>

    @pytest.mark.fast
    def test_run_command_inspect_code(monkeypatch):
        mock_manager = MagicMock()
        monkeypatch.setattr(steps, "inspect_code_cmd", lambda path:
print("Architecture"))
        command_context = {}
        with (
            patch(
                "devsynth.application.orchestration.workflow.workflow_manager",
mock_manager
            ),
>           patch("devsynth.core.workflow_manager", mock_manager),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior/test_an
alyze_commands_steps_unit.py:17:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e0993a0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.core'> does not have the attribute
'workflow_manager'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_run_command_inspect_config_update ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2951c0>

    @pytest.mark.fast
    def test_run_command_inspect_config_update(monkeypatch):
        mock_manager = MagicMock()
        monkeypatch.setattr(
            steps,
            "inspect_config_cmd",
            lambda path, upd, prn: print("Configuration updated successfully"),
        )
        command_context = {}
        with (
            patch(
                "devsynth.application.orchestration.workflow.workflow_manager",
mock_manager
            ),
>           patch("devsynth.core.workflow_manager", mock_manager),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior/test_an
alyze_commands_steps_unit.py:42:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x10cd93b90>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.core'> does not have the attribute
'workflow_manager'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________________ test_main_handles_run_cli_errors _______________________

name = 'errors'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'errors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e0993d0>

    @pytest.mark.fast
    def test_main_handles_run_cli_errors(monkeypatch):
        def failing_run_cli():
            raise RuntimeError("boom")

        handled = {}

        def fake_handle_error(_bridge, err):
            handled["error"] = err

        monkeypatch.setattr("devsynth.adapters.cli.typer_adapter.run_cli",
failing_run_cli)
>       monkeypatch.setattr(
            "devsynth.application.cli.errors.handle_error", fake_handle_error
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_cli_err
or_handling.py:35:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'errors', ann = 'devsynth.application.cli.errors'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.errors
has no attribute 'errors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ test_build_app_registers_commands_from_registry ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e09b5f0>

    @pytest.mark.fast
    def test_build_app_registers_commands_from_registry(monkeypatch):
        """Commands in COMMAND_REGISTRY should be registered with the CLI."""
        called = {}

        def sample_cmd():
            called["ran"] = True

        monkeypatch.setattr(adapter, "COMMAND_REGISTRY", {"sample": sample_cmd})
>       monkeypatch.setattr(adapter, "config_app", typer.Typer())
E       AttributeError: <module 'devsynth.adapters.cli.typer_adapter' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/cli/
typer_adapter.py'> has no attribute 'config_app'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_command
_registry.py:17: AttributeError
_________________ test_global_debug_flag_sets_log_level_debug __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5639e0>

    @pytest.mark.fast
    def test_global_debug_flag_sets_log_level_debug(monkeypatch):
        runner = CliRunner()
        # Ensure env does not force level
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)
        monkeypatch.delenv("DEVSYNTH_DEBUG", raising=False)

        app = build_app()
        result = runner.invoke(app, ["--debug", "--version"])  # triggers
callback and exit
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.DEBUG
E       assert 20 == 10
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at
0x103b23c40>()
E        +        where <function getLogger at 0x103b23c40> = logging.getLogger
E        +  and   10 = logging.DEBUG

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:29: AssertionError
__________________ test_env_debug_sets_log_level_when_no_flag __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e279250>

    @pytest.mark.fast
    def test_env_debug_sets_log_level_when_no_flag(monkeypatch):
        runner = CliRunner()
        monkeypatch.setenv("DEVSYNTH_DEBUG", "1")
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)

        app = build_app()
        result = runner.invoke(app, ["--version"])  # triggers callback
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.DEBUG
E       assert 20 == 10
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at
0x103b23c40>()
E        +        where <function getLogger at 0x103b23c40> = logging.getLogger
E        +  and   10 = logging.DEBUG

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:42: AssertionError
__________________ test_log_level_option_overrides_env_debug ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5638c0>

    @pytest.mark.fast
    def test_log_level_option_overrides_env_debug(monkeypatch):
        runner = CliRunner()
        monkeypatch.setenv("DEVSYNTH_DEBUG", "true")
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)

        app = build_app()
        result = runner.invoke(app, ["--log-level", "WARNING", "--version"])  #
eager
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.WARNING
E       assert 20 == 30
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at
0x103b23c40>()
E        +        where <function getLogger at 0x103b23c40> = logging.getLogger
E        +  and   30 = logging.WARNING

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:55: AssertionError
_____________ test_mvuu_dashboard_module_no_run_avoids_subprocess ______________

    @pytest.mark.fast
    @pytest.mark.smoke
    def test_mvuu_dashboard_module_no_run_avoids_subprocess():
        # Ensure running the module with --no-run exits cleanly and does not
spawn subprocesses
        with mock.patch.object(subprocess, "run") as mocked_run:
            proc = subprocess.run(
                [
                    sys.executable,
                    "-m",
                    "devsynth.application.cli.commands.mvuu_dashboard_cmd",
                    "--no-run",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=15,
            )
        # The outer subprocess.run executed our Python process; it should have
succeeded
>       assert proc.returncode == 0
E       AssertionError: assert <MagicMock name='run().returncode'
id='5072990720'> == 0
E        +  where <MagicMock name='run().returncode' id='5072990720'> =
<MagicMock name='run()' id='5072526160'>.returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_mvuu_da
shboard_smoke.py:26: AssertionError
__________ test_cli_run_tests_unit_fast_completes_with_non_zero_tests __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e589e20>

    @pytest.mark.fast
    def test_cli_run_tests_unit_fast_completes_with_non_zero_tests(monkeypatch):
        """
        Regression test: ensure the shared test runner completes in fast mode
and
        executes a non-zero number of tests, without hanging on optional
providers.
        """
        # Disable optional external providers to avoid network/UI stalls
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "false")

        success, output = run_tests(
            target="unit-tests",
            speed_categories=("fast",),
            verbose=False,
            report=False,
            parallel=False,  # run in-process to minimize flakiness on CI
            segment=False,
            segment_size=50,
            maxfail=1,
        )

        # Must succeed overall
>       assert success, f"Runner did not succeed. Output:\n{output}"
E       AssertionError: Runner did not succeed. Output:
E         Test timed out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)
E       assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_run_tes
ts_regression.py:29: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:13,231 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
____________________ test_configure_llm_settings_reads_env _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5128d0>

    @pytest.mark.fast
    def test_configure_llm_settings_reads_env(monkeypatch):
        """Environment variables update LLM settings when configured."""
        monkeypatch.setenv("DEVSYNTH_LLM_MODEL", "foo-model")
        monkeypatch.setenv("DEVSYNTH_LLM_MAX_TOKENS", "123")
        monkeypatch.setenv("DEVSYNTH_LLM_TEMPERATURE", "0.9")
        monkeypatch.setenv("DEVSYNTH_LLM_AUTO_SELECT_MODEL", "false")

>       importlib.reload(config)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/config/test_conf
ig_llm_env.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.config' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/config/__init
__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.config not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________________ test_load_config_merges_mvuu_settings _____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_load_config_merges_mvuu_s0')

    @pytest.mark.fast
    def test_load_config_merges_mvuu_settings(tmp_path):
        mvu_cfg = {"schema": "s.json", "storage": {"path": "db.json", "format":
"json"}}
        dev_dir = tmp_path / ".devsynth"
        dev_dir.mkdir()
        (dev_dir / "mvu.yml").write_text(yaml.safe_dump(mvu_cfg))
        cfg = load_config(start_path=str(tmp_path))
>       assert cfg.mvuu == mvu_cfg
E       AssertionError: assert {} == {'schema': 's...': 'db.json'}}
E
E         Right contains 2 more items:
E         {'schema': 's.json', 'storage': {'format': 'json', 'path': 'db.json'}}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_config
_loader_mvu.py:16: AssertionError
______________ test_bootstrap_script_rejects_invalid_environment _______________

    @pytest.mark.fast
    def test_bootstrap_script_rejects_invalid_environment():
        """bootstrap.sh should reject unknown environments.

        ReqID: DEP-02"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'bootstrap.sh'} invalid"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Invalid environment" in result.stderr
E           assert 'Invalid environment' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/.../github.com/ravenoak/devsyn
th/scripts/deployment/bootstrap.sh invalid"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:29: AssertionError
____________________ test_bootstrap_script_requires_docker _____________________

    @pytest.mark.fast
    def test_bootstrap_script_requires_docker():
        """bootstrap.sh should require docker to be installed.

        ReqID: DEP-03"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'bootstrap.sh'}
development"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Docker is required" in result.stderr
E           assert 'Docker is required' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...hub.com/ravenoak/devsynth/s
cripts/deployment/bootstrap.sh development"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:49: AssertionError
________________________ test_install_dev_installs_task ________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_install_dev_installs_task0')

    @pytest.mark.fast
    def test_install_dev_installs_task(tmp_path):
        """install_dev.sh should install go-task when absent.

        ReqID: DEP-05"""

        home = tmp_path / "home"
>       home.mkdir()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_install_dev_installs_task0/home')
mode = 511, parents = False, exist_ok = False

    def mkdir(self, mode=0o777, parents=False, exist_ok=False):
        """
        Create a new directory at this given path.
        """
        try:
>           os.mkdir(self, mode)
E           FileExistsError: [Errno 17] File exists:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_install_dev_installs_task0/home'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1311: FileExistsError
___________________ test_health_check_script_reports_healthy ___________________

    @pytest.mark.fast
    def test_health_check_script_reports_healthy():
        """health_check.sh should succeed when endpoints return 200.

        ReqID: DEP-02"""
        server = HTTPServer(("127.0.0.1", 0), _Handler)
        port = server.server_address[1]
        thread = threading.Thread(target=server.serve_forever)
        thread.daemon = True
        thread.start()
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            url = f"http://127.0.0.1:{port}"
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} {url}
{url} {url}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
>           assert result.returncode == 0
E           assert 1 == 0
E            +  where 1 = CompletedProcess(args=['su', 'nobody', '-s',
'/bin/bash', '-c', "cd '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...
http://127.0.0.1:57103 http://127.0.0.1:57103 http://127.0.0.1:57103"],
returncode=1, stdout='', stderr='su: Sorry\n').returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:49: AssertionError
__________________ test_health_check_script_rejects_root_user __________________

    @pytest.mark.fast
    def test_health_check_script_rejects_root_user():
        """health_check.sh should refuse to run as root.

        ReqID: CON-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            result = subprocess.run(
                [str(SCRIPTS_DIR / "health_check.sh")],
                cwd=workdir,
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Please run this script as a non-root user." in result.stderr
E           AssertionError: assert 'Please run this script as a non-root user.'
in 'Missing environment file: .env\n'
E            +  where 'Missing environment file: .env\n' =
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scr
ipts/deployment/health_check.sh'], returncode=1, stdout='', stderr='Missing
environment file: .env\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:72: AssertionError
__________________ test_health_check_script_requires_env_file __________________

    @pytest.mark.fast
    def test_health_check_script_requires_env_file():
        """health_check.sh should fail when the env file is missing.

        ReqID: DEP-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Missing environment file" in result.stderr
E           assert 'Missing environment file' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/ravenoak/d
evsynth/scripts/deployment/health_check.sh"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:92: AssertionError
_____________ test_health_check_script_requires_strict_permissions _____________

    @pytest.mark.fast
    def test_health_check_script_requires_strict_permissions():
        """health_check.sh should enforce 600 permissions on the env file.

        ReqID: CON-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o644)
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "must have 600 permissions" in result.stderr
E           assert 'must have 600 permissions' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/ravenoak/d
evsynth/scripts/deployment/health_check.sh"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:115: AssertionError
_________________ test_health_check_script_rejects_invalid_url _________________

    @pytest.mark.fast
    def test_health_check_script_rejects_invalid_url():
        """health_check.sh should validate provided URLs.

        ReqID: DEP-03"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            cmd = (
                f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} not-a-url"
                " not-a-url not-a-url"
            )
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Invalid URL" in result.stderr
E           assert 'Invalid URL' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...ynth/scripts/deployment/hea
lth_check.sh not-a-url not-a-url not-a-url"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:141: AssertionError
_____________ test_health_check_script_fails_on_unhealthy_endpoint _____________

    @pytest.mark.fast
    def test_health_check_script_fails_on_unhealthy_endpoint():
        """health_check.sh should report failure when an endpoint is unhealthy.

        ReqID: DEP-01"""
        server = HTTPServer(("127.0.0.1", 0), _FailHandler)
        port = server.server_address[1]
        thread = threading.Thread(target=server.serve_forever)
        thread.daemon = True
        thread.start()
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            url = f"http://127.0.0.1:{port}"
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} {url}
{url} {url}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert f"Health check failed for {url}" in result.stderr
E           assert 'Health check failed for http://127.0.0.1:57105' in 'su:
Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody',
'-s', '/bin/bash', '-c', "cd
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/... http://127.0.0.1:57105
http://127.0.0.1:57105 http://127.0.0.1:57105"], returncode=1, stdout='',
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:170: AssertionError
____________________ TestWSDETeam.test_get_primus_succeeds _____________________

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x119a50b30>

    @pytest.mark.fast
    def test_get_primus_succeeds(self):
        """Test getting the current Primus agent.

        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.primus_index = 1
        primus = self.team.get_primus()
>       assert primus == self.agent2
E       AssertionError: assert <MagicMock id='5075396240'> == <MagicMock
id='5075392352'>
E
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:108: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,463 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_team
2025-10-28 10:30:19,463 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_team
____________ TestWSDETeam.test_assign_roles_with_rotation_succeeds _____________

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x119a51eb0>

    @pytest.mark.fast
    def test_assign_roles_with_rotation_succeeds(self):
        """Test that role assignments change when the Primus rotates.

        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.add_agent(self.agent3)
        self.team.add_agent(self.agent4)
        self.team.primus_index = 0
        self.team.assign_roles()
        initial_roles = {
            self.agent1.name: self.agent1.current_role,
            self.agent2.name: self.agent2.current_role,
            self.agent3.name: self.agent3.current_role,
            self.agent4.name: self.agent4.current_role,
        }
        self.team.rotate_primus()
        self.team.assign_roles()
>       assert self.agent1.current_role != initial_roles[self.agent1.name]
E       AssertionError: assert 'Primus' != 'Primus'
E        +  where 'Primus' = <MagicMock id='5072488736'>.current_role
E        +    where <MagicMock id='5072488736'> =
<tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x119a51eb0>.agent1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:206: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,490 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_team
2025-10-28 10:30:19,490 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_team
2025-10-28 10:30:19,490 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_team
2025-10-28 10:30:19,491 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_team
2025-10-28 10:30:19,492 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': 'agent3', 'designer': 'agent4', 'evaluator': None}
2025-10-28 10:30:19,493 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': 'agent3', 'designer': 'agent4', 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor':
'agent3', 'designer': 'agent4', 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor':
'agent3', 'designer': 'agent4', 'evaluator': None}
_ TestWSDETeam.test_apply_dialectical_reasoning_with_knowledge_graph_succeeds __

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x119a52390>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_with_knowledge_graph_succeeds(self):
        """Test applying dialectical reasoning with knowledge graph integration.

        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        task = {
            "id": "task1",
            "type": "code_generation",
            "description": "Implement a secure authentication system",
            "requirements": ["user authentication", "password security"],
        }
        solution = {
            "id": "solution1",
            "agent": "agent1",
            "content": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        self.team.solutions = {task["id"]: [solution]}
        mock_wsde_memory = MagicMock()
        mock_wsde_memory.query_knowledge_for_task.return_value = [
            {"concept": "authentication", "relevance": 0.9},
            {"concept": "password", "relevance": 0.8},
            {"concept": "security", "relevance": 0.7},
        ]
        mock_wsde_memory.query_concept_relationships.return_value = [
            {"relationship": "requires", "direction": "outgoing"}
        ]
>       result = self.team.apply_dialectical_reasoning_with_knowledge_graph(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            task, self.critic_agent, mock_wsde_memory
        )
E       AttributeError: 'WSDETeam' object has no attribute
'apply_dialectical_reasoning_with_knowledge_graph'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:247: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,501 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_team
2025-10-28 10:30:19,501 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_team
____________________ TestWSDE.test_initialization_succeeds _____________________

self = <tests.unit.domain.models.test_wsde.TestWSDE object at 0x119a524b0>

    @pytest.mark.fast
    def test_initialization_succeeds(self):
        """Test that a WSDE is initialized correctly.

        ReqID: N/A"""
>       wsde = WSDE(name="Test WSDE")
               ^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: WSDE.__init__() got an unexpected keyword argument 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:278: TypeError
_____________ TestWSDE.test_initialization_with_metadata_succeeds ______________

self = <tests.unit.domain.models.test_wsde.TestWSDE object at 0x119a528d0>

    @pytest.mark.fast
    def test_initialization_with_metadata_succeeds(self):
        """Test that a WSDE is initialized correctly with metadata.

        ReqID: N/A"""
        metadata = {"key": "value", "another_key": 123}
>       wsde = WSDE(name="Test WSDE", metadata=metadata)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: WSDE.__init__() got an unexpected keyword argument 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:291: TypeError
__________ test_apply_dialectical_reasoning_invokes_hooks_and_memory ___________

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_invokes_hooks_and_memory() -> None:
        """End-to-end reasoning returns typed artefacts and integrates hooks."""

        team = WSDETeam(name="workflow-regression")
        task_factory = DialecticalTaskFactory()
        task = task_factory.build(
            solution={
                "content": "Short solution lacking examples.",
                "code": "print('hello world')",
            }
        )

        critic = SimpleNamespace(name="critic-agent")
        memory = MemoryRecorder()
        hook_calls: list[tuple[DialecticalTask, tuple[DialecticalSequence,
...]]] = []

        def hook(task_payload: DialecticalTask, sequences:
tuple[DialecticalSequence, ...]):
            hook_calls.append((task_payload, sequences))

        team.register_dialectical_hook(hook)

        result = team.apply_dialectical_reasoning(task, critic,
memory_integration=memory)

        assert isinstance(result, DialecticalSequence)
        serialized = result.to_dict()
        assert serialized["status"] == "completed"
        assert serialized["synthesis"]

        assert hook_calls, "Hook should be invoked with the generated sequence"
        hook_task, hook_sequences = hook_calls[0]
        assert isinstance(hook_task, DialecticalTask)
        assert hook_task.identifier == task.identifier
>       assert hook_sequences[0] is result
E       AssertionError: assert {'antithesis': {'agent': 'critic-agent',
'alternative_approaches': ['Consider a more structured format with sections',...
logging'], ...}, 'id': '343dcb4f-4647-429b-bb16-bb530e2c8c8e', 'metadata': {},
'method': 'dialectical_reasoning', ...} is
DialecticalSequence(sequence_id='9ddac139-6405-4206-a04f-4f75cb477b80',
steps=(DialecticalStep(step_id='343dcb4f-4647-... handling', 'Implement a more
modular design with smaller functions')),), status='completed', reason=None,
metadata={})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_workflow.py:60: AssertionError
__ TestWSDERoleReassignment.test_build_consensus_multiple_solutions_succeeds ___

self =
<tests.unit.domain.models.test_wsde_dynamic_workflows.TestWSDERoleReassignment
object at 0x119a75400>

    def test_build_consensus_multiple_solutions_succeeds(self):
        """Test that build consensus multiple solutions succeeds.

        ReqID: N/A"""
        task = {"id": "t1", "description": "demo"}
        self.team.add_solution(task, {"agent": "code", "content": "a"})
        self.team.add_solution(task, {"agent": "test", "content": "b"})
        consensus = self.team.build_consensus(task)
>       assert consensus["consensus"] != ""
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'consensus'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dynamic_workflows.py:43: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_core - INFO - Added agent
code to team test_dynamic_workflows_team
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team test_dynamic_workflows_team
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_core - INFO - Added agent
test to team test_dynamic_workflows_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent code
to team test_dynamic_workflows_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team test_dynamic_workflows_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent test
to team test_dynamic_workflows_team
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task t1
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task t1
2025-10-28 10:30:19,621 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task t1
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task t1
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
_______________ test_check_security_best_practices_detects_issue _______________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at
0x12e7e0d60>

    @pytest.mark.fast
    def test_check_security_best_practices_detects_issue(wsde_team_factory):
        """ReqID: WSDE-SECURITY-01 — flags insecure patterns for escalation."""

        team = wsde_team_factory()
        insecure_code = "password = 'secret'\nexec('print(1)')\n"
>       assert team._check_security_best_practices(insecure_code) is False
E       assert {'compliance_level': 'medium', 'issues': ['Hardcoded credentials
detected'], 'suggestions': ['Use environment variable..., 'Use parameterized
queries to prevent SQL injection', 'Validate all user input and implement proper
error handling']} is False
E        +  where {'compliance_level': 'medium', 'issues': ['Hardcoded
credentials detected'], 'suggestions': ['Use environment variable..., 'Use
parameterized queries to prevent SQL injection', 'Validate all user input and
implement proper error handling']} = _check_security_best_practices("password =
'secret'\nexec('print(1)')\n")
E        +    where _check_security_best_practices =
<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12e8c1190>._check_security_best_practices

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:12: AssertionError
____________ test_check_security_best_practices_accepts_clean_code _____________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at
0x12e7e2b60>

    @pytest.mark.fast
    def
test_check_security_best_practices_accepts_clean_code(wsde_team_factory):
        """ReqID: WSDE-SECURITY-02 — passes checklist when code avoids red
flags."""

        team = wsde_team_factory()
        secure_code = (
            "def process_items(items):\n"
            "    processed_items = []\n"
            "    for element in items:\n"
            "        processed_items.append(element)\n"
            "    return processed_items\n"
        )
>       assert team._check_security_best_practices(secure_code) is True
E       AssertionError: assert {'compliance_level': 'high', 'issues': [],
'suggestions': []} is True
E        +  where {'compliance_level': 'high', 'issues': [], 'suggestions': []}
= _check_security_best_practices('def process_items(items):\n    processed_items
= []\n    for element in items:\n        processed_items.append(element)\n
return processed_items\n')
E        +    where _check_security_best_practices =
<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12e8d2f90>._check_security_best_practices

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:27: AssertionError
_______________ test_balance_security_and_performance_idempotent _______________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at
0x12e7e1440>

    @pytest.mark.fast
    def test_balance_security_and_performance_idempotent(wsde_team_factory):
        """ReqID: WSDE-SECURITY-03 — avoids duplicating checklist
annotations."""

        team = wsde_team_factory()
        code = "def run():\n    return True"

        balanced_once = team._balance_security_and_performance(code)
        balanced_twice = team._balance_security_and_performance(balanced_once)

        assert balanced_once == balanced_twice
>       assert balanced_once.count("Security and performance balance") == 1
               ^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:41: AttributeError
__________________ test_role_assignment_uses_expertise_scores __________________

    @pytest.mark.fast
    def test_role_assignment_uses_expertise_scores():
        agents = [
            DummyAgent(name="lead", expertise=["leadership", "coordination"]),
            DummyAgent(name="builder", expertise=["development", "testing"]),
            DummyAgent(name="designer", expertise=["architecture", "planning"]),
            DummyAgent(name="reviewer", expertise=["evaluation", "analysis"]),
        ]
        team = _bind_team(WSDETeam(name="roles", agents=agents))

        assignments = team.assign_roles()
        primus = assignments.as_name_mapping()["primus"]
        assert primus is not None and primus.name == "lead"
>       role_map = team.get_role_map()
                   ^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'get_role_map'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_strategies.py:88: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,819 - devsynth.domain.models.wsde_core - INFO - Added agent
lead to team roles
2025-10-28 10:30:19,819 - devsynth.domain.models.wsde_core - INFO - Added agent
builder to team roles
2025-10-28 10:30:19,819 - devsynth.domain.models.wsde_core - INFO - Added agent
designer to team roles
2025-10-28 10:30:19,819 - devsynth.domain.models.wsde_core - INFO - Added agent
reviewer to team roles
2025-10-28 10:30:19,819 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team roles: {'primus': 'lead', 'worker': 'builder',
'supervisor': 'designer', 'designer': 'reviewer', 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent lead
to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
builder to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
designer to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
reviewer to team roles
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team roles: {'primus': 'lead', 'worker': 'builder', 'supervisor':
'designer', 'designer': 'reviewer', 'evaluator': None}
____________________ TestWSDETeam.test_get_primus_succeeds _____________________

self = <tests.unit.domain.models.test_wsde_team.TestWSDETeam object at
0x119aca300>

    @pytest.mark.fast
    def test_get_primus_succeeds(self):
        """Test getting the current Primus agent.

        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.primus_index = 1
        primus = self.team.get_primus()
>       assert primus == self.agent2
E       AssertionError: assert <MagicMock id='5080552304'> == <MagicMock
id='5080556048'>
E
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_team.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,858 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_wsde_team
2025-10-28 10:30:19,858 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_wsde_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_wsde_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_wsde_team
_______ TestWSDETeam.test_analyze_trade_offs_detects_conflicts_succeeds ________

self = <tests.unit.domain.models.test_wsde_team.TestWSDETeam object at
0x119acb1a0>

    @pytest.mark.fast
    def test_analyze_trade_offs_detects_conflicts_succeeds(self):
        """Trade-off analysis should flag options with similar scores as
conflicts.

        ReqID: N/A"""
        evaluated = [
            {"id": 1, "score": 0.8},
            {"id": 2, "score": 0.78},
            {"id": 3, "score": 0.2},
        ]
        trade_offs = self.team.analyze_trade_offs(
            evaluated, conflict_detection_threshold=0.7
        )
>       opt1 = next(o for o in trade_offs if o["id"] == 1)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       StopIteration

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_team.py:134: StopIteration
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:19,892 - devsynth.domain.models.wsde_decision_making - INFO -
Analyzing trade-offs between options
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Analyzing trade-offs between options
_________________ test_add_solution_appends_and_triggers_hooks _________________

    def test_add_solution_appends_and_triggers_hooks():
        hook = MagicMock()
        team = SimpleNamespace(solutions={}, dialectical_hooks=[hook])
        task = {"id": "t1"}
        solution = {"content": "data"}
>       result = add_solution(team, task, solution)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_utils.py:107:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

team = namespace(solutions={}, dialectical_hooks=[<MagicMock id='5080361696'>])
task = {'id': 't1'}, solution = {'content': 'data'}

    def add_solution(
        team: Any, task: TaskPayload, solution: SolutionRecord
    ) -> SolutionRecord:
        """Add a solution to the team and trigger dialectical hooks."""
        task_id = task.get("id")
        if not task_id:
            task_id = str(uuid4())
            task["id"] = task_id
        else:
            # Normalise the identifier on the task payload so future calls reuse
it.
            task["id"] = task_id

>       team.solutions.add(task_id, solution)
        ^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'add'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_utils.py:236: AttributeError
_______________ test_calculate_expertise_score_multiple_matches ________________

    @pytest.mark.fast
    def test_calculate_expertise_score_multiple_matches():
        """Test that calculate expertise score multiple matches.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeExpertiseScoreTeam")
        agent = DummyAgent(["python", "documentation"])
        team.add_agent(agent)
        task = {"language": "python", "description": "generate documentation"}
        score = team._calculate_expertise_score(agent, task)
>       assert score > 1
E       assert 1 > 1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_expertise_score.py:25: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,012 - devsynth.domain.models.wsde_core - INFO - Added agent
agent to team TestWsdeExpertiseScoreTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent
to team TestWsdeExpertiseScoreTeam
____________ test_summarize_voting_result_reports_winner_and_counts ____________

    @pytest.mark.fast
    def test_summarize_voting_result_reports_winner_and_counts():
        team = WSDETeam(name="facade")
        team.add_agents([SimpleAgent("a"), SimpleAgent("b")])
        voting_result = {
            "status": "completed",
            "result": {
                "method": "majority",
                "winner": "A",
                "tie_broken": True,
                "tie_breaker_method": "primus",
            },
            "vote_counts": {"A": 2, "B": 1},
            "vote_weights": {"a": 1.0, "b": 1.5},
        }
        summary = team.summarize_voting_result(voting_result)
>       assert "Voting was completed using majority." in summary
E       AssertionError: assert 'Voting was completed using majority.' in 'Vote
distribution: A: 2 votes, B: 1 votes\nVote weights: a: 1.00, b: 1.50'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade.py:50: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,022 - devsynth.domain.models.wsde_core - INFO - Added agent
a to team facade
2025-10-28 10:30:20,022 - devsynth.domain.models.wsde_core - INFO - Added agent
b to team facade
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a to
team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent b to
team facade
__________________ test_select_primus_updates_index_and_role ___________________

    @pytest.mark.fast
    def test_select_primus_updates_index_and_role():
        team = WSDETeam(name="facade")
        doc = SimpleAgent("doc", ["documentation"])
        coder = SimpleAgent("coder", ["python"])
        team.add_agents([doc, coder])
        team.select_primus_by_expertise({"type": "coding", "language":
"python"})
>       assert team.get_primus() is coder
E       assert <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at
0x12eda0a40> is <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at
0x12eda1df0>
E        +  where <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object
at 0x12eda0a40> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12edc4a70>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade_roles.py:23: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,029 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team facade
2025-10-28 10:30:20,029 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team facade
2025-10-28 10:30:20,029 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team facade
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
________________ test_dynamic_role_reassignment_rotates_primus _________________

    @pytest.mark.fast
    def test_dynamic_role_reassignment_rotates_primus():
        team = WSDETeam(name="facade")
        doc = SimpleAgent("doc", ["documentation"])
        coder = SimpleAgent("coder", ["python"])
        tester = SimpleAgent("tester", ["testing"])
        team.add_agents([doc, coder, tester])
        team.dynamic_role_reassignment({"type": "documentation"})
        first = team.get_primus()
        team.dynamic_role_reassignment({"type": "coding", "language": "python"})
        second = team.get_primus()
>       assert first is doc
E       assert <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at
0x12ed9e870> is <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at
0x12ed9e8d0>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade_roles.py:39: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team facade
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team facade
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team facade
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_roles - INFO - Selected
tester as primus based on expertise
2025-10-28 10:30:20,035 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team facade
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
_________ test_documentation_tasks_pick_documentation_experts_succeeds _________

    @pytest.mark.fast
    def test_documentation_tasks_pick_documentation_experts_succeeds():
        """Test that documentation tasks pick documentation experts succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePhaseRoleRotationTeam")
        coder = _agent("coder", ["python"])
        doc = _agent("doc", ["documentation", "markdown"])
        team.add_agents([coder, doc])
        team.select_primus_by_expertise({"type": "documentation"})
>       assert team.get_primus() is doc
E       AssertionError: assert <MagicMock id='5081381296'> is <MagicMock
id='5081377408'>
E        +  where <MagicMock id='5081381296'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12edfba40>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_phase_role_rotation.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,066 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdePhaseRoleRotationTeam
2025-10-28 10:30:20,066 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdePhaseRoleRotationTeam
2025-10-28 10:30:20,066 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdePhaseRoleRotationTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdePhaseRoleRotationTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
_____________ test_current_primus_considered_in_selection_succeeds _____________

    @pytest.mark.fast
    def test_current_primus_considered_in_selection_succeeds():
        """Current primus remains candidate when reselection occurs.

        This prevents ``select_primus_by_expertise`` from skipping the
        existing primus simply because they've served once."""

        team = WSDETeam(name="PrimusReselectionTeam")
        py = _agent("py", ["python"])
        js = _agent("js", ["javascript"])
        team.add_agents([py, js])

        team.assign_roles()
        team.select_primus_by_expertise({"language": "python"})

>       assert team.get_primus() is py
E       AssertionError: assert <MagicMock id='5081272336'> is <MagicMock
id='5080847312'>
E        +  where <MagicMock id='5081272336'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ee1ffb0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:69: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,102 - devsynth.domain.models.wsde_core - INFO - Added agent
py to team PrimusReselectionTeam
2025-10-28 10:30:20,102 - devsynth.domain.models.wsde_core - INFO - Added agent
js to team PrimusReselectionTeam
2025-10-28 10:30:20,102 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team PrimusReselectionTeam: {'primus': 'py', 'worker': 'js',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:30:20,102 - devsynth.domain.models.wsde_roles - INFO - Selected js
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to
team PrimusReselectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent js to
team PrimusReselectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team PrimusReselectionTeam: {'primus': 'py', 'worker': 'js', 'supervisor':
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected js as
primus based on expertise
_____________ test_documentation_tasks_prefer_doc_experts_succeeds _____________

    @pytest.mark.fast
    def test_documentation_tasks_prefer_doc_experts_succeeds():
        """Test that documentation tasks prefer doc experts succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePrimusSelectionTeam")
        coder = _agent("coder", ["python"])
        doc = _agent("doc", ["documentation", "markdown"])
        team.add_agents([coder, doc])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc
E       AssertionError: assert <MagicMock id='5081001632'> is <MagicMock
id='5080997072'>
E        +  where <MagicMock id='5081001632'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ee1f530>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,113 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,113 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,113 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
_______________ test_nested_task_metadata_is_flattened_succeeds ________________

    @pytest.mark.fast
    def test_nested_task_metadata_is_flattened_succeeds():
        """Test that nested task metadata is flattened succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePrimusSelectionTeam")
        py = _agent("py", ["python"])
        doc = _agent("doc", ["documentation"])
        team.add_agents([doc, py])
        task = {"context": {"info": [{"language": "python"}]}}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is py
E       AssertionError: assert <MagicMock id='5081011632'> is <MagicMock
id='5081009184'>
E        +  where <MagicMock id='5081011632'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ee1f4a0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:98: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,126 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,126 - devsynth.domain.models.wsde_core - INFO - Added agent
py to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,126 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
______________ test_select_primus_by_expertise_coverage_succeeds _______________

    @pytest.mark.fast
    def test_select_primus_by_expertise_coverage_succeeds():
        """Test that select primus by expertise coverage succeeds.

        ReqID: N/A"""
        import inspect
        import os
        from types import SimpleNamespace

        import coverage

        import devsynth.domain.models.wsde_facade as wsde

        team = wsde.WSDETeam(name="TestWsdePrimusSelectionTeam")
        cov = coverage.Coverage()
        cov.start()
        team.select_primus_by_expertise({})
        team.add_agent(
            SimpleNamespace(
                name="a1", expertise=["python"], current_role=None,
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a2",
                config=SimpleNamespace(
                    parameters={"expertise": ["doc_generation", "markdown"]}
                ),
                current_role=None,
                has_been_primus=False,
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a3", expertise=["testing"], current_role=None,
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a4", expertise=["security"], current_role=None,
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a5",
                expertise=["javascript"],
                current_role=None,
                has_been_primus=False,
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a6", expertise=["api"], current_role=None,
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a7", expertise=["design"], current_role=None,
has_been_primus=False
            )
        )
        team.select_primus_by_expertise(
            {"type": "documentation", "details": [1, 2], "extra": {"foo":
"bar"}}
        )
        team.select_primus_by_expertise({"language": "python"})
        team.select_primus_by_expertise({"type": "testing"})
        team.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "security"})
        team.select_primus_by_expertise({"type": "frontend"})
        team.select_primus_by_expertise({"type": "backend"})
        team.select_primus_by_expertise({"type": "design"})
        cov.stop()
        path = inspect.getsourcefile(wsde.WSDETeam.select_primus_by_expertise)
        lines, start =
inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)
        executable = []
        skip = False
        for i, line in enumerate(lines, start):
            stripped = line.strip()
            if stripped.startswith('"""'):
                if (
                    stripped.count('"""') == 2
                    and stripped.endswith('""')
                    and stripped != '"""'
                ):
                    continue
                skip = not skip
                continue
            if skip:
                if stripped.endswith('"""'):
                    skip = False
                continue
            if stripped:
                executable.append(i)
        executed = set(cov.get_data().lines(path))
        coverage_percent = len(set(executable) & executed) / len(executable) *
100
>       assert coverage_percent >= 30
E       assert 25.0 >= 30

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:210: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,174 - devsynth.domain.models.wsde_roles - WARNING - Cannot
select primus: no agents in team
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a4 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a5 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a6 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,175 - devsynth.domain.models.wsde_core - INFO - Added agent
a7 to team TestWsdePrimusSelectionTeam
2025-10-28 10:30:20,176 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
2025-10-28 10:30:20,176 - devsynth.domain.models.wsde_roles - INFO - Selected a2
as primus based on expertise
2025-10-28 10:30:20,176 - devsynth.domain.models.wsde_roles - INFO - Selected a3
as primus based on expertise
2025-10-28 10:30:20,176 - devsynth.domain.models.wsde_roles - INFO - Selected a4
as primus based on expertise
2025-10-28 10:30:20,177 - devsynth.domain.models.wsde_roles - INFO - Selected a5
as primus based on expertise
2025-10-28 10:30:20,177 - devsynth.domain.models.wsde_roles - INFO - Selected a6
as primus based on expertise
2025-10-28 10:30:20,177 - devsynth.domain.models.wsde_roles - INFO - Selected a7
as primus based on expertise
2025-10-28 10:30:20,177 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select
primus: no agents in team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a4 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a5 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a6 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a7 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a2 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a3 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a4 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a5 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a6 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a7 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
________ test_vote_on_critical_decision_tie_triggers_consensus_succeeds ________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12ed9f590>, <MagicMock id='5081003744'>, <MagicMock id='5081005520'>,
<MagicMock id='5081117280'>)

    @pytest.mark.fast
    def
test_vote_on_critical_decision_tie_triggers_consensus_succeeds(team_with_agents)
:
        """Test that vote on critical decision tie triggers consensus succeeds.

        ReqID: N/A"""
        team, doc, coder, _ = team_with_agents
        doc.process.return_value = {"vote": "A"}
        coder.process.return_value = {"vote": "B"}
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": [{"id": "A"}, {"id": "B"}],
        }
        with (
            patch.object(team, "get_primus", return_value=None),
            patch.object(team, "build_consensus", return_value={"consensus":
"AB"}) as bc,
        ):
            result = team.vote_on_critical_decision(task)
>           assert result["voting_initiated"]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:56: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:20,195 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,195 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,195 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
___________ test_vote_on_critical_decision_weighted_voting_succeeds ____________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12edf9d30>, <MagicMock id='5081374240'>, <MagicMock id='5081378992'>,
<MagicMock id='5081367328'>)

    @pytest.mark.fast
    def
test_vote_on_critical_decision_weighted_voting_succeeds(team_with_agents):
        """Test that vote on critical decision weighted voting succeeds.

        ReqID: N/A"""
        team, doc, coder, tester = team_with_agents
        for agent, level in [(doc, "expert"), (coder, "novice"), (tester,
"novice")]:
            cfg = MagicMock()
            cfg.name = agent.name
            cfg.parameters = {"expertise": agent.expertise, "expertise_level":
level}
            agent.config = cfg
        doc.process.return_value = {"vote": "A"}
        coder.process.return_value = {"vote": "B"}
        tester.process.return_value = {"vote": "B"}
        task = {
            "type": "critical_decision",
            "domain": "documentation",
            "is_critical": True,
            "options": [{"id": "A"}, {"id": "B"}],
        }
        result = team.vote_on_critical_decision(task)
        assert result["result"]["winner"] == "A"
>       assert result["result"]["method"] == "weighted_vote"
E       AssertionError: assert 'majority_vote' == 'weighted_vote'
E
E         - weighted_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:84: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:20,207 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,207 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,207 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
______________ test_build_consensus_multiple_and_single_succeeds _______________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12ee79700>, <MagicMock id='5081896944'>, <MagicMock id='5081125312'>,
<MagicMock id='5081904432'>)

    @pytest.mark.fast
    def test_build_consensus_multiple_and_single_succeeds(team_with_agents):
        """Test that build consensus multiple and single succeeds.

        ReqID: N/A"""
        team, doc, coder, _ = team_with_agents
        task = {"id": "t1", "description": "demo"}
        team.add_solution(task, {"agent": doc.name, "content": "First"})
        single = team.build_consensus(task)
>       assert single["method"] == "single_solution"
               ^^^^^^^^^^^^^^^^
E       KeyError: 'method'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:96: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:20,217 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,217 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,217 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,218 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task t1
2025-10-28 10:30:20,218 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task t1
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
__________ test_documentation_task_selects_unused_doc_agent_succeeds ___________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12ee88500>, <MagicMock id='5081957968'>, <MagicMock id='5081961760'>,
<MagicMock id='5081965504'>)

    @pytest.mark.fast
    def
test_documentation_task_selects_unused_doc_agent_succeeds(team_with_agents):
        """Test that documentation task selects unused doc agent succeeds.

        ReqID: N/A"""
        team, doc, coder, tester = team_with_agents
        team.select_primus_by_expertise({"type": "coding", "language":
"python"})
>       assert team.get_primus() is coder
E       AssertionError: assert <MagicMock id='5081957968'> is <MagicMock
id='5081961760'>
E        +  where <MagicMock id='5081957968'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ee88500>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:111: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:20,234 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,234 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,234 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,235 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
_______________ test_vote_on_critical_decision_coverage_succeeds _______________

    @pytest.mark.fast
    def test_vote_on_critical_decision_coverage_succeeds():
        """Test that vote on critical decision coverage succeeds.

        ReqID: N/A"""
        import inspect
        from types import SimpleNamespace

        import coverage

        import devsynth.domain.models.wsde_facade as wsde

        team = wsde.WSDETeam(name="TestWsdeTeamTeam")
        a1 = SimpleNamespace(
            name="a1",
            config=SimpleNamespace(
                name="a1", parameters={"expertise": ["python"],
"expertise_level": "expert"}
            ),
            process=lambda t: {"vote": "A"},
        )
        a2 = SimpleNamespace(
            name="a2",
            config=SimpleNamespace(
                name="a2", parameters={"expertise": ["docs"], "expertise_level":
"novice"}
            ),
            process=lambda t: {"vote": "B"},
        )
        a3 = SimpleNamespace(
            name="a3",
            config=SimpleNamespace(
                name="a3", parameters={"expertise": ["python"],
"expertise_level": "novice"}
            ),
            process=lambda t: {"vote": "A"},
        )
        team.add_agents([a1, a2, a3])
        cov = coverage.Coverage()
        cov.start()
        team.vote_on_critical_decision({"type": "other"})
        team.vote_on_critical_decision(
            {"type": "critical_decision", "is_critical": True, "options": []}
        )
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "domain": "python",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        a3.process = lambda t: {"vote": "B"}
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        path = wsde.__file__
        lines, start =
inspect.getsourcelines(wsde.WSDETeam.vote_on_critical_decision)
        executable = list(range(start, start + len(lines)))
>       executed = set(cov.get_data().lines(path))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:253: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,285 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestWsdeTeamTeam
2025-10-28 10:30:20,285 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestWsdeTeamTeam
2025-10-28 10:30:20,285 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team TestWsdeTeamTeam
2025-10-28 10:30:20,312 - devsynth.domain.models.wsde_voting - WARNING - Cannot
conduct vote: no options provided
2025-10-28 10:30:20,312 - devsynth.domain.models.wsde_voting - WARNING - Cannot
conduct vote: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestWsdeTeamTeam
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct
vote: no options provided
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct
vote: no options provided
_____________ test_expertise_selection_and_flag_rotation_succeeds ______________

    @pytest.mark.fast
    def test_expertise_selection_and_flag_rotation_succeeds():
        """Test that expertise selection and flag rotation succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeTeamTeam")
        doc = MagicMock()
        doc.name = "doc"
        doc.expertise = ["documentation"]
        coder = MagicMock()
        coder.name = "coder"
        coder.expertise = ["python"]
        tester = MagicMock()
        tester.name = "tester"
        tester.expertise = ["testing"]
        team.add_agents([doc, coder, tester])
        tasks = [
            {"type": "coding", "language": "python"},
            {"type": "documentation"},
            {"type": "testing"},
        ]
        expected = [coder, doc, tester]
        for task, agent in zip(tasks, expected):
            team.select_primus_by_expertise(task)
>           assert team.get_primus() is agent
E           AssertionError: assert <MagicMock id='5082276080'> is <MagicMock
id='5082268496'>
E            +  where <MagicMock id='5082276080'> = get_primus()
E            +    where get_primus =
<devsynth.domain.models.wsde_facade.WSDETeam object at 0x12eee69f0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:301: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,345 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,345 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,345 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
2025-10-28 10:30:20,346 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
_____________________ test_select_primus_coverage_succeeds _____________________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12eee1730>, <MagicMock id='5082322448'>, <MagicMock id='5082328880'>,
<MagicMock id='5082197760'>)

    @pytest.mark.fast
    def test_select_primus_coverage_succeeds(team_with_agents):
        """Ensure select_primus_by_expertise maintains >80% coverage.

        ReqID: N/A"""
        import inspect

        import coverage

        import devsynth.domain.models.wsde_facade as wsde

        team, doc, coder, tester = team_with_agents
        cov = coverage.Coverage()
        cov.start()
        empty = wsde.WSDETeam(name="TestWsdeTeamTeam")
        empty.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "coding", "language":
"python"})
        team.select_primus_by_expertise({"type": "testing"})
        team.select_primus_by_expertise({"type": "documentation"})
        path = wsde.__file__
        lines, start =
inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)
        executable = list(range(start, start + len(lines)))
>       executed = set(cov.get_data().lines(path))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:334: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:20,355 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestWsdeTeamTeam
2025-10-28 10:30:20,355 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestWsdeTeamTeam
2025-10-28 10:30:20,355 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,386 - devsynth.domain.models.wsde_roles - WARNING - Cannot
select primus: no agents in team
2025-10-28 10:30:20,386 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
2025-10-28 10:30:20,387 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
2025-10-28 10:30:20,387 - devsynth.domain.models.wsde_roles - INFO - Selected
tester as primus based on expertise
2025-10-28 10:30:20,387 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select
primus: no agents in team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
_________________________ test_majority_voting_simple __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee6a50>

    @pytest.mark.fast
    def test_majority_voting_simple(monkeypatch):
        agents = [DummyAgent("a1", ["a"]), DummyAgent("a2", ["b"]),
DummyAgent("a3", ["a"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda x: x[0])
        task = {"options": ["option_a", "option_b"], "voting_method":
"majority"}
        result = team.vote_on_critical_decision(task)
        assert result["status"] == "completed"
>       assert result["result"] == "option_a"
E       AssertionError: assert {'method': 'majority_vote', 'winner': 'option_a'}
== 'option_a'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:46: AssertionError
_____________________ test_handle_tied_vote_primus_breaks ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee65a0>

    @pytest.mark.fast
    def test_handle_tied_vote_primus_breaks(monkeypatch):
        agents = [DummyAgent("primus", ["x"]), DummyAgent("other", ["y"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        voting_result = {
            "votes": {"primus": "A", "other": "B"},
            "options": ["A", "B"],
            "status": "pending",
        }
        vote_counts = {"A": 1, "B": 1}
        res = team._handle_tied_vote(
            {"options": ["A", "B"]}, voting_result, vote_counts, ["A", "B"]
        )
>       assert res["result"] == "A"
E       assert {'consensus_result': {'explanation': "Partial consensus on option
'A' with 0.0% support after 3 rounds of discussion."...nces': {'other': {'A':
0.0, 'B': 0.0}, 'primus': {'A': 0.0, 'B': 0.0}}, ...}, 'tied': True,
'tied_options': ['A', 'B']} == 'A'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:62: AssertionError
__________________ test_weighted_voting_tie_primus_resolution __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee7560>

    @pytest.mark.fast
    def test_weighted_voting_tie_primus_resolution(monkeypatch):
        a1 = DummyAgent("p", ["frontend"])
        a2 = DummyAgent("s", ["frontend"])
        team = bind(DummyTeam([a1, a2], primus=a1))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {
            "options": ["frontend", "backend"],
            "voting_method": "weighted",
            "domain": "frontend",
        }
        # Force votes to create a tie
        voting_result = {
            "options": ["frontend", "backend"],
            "votes": {"p": "frontend", "s": "backend"},
            "method": "weighted",
            "status": "pending",
        }
        vote_counts = {"frontend": 1, "backend": 1}
        res = team._handle_tied_vote(
            task, voting_result, vote_counts, ["frontend", "backend"]
        )
>       assert res["result"] == "frontend"
E       assert {'consensus_result': {'explanation': "Consensus reached on option
'frontend' with 100.0% support after 1 rounds of dis...'frontend': 1.0}, 's':
{'backend': 0.0, 'frontend': 1.0}}, ...}, 'tied': True, 'tied_options':
['frontend', 'backend']} == 'frontend'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:88: AssertionError
___________________ test_vote_on_critical_decision_majority ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee6960>

    @pytest.mark.fast
    def test_vote_on_critical_decision_majority(monkeypatch):
        agents = [
            DummyAgent("a1", ["opt1"]),
            DummyAgent("a2", ["opt1"]),
            DummyAgent("a3", ["opt2"]),
        ]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {"id": "t", "options": ["opt1", "opt2"], "voting_method":
"majority"}
        result = team.vote_on_critical_decision(task)
>       assert result["result"] == "opt1"
E       AssertionError: assert {'method': 'majority_vote', 'winner': 'opt1'} ==
'opt1'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:102: AssertionError
___________________ test_vote_on_critical_decision_weighted ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eee7da0>

    @pytest.mark.fast
    def test_vote_on_critical_decision_weighted(monkeypatch):
        a1 = DummyAgent("a1", ["frontend"])
        a2 = DummyAgent("a2", ["backend"])
        team = bind(DummyTeam([a1, a2], primus=a1))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {
            "id": "t2",
            "options": ["frontend", "backend"],
            "voting_method": "weighted",
            "domain": "frontend",
        }
        result = team.vote_on_critical_decision(task)
        assert result["status"] == "completed"
>       assert result["result"] == "frontend"
E       AssertionError: assert {'method': 'weighted_vote', 'winner': 'frontend'}
== 'frontend'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:120: AssertionError
______________________ test_apply_majority_voting_no_tie _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee99cd0>

    @pytest.mark.fast
    def test_apply_majority_voting_no_tie(monkeypatch):
        agents = [DummyAgent("a1"), DummyAgent("a2")]
        team = bind(DummyTeam(agents, primus=agents[0]))
        voting_result = {
            "options": ["X", "Y"],
            "votes": {"a1": "X", "a2": "X"},
            "vote_counts": {"X": 2, "Y": 0},
            "status": "pending",
        }
        res = team._apply_majority_voting({}, voting_result)
>       assert res["result"] == "X"
               ^^^^^^^^^^^^^
E       KeyError: 'result'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:134: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:20,476 - devsynth.domain.models.wsde_voting - WARNING - Cannot
conduct vote: no options provided
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct
vote: no options provided
_____________________________ test_consensus_vote ______________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee9b800>

    @pytest.mark.fast
    def test_consensus_vote(monkeypatch):
        agents = [DummyAgent("a1", ["front"]), DummyAgent("a2", ["front"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {"options": ["front", "back"]}
        res = wsde_voting.consensus_vote(team, task)
>       assert res["decision"] in {"front", "back"}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: unhashable type: 'dict'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:145: TypeError
________________________ test_health_endpoint_succeeds _________________________

api_token_env = None

    @pytest.mark.fast
    def test_health_endpoint_succeeds(api_token_env):
        """Test that health endpoint succeeds.

        ReqID: N/A"""
>       resp = _get_client().get("/health", headers={"Authorization": "Bearer
test-token"})
               ^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:47:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _get_client():
        """Get the test client, initializing lazily."""
        global client
>       if client is None:
           ^^^^^^
E       NameError: name 'client' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:28: NameError
________________________ test_metrics_endpoint_succeeds ________________________

api_token_env = None

    @pytest.mark.fast
    def test_metrics_endpoint_succeeds(api_token_env):
        """Test that metrics endpoint succeeds.

        ReqID: N/A"""
>       resp = _get_client().get("/metrics", headers={"Authorization": "Bearer
test-token"})
               ^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:57:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _get_client():
        """Get the test client, initializing lazily."""
        global client
>       if client is None:
           ^^^^^^
E       NameError: name 'client' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:28: NameError
_________ test_skip_if_missing_backend_converts_find_spec_value_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12efbfb60>

    @pytest.mark.fast
    def
test_skip_if_missing_backend_converts_find_spec_value_error(monkeypatch):
        """ValueError from ``find_spec`` should result in a clean skip
marker."""

        from tests.fixtures import resources

        calls: list[tuple[str, str]] = []

        def raising_find_spec(_name: str):
            raise ValueError("bad spec")

        def fake_importorskip(name: str, *, reason: str) -> None:
            calls.append((name, reason))
            raise pytest.skip.Exception(reason)

        monkeypatch.setattr(resources.importlib.util, "find_spec",
raising_find_spec)
        monkeypatch.setattr(resources, "is_resource_available", lambda _: True)
        monkeypatch.setattr(resources.pytest, "importorskip", fake_importorskip)

        marks = resources.skip_if_missing_backend("faiss",
include_requires_resource=False)

        assert any(mark.name == "skip" for mark in marks)
>       assert calls and calls[0][0] == "faiss"
E       assert ([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_bac
kend_resource_flags.py:98: AssertionError
_________ TestChromaDBAdapter.test_store_and_retrieve_vector_succeeds __________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d85e80>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12eee1220>

    @pytest.mark.fast
    def test_store_and_retrieve_vector_succeeds(self, adapter):
        """Test storing and retrieving a vector.

        ReqID: N/A"""
        vector = MemoryVector(
            id="test-vector-1",
            content="This is a test vector",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"test": "metadata", "category": "test"},
        )
        vector_id = adapter.store_vector(vector)
        assert vector_id == "test-vector-1"
        retrieved_vector = adapter.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector.id
>       assert retrieved_vector.content == vector.content
E       AssertionError: assert None == 'This is a test vector'
E        +  where None = MemoryVector(id='test-vector-1', content=None,
embedding=<MagicMock
name='mock.get_collection().get().__getitem__().__getitem__()' id='5082383040'>,
metadata={}, created_at=datetime.datetime(2025, 10, 28, 10, 30, 24,
27818)).content
E        +  and   'This is a test vector' = MemoryVector(id='test-vector-1',
content='This is a test vector', embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
metadata={'test': 'metadata', 'category': 'test'},
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 27014)).content

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:70: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,026 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphq8mt1a_
2025-10-28 10:30:24,026 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphq8mt1a_
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,027 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-1 in ChromaDB
2025-10-28 10:30:24,027 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Retrieved vector with ID test-vector-1 from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Retrieved vector with ID test-vector-1 from ChromaDB
__________ TestChromaDBAdapter.test_store_vector_without_id_succeeds ___________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d86360>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12ed27320>

    @pytest.mark.fast
    def test_store_vector_without_id_succeeds(self, adapter):
        """Test storing a vector without an ID.

        ReqID: N/A"""
        vector = MemoryVector(
            id="",
            content="Vector without ID",
            embedding=[0.5, 0.4, 0.3, 0.2, 0.1],
            metadata={"test": "no-id"},
        )
        vector_id = adapter.store_vector(vector)
        assert vector_id is not None
        assert vector_id != ""
        retrieved_vector = adapter.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector_id
>       assert retrieved_vector.content == vector.content
E       AssertionError: assert None == 'Vector without ID'
E        +  where None = MemoryVector(id='49bea44d-59e5-4fdd-a5a9-d09c6fbe7631',
content=None, embedding=<MagicMock
name='mock.get_collection()...etitem__().__getitem__()' id='5081833472'>,
metadata={}, created_at=datetime.datetime(2025, 10, 28, 10, 30, 24,
37872)).content
E        +  and   'Vector without ID' =
MemoryVector(id='49bea44d-59e5-4fdd-a5a9-d09c6fbe7631', content='Vector without
ID', embedding=[0.5, 0.4, 0.3, 0.2, 0.1], metadata={'test': 'no-id'},
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 37066)).content

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:92: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,036 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp6iq9romq
2025-10-28 10:30:24,036 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp6iq9romq
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,037 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID 49bea44d-59e5-4fdd-a5a9-d09c6fbe7631 in ChromaDB
2025-10-28 10:30:24,037 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Retrieved vector with ID 49bea44d-59e5-4fdd-a5a9-d09c6fbe7631 from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID 49bea44d-59e5-4fdd-a5a9-d09c6fbe7631 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Retrieved vector with ID 49bea44d-59e5-4fdd-a5a9-d09c6fbe7631 from ChromaDB
_____________ TestChromaDBAdapter.test_similarity_search_succeeds ______________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d86840>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12efdc6b0>

    @pytest.mark.fast
    def test_similarity_search_succeeds(self, adapter):
        """Test similarity search functionality.

        ReqID: N/A"""
        vectors = [
            MemoryVector(
                id=f"test-vector-{i}",
                content=f"Test vector {i}",
                embedding=[(float(j) / 10) for j in range(i, i + 5)],
                metadata={"index": i},
            )
            for i in range(1, 6)
        ]
        for vector in vectors:
            adapter.store_vector(vector)
        query_embedding = [0.15, 0.25, 0.35, 0.45, 0.55]
        results = adapter.similarity_search(query_embedding, top_k=3)
>       assert len(results) == 3
E       assert 0 == 3
E        +  where 0 = len([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:113: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,045 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp0peypm9v
2025-10-28 10:30:24,045 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp0peypm9v
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-1 in ChromaDB
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-2 in ChromaDB
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-3 in ChromaDB
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-4 in ChromaDB
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-5 in ChromaDB
2025-10-28 10:30:24,046 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Found 0 similar vectors in ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-2 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-3 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-4 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-5 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Found 0
similar vectors in ChromaDB
_______________ TestChromaDBAdapter.test_delete_vector_succeeds ________________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d86d20>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12f00cc20>

    @pytest.mark.fast
    def test_delete_vector_succeeds(self, adapter):
        """Test deleting a vector.

        ReqID: N/A"""
        vector = MemoryVector(
            id="test-vector-delete",
            content="Vector to delete",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"test": "delete"},
        )
        adapter.store_vector(vector)
        result = adapter.delete_vector(vector.id)
        assert result is True
        retrieved_vector = adapter.retrieve_vector(vector.id)
>       assert retrieved_vector is None
E       AssertionError: assert MemoryVector(id='test-vector-delete',
content=None, embedding=<MagicMock
name='mock.get_collection().get().__getitem__().__getitem__()' id='5083442272'>,
metadata={}, created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 61615)) is
None

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:132: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,058 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpblh1wk38
2025-10-28 10:30:24,059 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpblh1wk38
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,060 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID test-vector-delete in ChromaDB
2025-10-28 10:30:24,061 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Deleted vector with ID test-vector-delete from ChromaDB
2025-10-28 10:30:24,061 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Retrieved vector with ID test-vector-delete from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID test-vector-delete in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted
vector with ID test-vector-delete from ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Retrieved vector with ID test-vector-delete from ChromaDB
_________ TestChromaDBAdapter.test_delete_nonexistent_vector_succeeds __________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d87200>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12ee788f0>

    @pytest.mark.fast
    def test_delete_nonexistent_vector_succeeds(self, adapter):
        """Test deleting a vector that doesn't exist.

        ReqID: N/A"""
        result = adapter.delete_vector("nonexistent-vector")
>       assert result is False
E       assert True is False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:140: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,069 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnza4k4w4
2025-10-28 10:30:24,070 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnza4k4w4
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,070 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Deleted vector with ID nonexistent-vector from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted
vector with ID nonexistent-vector from ChromaDB
____________ TestChromaDBAdapter.test_get_collection_stats_succeeds ____________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at
0x119d876e0>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at
0x12edbd9a0>

    @pytest.mark.fast
    def test_get_collection_stats_succeeds(self, adapter):
        """Test getting collection statistics.

        ReqID: N/A"""
        vectors = [
            MemoryVector(
                id=f"stats-vector-{i}",
                content=f"Stats vector {i}",
                embedding=[(float(j) / 10) for j in range(i, i + 5)],
                metadata={"index": i},
            )
            for i in range(1, 4)
        ]
        for vector in vectors:
            adapter.store_vector(vector)
        stats = adapter.get_collection_stats()
        assert stats["collection_name"] == "test_vectors"
>       assert stats["vector_count"] >= 3
E       assert 0 >= 3

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:160: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:24,078 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpagouq47k
2025-10-28 10:30:24,078 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Initialized ChromaDB client with persist directory:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpagouq47k
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:24,079 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID stats-vector-1 in ChromaDB
2025-10-28 10:30:24,079 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID stats-vector-2 in ChromaDB
2025-10-28 10:30:24,079 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Stored vector with ID stats-vector-3 in ChromaDB
2025-10-28 10:30:24,079 - devsynth.adapters.memory.chroma_db_adapter - INFO -
Retrieved collection statistics: {'collection_name': 'test_vectors',
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory':
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpagouq47k'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID stats-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID stats-vector-2 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored
vector with ID stats-vector-3 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615
Retrieved collection statistics: {'collection_name': 'test_vectors',
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory':
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpagouq47k'}
_____________ TestDialecticalReasoner.test_assess_impact_succeeds ______________

self = <MagicMock name='mock.notify_impact_assessment_completed'
id='5081912464'>
args = (ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5...tion 2', 'Recommendation
3'], created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'),)
kwargs = {}
expected =
call(ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d495...ation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'))
actual =
call(ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID('e38a6c52-3ec
3-471c-b09c-6dad1bec3718'), change_id=...est_user'),
edrr_phase=<EDRRPhase.REFINE: 'REFINE'>, triggered_at=datetime.datetime(2025,
10, 28, 10, 30, 24, 610657)))
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x12ef84040>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected:
notify_impact_assessment_completed(ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-
b09c-6dad1bec3718'), change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'))
E             Actual:
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'), edrr_phase=<EDRRPhase.REFINE: 'REFINE'>,
triggered_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610657)))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='mock.notify_impact_assessment_completed'
id='5081912464'>
args = (ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5...tion 2', 'Recommendation
3'], created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'),)
kwargs = {}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected:
notify_impact_assessment_completed(ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-
b09c-6dad1bec3718'), change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'))
E         Actual:
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'), edrr_phase=<EDRRPhase.REFINE: 'REFINE'>,
triggered_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610657)))
E
E       pytest introspection follows:
E
E       Args:
E       assert (ImpactNotifi...24, 610657)),) == (ImpactAssess...'test_user'),)
E
E         At index 0 diff:
ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID('e38a6c52-3ec3-471
c-b09c-6dad1bec3718'), change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'), edrr_phase=<...
E
E         ...Full output truncated (2 lines hidden), use '-vv' to show

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner
testMethod=test_assess_impact_succeeds>

        def test_assess_impact_succeeds(self):
            """Test assessing the impact of a change.

            ReqID: N/A"""
            self.llm_service.query.side_effect = [
                "This is an impact analysis",
                """- Recommendation 1
    - Recommendation 2
    - Recommendation 3""",
            ]
            dependent_requirement = Requirement(
                id=uuid4(),
                title="Dependent Requirement",
                description="This requirement depends on the test requirement",
                status=RequirementStatus.DRAFT,
                priority=RequirementPriority.MEDIUM,
                type=RequirementType.FUNCTIONAL,
                created_by="test_user",
                dependencies=[self.requirement.id],
            )
            self.requirement_repository.save_requirement(dependent_requirement)
            impact = self.reasoner.assess_impact(self.change)
            self.assertEqual(impact.change_id, self.change.id)
            self.assertEqual(impact.analysis, "This is an impact analysis")
            self.assertEqual(len(impact.recommendations), 3)
            self.assertEqual(impact.recommendations[0], "Recommendation 1")
            self.assertEqual(impact.recommendations[1], "Recommendation 2")
            self.assertEqual(impact.recommendations[2], "Recommendation 3")
            self.assertEqual(len(impact.affected_requirements), 2)
            self.assertIn(self.requirement.id, impact.affected_requirements)
            self.assertIn(dependent_requirement.id,
impact.affected_requirements)
            saved_impact =
self.impact_repository.get_impact_assessment(impact.id)
            self.assertIsNotNone(saved_impact)
            self.assertEqual(saved_impact.id, impact.id)
            impact_by_change =
self.impact_repository.get_impact_assessment_for_change(
                self.change.id
            )
            self.assertIsNotNone(impact_by_change)
            self.assertEqual(impact_by_change.id, impact.id)
>
self.notification_service.notify_impact_assessment_completed.assert_called_once_
with(
                impact
            )
E           AssertionError: expected call not found.
E           Expected:
notify_impact_assessment_completed(ImpactAssessment(id=UUID('e38a6c52-3ec3-471c-
b09c-6dad1bec3718'), change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'))
E             Actual:
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID('e38a6c52-3ec3-471c-b09c-6dad1bec3718'),
change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'), edrr_phase=<EDRRPhase.REFINE: 'REFINE'>,
triggered_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610657)))
E
E           pytest introspection follows:
E
E           Args:
E           assert (ImpactNotifi...24, 610657)),) ==
(ImpactAssess...'test_user'),)
E
E             At index 0 diff:
ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID('e38a6c52-3ec3-471
c-b09c-6dad1bec3718'), change_id=UUID('064b22b5-ae70-4fd0-a704-f32d49553f5d'),
affected_requirements=[UUID('54cd3c07-8f97-4bbf-8d33-4d55e4e943c2'),
UUID('f635d5ec-dca8-47ee-9d07-05ebd5b07046')], affected_components=[],
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'],
created_at=datetime.datetime(2025, 10, 28, 10, 30, 24, 610481),
created_by='test_user'), edrr_phase=<...
E
E             ...Full output truncated (2 lines hidden), use '-vv' to show

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:213: AssertionError
________ TestDialecticalReasoner.test_evaluate_change_consensus_failure ________

self = <MagicMock name='mock.store_with_edrr_phase' id='5081841008'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df', 'reasoning_id':
'aa13d6df-de11-4edc-85c9-9d69c8109102'}, memory_type=<MemoryType.RELATIONSHIP:
'relationship'>, edrr_phase='RETROSPECT', metadata={'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('aa13d6df-de11-4edc-85c9-9d69c8109102'),
'change_id': UUID('535a46b7-de95-4a46-82c4-b94a6b92d8df'), 'thesis': 'This is a
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion',
'recommendation': 'This is a recommendation', 'created_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 703280), 'updated_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 703282), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>,
edrr_phase='RETROSPECT', metadata={'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df'})].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner
testMethod=test_evaluate_change_consensus_failure>

        def test_evaluate_change_consensus_failure(self):
            """Ensure consensus failure triggers logging and memory storage."""
            self.llm_service.query.side_effect = [
                "This is a thesis",
                "This is an antithesis",
                """Argument 1:
    Position: Thesis
    Content: Argument for thesis

    Argument 2:
    Position: Antithesis
    Content: Argument for antithesis""",
                "This is a synthesis",
                """Conclusion: This is a conclusion

    Recommendation: This is a recommendation""",
                "no",
            ]
            with patch(
                "devsynth.application.requirements.dialectical_reasoner.logger"
            ) as mock_logger:
                with self.assertRaises(ConsensusError):
                    self.reasoner.evaluate_change(self.change)
                mock_logger.error.assert_any_call(
                    "Consensus not reached for change",
                    extra={"change_id": str(self.change.id), "event":
"consensus_failed"},
                )
>           self.memory_manager.store_with_edrr_phase.assert_called_once()
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df', 'reasoning_id':
'aa13d6df-de11-4edc-85c9-9d69c8109102'}, memory_type=<MemoryType.RELATIONSHIP:
'relationship'>, edrr_phase='RETROSPECT', metadata={'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('aa13d6df-de11-4edc-85c9-9d69c8109102'),
'change_id': UUID('535a46b7-de95-4a46-82c4-b94a6b92d8df'), 'thesis': 'This is a
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion',
'recommendation': 'This is a recommendation', 'created_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 703280), 'updated_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 703282), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>,
edrr_phase='RETROSPECT', metadata={'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df'})].
E
E           pytest introspection follows:
E
E           Args:
E           assert ({'antithesis...usion', ...},) == ()
E
E             Left contains one more item: {'antithesis': 'This is an
antithesis', 'arguments': [{'content': 'Argument for thesis', 'counterargument':
'', 'posit...: 'Antithesis'}], 'change_id':
UUID('535a46b7-de95-4a46-82c4-b94a6b92d8df'), 'conclusion': 'This is a
conclusion', ...}
E             Use -v to get more diff
E           Kwargs:
E           assert {'edrr_phase'...94a6b92d8df'}} == {}
E
E             Left contains 3 more items:
E             {'edrr_phase': 'RETROSPECT',
E              'memory_type': <MemoryType.DIALECTICAL_REASONING:
'dialectical_reasoning'>,
E              'metadata': {'change_id':
'535a46b7-de95-4a46-82c4-b94a6b92d8df'}}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:170: AssertionError
____________ TestDialecticalReasoner.test_evaluate_change_succeeds _____________

self = <MagicMock name='mock.store_with_edrr_phase' id='5082317456'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id':
'9df509d5-6e18-455b-974d-5c928758523b', 'reasoning_id':
'0138c63c-cdd0-4344-a366-0e671643c7f9'}, memory_type=<MemoryType.RELATIONSHIP:
'relationship'>, edrr_phase='REFINE', metadata={'change_id':
'9df509d5-6e18-455b-974d-5c928758523b', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('0138c63c-cdd0-4344-a366-0e671643c7f9'),
'change_id': UUID('9df509d5-6e18-455b-974d-5c928758523b'), 'thesis': 'This is a
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion',
'recommendation': 'This is a recommendation', 'created_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 751498), 'updated_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 751500), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>,
edrr_phase='REFINE', metadata={'change_id':
'9df509d5-6e18-455b-974d-5c928758523b'})].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner
testMethod=test_evaluate_change_succeeds>

        def test_evaluate_change_succeeds(self):
            """Test evaluating a change using dialectical reasoning.

            ReqID: N/A"""
            self.llm_service.query.side_effect = [
                "This is a thesis",
                "This is an antithesis",
                """Argument 1:
    Position: Thesis
    Content: Argument for thesis

    Argument 2:
    Position: Antithesis
    Content: Argument for antithesis""",
                "This is a synthesis",
                """Conclusion: This is a conclusion

    Recommendation: This is a recommendation""",
                "yes",
            ]
            with patch(
                "devsynth.application.requirements.dialectical_reasoner.logger"
            ) as mock_logger:
                reasoning = self.reasoner.evaluate_change(self.change)
                mock_logger.info.assert_any_call(
                    "Consensus reached for change",
                    extra={"change_id": str(self.change.id), "event":
"consensus_reached"},
                )
>           self.memory_manager.store_with_edrr_phase.assert_called_once()
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id':
'9df509d5-6e18-455b-974d-5c928758523b', 'reasoning_id':
'0138c63c-cdd0-4344-a366-0e671643c7f9'}, memory_type=<MemoryType.RELATIONSHIP:
'relationship'>, edrr_phase='REFINE', metadata={'change_id':
'9df509d5-6e18-455b-974d-5c928758523b', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('0138c63c-cdd0-4344-a366-0e671643c7f9'),
'change_id': UUID('9df509d5-6e18-455b-974d-5c928758523b'), 'thesis': 'This is a
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion',
'recommendation': 'This is a recommendation', 'created_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 751498), 'updated_at':
datetime.datetime(2025, 10, 28, 10, 30, 24, 751500), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>,
edrr_phase='REFINE', metadata={'change_id':
'9df509d5-6e18-455b-974d-5c928758523b'})].
E
E           pytest introspection follows:
E
E           Args:
E           assert ({'antithesis...usion', ...},) == ()
E
E             Left contains one more item: {'antithesis': 'This is an
antithesis', 'arguments': [{'content': 'Argument for thesis', 'counterargument':
'', 'posit...: 'Antithesis'}], 'change_id':
UUID('9df509d5-6e18-455b-974d-5c928758523b'), 'conclusion': 'This is a
conclusion', ...}
E             Use -v to get more diff
E           Kwargs:
E           assert {'edrr_phase'...c928758523b'}} == {}
E
E             Left contains 3 more items:
E             {'edrr_phase': 'REFINE',
E              'memory_type': <MemoryType.DIALECTICAL_REASONING:
'dialectical_reasoning'>,
E              'metadata': {'change_id':
'9df509d5-6e18-455b-974d-5c928758523b'}}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:120: AssertionError
__________________________ test_dpg_command_disabled ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eef2de0>

    def test_dpg_command_disabled(monkeypatch):
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.dpg_cmd.get_settings",
            lambda reload=True: types.SimpleNamespace(gui_enabled=False),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dpg
_flag.py:88:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________________ test_dpg_command_missing_dependency ______________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12eef1430>

    def test_dpg_command_missing_dependency(monkeypatch):
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.dpg_cmd.get_settings",
            lambda reload=True: types.SimpleNamespace(gui_enabled=True),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dpg
_flag.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________________ TestPhases.test_retrospect_phase_has_expected _________________

self = <tests.unit.general.test_ingest_cmd.TestPhases object at 0x119f811f0>
mock_bridge = <MagicMock id='5084214048'>
mock_memory_manager = <MagicMock name='MemoryManager()' id='5082388320'>

    @pytest.mark.fast
    def test_retrospect_phase_has_expected(self, mock_bridge,
mock_memory_manager):
        """Test retrospect_phase function.

        ReqID: N/A"""
        mock_memory_manager.reset_mock()
>       result = retrospect_phase(
            {"projectName": "TestProject"},
            {"relationships_created": 75},
            verbose=True,
            bridge=mock_bridge,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ing
est_cmd.py:504:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

manifest = {'projectName': 'TestProject'}
refine_results = {'relationships_created': 75}, verbose = True

    def retrospect_phase(
        manifest: ManifestModel,
        refine_results: RefinePhaseResult,
        verbose: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
        memory_manager: Union[MemoryManager, None] = None,
        code_analyzer: Union[CodeAnalyzer, None] = None,
        wsde_team: Union[WSDETeam, None] = None,
    ) -> RetrospectPhaseResult:
        """Summarize results and suggest improvements."""

        start = time.perf_counter()

        bridge = bridge or DEFAULT_BRIDGE
        memory_manager = memory_manager or MemoryManager(
            adapters={"tinydb": TinyDBMemoryAdapter()}
        )
        analyzer = code_analyzer or CodeAnalyzer()
        wsde_team = wsde_team or WSDETeam(name="IngestCmdTeam")

        if verbose:
            bridge.print("  Generating retrospective report...")

        improvements = refine_results.get("relationships_created", 0)
        gaps = refine_results.get("outdated_items_archived", 0)

        learnings = cast(JSONValue, wsde_team.extract_learnings(refine_results,
True))
        patterns = cast(
            JSONValue,
            wsde_team.recognize_patterns(
                learnings,
                historical_context=memory_manager.retrieve_historical_patterns()
,
                code_analyzer=analyzer,
            ),
        )
        integrated = cast(
            JSONValue,
>           wsde_team.integrate_knowledge(learnings, patterns, memory_manager),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       TypeError: _integrate_knowledge() takes 3 positional arguments but 4
were given

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/ingest_cmd.py:654: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:25,454 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
_____________ test_offline_mode_selects_offline_provider_succeeds ______________

obj = <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.application.llm' has no attribute
'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f09ec90>

    @pytest.mark.fast
    def test_offline_mode_selects_offline_provider_succeeds(monkeypatch):
        """Test that offline mode selects offline provider succeeds.

        ReqID: N/A"""
        monkeypatch.setattr(
            "devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE", False
        )
        monkeypatch.setattr(
            "devsynth.application.llm.load_config", lambda: _mock_config(True)
        )
>       monkeypatch.setattr(
            "devsynth.application.llm.get_llm_settings",
            lambda: {
                "provider": "openai",
                "openai_api_key": "key",
                "openai_model": "gpt-3.5-turbo",
            },
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_llm
_provider_selection.py:30:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.llm has no
attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ test_online_mode_uses_configured_provider_succeeds ______________

obj = <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.application.llm' has no attribute
'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f09d8b0>

    @pytest.mark.fast
    def test_online_mode_uses_configured_provider_succeeds(monkeypatch):
        """Test that online mode uses configured provider succeeds.

        ReqID: N/A"""
        monkeypatch.setattr(
            "devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE", False
        )
        monkeypatch.setattr(
            "devsynth.application.llm.load_config", lambda: _mock_config(False)
        )
>       monkeypatch.setattr(
            "devsynth.application.llm.get_llm_settings",
            lambda: {
                "provider": "openai",
                "openai_api_key": "key",
                "openai_model": "gpt-3.5-turbo",
            },
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_llm
_provider_selection.py:53:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.llm has no
attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____ TestLMStudioIntegrationRegression.test_lmstudio_settings_extraction ______

self =
<tests.unit.general.test_lmstudio_integration_regression.TestLMStudioIntegration
Regression object at 0x119fbab10>

    def test_lmstudio_settings_extraction(self):
        """Test that LLM settings can be extracted for LM Studio.

        ReqID: LMSTUDIO-REG-3
        """
>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_lms
tudio_integration_regression.py:91: ImportError
_ TestLMStudioIntegrationRegression.test_lmstudio_provider_mock_initialization _

self =
<tests.unit.general.test_lmstudio_integration_regression.TestLMStudioIntegration
Regression object at 0x119fbb470>
mock_lmstudio = <MagicMock name='lmstudio' id='5084485376'>

    @patch("devsynth.application.llm.lmstudio_provider.lmstudio")
    def test_lmstudio_provider_mock_initialization(self, mock_lmstudio):
        """Test LM Studio provider with mocked LM Studio service.

        ReqID: LMSTUDIO-REG-5
        """
>       from ....application.llm.lmstudio_provider import LMStudioProvider
E       ImportError: attempted relative import beyond top-level package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_lms
tudio_integration_regression.py:149: ImportError
_
TestMultiAgentAdapterWorkflow.test_multi_agent_consensus_and_primus_selection_su
cceeds _

self = <MagicMock name='build_consensus' id='5088070736'>
args = ({'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'confiden...s', 'reasoning': ''},
{'agent': 'DocAgent', 'confidence': 1.0, 'content': 'doc', 'reasoning': ''}],
'type': 'coding'},)
kwargs = {}
expected = call({'type': 'coding', 'language': 'python', 'solutions': [{'agent':
'PythonAgent', 'content': 'py', 'confidence': 1....nt': 'DocAgent', 'content':
'doc', 'confidence': 1.0, 'reasoning': ''}], 'id':
'12e59bb4-b16a-40c8-b924-0054a7a14695'})
actual = call({'type': 'coding', 'language': 'python', 'solutions': [{'agent':
'PythonAgent', 'content': 'py', 'confidence': 1....', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options':
['py', 'js', 'doc']})
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x12efb6020>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695'})
E             Actual: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options':
['py', 'js', 'doc']})

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='build_consensus' id='5088070736'>
args = ({'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'confiden...s', 'reasoning': ''},
{'agent': 'DocAgent', 'confidence': 1.0, 'content': 'doc', 'reasoning': ''}],
'type': 'coding'},)
kwargs = {}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695'})
E         Actual: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options':
['py', 'js', 'doc']})
E
E       pytest introspection follows:
E
E       Args:
E       assert ({'id': '12e5...: ''}], ...},) == ({'id': '12e5...': 'coding'},)
E
E         At index 0 diff: {'type': 'coding', 'language': 'python', 'solutions':
[{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0, 'reasoning': ''},
{'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0, 'reasoning': ''},
{'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0, 'reasoning': ''}],
'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options': ['py', 'js', 'doc']} !=
{'type': 'coding', 'language': 'python', 'solutions': [{'agent': 'PythonAgent',
'content': 'py', 'confidence': 1.0, 'reasoning': ''}, {'agent': 'JSAgent',
'content': 'js', 'confidence': 1.0, ...
E
E         ...Full output truncated (2 lines hidden), use '-vv' to show

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

self =
<tests.unit.general.test_multi_agent_adapter_workflow.TestMultiAgentAdapterWorkf
low object at 0x11a0164e0>

    @pytest.mark.fast
    def test_multi_agent_consensus_and_primus_selection_succeeds(self):
        """Test that multi agent consensus and primus selection succeeds.

        ReqID: N/A"""
        task = {"type": "coding", "language": "python"}
        with (
            patch.object(
                self.team,
                "build_consensus",
                return_value={
                    "consensus": "done",
                    "contributors": ["PythonAgent", "JSAgent", "DocAgent"],
                    "method": "consensus_synthesis",
                    "reasoning": "",
                },
            ) as mock_consensus,
            patch.object(
                self.team,
                "select_primus_by_expertise",
                wraps=self.team.select_primus_by_expertise,
            ) as mock_select,
        ):
            result = self.adapter.process_task(task)
>           mock_consensus.assert_called_once_with(task)
E           AssertionError: expected call not found.
E           Expected: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695'})
E             Actual: build_consensus({'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options':
['py', 'js', 'doc']})
E
E           pytest introspection follows:
E
E           Args:
E           assert ({'id': '12e5...: ''}], ...},) == ({'id': '12e5...':
'coding'},)
E
E             At index 0 diff: {'type': 'coding', 'language': 'python',
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0,
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0,
'reasoning': ''}], 'id': '12e59bb4-b16a-40c8-b924-0054a7a14695', 'options':
['py', 'js', 'doc']} != {'type': 'coding', 'language': 'python', 'solutions':
[{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0, 'reasoning': ''},
{'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0, ...
E
E             ...Full output truncated (2 lines hidden), use '-vv' to show

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mul
ti_agent_adapter_workflow.py:63: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:26,109 - devsynth.domain.models.wsde_core - INFO - Added agent
PythonAgent to team team1
2025-10-28 10:30:26,109 - devsynth.domain.models.wsde_core - INFO - Added agent
JSAgent to team team1
2025-10-28 10:30:26,110 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:30:26,110 - devsynth.domain.models.wsde_core - INFO - Added agent
DocAgent to team team1
2025-10-28 10:30:26,110 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent',
'supervisor': 'DocAgent', 'designer': None, 'evaluator': None}
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
PythonAgent to team team1
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
JSAgent to team team1
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 'supervisor':
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
DocAgent to team team1
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 'supervisor':
'DocAgent', 'designer': None, 'evaluator': None}
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_roles - INFO - Selected
JSAgent as primus based on expertise
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_enhanced_dialectical -
INFO - Applying enhanced multi-solution dialectical reasoning to task:
12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 1 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 2 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 3 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
2025-10-28 10:30:26,111 - devsynth.domain.models.wsde_solution_analysis - INFO -
Generating comparative analysis for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected JSAgent
as primus based on expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_enhanced_dialectical:logging_setup.py:615
Applying enhanced multi-solution dialectical reasoning to task:
12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 1 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 2 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 3 for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Generating comparative analysis for task: 12e59bb4-b16a-40c8-b924-0054a7a14695
__________________________ test_mvu_lint_cli_success ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ed02e40>

    @pytest.mark.fast
    def test_mvu_lint_cli_success(monkeypatch):
        """CLI should report success when no errors are returned."""
        runner = CliRunner()
        app = build_app()
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.mvu_lint_cmd.lint_range",
            lambda _rev: [],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mvu
_lint_cli.py:36:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
__________________________ test_mvu_lint_cli_failure ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ed003b0>

    @pytest.mark.fast
    def test_mvu_lint_cli_failure(monkeypatch):
        """CLI should exit with error when linter reports problems."""
        runner = CliRunner()
        app = build_app()
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.mvu_lint_cmd.lint_range",
            lambda _rev: ["abc123: error"],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mvu
_lint_cli.py:50:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'devsynth.application.cli' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________ test_ensure_path_exists_within_project_dir_succeeds ______________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_ensure_path_exists_within0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f0f1820>

    @pytest.mark.fast
    def test_ensure_path_exists_within_project_dir_succeeds(tmp_path,
monkeypatch):
        """Test that ensure path exists within project dir succeeds.

        ReqID: N/A"""
        project_dir = tmp_path / "project"
        outside_dir = tmp_path / "outside"
        project_dir.mkdir(exist_ok=True)
        outside_dir.mkdir(exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.setenv("DEVSYNTH_NO_FILE_LOGGING", "1")
>       settings = importlib.reload(settings_module)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pat
h_restrictions.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.config.settings' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/config/settin
gs.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.config.settings not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________ test_documentation_tasks_prefer_documentation_experts_succeeds ________

    def test_documentation_tasks_prefer_documentation_experts_succeeds():
        """Test that documentation tasks prefer documentation experts succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestPrimusSelectionTeam")
        coder = create_agent("Coder", ["python"])
        doc_agent = create_agent("Doc", ["documentation", "markdown"])
        team.add_agents([coder, doc_agent])
        task = {"type": "documentation", "description": "Update docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc_agent
E       AssertionError: assert <MagicMock id='5086279536'> is <MagicMock
id='5085808256'>
E        +  where <MagicMock id='5086279536'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12f0f0a10>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:91: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:28,309 - devsynth.domain.models.wsde_core - INFO - Added agent
Coder to team TestPrimusSelectionTeam
2025-10-28 10:30:28,310 - devsynth.domain.models.wsde_core - INFO - Added agent
Doc to team TestPrimusSelectionTeam
2025-10-28 10:30:28,310 - devsynth.domain.models.wsde_roles - INFO - Selected
Coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder
as primus based on expertise
_____________ test_weighted_expertise_prefers_specialist_succeeds ______________

weighted_team = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12f2165d0>, <MagicMock id='5085684960'>, <MagicMock id='5085677280'>)

    def test_weighted_expertise_prefers_specialist_succeeds(weighted_team):
        """Test that weighted expertise prefers specialist succeeds.

        ReqID: N/A"""
        team, generalist, specialist = weighted_team
        task = {"type": "coding", "language": "python", "domain": "backend"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is specialist
E       AssertionError: assert <MagicMock id='5085684960'> is <MagicMock
id='5085677280'>
E        +  where <MagicMock id='5085684960'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12f2165d0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:102: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:28,325 - devsynth.domain.models.wsde_core - INFO - Added agent
Generalist to team TestPrimusSelectionTeam
2025-10-28 10:30:28,325 - devsynth.domain.models.wsde_core - INFO - Added agent
Specialist to team TestPrimusSelectionTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Generalist to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Specialist to team TestPrimusSelectionTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:28,325 - devsynth.domain.models.wsde_roles - INFO - Selected
Generalist as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Generalist as primus based on expertise
_________ test_documentation_tasks_prioritize_best_doc_expert_succeeds _________

documentation_team = (<devsynth.domain.models.wsde_facade.WSDETeam object at
0x12f42c8f0>, <MagicMock id='5087873024'>, <MagicMock id='5087807152'>,
<MagicMock id='5087818672'>)

    def
test_documentation_tasks_prioritize_best_doc_expert_succeeds(documentation_team)
:
        """Test that documentation tasks prioritize best doc expert succeeds.

        ReqID: N/A"""
        team, coder, writer, doc = documentation_team
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
        primus = team.get_primus()
>       assert primus in (writer, doc)
E       AssertionError: assert <MagicMock id='5087873024'> in (<MagicMock
id='5087807152'>, <MagicMock id='5087818672'>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:128: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:28,640 - devsynth.domain.models.wsde_core - INFO - Added agent
Coder to team TestPrimusSelectionTeam
2025-10-28 10:30:28,640 - devsynth.domain.models.wsde_core - INFO - Added agent
Writer to team TestPrimusSelectionTeam
2025-10-28 10:30:28,640 - devsynth.domain.models.wsde_core - INFO - Added agent
Doc to team TestPrimusSelectionTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Writer to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc
to team TestPrimusSelectionTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:28,641 - devsynth.domain.models.wsde_roles - INFO - Selected
Coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder
as primus based on expertise
________________________ test_is_cli_available_succeeds ________________________

    @pytest.mark.fast
    def test_is_cli_available_succeeds():
        """Test that is_cli_available checks environment variables and CLI
availability.

        ReqID: N/A"""
        with patch.dict(os.environ, {"DEVSYNTH_RESOURCE_CLI_AVAILABLE":
"false"}):
            from tests.conftest import is_cli_available

            assert not is_cli_available()
        with patch("subprocess.run") as mock_run:
            mock_run.return_value.returncode = 0
            from tests.conftest import is_cli_available

            assert is_cli_available()
        with patch("subprocess.run") as mock_run:
            mock_run.return_value.returncode = 1
            from tests.conftest import is_cli_available

            assert not is_cli_available()
        with patch("subprocess.run", side_effect=Exception("Command not
found")):
            from tests.conftest import is_cli_available

>           assert not is_cli_available()
E           assert not True
E            +  where True = <function is_cli_available at 0x109d81d00>()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_res
ource_markers.py:89: AssertionError
_________________ test_pytest_collection_modifyitems_succeeds __________________

    @pytest.mark.fast
    def test_pytest_collection_modifyitems_succeeds():
        """Test that pytest_collection_modifyitems skips tests with unavailable
resources.

        ReqID: N/A"""
        from tests.conftest import pytest_collection_modifyitems

        class MockMarker:

            def __init__(self, name, args):
                self.name = name
                self.args = args

        class MockItem:

            def __init__(self, name, markers=None):
                self.name = name
                self._markers = markers or []
                self.user_properties = []

            def iter_markers(self, name=None):
                if name:
                    return [m for m in self._markers if m.name == name]
                return self._markers

            def add_marker(self, marker):
                self._markers.append(marker)

        class MockConfig:

            def __init__(self):
                pass

        item1 = MockItem(
            "test_unavailable_resource",
            [MockMarker("requires_resource", ["unavailable_resource"])],
        )
        item2 = MockItem(
            "test_available_resource",
            [MockMarker("requires_resource", ["available_resource"])],
        )
        item3 = MockItem("test_no_resource_marker")
        items = [item1, item2, item3]
        with patch(
            "tests.conftest.is_resource_available", lambda r: r ==
"available_resource"
        ):
>           pytest_collection_modifyitems(MockConfig(), items)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_res
ource_markers.py:161:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config =
<tests.unit.general.test_resource_markers.test_pytest_collection_modifyitems_suc
ceeds.<locals>.MockConfig object at 0x12f470d40>
items =
[<tests.unit.general.test_resource_markers.test_pytest_collection_modifyitems_su
cceeds.<locals>.MockItem object at
0x1...nit.general.test_resource_markers.test_pytest_collection_modifyitems_succe
eds.<locals>.MockItem object at 0x12f470d10>]

    def pytest_collection_modifyitems(config, items):
        """
        Normalize and enforce resource gating, smoke-mode behavior,
property-based testing collection,
        and conservative xdist-parallel safety.

        - Resource gating: validate @pytest.mark.requires_resource("<name>") and
skip when unavailable or malformed/unknown.
        - Smoke mode: when PYTEST_DISABLE_PLUGIN_AUTOLOAD=1, skip tests under
tests/behavior/ that rely on third-party plugins.
        - Property-based tests: skip unless DEVSYNTH_PROPERTY_TESTING is
enabled.
        - Xdist safety: conservatively mark integration and performance tests as
@pytest.mark.isolation to avoid parallelization issues.
        """
        # Validate and apply resource gating
        for item in items:
            for marker in item.iter_markers(name="requires_resource"):
                # Validate marker arguments
                if (
                    not marker.args
                    or not isinstance(marker.args[0], str)
                    or not marker.args[0].strip()
                ):
                    item.add_marker(
                        pytest.mark.skip(
                            reason="Malformed requires_resource marker: expected
a non-empty resource name"
                        )
                    )
                    continue
                resource = marker.args[0].strip()

                # Validate known resource
                known_resources = {
                    "anthropic",
                    "llm_provider",
                    "lmstudio",
                    "openai",
                    "codebase",
                    "cli",
                    "chromadb",
                    "tinydb",
                    "duckdb",
                    "faiss",
                    "kuzu",
                    "lmdb",
                    "rdflib",
                    "memory",
                    "test_resource",
                    "webui",
                }
                if resource not in known_resources:
                    item.add_marker(
                        pytest.mark.skip(
                            reason=f"Unknown resource '{resource}' not
recognized by test harness"
                        )
                    )
                    continue

                # Add a derived static marker to enable '-m resource_<name>'
selection
                try:
                    item.add_marker(getattr(pytest.mark,
f"resource_{resource}"))
                except Exception:
                    # Defensive: do not fail collection if dynamic marker
attachment has issues
                    pass

                # Skip if resource is not available
                if not is_resource_available(resource):
                    item.add_marker(
                        pytest.mark.skip(reason=f"Resource '{resource}' not
available")
                    )

        # Smoke-mode behavior: skip behavior tests when plugins are disabled
        smoke = os.environ.get("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "0").lower() in
{
            "1",
            "true",
            "yes",
        }
        if smoke:
            skip_behavior = pytest.mark.skip(
                reason=(
                    "Smoke mode (plugins disabled): skipping behavior tests that
require third-party plugins"
                )
            )
            for item in items:
                try:
                    fspath = getattr(item, "fspath", None)
                    path_str = str(fspath) if fspath is not None else ""
                except Exception:
                    path_str = ""
                norm = path_str.replace("\\", "/")
                if "/tests/behavior/" in norm or
norm.endswith("/tests/behavior"):
                    item.add_marker(skip_behavior)

        # Conservatively mark integration and performance tests as isolation for
xdist safety
        for item in items:
            try:
                fspath = getattr(item, "fspath", None)
                path_str = str(fspath) if fspath is not None else ""
            except Exception:
                path_str = ""
            norm = path_str.replace("\\", "/")
            if "/tests/integration/" in norm or "/tests/performance/" in norm:
                if not item.get_closest_marker("isolation"):
                    item.add_marker(pytest.mark.isolation)

        # Broaden isolation auto-marking heuristics for fragile tests that touch
filesystem/network
        # Apply only when a test lacks an explicit @pytest.mark.isolation
        network_keywords = ("network", "http", "https", "socket", "requests",
"ftp")
        fs_fixtures = {
            "tmp_path",
            "tmpdir",
            "tmpdir_factory",
            "temp_log_dir",
            "tmp_project_dir",
        }
        for item in items:
            try:
                name = getattr(item, "name", "") or ""
                nodeid = getattr(item, "nodeid", "") or ""
                fixturenames = set(getattr(item, "fixturenames", []) or [])
            except Exception:
                name, nodeid, fixturenames = "", "", set()
>           if item.get_closest_marker("isolation"):
               ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'MockItem' object has no attribute
'get_closest_marker'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/conftest.py:1158:
AttributeError
_________________________ test_speed_option_recognized _________________________

    @pytest.mark.fast
    def test_speed_option_recognized():
        """Verify that the ``--speed`` option filters tests by marker.

        ReqID: DEV-0000"""
        repo_root = Path(__file__).resolve().parents[3]
        test_file = repo_root / "tests" / "test_speed_temp.py"
        try:
            test_file.write_text(
                "import pytest\n\n@pytest.mark.fast\ndef test_dummy():\n
assert True\n"
            )
>           result = subprocess.run(
                [
                    sys.executable,
                    "-m",
                    "pytest",
                    "-p",
                    "no:cov",
                    "-o",
                    "addopts=",
                    "--speed=fast",
                    "-m",
                    "fast",
                    str(test_file),
                ],
                capture_output=True,
                text=True,
                cwd=repo_root,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_spe
ed_option.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:550: in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:1209: in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:2115: in _communicate
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/selectors.py:415: in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x12e8f3240, file
'/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/selectors.py', line 416, code select>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
_______________________ test_cli_bridge_methods_succeeds _______________________

self = <MagicMock name='ask' id='5075732288'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'ask' to have been called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

    @pytest.mark.fast
    def test_cli_bridge_methods_succeeds():
        """Test that cli bridge methods succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.prompt.Prompt.ask", return_value="ans") as ask:
            resp = bridge.ask_question("Q?")
>           ask.assert_called_once()
E           AssertionError: Expected 'ask' to have been called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ux_
bridge.py:19: AssertionError
----------------------------- Captured stdout call -----------------------------
Q?  Q?
______________________ test_webui_bridge_methods_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f02d820>

    @pytest.mark.fast
    def test_webui_bridge_methods_succeeds(monkeypatch):
        """Test that webui bridge methods succeeds.

        ReqID: N/A"""
        st = ModuleType("streamlit")
        st.text_input = MagicMock(return_value="text")
        st.selectbox = MagicMock(return_value="choice")
        st.checkbox = MagicMock(return_value=True)
        st.write = MagicMock()
        st.markdown = MagicMock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ux_
bridge.py:47:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ TestWorkflowManager.test_handle_human_intervention_succeeds __________

self = <tests.unit.general.test_workflow.TestWorkflowManager object at
0x11a11eb10>
workflow_manager = <devsynth.application.orchestration.workflow.WorkflowManager
object at 0x12e54da00>

    @pytest.mark.fast
    def test_handle_human_intervention_succeeds(self, workflow_manager):
        """Test handling human intervention.

        ReqID: N/A"""
        with (
            patch(
                "devsynth.application.orchestration.workflow.console"
            ) as mock_console,
            patch(
                "devsynth.application.orchestration.workflow.Prompt.ask",
                return_value="User input",
            ),
        ):
            response = workflow_manager._handle_human_intervention(
                "workflow-id", "step-id", "Need your input"
            )
>           assert response == "User input"
E           AssertionError: assert '' == 'User input'
E
E             - User input

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wor
kflow.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
Human intervention required:
Need your input
Your input          Your input
__________ TestWorkflowManager.test_add_init_workflow_steps_succeeds ___________

self = <tests.unit.general.test_workflow.TestWorkflowManager object at
0x11a11f3e0>
workflow_manager = <devsynth.application.orchestration.workflow.WorkflowManager
object at 0x12e7aa510>

    @pytest.mark.fast
    def test_add_init_workflow_steps_succeeds(self, workflow_manager):
        """Test adding steps for init workflow.

        ReqID: N/A"""
        mock_workflow = MagicMock()
        workflow_manager._add_init_workflow_steps(
            mock_workflow, {"path": "./test-project"}
        )
>       assert workflow_manager.orchestration_port.add_step.call_count == 3
E       AssertionError: assert 6 == 3
E        +  where 6 = <MagicMock name='OrchestrationPort.add_step'
id='5072646128'>.call_count
E        +    where <MagicMock name='OrchestrationPort.add_step'
id='5072646128'> = <MagicMock name='OrchestrationPort' id='5074756112'>.add_step
E        +      where <MagicMock name='OrchestrationPort' id='5074756112'> =
<devsynth.application.orchestration.workflow.WorkflowManager object at
0x12e7aa510>.orchestration_port

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wor
kflow.py:83: AssertionError
_______________ test_assign_roles_with_explicit_mapping_succeeds _______________

    def test_assign_roles_with_explicit_mapping_succeeds():
        """Test that assign roles with explicit mapping succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeRoleMappingTeam")
        a1 = MagicMock()
        a2 = MagicMock()
        a3 = MagicMock()
        team.add_agents([a1, a2, a3])
        # The primus agent should not also be listed as a worker or the
        # assignment logic will overwrite the Primus role.  Provide a distinct
        # worker mapping to verify role assignment order.
        mapping = {
            "primus": a1,
            "worker": [a2],
            "supervisor": a2,
            "designer": a3,
            "evaluator": None,
        }
>       team.assign_roles(mapping)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_role_mapping.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:663: in assign_roles
    return _manager(self).assign(role_mapping)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:372: in assign
    self._validate_role_mapping(normalised)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = RoleAssignmentManager(team=<devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12e78d280>)
mapping = {<RoleName.DESIGNER: 'designer'>: <MagicMock id='5072636960'>,
<RoleName.EVALUATOR: 'evaluator'>: None, <RoleName.PRIMUS: 'primus'>: <MagicMock
id='5074669392'>, <RoleName.SUPERVISOR: 'supervisor'>: <MagicMock
id='5074674384'>, ...}

    def _validate_role_mapping(
        self, mapping: Mapping[RoleName, SupportsTeamAgent | None]
    ) -> None:
        valid_roles = set(RoleName)
        for role, agent in mapping.items():
            if role not in valid_roles:
                raise ValueError(
                    f"Invalid role: {role}. Valid roles are: {', '.join(r.value
for r in valid_roles)}"
                )
            if agent is not None and agent not in self.team.agents:
>               raise ValueError(f"Agent {agent.name} is not a member of this
team")
                                          ^^^^^^^^^^
E               AttributeError: 'list' object has no attribute 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:520: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:32,942 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5072904000'> to team TestWsdeRoleMappingTeam
2025-10-28 10:30:32,943 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5074761968'> to team TestWsdeRoleMappingTeam
2025-10-28 10:30:32,944 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5072834432'> to team TestWsdeRoleMappingTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5072904000'> to team TestWsdeRoleMappingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5074761968'> to team TestWsdeRoleMappingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5072834432'> to team TestWsdeRoleMappingTeam
_____________ TestWSDETeam.test_get_role_specific_agents_succeeds ______________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a151be0>
mock_agent = <MagicMock spec='BaseAgent' id='5072042992'>

    def test_get_role_specific_agents_succeeds(self, mock_agent):
        """Test getting agents by their specific roles.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = MagicMock(spec=BaseAgent)
        agent2 = MagicMock(spec=BaseAgent)
        agent3 = MagicMock(spec=BaseAgent)
        agent4 = MagicMock(spec=BaseAgent)
        agent5 = MagicMock(spec=BaseAgent)
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        team.add_agent(agent4)
        team.add_agent(agent5)
        agent1.current_role = "Primus"
        agent2.current_role = "Worker"
        agent3.current_role = "Supervisor"
        agent4.current_role = "Designer"
        agent5.current_role = "Evaluator"
>       assert team.get_worker() == agent2
E       AssertionError: assert None == <MagicMock spec='BaseAgent'
id='5072031280'>
E        +  where None = _get_worker()
E        +    where _get_worker = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12e56c920>.get_worker

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:153: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,016 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5075733392'> to team TestTeam
2025-10-28 10:30:33,017 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5072836352'> to team TestTeam
2025-10-28 10:30:33,017 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5072982272'> to team TestTeam
2025-10-28 10:30:33,017 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5074749824'> to team TestTeam
2025-10-28 10:30:33,017 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='5074752704'> to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5075733392'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5072836352'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5072982272'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5074749824'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='5074752704'> to team TestTeam
_______________ TestWSDETeam.test_peer_based_structure_succeeds ________________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a1525a0>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f240540>

    def test_peer_based_structure_succeeds(self, mock_agent_with_expertise):
        """Test that all agents are treated as peers with no permanent
hierarchy.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["skill1"])
        agent2 = mock_agent_with_expertise("Agent2", ["skill2"])
        agent3 = mock_agent_with_expertise("Agent3", ["skill3"])
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        assert team.get_primus() == agent1
        task2 = {"type": "task", "requires": "skill2"}
        team.select_primus_by_expertise(task2)
>       assert team.get_primus() == agent2
E       AssertionError: assert <MagicMock spec='BaseAgent' id='5072521984'> ==
<MagicMock spec='BaseAgent' id='5083683664'>
E        +  where <MagicMock spec='BaseAgent' id='5072521984'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12e588890>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:207: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,042 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team TestTeam
2025-10-28 10:30:33,042 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team TestTeam
2025-10-28 10:30:33,042 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team TestTeam
2025-10-28 10:30:33,042 - devsynth.domain.models.wsde_roles - INFO - Selected
Agent1 as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Agent1
as primus based on expertise
__________ TestWSDETeam.test_consensus_based_decision_making_succeeds __________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a152f60>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f242520>

    def test_consensus_based_decision_making_succeeds(self,
mock_agent_with_expertise):
        """Test facilitating consensus building among agents.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["skill1"])
        agent2 = mock_agent_with_expertise("Agent2", ["skill2"])
        agent3 = mock_agent_with_expertise("Agent3", ["skill3"])
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        task = {"type": "decision_task", "description": "A task requiring
consensus"}
        solution1 = {"agent": "Agent1", "content": "Solution from Agent1"}
        solution2 = {"agent": "Agent2", "content": "Solution from Agent2"}
        solution3 = {"agent": "Agent3", "content": "Solution from Agent3"}
        team.add_solution(task, solution1)
        team.add_solution(task, solution2)
        team.add_solution(task, solution3)
        consensus = team.build_consensus(task)
>       assert "Agent1" in consensus["contributors"]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'contributors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:262: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team TestTeam
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team TestTeam
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team TestTeam
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task df0bb2f7-d592-4853-9107-7c6d15a6548b
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task df0bb2f7-d592-4853-9107-7c6d15a6548b
2025-10-28 10:30:33,068 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task df0bb2f7-d592-4853-9107-7c6d15a6548b
2025-10-28 10:30:33,069 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task df0bb2f7-d592-4853-9107-7c6d15a6548b
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task df0bb2f7-d592-4853-9107-7c6d15a6548b
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task df0bb2f7-d592-4853-9107-7c6d15a6548b
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
____________ TestWSDETeam.test_dialectical_review_process_succeeds _____________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a153440>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f26bc40>

    def test_dialectical_review_process_succeeds(self,
mock_agent_with_expertise):
        """Test the dialectical review process with thesis, antithesis, and
synthesis.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        code_agent = mock_agent_with_expertise("CodeAgent", ["python",
"coding"])
        test_agent = mock_agent_with_expertise("TestAgent", ["testing",
"quality"])
        critic_agent = mock_agent_with_expertise(
            "CriticAgent", ["dialectical_reasoning", "critique"]
        )
        team.add_agent(code_agent)
        team.add_agent(test_agent)
        team.add_agent(critic_agent)
        task = {
            "type": "implementation_task",
            "description": "Implement a user authentication system",
        }
        thesis = {
            "agent": "CodeAgent",
            "content": "Implement authentication using a simple
username/password check",
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
        }
        team.add_solution(task, thesis)
        dialectical_result = team.apply_dialectical_reasoning(task,
critic_agent)
        assert "thesis" in dialectical_result
        assert "antithesis" in dialectical_result
        assert "synthesis" in dialectical_result
>       assert dialectical_result["thesis"]["agent"] == "CodeAgent"
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:295: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,095 - devsynth.domain.models.wsde_core - INFO - Added agent
CodeAgent to team TestTeam
2025-10-28 10:30:33,095 - devsynth.domain.models.wsde_core - INFO - Added agent
TestAgent to team TestTeam
2025-10-28 10:30:33,095 - devsynth.domain.models.wsde_core - INFO - Added agent
CriticAgent to team TestTeam
2025-10-28 10:30:33,095 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 3d83b1aa-a64a-4254-ac74-f5a45dcc9cee
2025-10-28 10:30:33,095 - devsynth.domain.models.wsde_core - WARNING - Cannot
apply dialectical reasoning: no solution provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
CodeAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
TestAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 3d83b1aa-a64a-4254-ac74-f5a45dcc9cee
WARNING  devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply
dialectical reasoning: no solution provided
_______ TestWSDETeam.test_peer_review_with_acceptance_criteria_succeeds ________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a153920>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f242b60>

    def test_peer_review_with_acceptance_criteria_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test the peer review process with specific acceptance criteria.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        author_agent = mock_agent_with_expertise("AuthorAgent", ["python",
"coding"])
        reviewer1 = mock_agent_with_expertise("ReviewerAgent1", ["testing",
"quality"])
        reviewer2 = mock_agent_with_expertise(
            "ReviewerAgent2", ["security", "best_practices"]
        )
        team.add_agent(author_agent)
        team.add_agent(reviewer1)
        team.add_agent(reviewer2)
        work_product = {
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        acceptance_criteria = [
            "Code follows security best practices",
            "Function handles edge cases",
            "Code is well-documented",
        ]
        review = team.request_peer_review(
            work_product=work_product,
            author=author_agent,
            reviewer_agents=[reviewer1, reviewer2],
        )
>       review.acceptance_criteria = acceptance_criteria
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: property 'acceptance_criteria' of 'PeerReview' object
has no setter

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:330: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,124 - devsynth.domain.models.wsde_core - INFO - Added agent
AuthorAgent to team TestTeam
2025-10-28 10:30:33,124 - devsynth.domain.models.wsde_core - INFO - Added agent
ReviewerAgent1 to team TestTeam
2025-10-28 10:30:33,124 - devsynth.domain.models.wsde_core - INFO - Added agent
ReviewerAgent2 to team TestTeam
2025-10-28 10:30:33,124 - devsynth.domain.models.wsde_roles - INFO - Selected
AuthorAgent as primus based on expertise
2025-10-28 10:30:33,124 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent',
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
ReviewerAgent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
ReviewerAgent2 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent',
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
__________ TestWSDETeam.test_peer_review_with_revision_cycle_succeeds __________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a1539b0>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f26cae0>

    def test_peer_review_with_revision_cycle_succeeds(self,
mock_agent_with_expertise):
        """Test the peer review process with a revision cycle.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        author_agent = mock_agent_with_expertise("AuthorAgent", ["python",
"coding"])
        reviewer1 = mock_agent_with_expertise("ReviewerAgent1", ["testing",
"quality"])
        reviewer2 = mock_agent_with_expertise(
            "ReviewerAgent2", ["security", "best_practices"]
        )
        team.add_agent(author_agent)
        team.add_agent(reviewer1)
        team.add_agent(reviewer2)
        work_product = {
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        review = team.request_peer_review(
            work_product=work_product,
            author=author_agent,
            reviewer_agents=[reviewer1, reviewer2],
        )
        for reviewer in review.reviewers:
            review.reviews[reviewer.name] = {
                "overall_feedback": "The code needs improvement",
                "suggestions": [
                    "Use a secure password hashing algorithm",
                    "Add input validation",
                ],
                "approved": False,
            }
        review.collect_reviews()
        review.request_revision()
        assert review.status == "revision_requested"
        revised_work = {
            "code": """def authenticate(username, password):
    # Validate inputs
    if not username or not password:
        return False

    # In a real system, this would use a secure password hashing algorithm
    # and compare against stored hashed passwords
    import hashlib
    hashed_password = hashlib.sha256(password.encode()).hexdigest()
    return username == 'admin' and hashed_password ==
'5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8'
    """,
            "description": "Improved authentication function with input
validation and hashing",
        }
        new_review = review.submit_revision(revised_work)
        assert new_review.previous_review == review
        assert new_review.work_product == revised_work
        for reviewer in new_review.reviewers:
            new_review.reviews[reviewer.name] = {
                "overall_feedback": "The code is now acceptable",
                "suggestions": [],
                "approved": True,
            }
        new_review.collect_reviews()
        new_review.quality_score = 0.9
        final_result = new_review.finalize(approved=True)
        assert final_result["status"] == "approved"
>       assert final_result["previous_review_id"] == review.review_id
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'previous_review_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:424: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,140 - devsynth.domain.models.wsde_core - INFO - Added agent
AuthorAgent to team TestTeam
2025-10-28 10:30:33,140 - devsynth.domain.models.wsde_core - INFO - Added agent
ReviewerAgent1 to team TestTeam
2025-10-28 10:30:33,140 - devsynth.domain.models.wsde_core - INFO - Added agent
ReviewerAgent2 to team TestTeam
2025-10-28 10:30:33,140 - devsynth.domain.models.wsde_roles - INFO - Selected
AuthorAgent as primus based on expertise
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent',
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'ReviewerAgent1', 'worker':
'ReviewerAgent2', 'supervisor': 'AuthorAgent', 'designer': None, 'evaluator':
None}
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task aeccd73e-bef0-45bf-b3f9-9baf217d3722
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task aeccd73e-bef0-45bf-b3f9-9baf217d3722
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'ReviewerAgent1',
'supervisor': 'ReviewerAgent2', 'designer': None, 'evaluator': None}
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task e6a63dc5-2a74-4214-a4a3-5f602ddd92d3
2025-10-28 10:30:33,141 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task e6a63dc5-2a74-4214-a4a3-5f602ddd92d3
2025-10-28 10:30:33,142 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
2025-10-28 10:30:33,142 - devsynth.application.collaboration.peer_review - ERROR
- Error in finalize: 'dict' object has no attribute 'reviewer'
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
ReviewerAgent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
ReviewerAgent2 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent',
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'ReviewerAgent1', 'worker': 'ReviewerAgent2',
'supervisor': 'AuthorAgent', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task aeccd73e-bef0-45bf-b3f9-9baf217d3722
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task aeccd73e-bef0-45bf-b3f9-9baf217d3722
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'ReviewerAgent1',
'supervisor': 'ReviewerAgent2', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task e6a63dc5-2a74-4214-a4a3-5f602ddd92d3
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task e6a63dc5-2a74-4214-a4a3-5f602ddd92d3
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
ERROR    devsynth.application.collaboration.peer_review:logging_setup.py:615
Error in finalize: 'dict' object has no attribute 'reviewer'
_______ TestWSDETeam.test_peer_review_with_dialectical_analysis_succeeds _______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a150e60>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f261940>

        def test_peer_review_with_dialectical_analysis_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the peer review process with dialectical analysis.

            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            author_agent = mock_agent_with_expertise("AuthorAgent", ["python",
"coding"])
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique"]
            )
            team.add_agent(author_agent)
            team.add_agent(critic_agent)
            work_product = {
                "code": """def authenticate(username, password):
        return username == 'admin' and password == 'password'""",
                "description": "Simple authentication function",
            }
            review = team.request_peer_review(
                work_product=work_product,
                author=author_agent,
                reviewer_agents=[critic_agent],
            )
            dialectical_analysis = {
                "thesis": {
                    "strengths": [
                        "Simple and easy to understand",
                        "Functional for basic use cases",
                    ],
                    "key_points": ["Direct string comparison for
authentication"],
                },
                "antithesis": {
                    "weaknesses": [
                        "Security vulnerability: Hardcoded credentials",
                        "No input validation",
                        "No error handling",
                        "No password hashing",
                    ],
                    "challenges": [
                        "Insecure for production use",
                        "Vulnerable to timing attacks",
                    ],
                },
                "synthesis": {
                    "improvements": [
                        "Use secure password hashing",
                        "Add input validation",
                        "Implement proper error handling",
                        "Use environment variables or a secure configuration for
credentials",
                    ],
                    "improved_solution": """def authenticate(username,
password):
        # Validate inputs
        if not username or not password:
            return False

        try:
            # In a real system, this would use a secure password hashing
algorithm
            # and compare against stored hashed passwords
            import hashlib
            import hmac
            import os

            # Use constant-time comparison to prevent timing attacks
            stored_hash = hashlib.sha256(os.environ.get('ADMIN_PASSWORD',
'').encode()).hexdigest()
            user_hash = hashlib.sha256(password.encode()).hexdigest()

            return username == os.environ.get('ADMIN_USERNAME', '') and
hmac.compare_digest(stored_hash, user_hash)
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False
    """,
                },
            }
            review.reviews[critic_agent.name] = {
                "overall_feedback": "The code needs significant improvement for
security",
                "dialectical_analysis": dialectical_analysis,
                "approved": False,
            }
            review.collect_reviews()
>           feedback = review.aggregate_feedback()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:505:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:945: in aggregate_feedback
    record = self._build_peer_review_record(decisions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:537: in _build_peer_review_record
    reviewer_names = tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <tuple_iterator object at 0x12e5fa140>

    reviewer_names = tuple(
>       decision.reviewer or "unknown" for decision in decision_list
        ^^^^^^^^^^^^^^^^^
    )
E   AttributeError: 'dict' object has no attribute 'reviewer'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:538: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,156 - devsynth.domain.models.wsde_core - INFO - Added agent
AuthorAgent to team TestTeam
2025-10-28 10:30:33,157 - devsynth.domain.models.wsde_core - INFO - Added agent
CriticAgent to team TestTeam
2025-10-28 10:30:33,157 - devsynth.domain.models.wsde_roles - INFO - Selected
AuthorAgent as primus based on expertise
2025-10-28 10:30:33,157 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'CriticAgent', 'worker': 'AuthorAgent',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:30:33,158 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'CriticAgent',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:30:33,158 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 1ef5a6db-4e68-45f7-9a28-d7c3a81f80ef
2025-10-28 10:30:33,158 - devsynth.domain.models.wsde_core - WARNING - Cannot
apply dialectical reasoning: no solution provided
2025-10-28 10:30:33,158 - devsynth.application.collaboration.peer_review -
WARNING - Error applying dialectical reasoning: 'NoneType' object has no
attribute 'get'
2025-10-28 10:30:33,158 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 57c79e31-4b5b-4827-b040-018626066d80
2025-10-28 10:30:33,158 - devsynth.domain.models.wsde_voting - WARNING - Cannot
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'CriticAgent', 'worker': 'AuthorAgent',
'supervisor': None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'CriticAgent',
'supervisor': None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 1ef5a6db-4e68-45f7-9a28-d7c3a81f80ef
WARNING  devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply
dialectical reasoning: no solution provided
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615
Error applying dialectical reasoning: 'NoneType' object has no attribute 'get'
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 57c79e31-4b5b-4827-b040-018626066d80
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build
consensus: no options provided
_____________ TestWSDETeam.test_contextdriven_leadership_succeeds ______________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a153e30>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f2631a0>

    def test_contextdriven_leadership_succeeds(self, mock_agent_with_expertise):
        """Test context-driven leadership in the WSDE team.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        python_agent = mock_agent_with_expertise("PythonAgent", ["python",
"backend"])
        js_agent = mock_agent_with_expertise("JSAgent", ["javascript",
"frontend"])
        security_agent = mock_agent_with_expertise(
            "SecurityAgent", ["security", "authentication"]
        )
        design_agent = mock_agent_with_expertise("DesignAgent", ["design", "ui",
"ux"])
        doc_agent = mock_agent_with_expertise(
            "DocAgent", ["documentation", "technical_writing"]
        )
        team.add_agent(python_agent)
        team.add_agent(js_agent)
        team.add_agent(security_agent)
        team.add_agent(design_agent)
        team.add_agent(doc_agent)
        doc_task = {
            "type": "documentation_task",
            "description": "Write API documentation",
            "domain": "documentation",
            "requirements": ["Clear examples", "Complete coverage"],
        }
        team.select_primus_by_expertise(doc_task)
        assert team.get_primus() == doc_agent
        assert doc_agent.current_role == "Primus"
        roles = [agent.current_role for agent in team.agents]
>       assert "Worker" in roles
E       AssertionError: assert 'Worker' in [None, None, None, None, 'Primus']

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:547: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,222 - devsynth.domain.models.wsde_core - INFO - Added agent
PythonAgent to team TestTeam
2025-10-28 10:30:33,222 - devsynth.domain.models.wsde_core - INFO - Added agent
JSAgent to team TestTeam
2025-10-28 10:30:33,222 - devsynth.domain.models.wsde_core - INFO - Added agent
SecurityAgent to team TestTeam
2025-10-28 10:30:33,222 - devsynth.domain.models.wsde_core - INFO - Added agent
DesignAgent to team TestTeam
2025-10-28 10:30:33,223 - devsynth.domain.models.wsde_core - INFO - Added agent
DocAgent to team TestTeam
2025-10-28 10:30:33,225 - devsynth.domain.models.wsde_roles - INFO - Selected
DocAgent as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
PythonAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
JSAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
SecurityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
DesignAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
DocAgent to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
DocAgent as primus based on expertise
___ TestWSDETeam.test_dialectical_reasoning_with_external_knowledge_succeeds ___

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a164350>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f262840>

        def test_dialectical_reasoning_with_external_knowledge_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the dialectical reasoning process with external knowledge
integration.

            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            code_agent = mock_agent_with_expertise("CodeAgent", ["python",
"coding"])
            security_agent = mock_agent_with_expertise(
                "SecurityAgent", ["security", "authentication"]
            )
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique"]
            )
            task = {
                "type": "implementation_task",
                "description": "Implement a secure user authentication system
with multi-factor authentication",
            }
            thesis = {
                "agent": "CodeAgent",
                "content": "Implement authentication using username/password
with JWT tokens",
                "code": """
    def authenticate(username, password):
        if username == 'admin' and password == 'password':
            token = generate_jwt_token(username)
            return token
        return None

    def generate_jwt_token(username):
        # Generate a JWT token
        return "jwt_token_placeholder"
                """,
            }
            team.add_solution(task, thesis)
            external_knowledge = {
                "security_best_practices": {
                    "authentication": [
                        "Use multi-factor authentication for sensitive
operations",
                        "Store passwords using strong, adaptive hashing
algorithms (e.g., bcrypt, Argon2)",
                        "Implement rate limiting to prevent brute force
attacks",
                        "Use HTTPS for all authentication requests",
                        "Set secure and HttpOnly flags on authentication
cookies",
                    ],
                    "data_protection": [
                        "Encrypt sensitive data at rest and in transit",
                        "Implement proper access controls",
                        "Follow the principle of least privilege",
                        "Regularly audit access to sensitive data",
                        "Have a data breach response plan",
                    ],
                },
                "industry_standards": {
                    "OWASP": [
                        "OWASP Top 10 Web Application Security Risks",
                        "OWASP Application Security Verification Standard
(ASVS)",
                        "OWASP Secure Coding Practices",
                    ],
                    "ISO": [
                        "ISO/IEC 27001 - Information security management",
                        "ISO/IEC 27002 - Code of practice for information
security controls",
                    ],
                    "NIST": [
                        "NIST Special Publication 800-53 - Security and Privacy
Controls",
                        "NIST Cybersecurity Framework",
                    ],
                },
                "compliance_requirements": {
                    "GDPR": [
                        "Obtain explicit consent for data collection",
                        "Provide mechanisms for users to access, modify, and
delete their data",
                        "Report data breaches within 72 hours",
                        "Conduct Data Protection Impact Assessments (DPIA)",
                    ],
                    "HIPAA": [
                        "Implement technical safeguards for PHI",
                        "Conduct regular risk assessments",
                        "Maintain audit trails of PHI access",
                        "Have Business Associate Agreements (BAA) in place",
                    ],
                    "PCI-DSS": [
                        "Maintain a secure network and systems",
                        "Protect cardholder data",
                        "Implement strong access control measures",
                        "Regularly test security systems and processes",
                    ],
                },
            }
>           dialectical_result =
team.apply_enhanced_dialectical_reasoning_with_knowledge(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^
                task, critic_agent, external_knowledge
            )
E           AttributeError: 'WSDETeam' object has no attribute
'apply_enhanced_dialectical_reasoning_with_knowledge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:639: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,252 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 1b50e969-72cb-4dbe-a822-5bebed788b03
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 1b50e969-72cb-4dbe-a822-5bebed788b03
_____ TestWSDETeam.test_multi_disciplinary_dialectical_reasoning_succeeds ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a164830>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f242a20>

        def test_multi_disciplinary_dialectical_reasoning_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the dialectical reasoning process with multiple disciplinary
perspectives.

            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            code_agent = mock_agent_with_expertise("CodeAgent", ["python",
"coding"])
            security_agent = mock_agent_with_expertise(
                "SecurityAgent", ["security", "authentication"]
            )
            ux_agent = mock_agent_with_expertise(
                "UXAgent", ["user_experience", "interface_design"]
            )
            performance_agent = mock_agent_with_expertise(
                "PerformanceAgent", ["performance", "optimization"]
            )
            accessibility_agent = mock_agent_with_expertise(
                "AccessibilityAgent", ["accessibility", "inclusive_design"]
            )
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique",
"synthesis"]
            )
            team.add_agent(code_agent)
            team.add_agent(security_agent)
            team.add_agent(ux_agent)
            team.add_agent(performance_agent)
            team.add_agent(accessibility_agent)
            team.add_agent(critic_agent)
            task = {
                "type": "implementation_task",
                "description": "Implement a user authentication system with a
focus on security, usability, performance, and accessibility",
            }
            thesis = {
                "agent": "CodeAgent",
                "content": "Implement authentication using username/password
with JWT tokens",
                "code": """
    def authenticate(username, password):
        if username == 'admin' and password == 'password':
            token = generate_jwt_token(username)
            return token
        return None

    def generate_jwt_token(username):
        # Generate a JWT token
        return "jwt_token_placeholder"
                """,
            }
            team.add_solution(task, thesis)
            disciplinary_knowledge = {
                "security": {
                    "authentication_best_practices": [
                        "Use multi-factor authentication for sensitive
operations",
                        "Store passwords using strong, adaptive hashing
algorithms (e.g., bcrypt, Argon2)",
                        "Implement rate limiting to prevent brute force
attacks",
                        "Use HTTPS for all authentication requests",
                        "Set secure and HttpOnly flags on authentication
cookies",
                    ]
                },
                "user_experience": {
                    "authentication_ux_principles": [
                        "Minimize friction in the authentication process",
                        "Provide clear error messages for failed authentication
attempts",
                        "Offer password recovery options",
                        "Remember user preferences where appropriate",
                        "Support single sign-on where possible",
                    ]
                },
                "performance": {
                    "authentication_performance_considerations": [
                        "Optimize token validation for minimal latency",
                        "Cache frequently used authentication data",
                        "Use asynchronous processing for non-critical
authentication tasks",
                        "Implement efficient database queries for user lookup",
                        "Monitor and optimize authentication service response
times",
                    ]
                },
                "accessibility": {
                    "authentication_accessibility_guidelines": [
                        "Ensure all authentication forms are keyboard
navigable",
                        "Provide appropriate ARIA labels for authentication form
elements",
                        "Support screen readers for error messages and
instructions",
                        "Maintain sufficient color contrast for text and
interactive elements",
                        "Allow authentication timeout extensions for users who
need more time",
                    ]
                },
            }
            dialectical_result =
team.apply_multi_disciplinary_dialectical_reasoning(
                task,
                critic_agent,
                disciplinary_knowledge,
                [security_agent, ux_agent, performance_agent,
accessibility_agent],
            )
>           assert "thesis" in dialectical_result
E           AssertionError: assert 'thesis' in {'reason': 'no_solution',
'status': 'failed'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:751: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,277 - devsynth.domain.models.wsde_core - INFO - Added agent
CodeAgent to team TestTeam
2025-10-28 10:30:33,277 - devsynth.domain.models.wsde_core - INFO - Added agent
SecurityAgent to team TestTeam
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_core - INFO - Added agent
UXAgent to team TestTeam
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_core - INFO - Added agent
PerformanceAgent to team TestTeam
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_core - INFO - Added agent
AccessibilityAgent to team TestTeam
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_core - INFO - Added agent
CriticAgent to team TestTeam
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 9c2b2ef1-da5a-4cd7-97f0-e615d2225ad5
2025-10-28 10:30:33,278 - devsynth.domain.models.wsde_multidisciplinary -
WARNING - Cannot apply multi-disciplinary dialectical reasoning: no solution
provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
CodeAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
SecurityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
UXAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
PerformanceAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
AccessibilityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 9c2b2ef1-da5a-4cd7-97f0-e615d2225ad5
WARNING  devsynth.domain.models.wsde_multidisciplinary:logging_setup.py:615
Cannot apply multi-disciplinary dialectical reasoning: no solution provided
____ TestWSDETeam.test_assign_roles_for_phase_varied_contexts_has_expected _____

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a164d10>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f262b60>

    def test_assign_roles_for_phase_varied_contexts_has_expected(
        self, mock_agent_with_expertise
    ):
        """Test that different phases can have different primus agents.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expand_agent = mock_agent_with_expertise(
            "Expand", ["brainstorming", "exploration", "creativity"]
        )
        diff_agent = mock_agent_with_expertise(
            "Diff", ["comparison", "analysis", "evaluation"]
        )
        refine_agent = mock_agent_with_expertise(
            "Refine", ["implementation", "coding", "development"]
        )
        doc_agent = mock_agent_with_expertise(
            "Doc", ["documentation", "reflection", "learning"]
        )
        team.add_agents([expand_agent, diff_agent, refine_agent, doc_agent])
        team.assign_roles_for_phase(Phase.EXPAND, {"description": "demo"})
        team.primus_index = team.agents.index(expand_agent)
>       team.role_assignments["primus"] = expand_agent
        ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'.
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:800: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_core - INFO - Added agent
Expand to team TestTeam
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_core - INFO - Added agent
Diff to team TestTeam
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_core - INFO - Added agent
Refine to team TestTeam
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_core - INFO - Added agent
Doc to team TestTeam
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_roles - INFO - Assigning
roles for phase EXPAND in team TestTeam
2025-10-28 10:30:33,298 - devsynth.domain.models.wsde_roles - INFO - Selected
Expand as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Expand to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Diff
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Refine to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles
for phase EXPAND in team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Expand
as primus based on expertise
______ TestWSDETeam.test_vote_on_critical_decision_majority_path_succeeds ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a1651f0>
mock_agent = <MagicMock spec='BaseAgent' id='4510892592'>

    def test_vote_on_critical_decision_majority_path_succeeds(self, mock_agent):
        """Test that vote on critical decision majority path succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = mock_agent
        a1.name = "a1"
        a2 = MagicMock(spec=BaseAgent)
        a2.name = "a2"
        a3 = MagicMock(spec=BaseAgent)
        a3.name = "a3"
        for a in [a1, a2, a3]:
            a.process = MagicMock()
            team.add_agent(a)
        a1.process.return_value = {"vote": "o1"}
        a2.process.return_value = {"vote": "o1"}
        a3.process.return_value = {"vote": "o2"}
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": [{"id": "o1"}, {"id": "o2"}],
        }
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["winner"] == "o1"
E       AssertionError: assert 'o2' == 'o1'
E
E         - o1
E         + o2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:838: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,335 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestTeam
2025-10-28 10:30:33,335 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestTeam
2025-10-28 10:30:33,335 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestTeam
______ TestWSDETeam.test_vote_on_critical_decision_weighted_path_succeeds ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a1656d0>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f261bc0>

    def test_vote_on_critical_decision_weighted_path_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that vote on critical decision weighted path succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expert = mock_agent_with_expertise("expert", ["security"])
        expert.config = MagicMock()
        expert.config.name = "expert"
        expert.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "expert",
        }
        intermediate = mock_agent_with_expertise("inter", ["security"])
        intermediate.config = MagicMock()
        intermediate.config.name = "inter"
        intermediate.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "intermediate",
        }
        novice = mock_agent_with_expertise("novice", ["python"])
        novice.config = MagicMock()
        novice.config.name = "novice"
        novice.config.parameters = {
            "expertise": ["python"],
            "expertise_level": "novice",
        }
        for a in [expert, intermediate, novice]:
            a.process = MagicMock()
            team.add_agent(a)
        expert.process.return_value = {"vote": "b"}
        intermediate.process.return_value = {"vote": "a"}
        novice.process.return_value = {"vote": "a"}
        task = {
            "type": "critical_decision",
            "domain": "security",
            "is_critical": True,
            "options": [{"id": "a"}, {"id": "b"}],
        }
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "weighted_vote"
E       AssertionError: assert 'majority_vote' == 'weighted_vote'
E
E         - weighted_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:882: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,350 - devsynth.domain.models.wsde_core - INFO - Added agent
expert to team TestTeam
2025-10-28 10:30:33,351 - devsynth.domain.models.wsde_core - INFO - Added agent
inter to team TestTeam
2025-10-28 10:30:33,351 - devsynth.domain.models.wsde_core - INFO - Added agent
novice to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
expert to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent inter
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
novice to team TestTeam
_
TestWSDETeam.test_documentation_task_selects_doc_agent_and_updates_role_assignme
nts_succeeds _

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a165bb0>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f262700>

    def
test_documentation_task_selects_doc_agent_and_updates_role_assignments_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that documentation task selects doc agent and updates role
assignments succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        coder = mock_agent_with_expertise("Coder", ["python"])
        coder.has_been_primus = True
        doc_agent = mock_agent_with_expertise("Doc", ["documentation",
"markdown"])
        doc_agent.has_been_primus = False
        team.add_agents([coder, doc_agent])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
        assert team.get_primus() is doc_agent
>       assert team.role_assignments["primus"] is doc_agent
               ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'.
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:901: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,371 - devsynth.domain.models.wsde_core - INFO - Added agent
Coder to team TestTeam
2025-10-28 10:30:33,371 - devsynth.domain.models.wsde_core - INFO - Added agent
Doc to team TestTeam
2025-10-28 10:30:33,371 - devsynth.domain.models.wsde_roles - INFO - Selected
Doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Doc as
primus based on expertise
______ TestWSDETeam.test_select_primus_fallback_when_no_expertise_matches ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a166090>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f260360>

    def test_select_primus_fallback_when_no_expertise_matches(
        self, mock_agent_with_expertise
    ):
        """Test that select primus fallback when no expertise matches.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = mock_agent_with_expertise("A1", ["python"])
        a2 = mock_agent_with_expertise("A2", ["javascript"])
        a3 = mock_agent_with_expertise("A3", ["design"])
        for a in [a1, a2, a3]:
            a.has_been_primus = True
            team.add_agent(a)
        task = {"type": "unknown", "topic": "nothing"}
        team.select_primus_by_expertise(task)
        assert team.get_primus() is a1
>       assert team.role_assignments["primus"] is a1
               ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'.
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:920: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,418 - devsynth.domain.models.wsde_core - INFO - Added agent
A1 to team TestTeam
2025-10-28 10:30:33,418 - devsynth.domain.models.wsde_core - INFO - Added agent
A2 to team TestTeam
2025-10-28 10:30:33,418 - devsynth.domain.models.wsde_core - INFO - Added agent
A3 to team TestTeam
2025-10-28 10:30:33,418 - devsynth.domain.models.wsde_roles - INFO - Selected A1
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A3 to
team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected A1 as
primus based on expertise
________ TestWSDETeam.test_documentation_expert_becomes_primus_succeeds ________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at
0x11a166570>
mock_agent_with_expertise = <function
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x12f262520>

    def test_documentation_expert_becomes_primus_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that documentation expert becomes primus succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        generalist = mock_agent_with_expertise("Generalist", ["python"])
        doc_agent = mock_agent_with_expertise("Doc", ["documentation"])
        team.add_agents([generalist, doc_agent])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc_agent
E       AssertionError: assert <MagicMock spec='BaseAgent' id='5081904192'> is
<MagicMock spec='BaseAgent' id='5072532208'>
E        +  where <MagicMock spec='BaseAgent' id='5081904192'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ee7b020>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:934: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,448 - devsynth.domain.models.wsde_core - INFO - Added agent
Generalist to team TestTeam
2025-10-28 10:30:33,449 - devsynth.domain.models.wsde_core - INFO - Added agent
Doc to team TestTeam
2025-10-28 10:30:33,449 - devsynth.domain.models.wsde_roles - INFO - Selected
Generalist as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Generalist to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Generalist as primus based on expertise
___________ test_vote_on_critical_decision_not_critical_raises_error ___________

    def test_vote_on_critical_decision_not_critical_raises_error():
        """Test that vote_on_critical_decision returns an error when task is not
critical.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        result = team.vote_on_critical_decision({"type": "other"})
>       assert result["voting_initiated"] is False
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_voting_invalid.py:14: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,468 - devsynth.domain.models.wsde_voting - WARNING - Cannot
conduct vote: no agents in team
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct
vote: no agents in team
____________ test_vote_on_critical_decision_no_options_raises_error ____________

    def test_vote_on_critical_decision_no_options_raises_error():
        """Test that vote_on_critical_decision returns an error when no options
are provided.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        task = {"type": "critical_decision", "is_critical": True, "options": []}
        result = team.vote_on_critical_decision(task)
>       assert result["voting_initiated"] is False
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_voting_invalid.py:25: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,475 - devsynth.domain.models.wsde_voting - WARNING - Cannot
conduct vote: no agents in team
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct
vote: no agents in team
____________ test_majority_vote_with_three_unique_choices_succeeds _____________

    def test_majority_vote_with_three_unique_choices_succeeds():
        """Test that when three agents vote for three different options, it
results in a tie.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", "option1")
        a2 = _make_agent("a2", "option2")
        a3 = _make_agent("a3", "option3")
        team.add_agents([a1, a2, a3])
        task = _basic_task()
        with patch.object(team, "build_consensus", return_value={"consensus":
""}):
            result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "tied_vote"
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'method'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:47: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,487 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestTeam
2025-10-28 10:30:33,488 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestTeam
2025-10-28 10:30:33,488 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestTeam
_________________ test_tie_triggers_handle_tied_vote_succeeds __________________

self = <MagicMock name='_handle_tied_vote' id='5067374560'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected '_handle_tied_vote' to have been called
once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

    def test_tie_triggers_handle_tied_vote_succeeds():
        """Test that a tie between two options triggers the _handle_tied_vote
method.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", "option1")
        a2 = _make_agent("a2", "option2")
        team.add_agents([a1, a2])
        task = _basic_task()
        with (
            patch.object(team, "build_consensus", return_value={"consensus":
""}),
            patch.object(team, "_handle_tied_vote",
wraps=team._handle_tied_vote) as mocked,
        ):
            team.vote_on_critical_decision(task)
>           mocked.assert_called_once()
E           AssertionError: Expected '_handle_tied_vote' to have been called
once. Called 0 times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:65: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,499 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestTeam
2025-10-28 10:30:33,499 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
______________ test_weighted_voting_prefers_expert_vote_succeeds _______________

    def test_weighted_voting_prefers_expert_vote_succeeds():
        """Test that weighted voting gives preference to expert votes over
novice votes.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expert = _make_agent("expert", "option2", ["security"], level="expert")
        novice1 = _make_agent("novice1", "option1", ["security"],
level="novice")
        novice2 = _make_agent("novice2", "option1", ["security"],
level="novice")
        team.add_agents([expert, novice1, novice2])
        task = _basic_task()
        task["domain"] = "security"
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "weighted_vote"
E       AssertionError: assert 'majority_vote' == 'weighted_vote'
E
E         - weighted_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:80: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,542 - devsynth.domain.models.wsde_core - INFO - Added agent
expert to team TestTeam
2025-10-28 10:30:33,542 - devsynth.domain.models.wsde_core - INFO - Added agent
novice1 to team TestTeam
2025-10-28 10:30:33,542 - devsynth.domain.models.wsde_core - INFO - Added agent
novice2 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
expert to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
novice1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
novice2 to team TestTeam
_______________ test_vote_on_critical_decision_no_votes_succeeds _______________

    def test_vote_on_critical_decision_no_votes_succeeds():
        """Test that vote_on_critical_decision handles the case when no votes
are cast.

        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", vote=None)
        a1.process.return_value = {}
        team.add_agent(a1)
        task = _basic_task()
        result = team.vote_on_critical_decision(task)
>       assert result["voting_initiated"] is True
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:94: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:33,551 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
_
TestWSDEVotingMechanisms.test_vote_on_critical_decision_initiates_voting_succeed
s _

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a178710>

    def test_vote_on_critical_decision_initiates_voting_succeeds(self):
        """Test that vote_on_critical_decision initiates a voting process.

        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option3"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
>       assert "voting_initiated" in result
E       AssertionError: assert 'voting_initiated' in {'explanation': 'Vote tied;
initiated consensus process.', 'id': '7e5d0934-c92e-45b3-adde-416ddb25c694',
'method': 'majority', 'options': ['option1', 'option2', 'option3'], ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:104: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,560 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,560 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,560 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,560 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
_ TestWSDEVotingMechanisms.test_vote_on_critical_decision_majority_vote_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a178b60>

    def test_vote_on_critical_decision_majority_vote_succeeds(self):
        """Test that vote_on_critical_decision uses majority vote to make
decisions.

        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option3"})
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert "result" in result
        assert result["result"] is not None
        assert "winner" in result["result"]
        assert result["result"]["winner"] == "option1"
>       assert "vote_counts" in result["result"]
E       AssertionError: assert 'vote_counts' in {'method': 'majority_vote',
'winner': 'option1'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:130: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,574 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,574 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,575 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,575 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
__ TestWSDEVotingMechanisms.test_vote_on_critical_decision_tied_vote_succeeds __

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a179040>

    def test_vote_on_critical_decision_tied_vote_succeeds(self):
        """Test that vote_on_critical_decision handles tied votes correctly.

        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option2"})
        self.team.build_consensus = MagicMock(
            return_value={
                "consensus": "Use a hybrid architecture combining microservices
and monolith",
                "contributors": ["agent1", "agent2", "agent3", "agent4"],
                "method": "consensus_synthesis",
                "reasoning": "Combined the best elements from both options",
            }
        )
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert "result" in result
>       assert "tied" in result["result"]
E       AssertionError: assert 'tied' in {'method': 'majority_vote', 'winner':
'option1'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:155: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,595 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,596 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,596 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,596 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
_ TestWSDEVotingMechanisms.test_vote_on_critical_decision_weighted_vote_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a179520>

    def test_vote_on_critical_decision_weighted_vote_succeeds(self):
        """Test that vote_on_critical_decision uses weighted voting for
domain-specific decisions.

        ReqID: N/A"""
        self.agent1.config.parameters = {
            "expertise": ["security", "encryption", "authentication"],
            "expertise_level": "expert",
        }
        self.agent2.config.parameters = {
            "expertise": ["security", "firewalls"],
            "expertise_level": "intermediate",
        }
        self.agent3.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "novice",
        }
        self.agent4.config.parameters = {
            "expertise": ["python", "javascript"],
            "expertise_level": "intermediate",
        }
        self.agent1.process = MagicMock(return_value={"vote": "option2"})
        self.agent2.process = MagicMock(return_value={"vote": "option1"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option3"})
        result = self.team.vote_on_critical_decision(self.domain_task)
        assert "result" in result
        assert result["result"] is not None
        if "winner" in result["result"]:
            assert result["result"]["winner"] == "option2"
        elif "tied" in result["result"] and result["result"]["tied"]:
            assert "tie_breaking_attempts" in result["result"]
            assert len(result["result"]["tie_breaking_attempts"]) > 0
            assert result["result"]["tie_breaking_attempts"][0]["winner"] ==
"option2"
        assert "votes" in result
        assert result["votes"]["agent1"] == "option2"
>       assert result["votes"]["agent2"] == "option1"
E       AssertionError: assert 'option2' == 'option1'
E
E         - option1
E         ?       ^
E         + option2
E         ?       ^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:209: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,607 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,607 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,607 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,607 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
_
TestWSDEVotingMechanisms.test_vote_on_critical_decision_records_results_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a179a00>

    def test_vote_on_critical_decision_records_results_succeeds(self):
        """Test that vote_on_critical_decision records the voting results.

        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
>       assert "voting_initiated" in result
E       AssertionError: assert 'voting_initiated' in {'explanation': 'Vote tied;
initiated consensus process.', 'id': 'ecd2c3fe-ae50-4f4c-acb3-aa49f6275f68',
'method': 'majority', 'options': ['option1', 'option2', 'option3'], ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:222: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,626 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,626 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,627 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,627 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
_
TestWSDEVotingMechanisms.test_vote_on_critical_decision_updates_history_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms
object at 0x11a179ee0>

    def test_vote_on_critical_decision_updates_history_succeeds(self):
        """Ensure voting history is recorded after a vote.

        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert len(self.team.voting_history) == 1
        entry = self.team.voting_history[0]
>       assert entry["task_id"] == self.team._get_task_id(self.critical_task)
                                   ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute '_get_task_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:245: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:33,639 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_voting_mechanisms_team
2025-10-28 10:30:33,639 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_voting_mechanisms_team
2025-10-28 10:30:33,639 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_voting_mechanisms_team
2025-10-28 10:30:33,639 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_voting_mechanisms_team
_______________ test_enhanced_init_endpoint_returns_typed_error ________________

enhanced_api = <module 'devsynth.interface.agentapi_enhanced' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/age
ntapi_enhanced.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_enhanced_init_endpoint_re0')

    @pytest.mark.fast
    def test_enhanced_init_endpoint_returns_typed_error(enhanced_api, tmp_path):
        """Initialization failures surface typed error payloads.",

        ReqID: N/A"""

        request = SimpleNamespace(client=SimpleNamespace(host="9.9.9.9"))
        init_request = InitRequest(path=str(tmp_path / "does-not-exist"))

>       with pytest.raises(enhanced_api.HTTPException) as exc:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'fastapi.exceptions.HTTPException'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
pi_endpoints.py:451: Failed
_____________________ test_display_result_logging_branches _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f16bdd0>

    @pytest.mark.fast
    def test_display_result_logging_branches(monkeypatch):
        """display_result should use appropriate logging level and output; error
path uses handler."""
        import devsynth.interface.cli as cli_mod

        # Patch logger methods to capture level usage
        debug_mock = MagicMock()
        info_mock = MagicMock()
        warn_mock = MagicMock()
        error_mock = MagicMock()
        monkeypatch.setattr(cli_mod.logger, "debug", debug_mock)
        monkeypatch.setattr(cli_mod.logger, "info", info_mock)
        monkeypatch.setattr(cli_mod.logger, "warning", warn_mock)
        monkeypatch.setattr(cli_mod.logger, "error", error_mock)

        # Patch console printing and formatting
        print_mock = MagicMock()
        monkeypatch.setattr("rich.console.Console.print", print_mock)

        # Patch SharedBridgeMixin formatter to a simple pass-through marker
        monkeypatch.setattr(
            "devsynth.interface.shared_bridge.SharedBridgeMixin._format_for_outp
ut",
            lambda self, message, *, highlight=False, message_type=None:
f"FMT:{message}",
        )

        bridge = CLIUXBridge()

        # Warning branch
        bridge.display_result("warn-msg", message_type="warning")
        warn_mock.assert_called()  # logged at warning
        print_mock.assert_called()  # printed something
        args, kwargs = print_mock.call_args
>       assert args[0] == "FMT:warn-msg"
E       AssertionError: assert 'warn-msg' == 'FMT:warn-msg'
E
E         - FMT:warn-msg
E         ? ----
E         + warn-msg

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_uxbridge_noninteractive.py:77: AssertionError
________________ test_cliuxbridge_sanitizes_script_tag_succeeds ________________

clean_state = None

    @pytest.mark.fast
    def test_cliuxbridge_sanitizes_script_tag_succeeds(clean_state):
        """Test that cliuxbridge sanitizes output succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("<script>alert('x')</script>Hello")
            out.assert_called_once()
            printed_text = out.call_args[0][0]
>           assert getattr(printed_text, "plain", str(printed_text)) == "Hello"
E           AssertionError: assert '<script>aler.../script>Hello' == 'Hello'
E
E             - Hello
E             + <script>alert('x')</script>Hello

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_o
utput_sanitization.py:28: AssertionError
__________________________ test_print_alias_delegates __________________________

    def test_print_alias_delegates():
        bridge = DummyBridge()
>       bridge.print("msg", highlight=True)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_aliases.py:39:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.unit.interface.test_uxbridge_aliases.DummyBridge object at
0x12f7405f0>
message = 'msg'

    def print(
        self,
        message: str,
        *,
        highlight: bool = False,
        message_type: str | None = None,
    ) -> None:
        """Backward compatible alias for :meth:`display_result`.

        Args:
            message: Message to display to the user.
            highlight: Whether to emphasise the message.
            message_type: Optional semantic type forwarded to
                :meth:`display_result`.
        """
>       self.display_result(message, highlight=highlight,
message_type=message_type)
E       TypeError: DummyBridge.display_result() got an unexpected keyword
argument 'message_type'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: TypeError
___________________ test_lazy_streamlit_forwards_attributes ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7407d0>

    def test_lazy_streamlit_forwards_attributes(monkeypatch: pytest.MonkeyPatch)
-> None:
        """`_LazyStreamlit` proxies attribute lookups through
`_require_streamlit`."""

        calls: list[tuple[str, str | None]] = []

        class SentinelStreamlit:
            def header(self, text: str) -> str:
                calls.append(("header", text))
                return f"header::{text}"

        sentinel = SentinelStreamlit()

        def fake_require_streamlit() -> SentinelStreamlit:
            calls.append(("require", None))
            return sentinel

        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
>       monkeypatch.setattr(webui, "_require_streamlit", fake_require_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_require_streamlit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:317: AttributeError
__________________ test_require_streamlit_guidance_and_cache ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7477d0>

    def test_require_streamlit_guidance_and_cache(monkeypatch:
pytest.MonkeyPatch) -> None:
        """Lazy loader emits install guidance once and caches the module."""

        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
        sentinel = SimpleNamespace(name="streamlit-sentinel")
        import_attempts: list[str] = []

        def import_once(name: str) -> SimpleNamespace:
            import_attempts.append(name)
            return sentinel

>       monkeypatch.setattr(webui.importlib, "import_module", import_once)
                            ^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'importlib'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:344: AttributeError
____________ test_ask_question_and_confirm_choice_respects_defaults ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7447a0>

    def test_ask_question_and_confirm_choice_respects_defaults(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Select boxes and checkboxes mirror CLI defaults without Streamlit."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:383: TypeError
_____________ test_display_result_routes_error_and_highlight_paths _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f747440>

    def test_display_result_routes_error_and_highlight_paths(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Error prompts, highlights, headers, and markdown obey spec
routing."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:459: TypeError
______________ test_display_result_handles_multiple_message_types ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f747aa0>

    def test_display_result_handles_multiple_message_types(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Warnings, successes, info, and unknown types route to the right
channels."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:545: TypeError
____________ test_display_result_info_and_error_fallbacks_sanitize _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f746150>

    def test_display_result_info_and_error_fallbacks_sanitize(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Missing Streamlit channels fall back to ``write`` with sanitized
payloads."""

        monkeypatch.delattr(BehaviorStreamlitStub, "info")
        stub = install_streamlit_stub(monkeypatch)

        sanitized_inputs: list[str] = []

        def fake_sanitize(text: str) -> str:
            sanitized_inputs.append(text)
            return text.replace("<", "&lt;").replace(">", "&gt;")

>       monkeypatch.setattr(webui, "sanitize_output", fake_sanitize)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'sanitize_output'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:590: AttributeError
________________ test_display_result_markup_fallback_uses_write ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f745430>

    def test_display_result_markup_fallback_uses_write(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Markdown rendering falls back to ``write`` when ``markdown`` is
absent."""

        monkeypatch.delattr(BehaviorStreamlitStub, "markdown")
        stub = install_streamlit_stub(monkeypatch)

        sanitized_inputs: list[str] = []

        def fake_sanitize(text: str) -> str:
            sanitized_inputs.append(text)
            return text.replace("<", "&lt;")

>       monkeypatch.setattr(webui, "sanitize_output", fake_sanitize)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'sanitize_output'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:636: AttributeError
______________ test_display_result_error_prefix_triggers_guidance ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74d610>

    def test_display_result_error_prefix_triggers_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Error prefixes emit suggestions, docs, warnings, and success
markers."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:660: TypeError
_______________ test_display_result_covers_all_message_channels ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f745a60>

    def test_display_result_covers_all_message_channels(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Sanitized conversion and every message channel execute as
specified."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:707: TypeError
____________________ test_render_traceback_captures_output _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f746a80>

    def test_render_traceback_captures_output(monkeypatch: pytest.MonkeyPatch)
-> None:
        """Traceback rendering opens an expander and streams the code block."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:859: TypeError
____________________ test_error_mapping_helpers_cover_cases ____________________

    def test_error_mapping_helpers_cover_cases() -> None:
        """Error type and helper tables provide consistent guidance across
keywords."""

>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:881: TypeError
___________________ test_ui_progress_estimates_and_subtasks ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f832ea0>

    def test_ui_progress_estimates_and_subtasks(monkeypatch: pytest.MonkeyPatch)
-> None:
        """Progress lifecycle mirrors CLI telemetry with sanitized subtasks."""

        stub = install_streamlit_stub(monkeypatch)
        times = count(start=0, step=10)
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:931: AttributeError
__________ test_ui_progress_complete_cascades_and_falls_back_to_write __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f830680>

    def test_ui_progress_complete_cascades_and_falls_back_to_write(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Completing the root task finalizes subtasks and falls back when
``success`` is absent."""

        monkeypatch.delattr(BehaviorStreamlitStub, "success")
        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 20))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1037: AttributeError
______________________ test_ui_progress_eta_formats_hours ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f830920>

    def test_ui_progress_eta_formats_hours(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Long running tasks display hour-level ETAs."""

        stub = install_streamlit_stub(monkeypatch)
        times = iter([0, 1000, 2000, 3000, 4000, 5000])
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1070: AttributeError
___________ test_ui_progress_status_transitions_cover_all_thresholds ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8306b0>

    def test_ui_progress_status_transitions_cover_all_thresholds(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Status messages progress from starting through completion with
sanitized text."""

        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 40))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1092: AttributeError
_____________________ test_ui_progress_eta_minutes_branch ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f868800>

    def test_ui_progress_eta_minutes_branch(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Slow progress projections report minute-level ETAs."""

        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 10))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1158: AttributeError
________________ test_get_layout_config_breakpoints[500-1-True] ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8e8710>
screen_width = 500, expected_columns = 1, expected_mobile = True

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1190: TypeError
_______________ test_get_layout_config_breakpoints[800-2-False] ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f786cf0>
screen_width = 800, expected_columns = 2, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1190: TypeError
_______________ test_get_layout_config_breakpoints[1300-3-False] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7866f0>
screen_width = 1300, expected_columns = 3, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1190: TypeError
______________ test_get_layout_config_breakpoints[absent-3-False] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f787650>
screen_width = 'absent', expected_columns = 3, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""

        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1190: TypeError
_______________ test_run_responsive_layout_and_router_invocation _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f785b80>

    def test_run_responsive_layout_and_router_invocation(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """`run()` applies defaults, injects resize JS, and invokes the
router."""

        stub = install_streamlit_stub(monkeypatch)

        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self:
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1219: AttributeError
________________________ test_run_handles_html_failure _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7877a0>

    def test_run_handles_html_failure(monkeypatch: pytest.MonkeyPatch) -> None:
        """HTML injection failures surface as display_result messages."""

        stub = install_streamlit_stub(monkeypatch)
        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self:
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1282: AttributeError
______________________ test_run_handles_page_config_error ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f787ec0>

    def test_run_handles_page_config_error(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Configuration errors surface as WebUI messages without router
execution."""

        stub = install_streamlit_stub(monkeypatch)
        stub.page_config_error = RuntimeError("No display")
        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self:
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1332: AttributeError
__________________ test_run_without_components_invokes_router __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f761cd0>

    def test_run_without_components_invokes_router(monkeypatch:
pytest.MonkeyPatch) -> None:
        """Router still runs when components module is absent."""

        stub = install_streamlit_stub(monkeypatch)
        stub.components = None
        navigation = {"Docs": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self:
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1382: AttributeError
__________________ test_ensure_router_caches_router_instance ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7871a0>

    def test_ensure_router_caches_router_instance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """`_ensure_router` only instantiates the router once."""

        install_streamlit_stub(monkeypatch)
        init_calls: list[str] = []

        class RouterSpy:
            def __init__(
                self, owner: webui.WebUI, pages: dict[str, Callable[[], None]]
            ) -> None:
                init_calls.append("init")

            def run(self) -> None:  # pragma: no cover - not exercised in this
test
                raise AssertionError("run should not be called")

        monkeypatch.setattr(webui, "Router", RouterSpy)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1425: TypeError
_________________ test_run_module_entrypoint_invokes_webui_run _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cd0d0>

    def test_run_module_entrypoint_invokes_webui_run(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The module-level run() helper instantiates WebUI and executes
run()."""

        install_streamlit_stub(monkeypatch)
        call_sequence: list[str] = []

>       class Runner(webui.WebUI):
E       TypeError: NoneType takes no arguments

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1449: TypeError
_____________ test_webui_run_registers_router_and_hydrates_session _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cfb90>

    @pytest.mark.fast
    def test_webui_run_registers_router_and_hydrates_session(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
        recorded: dict[str, object] = {}

        class DummyRouter:
            def __init__(
                self, ui, pages
            ) -> None:  # noqa: ANN001 - interface dictated by Router
                recorded["ui"] = ui
                recorded["pages"] = dict(pages)

            def run(self) -> None:
                recorded["ran"] = True

>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:26: AttributeError
_______________ test_webui_command_dispatch_invokes_cli_targets ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cdbe0>

    @pytest.mark.fast
    def test_webui_command_dispatch_invokes_cli_targets(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:45: AttributeError
_______________ test_webui_command_dispatch_reports_value_errors _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cf800>

    @pytest.mark.fast
    def test_webui_command_dispatch_reports_value_errors(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:70: AttributeError
__________ test_z_progress_indicator_extensive_paths_cover_hierarchy ___________

bridge_live = namespace(module=<module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cf230>

    def test_z_progress_indicator_extensive_paths_cover_hierarchy(
        bridge_live: SimpleNamespace, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        """SpecRef: docs/specifications/webui-integration.md §Progress
indicators;
        docs/developer_guides/progress_indicators.md §Testing with Progress
Indicators.

        Exercises fallback descriptions, nested subtasks, and default status
        thresholds so the hierarchy mirrors the documented UX affordances.
        """

        bridge = bridge_live.module

        class Explodes:
            def __str__(self) -> str:
                raise ValueError("cannot stringify")

        sanitize_inputs: list[str] = []

        def fake_sanitize(value: str) -> str:
            sanitize_inputs.append(value)
            if value == "raise":
                raise ValueError("boom")
            return f"san::{value}"

        monkeypatch.setattr(bridge, "sanitize_output", fake_sanitize)
        tick = iter(range(1000))
        monkeypatch.setattr(bridge.time, "time", lambda: next(tick))

        indicator = bridge.WebUIProgressIndicator("start", 12)

        indicator.update(description="step-1", status="ok", advance=2)
        assert indicator._description == "san::step-1"
        assert indicator._status == "san::ok"

        indicator.update(description=Explodes(), advance=0)
        assert indicator._description == "san::step-1"

        indicator.update(status="raise", advance=0)
>       assert indicator._status == "In progress..."
E       AssertionError: assert 'Starting...' == 'In progress...'
E
E         - In progress...
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_aa_coverage.py:66: AssertionError
__________ test_nested_subtask_handles_fallbacks_and_missing_parents ___________

webui_bridge_module = (<module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int...ce/webu
i_bridge.py'>, <module 'streamlit' from <function
StreamlitRecorder.__getattr__.<locals>._noop at 0x12fa649a0>>)

    def test_nested_subtask_handles_fallbacks_and_missing_parents(
        webui_bridge_module: tuple[ModuleType, StreamlitRecorder],
    ) -> None:
        """Nested subtasks fall back to safe labels and ignore missing
parents."""

        bridge, _ = webui_bridge_module
        indicator = bridge.WebUIProgressIndicator("Main task", 10)

        parent_id = indicator.add_subtask(BadString(), total=5)
        parent = indicator._subtasks[parent_id]
>       assert parent["description"] == "<subtask>"
               ^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_fast_suite.py:82: TypeError
________ test_nested_subtask_status_progression_without_explicit_status ________

webui_bridge_module = (<module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int...ce/webu
i_bridge.py'>, <module 'streamlit' from <function
StreamlitRecorder.__getattr__.<locals>._noop at 0x12f80f880>>)

    def test_nested_subtask_status_progression_without_explicit_status(
        webui_bridge_module: tuple[ModuleType, StreamlitRecorder],
    ) -> None:
        """Omitting ``status`` triggers the automatic status lifecycle."""

        bridge, _ = webui_bridge_module
        indicator = bridge.WebUIProgressIndicator("Main task", 100)
        parent_id = indicator.add_subtask("Parent", total=100)
        nested_id = indicator.add_nested_subtask(parent_id, "Child", total=100)
>       nested = indicator._subtasks[parent_id]["nested_subtasks"][nested_id]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_fast_suite.py:121: TypeError
_____________ test_progress_indicator_nested_tasks_cover_fallbacks _____________

bridge_module = <module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>

    def test_progress_indicator_nested_tasks_cover_fallbacks(bridge_module):
        """Nested subtasks use safe placeholders and default statuses."""

        indicator = bridge_module.WebUIProgressIndicator("Task", 4)

        parent_id = indicator.add_subtask(RaisingStr(), total=4)
        parent = indicator._subtasks[parent_id]
>       assert parent["description"] == "<subtask>"
               ^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_handshake.py:108: TypeError
____________ test_progress_indicator_status_defaults_and_fallbacks _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f877a40>
bridge_module = <module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>

    def test_progress_indicator_status_defaults_and_fallbacks(
        monkeypatch: pytest.MonkeyPatch, bridge_module
    ) -> None:
        """Status strings fall back to defaults and sanitize valid updates."""

        monkeypatch.setattr(
            bridge_module, "sanitize_output", lambda value: f"S:{value}",
raising=False
        )
        indicator = bridge_module.WebUIProgressIndicator("Task", 100)

        indicator.update(description="Start")
        assert indicator._description == "S:Start"

        indicator.update(description=RaisingStr(), status=RaisingStr())
        assert indicator._description == "S:Start"
>       assert indicator._status == "In progress..."
E       AssertionError: assert 'Starting...' == 'In progress...'
E
E         - In progress...
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_handshake.py:147: AssertionError
____________ test_progress_indicator_subtasks_and_nested_operations ____________

sanitize_spy = <tests.unit.interface.test_webui_bridge_progress._SanitizeSpy
object at 0x12fa87260>

    @pytest.mark.fast
    def test_progress_indicator_subtasks_and_nested_operations(
        sanitize_spy: _SanitizeSpy,
    ) -> None:
        """Subtask lifecycle sanitizes text, handles fallbacks, and ignores
invalid IDs.

        ReqID: N/A
        """

        indicator = webui_bridge.WebUIProgressIndicator("main", 100)

        task_id = indicator.add_subtask("alpha", total=10)
>       assert sanitize_spy.calls[-1] == "alpha"
E       AssertionError: assert 'Starting...' == 'alpha'
E
E         - alpha
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_progress.py:146: AssertionError
___________________ test_nested_subtask_default_status_cycle ___________________

sanitize_spy = <tests.unit.interface.test_webui_bridge_progress._SanitizeSpy
object at 0x12f757d10>

    @pytest.mark.fast
    def test_nested_subtask_default_status_cycle(sanitize_spy: _SanitizeSpy) ->
None:
        """Nested subtasks update status text according to default progress
thresholds.

        ReqID: N/A
        """

        indicator = webui_bridge.WebUIProgressIndicator("root", 100)

        subtask_id = indicator.add_subtask("outer", total=100)
        nested_id = indicator.add_nested_subtask(subtask_id, "inner", total=100)

        # Descriptions are sanitized through the shared spy helper.
>       assert sanitize_spy.calls[-2:] == ["outer", "inner"]
E       AssertionError: assert ['inner', 'Starting...'] == ['outer', 'inner']
E
E         At index 0 diff: 'inner' != 'outer'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_progress.py:359: AssertionError
_______________ test_nested_progress_status_defaults_follow_spec _______________

bridge_env = namespace(module=<module 'devsynth.interface.webui_bridge' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f756720>

    def test_nested_progress_status_defaults_follow_spec(
        bridge_env: SimpleNamespace, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        """SpecRef: docs/specifications/webui-integration.md §Progress
indicators;
        docs/developer_guides/progress_indicators.md §UXBridge Integration.

        Nested subtasks surface the documented status thresholds and cascade
        completion through parent subtasks.
        """

        bridge = bridge_env.module
        tick = iter(range(1000))
        monkeypatch.setattr(bridge.time, "time", lambda: next(tick))

        indicator = bridge.WebUIProgressIndicator("Collect", 100)
        parent_id = indicator.add_subtask("Gather", total=40)
        nested_id = indicator.add_nested_subtask(parent_id, "Validate", total=8)

        expectations = [
            (0, "Starting..."),
            (2, "Processing..."),
            (4, "Halfway there..."),
            (6, "Almost done..."),
            (8 * 0.99, "Finalizing..."),
            (8, "Complete"),
        ]

        for progress, status in expectations:
>           indicator._subtasks[parent_id]["nested_subtasks"][nested_id][
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                "current"
            ] = progress
E           TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_spec_alignment.py:90: TypeError
______________________ test_cli_returns_module_attribute _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb6b980>

    def test_cli_returns_module_attribute(monkeypatch):
        """ReqID: FR-203 prefer WebUI overrides when resolving commands."""
        dummy = DummyCommands()

        def sentinel():
            return "ok"

        monkeypatch.setattr(webui, "dummy_cmd", sentinel, raising=False)
>       assert dummy._cli("dummy_cmd") is sentinel
E       AssertionError: assert None is <function
test_cli_returns_module_attribute.<locals>.sentinel at 0x12f8b94e0>
E        +  where None = _cli('dummy_cmd')
E        +    where _cli =
<tests.unit.interface.test_webui_commands.DummyCommands object at
0x12fcaea50>._cli

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_commands.py:40: AssertionError
______________________ test_require_streamlit_lazy_loader ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edf3da0>

    def test_require_streamlit_lazy_loader(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Lazy ``st`` proxy loads Streamlit only when accessed."""

        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:40:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_webui_display_result_highlight_succeeds _________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    def test_webui_display_result_highlight_succeeds(mock_streamlit,
clean_state):
        """Test that highlighted messages use st.info.

        ReqID: N/A"""
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:101:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ test_webui_display_result_error_raises_error _________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_error_raises_error(mock_streamlit):
        """Test that error messages use st.error.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:116:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_warning_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_warning_succeeds(mock_streamlit):
        """Test that warning messages use st.warning.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:132:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_success_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_success_succeeds(mock_streamlit):
        """Test that success messages use st.success.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:148:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_heading_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_heading_succeeds(mock_streamlit):
        """Test that heading messages use st.header.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:164:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_webui_display_result_subheading_succeeds _________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_subheading_succeeds(mock_streamlit):
        """Test that subheading messages use st.subheader.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:180:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_webui_display_result_rich_markup_succeeds ________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_rich_markup_succeeds(mock_streamlit):
        """Test that Rich markup is converted to Markdown/HTML.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:196:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_normal_succeeds ___________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_normal_succeeds(mock_streamlit):
        """Test that normal messages use st.write.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:214:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_webui_progress_indicator_succeeds ____________________

mock_streamlit = <module 'streamlit'>

    def test_webui_progress_indicator_succeeds(mock_streamlit):
        """Test the enhanced progress indicator.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_get_layout_config_breakpoints[640-expected0] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fc8ede0>
screen_width = 640
expected = {'columns': 1, 'content_width': '100%', 'font_size': 'small',
'is_mobile': True, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen
width."""

        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py💯
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fc8ede0>
stub = namespace(calls=[], session_state=namespace(screen_width=640),
markdown=<function _make_streamlit_stub.<locals>.record...cals>.method at
0x12f26ad40>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12f26aac0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_get_layout_config_breakpoints[820-expected1] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff167e0>
screen_width = 820
expected = {'columns': 2, 'content_width': '70%', 'font_size': 'medium',
'is_mobile': False, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen
width."""

        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py💯
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff167e0>
stub = namespace(calls=[], session_state=namespace(screen_width=820),
markdown=<function _make_streamlit_stub.<locals>.record...cals>.method at
0x12f268cc0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12f268d60>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_get_layout_config_breakpoints[1200-expected2] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff16060>
screen_width = 1200
expected = {'columns': 3, 'content_width': '80%', 'font_size': 'medium',
'is_mobile': False, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen
width."""

        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py💯
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff16060>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12f268720>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12f2687c0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
________________ test_display_result_rich_markup_uses_markdown _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff15d00>

    def test_display_result_rich_markup_uses_markdown(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: N/A - Rich markup renders via ``markdown`` with HTML spans."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:113:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff15d00>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12f268360>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12f268860>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
________________ test_display_result_error_type_renders_context ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b650>

    def test_display_result_error_type_renders_context(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: N/A - ``message_type='error'`` surfaces suggestions and
docs."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b650>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12f2680e0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12f268040>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_display_result_message_types[warning-warning] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0a060>
message_type = 'warning', expected_method = 'warning'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0a060>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12e77f1a0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12e77dbc0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_display_result_message_types[success-success] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b560>
message_type = 'success', expected_method = 'success'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b560>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12e778900>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12e77a980>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_________________ test_display_result_message_types[info-info] _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b4d0>
message_type = 'info', expected_method = 'info'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b4d0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x10cc8a3e0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x10cc894e0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_____________ test_display_result_message_types[unexpected-write] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff09fd0>
message_type = 'unexpected', expected_method = 'write'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff09fd0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12e2bb880>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12e2b8e00>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
___________________ test_display_result_highlight_uses_info ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b5c0>

    def test_display_result_highlight_uses_info(monkeypatch: pytest.MonkeyPatch)
-> None:
        """ReqID: N/A - Highlighting without type uses ``info`` output."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:196:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff0b5c0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x10bf6cea0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x10bf6d760>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
____________________ test_display_result_defaults_to_write _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff08320>

    def test_display_result_defaults_to_write(monkeypatch: pytest.MonkeyPatch)
-> None:
        """ReqID: N/A - Plain messages fall back to ``write`` output."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:208:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff08320>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12ef347c0>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12ef36520>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_______ test_display_result_renders_headings[# Overview-expected_calls0] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5dac30>
message = '# Overview', expected_calls = [('header', ('Overview',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5dac30>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12ef37560>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12ef36de0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_______ test_display_result_renders_headings[## Section-expected_calls1] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1f82f0>
message = '## Section', expected_calls = [('subheader', ('Section',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1f82f0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12efb6020>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12efb6e80>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_____ test_display_result_renders_headings[### Deep Dive-expected_calls2] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd1dc0>
message = '### Deep Dive'
expected_calls = [('markdown', ('**Deep Dive**',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""

        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd1dc0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200),
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at
0x12e2b8e00>, subheader=<function
_make_streamlit_stub.<locals>.record.<locals>.method at 0x12e2b8b80>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit
stub."""

>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
____________________ test_lazy_streamlit_proxy_imports_once ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db2a600>

    def test_lazy_streamlit_proxy_imports_once(monkeypatch: pytest.MonkeyPatch)
-> None:
        """``st`` forwards attribute access while caching the loaded module."""

        sentinel = SimpleNamespace(marker="streamlit-sentinel")
        imports: list[str] = []

        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)

        def fake_import(name: str) -> SimpleNamespace:
            imports.append(name)
            return sentinel

>       monkeypatch.setattr(webui.importlib, "import_module", fake_import)
                            ^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'importlib'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:107: AttributeError
____________________ test_ui_progress_tracks_status_and_eta ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db29ee0>

    def test_ui_progress_tracks_status_and_eta(monkeypatch: pytest.MonkeyPatch)
-> None:
        """Progress lifecycle updates sanitize text, surface ETA, and mark
completion."""

        stub = DummyStreamlit()
        monkeypatch.setattr(webui, "st", stub, raising=False)

        time_ticks = count(start=100, step=10)
>       monkeypatch.setattr(webui.time, "time", lambda: next(time_ticks))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:126: AttributeError
__________________ test_ensure_router_creates_single_instance __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db18aa0>

    def test_ensure_router_creates_single_instance(monkeypatch:
pytest.MonkeyPatch) -> None:
        """Router initialization defers until first access and memoizes the
instance."""

        created: list[tuple[webui.WebUI, dict[str, str]]] = []

        class RecordingRouter:
            def __init__(self, owner: webui.WebUI, navigation: dict[str, str])
-> None:
                created.append((owner, navigation))
                self.owner = owner
                self.navigation = navigation

        monkeypatch.setattr(webui, "Router", RecordingRouter)

        navigation = {"Home": "render_home", "Docs": "render_docs"}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self:
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:189: AttributeError
_______________ test_missing_streamlit_surfaces_install_guidance _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db1a300>

    def test_missing_streamlit_surfaces_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Import failures raise :class:`DevSynthError` with actionable
guidance."""

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:140: AttributeError
_____________________ test_lazy_streamlit_import_is_cached _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db18680>

    def test_lazy_streamlit_import_is_cached(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Feature: webui_core.feature Scenario: Lazy Streamlit loader caches
module."""

        from devsynth.interface import webui

        call_log: List[str] = []
        streamlit_stub = make_streamlit_mock()

        original_import = importlib.import_module

        def fake_import(name: str, package: str | None = None):
            call_log.append(name)
            if name == "streamlit":
                return streamlit_stub
            return original_import(name, package)

        monkeypatch.setattr(importlib, "import_module", fake_import)

>       module = importlib.reload(webui)
                 ^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_streamlit_and_wizard.py:60:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________ test_ui_progress_eta_displays_seconds_when_under_minute ____________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db6d280>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_seconds_when_under_minute(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA in seconds when less than a minute remains."""

>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 5.0, 10.0],
            description="ETA Seconds",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:244:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_ui_progress_eta_displays_minutes_when_under_hour _____________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5f9e0>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_minutes_when_under_hour(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA rounded to whole minutes when under an hour remains."""

>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 100.0, 200.0],
            description="ETA Minutes",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:263:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ test_ui_progress_eta_displays_hours_and_minutes ________________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5f380>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_hours_and_minutes(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA with hours and minutes when exceeding an hour."""

>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 100.0, 200.0],
            description="ETA Hours",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:282:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ test_ui_progress_status_transitions_without_explicit_status __________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5f710>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_status_transitions_without_explicit_status(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Ensure automatic status text transitions at documented thresholds."""

>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
            description="Status Thresholds",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:301:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_ui_progress_subtasks_update_with_frozen_time _______________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6b92b0>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_subtasks_update_with_frozen_time(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Subtask updates still render when the clock is frozen."""

>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[100.0],
            description="Frozen Subtasks",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:335:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ TestProjectSetupPages.test_project_setup_pages_inheritance __________

self = <tests.unit.interface.test_webui_rendering.TestProjectSetupPages object
at 0x11e8e1fd0>

    def test_project_setup_pages_inheritance(self):
        """Test that ProjectSetupPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:98: ImportError
_____________ TestLifecyclePages.test_lifecycle_pages_inheritance ______________

self = <tests.unit.interface.test_webui_rendering.TestLifecyclePages object at
0x11e8e2d80>

    def test_lifecycle_pages_inheritance(self):
        """Test that LifecyclePages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:129: ImportError
____________ TestOperationsPages.test_operations_pages_inheritance _____________

self = <tests.unit.interface.test_webui_rendering.TestOperationsPages object at
0x11e8e3b00>

    def test_operations_pages_inheritance(self):
        """Test that OperationsPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:161: ImportError
_______________ TestSupportPages.test_support_pages_inheritance ________________

self = <tests.unit.interface.test_webui_rendering.TestSupportPages object at
0x11e8f4980>

    def test_support_pages_inheritance(self):
        """Test that SupportPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:192: ImportError
________ TestWebUIRenderingUtilities.test_rendering_import_dependencies ________

self = <tests.unit.interface.test_webui_rendering.TestWebUIRenderingUtilities
object at 0x11e8f6810>

    def test_rendering_import_dependencies(self):
        """Test that rendering imports work correctly."""
        # Test that key imports are available
>       from devsynth.interface.webui.rendering import (
            LifecyclePages,
            OperationsPages,
            PageRenderer,
            ProjectSetupPages,
            SupportPages,
        )
E       ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:295: ImportError
____________________ test_require_streamlit_returns_module _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edf9970>

    @pytest.mark.fast
    def test_require_streamlit_returns_module(monkeypatch):
        """Successful import returns module.

        ReqID: N/A"""
        module = types.SimpleNamespace()
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_require_streamlit.py:16: AttributeError
________________________ test_require_streamlit_raises _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f46ff80>

    @pytest.mark.fast
    def test_require_streamlit_raises(monkeypatch):
        """Import failure raises DevSynthError.

        ReqID: N/A"""
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_require_streamlit.py:26: AttributeError
___________________ test_requirements_wizard_initialization ____________________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_initialization(stub_streamlit, clean_state):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_requirements_wizard_step_navigation_succeeds _______________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_step_navigation_succeeds(stub_streamlit,
clean_state):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:60:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_requirements_wizard_save_requirements_succeeds ______________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_save_requirements_succeeds(stub_streamlit,
clean_state):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:78:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________________ test_validate_requirements_step ________________________

stub_streamlit = <module 'streamlit'>

    def test_validate_requirements_step(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:102:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________________ test_handle_requirements_navigation_next ___________________

stub_streamlit = <module 'streamlit'>

    def test_handle_requirements_navigation_next(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:113:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_save_requirements_writes_file ______________________

stub_streamlit = <module 'streamlit'>

    def test_save_requirements_writes_file(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:130:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_priority_persists_through_navigation ___________________

stub_streamlit = <module 'streamlit'>

    def test_priority_persists_through_navigation(stub_streamlit):
        """Priority selection should persist when navigating steps."""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:156:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_title_and_description_persist ______________________

stub_streamlit = <module 'streamlit'>

    def test_title_and_description_persist(stub_streamlit):
        """Title and description should persist across navigation."""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:181:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_run_method_with_invalid_navigation_option ________________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_run_method_with_invalid_navigation_option(stub_streamlit,
clean_state):
        """Test the run method with an invalid navigation option.

        ReqID: N/A"""
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:59:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_______________ test_run_method_with_page_exception_raises_error _______________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_page_exception_raises_error(stub_streamlit):
        """Test the run method when a page method raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:82:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_run_method_with_streamlit_exception_raises_error _____________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f105ee0>

    def test_run_method_with_streamlit_exception_raises_error(stub_streamlit,
monkeypatch):
        """Test the run method when streamlit raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:106:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_run_method_with_sidebar_exception_raises_error ______________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_sidebar_exception_raises_error(stub_streamlit):
        """Test the run method when sidebar raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:126:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_run_method_with_multiple_exceptions_raises_error _____________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_multiple_exceptions_raises_error(stub_streamlit):
        """Test the run method when multiple exceptions occur.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:146:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_standalone_run_function_succeeds _____________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee420c0>

    def test_standalone_run_function_succeeds(stub_streamlit, monkeypatch):
        """Test the standalone run function.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:169:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________________ test_run_webui_alias_succeeds _________________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee40200>

    def test_run_webui_alias_succeeds(stub_streamlit, monkeypatch):
        """Test the run_webui alias function.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:187:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_rendering_simulation_records_summary_and_errors _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee401d0>

    @pytest.mark.fast
    def test_rendering_simulation_records_summary_and_errors(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """simulate_progress_rendering mirrors CLI telemetry and sanitises
errors."""

        stub = BehaviorStreamlitStub()
        harness = RenderHarness(stub)

        summary = {
            "description": "Gather <docs>",
            "progress": 0.75,
            "remaining": 45.0,
            "elapsed": 90.0,
            "eta": 200.0,
            "history": [
                {"status": "Queued <init>", "progress": 0.25, "time": 100.0},
                {"status": "Collecting", "progress": 0.5, "time": 150.0},
            ],
            "checkpoints": [
                {"progress": 0.25, "time": 110.0, "eta": 200.0},
                {"progress": 0.5, "time": 160.0, "eta": 200.0},
            ],
            "subtasks_detail": [
                {
                    "description": "Docs <survey>",
                    "progress": 1.0,
                    "status": "Complete",
                    "history": [
                        {"status": "Scanning", "progress": 0.5, "time": 140.0},
                    ],
                    "checkpoints": [
                        {"progress": 0.5, "time": 145.0, "eta": 150.0},
                    ],
                }
            ],
        }

        clock = _LinearClock(start=100.0, step=5.0)
>       result = rendering.simulate_progress_rendering(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            harness,
            summary,
            errors=["<boom & fail>"],
            clock=clock,
        )
E       AttributeError: module 'devsynth.interface.webui.rendering' has no
attribute 'simulate_progress_rendering'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:84: AttributeError
__________ test_rendering_simulation_handles_nested_summary_and_clock __________

    @pytest.mark.fast
    def test_rendering_simulation_handles_nested_summary_and_clock() -> None:
        """simulate_progress_rendering formats nested history with a scripted
clock."""

        stub = BehaviorStreamlitStub()
        harness = RenderHarness(stub)
        primary = stub.container()

        summary = {
            "description": "Main <summary>",
            "progress": 0.5,
            "eta": 250.0,
            "remaining": 120.0,
            "elapsed": 80.0,
            "history": [
                {"status": "Queued <1>", "progress": 0.1, "time": 50.0},
                {
                    "status": "Processing",
                    "completed": 30,
                    "total": 100,
                    "time": 70.0,
                },
            ],
            "checkpoints": [
                {"progress": 0.25, "time": 60.0, "eta": 250.0},
                {"completed": 80, "total": 100, "time": 80.0, "eta": 250.0},
            ],
            "subtasks_detail": [
                {
                    "description": "stage <alpha>",
                    "progress": 0.75,
                    "status": "Working <soon>",
                    "history": [
                        {"status": "Primed <1>", "progress": 0.25, "time":
40.0},
                        {
                            "status": "Working",
                            "completed": 30,
                            "total": 40,
                            "time": 70.0,
                        },
                    ],
                    "checkpoints": [
                        {"progress": 0.5, "time": 45.0, "eta": 250.0},
                    ],
                },
                {
                    "description": "stage beta",
                    "completed": 20,
                    "total": 40,
                    "status": "Queued",
                    "history": [
                        {
                            "status": "Queued",
                            "completed": 10,
                            "total": 40,
                            "time": 65.0,
                        }
                    ],
                    "checkpoints": [
                        {"completed": 20, "total": 40, "time": 75.0, "eta":
250.0},
                    ],
                },
            ],
        }

        clock = _LinearClock(start=200.0, step=0.0)
>       result = rendering.simulate_progress_rendering(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            harness,
            summary,
            container=primary,
            errors=["<primary>", "<secondary>"],
            clock=clock,
        )
E       AttributeError: module 'devsynth.interface.webui.rendering' has no
attribute 'simulate_progress_rendering'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:170: AttributeError
____________ test_ui_progress_simulation_drives_eta_and_completion _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee41ee0>

    @pytest.mark.fast
    def test_ui_progress_simulation_drives_eta_and_completion(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """WebUI._UIProgress under a stubbed Streamlit emits ETA and success
messages."""

        stub = BehaviorStreamlitStub()
        monkeypatch.setattr(webui, "st", stub, raising=False)
        monkeypatch.setattr(webui, "_STREAMLIT", stub, raising=False)

        clock_values = iter([0.0, 2.0, 4.0, 6.5, 9.0, 12.0, 15.5, 18.0, 20.0])

        def fake_time() -> float:
            try:
                return float(next(clock_values))
            except StopIteration:
                return 20.0

>       monkeypatch.setattr(webui.time, "time", fake_time, raising=False)
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:244: AttributeError
__________________ test_webui_display_result_sanitises_error ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee43fb0>

    @pytest.mark.fast
    def test_webui_display_result_sanitises_error(monkeypatch:
pytest.MonkeyPatch) -> None:
        """display_result escapes markup before routing to Streamlit error
channel."""

        stub = BehaviorStreamlitStub()
        monkeypatch.setattr(webui, "st", stub, raising=False)
        monkeypatch.setattr(webui, "_STREAMLIT", stub, raising=False)

>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:279: TypeError
______________________ test_webui_require_streamlit_cache ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee41970>

    @pytest.mark.fast
    def test_webui_require_streamlit_cache(monkeypatch: pytest.MonkeyPatch) ->
None:
        """The WebUI lazy loader imports Streamlit once and caches the
module."""

        sentinel = object()
        calls: list[str] = []

        def fake_import(name: str) -> object:
            calls.append(name)
            return sentinel

        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
        monkeypatch.setattr(importlib, "import_module", fake_import)

>       loaded = webui._require_streamlit()
                 ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'_require_streamlit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:336: AttributeError
____________ test_webui_require_streamlit_reports_install_guidance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f169100>

    @pytest.mark.fast
    def test_webui_require_streamlit_reports_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Missing Streamlit surfaces actionable guidance."""

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:356: AttributeError
__ test_webui_display_result_sanitizes_without_streamlit[error-kwargs0-error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f16a8a0>
branch = 'error', kwargs = {'message_type': 'error'}, expected_method = 'error'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed
Streamlit API."""

        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[warning-kwargs1-warning]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f169010>
branch = 'warning', kwargs = {'message_type': 'warning'}
expected_method = 'warning'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed
Streamlit API."""

        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[success-kwargs2-success]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb33b90>
branch = 'success', kwargs = {'message_type': 'success'}
expected_method = 'success'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed
Streamlit API."""

        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[highlight-kwargs3-info]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f16acf0>
branch = 'highlight', kwargs = {'highlight': True}, expected_method = 'info'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed
Streamlit API."""

        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
______________________ test_webui_ui_progress_eta_formats ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcae330>

    @pytest.mark.fast
    def test_webui_ui_progress_eta_formats(monkeypatch: pytest.MonkeyPatch) ->
None:
        """The Streamlit UI progress indicator formats ETA strings across
ranges."""

        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:521: AttributeError
_______________ test_missing_streamlit_surfaces_install_guidance _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcad880>

    def test_missing_streamlit_surfaces_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Import failures raise DevSynthError with installation
instructions."""

>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:248: AttributeError
__________ TestProjectSetupPages.test_project_setup_pages_inheritance __________

self = <test_rendering.TestProjectSetupPages object at 0x11e9bc890>

    def test_project_setup_pages_inheritance(self):
        """Test that ProjectSetupPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:98: ImportError
_____________ TestLifecyclePages.test_lifecycle_pages_inheritance ______________

self = <test_rendering.TestLifecyclePages object at 0x11e9bd730>

    def test_lifecycle_pages_inheritance(self):
        """Test that LifecyclePages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:129: ImportError
____________ TestOperationsPages.test_operations_pages_inheritance _____________

self = <test_rendering.TestOperationsPages object at 0x11e9be570>

    def test_operations_pages_inheritance(self):
        """Test that OperationsPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:161: ImportError
_______________ TestSupportPages.test_support_pages_inheritance ________________

self = <test_rendering.TestSupportPages object at 0x11e9bf380>

    def test_support_pages_inheritance(self):
        """Test that SupportPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:192: ImportError
________ TestWebUIRenderingUtilities.test_rendering_import_dependencies ________

self = <test_rendering.TestWebUIRenderingUtilities object at 0x11e9d51f0>

    def test_rendering_import_dependencies(self):
        """Test that rendering imports work correctly."""
        # Test that key imports are available
>       from devsynth.interface.webui.rendering import (
            LifecyclePages,
            OperationsPages,
            PageRenderer,
            ProjectSetupPages,
            SupportPages,
        )
E       ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:295: ImportError
__ TestLMStudioProviderAvailabilityProbing.test_server_availability_detection __

self = <MagicMock name='_require_lmstudio().sync_api.models.list'
id='5090286416'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'list' to have been called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11e9e9cd0>

    @pytest.mark.fast
    def test_server_availability_detection(self):
        """Test detection of LM Studio server availability."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock successful model list response
            mock_lmstudio.sync_api.models.list.return_value = [
                {"id": "model1", "object": "model"},
                {"id": "model2", "object": "model"},
            ]

            provider = LMStudioProvider(config)

            # Should probe server availability on initialization
>           mock_lmstudio.sync_api.models.list.assert_called_once()
E           AssertionError: Expected 'list' to have been called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:470: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,319 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,322 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,322 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,324 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,324 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___ TestLMStudioProviderAvailabilityProbing.test_server_unavailable_handling ___

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11e9ea180>

    @pytest.mark.fast
    def test_server_unavailable_handling(self):
        """Test handling when LM Studio server is unavailable."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock connection error
            from requests.exceptions import ConnectionError

            mock_lmstudio.sync_api.models.list.side_effect = ConnectionError(
                "Connection refused"
            )

            provider = LMStudioProvider(config)

            # Should handle gracefully and set server_unavailable flag
>           assert hasattr(provider, "server_unavailable")
E           AssertionError: assert False
E            +  where False =
hasattr(<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12f631760>, 'server_unavailable')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:493: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,385 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,388 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,388 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,390 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,391 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______ TestLMStudioProviderAvailabilityProbing.test_model_list_retrieval _______

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11e9ea630>

    @pytest.mark.fast
    def test_model_list_retrieval(self):
        """Test retrieval of available models list."""
        config = {"base_url": "http://localhost:1234/v1"}

        available_models = [
            {"id": "llama-2-7b", "object": "model"},
            {"id": "codellama-7b", "object": "model"},
            {"id": "mistral-7b", "object": "model"},
        ]

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.models.list.return_value = available_models

            provider = LMStudioProvider(config)

            # Should retrieve and store model list
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:518: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,416 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,420 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,420 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,422 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,422 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______ TestLMStudioProviderConfiguration.test_configuration_with_defaults ______

self = <test_lmstudio_provider.TestLMStudioProviderConfiguration object at
0x11e9eaff0>

    @pytest.mark.fast
    def test_configuration_with_defaults(self):
        """Test configuration with default values."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            provider = LMStudioProvider(config)

            assert provider.temperature == 0.7  # Default
>           assert provider.max_tokens == 4096  # Default
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           assert 1024 == 4096
E            +  where 1024 =
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12f20b200>.max_tokens

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:561: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,472 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,474 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,475 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,477 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,477 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_______ TestLMStudioProviderErrorHandling.test_invalid_temperature_range _______

self = <test_lmstudio_provider.TestLMStudioProviderErrorHandling object at
0x11e9f8bc0>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            provider = LMStudioProvider(config)

            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello", {"temperature": -0.1})

>           assert "temperature must be between" in str(exc_info.value)
E           AssertionError: assert 'temperature must be between' in 'LM Studio
API error: Network access disabled during tests (httpx). Check that LM Studio is
running and accessible.'
E            +  where 'LM Studio API error: Network access disabled during tests
(httpx). Check that LM Studio is running and accessible.' =
str(LMStudioConnectionError('LM Studio API error: Network access disabled during
tests (httpx). Check that LM Studio is running and accessible.'))
E            +    where LMStudioConnectionError('LM Studio API error: Network
access disabled during tests (httpx). Check that LM Studio is running and
accessible.') = <ExceptionInfo LMStudioConnectionError('LM Studio API error:
Network access disabled during tests (httpx). Check that LM Studio is running
and accessible.') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:684: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,622 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,624 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,624 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,627 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,627 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:30:41,629 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12f60cfe0>", "event": "Websocket handling
thread started", "thread_id": "Thread-32"}
2025-10-28 10:30:41,635 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
INFO     AsyncWebsocketThread:_ws_thread.py:138 {"client":
"<lmstudio.sync_api.Client object at 0x12f60cfe0>", "event": "Websocket handling
thread started", "thread_id": "Thread-32"}
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
__________ TestLMStudioProviderErrorHandling.test_invalid_max_tokens ___________

self = <test_lmstudio_provider.TestLMStudioProviderErrorHandling object at
0x11e9f9070>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            provider = LMStudioProvider(config)

            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello", {"max_tokens": 0})

>           assert "max_tokens must be positive" in str(exc_info.value)
E           AssertionError: assert 'max_tokens must be positive' in 'LM Studio
API error: Network access disabled during tests (httpx). Check that LM Studio is
running and accessible.'
E            +  where 'LM Studio API error: Network access disabled during tests
(httpx). Check that LM Studio is running and accessible.' =
str(LMStudioConnectionError('LM Studio API error: Network access disabled during
tests (httpx). Check that LM Studio is running and accessible.'))
E            +    where LMStudioConnectionError('LM Studio API error: Network
access disabled during tests (httpx). Check that LM Studio is running and
accessible.') = <ExceptionInfo LMStudioConnectionError('LM Studio API error:
Network access disabled during tests (httpx). Check that LM Studio is running
and accessible.') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:702: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:41,653 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12f60cfe0>", "event": "Websocket thread
terminated", "thread_id": "Thread-32"}
------------------------------ Captured log setup ------------------------------
INFO     AsyncWebsocketThread:_ws_thread.py:149 {"client":
"<lmstudio.sync_api.Client object at 0x12f60cfe0>", "event": "Websocket thread
terminated", "thread_id": "Thread-32"}
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,666 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,669 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,669 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,671 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,671 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:30:41,673 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12f6186e0>", "event": "Websocket handling
thread started", "thread_id": "Thread-33"}
2025-10-28 10:30:41,683 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
INFO     AsyncWebsocketThread:_ws_thread.py:138 {"client":
"<lmstudio.sync_api.Client object at 0x12f6186e0>", "event": "Websocket handling
thread started", "thread_id": "Thread-33"}
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
_________ TestLMStudioProviderEdgeCases.test_empty_model_list_handling _________

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at
0x11e9f9ac0>

    @pytest.mark.fast
    def test_empty_model_list_handling(self):
        """Test handling of empty model list."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock empty model list
            mock_lmstudio.sync_api.models.list.return_value = []

            provider = LMStudioProvider(config)

            # Should handle empty model list gracefully
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:774: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:30:41,694 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12f6186e0>", "event": "Websocket thread
terminated", "thread_id": "Thread-33"}
------------------------------ Captured log setup ------------------------------
INFO     AsyncWebsocketThread:_ws_thread.py:149 {"client":
"<lmstudio.sync_api.Client object at 0x12f6186e0>", "event": "Websocket thread
terminated", "thread_id": "Thread-33"}
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,710 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,713 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,713 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,716 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,716 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_____________ TestLMStudioProviderEdgeCases.test_timeout_handling ______________

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at
0x11e9f9ee0>

    @pytest.mark.fast
    def test_timeout_handling(self):
        """Test timeout handling during model listing."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock timeout during model listing
            import requests

            mock_lmstudio.sync_api.models.list.side_effect = (
                requests.exceptions.Timeout("Request timed out")
            )

            provider = LMStudioProvider(config)

            # Should handle timeout gracefully
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:798: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,745 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,747 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,747 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,749 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,749 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_________ TestLMStudioProviderEdgeCases.test_unicode_content_handling __________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12fc26e70>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
>               self._lmstudio.llm(self.model).complete,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                prompt,
                config=params,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:500:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: in __call__
    return getattr(real, self._attr)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1753: in llm
    return get_default_client().llm.model(model_key, ttl=ttl, config=config)
           ^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1726: in get_default_client
    _default_client._ensure_api_host_is_valid()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1599: in _ensure_api_host_is_valid
    elif self.is_valid_api_host(specified_api_host):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1581: in is_valid_api_host
    probe_response = cls._query_probe_url(probe_url)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1573: in _query_probe_url
    return httpx.get(url, timeout=1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_api.py:195: in get
    return request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_api.py:109: in request
    return client.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x12fc26270>, method = 'GET'
url = 'http://127.0.0.1:1234/lmstudio-greeting', args = ()
kwargs = {'auth': None, 'content': None, 'data': None, 'files': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at
0x11e9fa390>

    @pytest.mark.fast
    def test_unicode_content_handling(self):
        """Test handling of Unicode content."""
        config = {"base_url": "http://localhost:1234/v1"}

        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "test-model",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! 你好! ¡Hola! 🌟",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10,
"total_tokens": 15},
        }

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                unicode_response
            )

            provider = LMStudioProvider(config)
>           response = provider.generate("Say hello in multiple languages")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:835:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12fc26e70>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
                self._lmstudio.llm(self.model).complete,
                prompt,
                config=params,
            )
            content = getattr(result, "content", None)
            if isinstance(content, str) and content:
                return content
            raise LMStudioModelError("Invalid response from LM Studio")
        except LMStudioModelError:
            raise
        except LMStudioTokenLimitError:
            raise  # Re-raise token limit errors as-is
        except LMStudioConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:  # noqa: BLE001
            error_msg = f"LM Studio API error: {str(e)}. Check that LM Studio is
running and accessible."
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:517: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,773 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:41,776 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,776 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:30:41,778 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:30:41,778 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:30:41,779 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12fc259a0>", "event": "Websocket handling
thread started", "thread_id": "Thread-34"}
2025-10-28 10:30:41,788 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
INFO     AsyncWebsocketThread:_ws_thread.py:138 {"client":
"<lmstudio.sync_api.Client object at 0x12fc259a0>", "event": "Websocket handling
thread started", "thread_id": "Thread-34"}
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
___ TestOpenAIProviderInitialization.test_initialization_with_default_model ____

self = <test_openai_provider.TestOpenAIProviderInitialization object at
0x11ea30ce0>

    @pytest.mark.fast
    def test_initialization_with_default_model(self):
        """Test initialization with default model."""
        config = {"api_key": "test-key"}

        provider = OpenAIProvider(config)

>       assert provider.model == "gpt-3.5-turbo"  # Default model
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 'stub-model' == 'gpt-3.5-turbo'
E
E         - gpt-3.5-turbo
E         + stub-model

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:41,986 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,008 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
________ TestOpenAIProviderErrorHandling.test_invalid_temperature_range ________

self = <test_openai_provider.TestOpenAIProviderErrorHandling object at
0x11ea31730>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)

        with pytest.raises(DevSynthError) as exc_info:
            provider.generate("Hello", {"temperature": -0.1})

>       assert "temperature must be between" in str(exc_info.value)
E       AssertionError: assert 'temperature must be between' in 'OpenAI
temperature must be a number between 0.0 and 2.0, got -0.1'
E        +  where 'OpenAI temperature must be a number between 0.0 and 2.0, got
-0.1' = str(OpenAIConfigurationError('OpenAI temperature must be a number
between 0.0 and 2.0, got -0.1'))
E        +    where OpenAIConfigurationError('OpenAI temperature must be a
number between 0.0 and 2.0, got -0.1') = <ExceptionInfo
OpenAIConfigurationError('OpenAI temperature must be a number between 0.0 and
2.0, got -0.1') tblen=3>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:462: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,089 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,110 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
___________ TestOpenAIProviderErrorHandling.test_invalid_max_tokens ____________

self = <test_openai_provider.TestOpenAIProviderErrorHandling object at
0x11e9bfce0>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)

        with pytest.raises(DevSynthError) as exc_info:
            provider.generate("Hello", {"max_tokens": 0})

>       assert "max_tokens must be positive" in str(exc_info.value)
E       AssertionError: assert 'max_tokens must be positive' in 'OpenAI
max_tokens must be a positive integer, got 0'
E        +  where 'OpenAI max_tokens must be a positive integer, got 0' =
str(OpenAIConfigurationError('OpenAI max_tokens must be a positive integer, got
0'))
E        +    where OpenAIConfigurationError('OpenAI max_tokens must be a
positive integer, got 0') = <ExceptionInfo OpenAIConfigurationError('OpenAI
max_tokens must be a positive integer, got 0') tblen=3>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:473: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,131 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,154 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
______________ TestOpenAIProviderHeaders.test_correct_headers_set ______________

self = <test_openai_provider.TestOpenAIProviderHeaders object at 0x11ea44dd0>

    @pytest.mark.fast
    def test_correct_headers_set(self):
        """Test that correct headers are set for OpenAI."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)

        expected_headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer test-key",
        }

        # Verify headers are set correctly
        for key, value in expected_headers.items():
>           assert provider.headers[key] == value
                   ^^^^^^^^^^^^^^^^
E           AttributeError: 'OpenAIProvider' object has no attribute 'headers'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:653: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,487 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,509 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
_____________ TestOpenAIProviderHeaders.test_custom_api_key_header _____________

self = <test_openai_provider.TestOpenAIProviderHeaders object at 0x11ea451f0>

    @pytest.mark.fast
    def test_custom_api_key_header(self):
        """Test custom API key header configuration."""
        config = {"api_key": "custom-key"}
        provider = OpenAIProvider(config)

>       assert provider.headers["Authorization"] == "Bearer custom-key"
               ^^^^^^^^^^^^^^^^
E       AttributeError: 'OpenAIProvider' object has no attribute 'headers'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:661: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,525 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,550 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
___________ TestOpenAIProviderEdgeCases.test_empty_response_handling ___________

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11ea45730>

    @pytest.mark.fast
    def test_empty_response_handling(self):
        """Test handling of empty responses."""
        config = {"api_key": "test-key"}

        empty_response = {
            "id": "chatcmpl-empty",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-3.5-turbo",
            "choices": [],
        }

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=empty_response,
                status=200,
            )

            provider = OpenAIProvider(config)

            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello")

>           assert "Invalid response" in str(exc_info.value)
E           assert 'Invalid response' in "OpenAI API error: 'NoneType' object
has no attribute 'choices'. Check your API key and model configuration."
E            +  where "OpenAI API error: 'NoneType' object has no attribute
'choices'. Check your API key and model configuration." =
str(OpenAIConnectionError("OpenAI API error: 'NoneType' object has no attribute
'choices'. Check your API key and model configuration."))
E            +    where OpenAIConnectionError("OpenAI API error: 'NoneType'
object has no attribute 'choices'. Check your API key and model configuration.")
= <ExceptionInfo OpenAIConnectionError("OpenAI API error: 'NoneType' object has
no attribute 'choices'. Check your API key and model configuration.")
tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:693: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,564 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:42,580 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:30:42,581 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
_________ TestOpenAIProviderEdgeCases.test_malformed_response_handling _________

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11ea30410>

    @pytest.mark.fast
    def test_malformed_response_handling(self):
        """Test handling of malformed responses."""
        config = {"api_key": "test-key"}

        malformed_response = {"invalid": "response", "structure": True}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=malformed_response,
                status=200,
            )

            provider = OpenAIProvider(config)

            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello")

>           assert "Invalid response" in str(exc_info.value)
E           assert 'Invalid response' in "OpenAI API error: 'NoneType' object
has no attribute 'choices'. Check your API key and model configuration."
E            +  where "OpenAI API error: 'NoneType' object has no attribute
'choices'. Check your API key and model configuration." =
str(OpenAIConnectionError("OpenAI API error: 'NoneType' object has no attribute
'choices'. Check your API key and model configuration."))
E            +    where OpenAIConnectionError("OpenAI API error: 'NoneType'
object has no attribute 'choices'. Check your API key and model configuration.")
= <ExceptionInfo OpenAIConnectionError("OpenAI API error: 'NoneType' object has
no attribute 'choices'. Check your API key and model configuration.")
tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:715: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,603 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:42,623 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:30:42,624 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
______________ TestOpenAIProviderEdgeCases.test_unicode_handling _______________

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12fb3f380>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenAI.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Validate runtime parameters
        self._validate_runtime_parameters(parameters or {})

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
>           message = getattr(response.choices[0], "message", None)
                              ^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'choices'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:346: AttributeError

During handling of the above exception, another exception occurred:

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11ea456a0>

    @pytest.mark.fast
    def test_unicode_handling(self):
        """Test handling of Unicode content."""
        config = {"api_key": "test-key"}

        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-3.5-turbo",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! 你好! ¡Hola! 🌟",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10,
"total_tokens": 15},
        }

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=unicode_response,
                status=200,
            )

            provider = OpenAIProvider(config)
>           response = provider.generate("Say hello in multiple languages")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:749:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12fb3f380>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenAI.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Validate runtime parameters
        self._validate_runtime_parameters(parameters or {})

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
            message = getattr(response.choices[0], "message", None)
            content = getattr(message, "content", None)
            if content is None:
                raise OpenAIModelError("Invalid response from OpenAI")
            return content
        except OpenAIModelError:
            raise
        except OpenAITokenLimitError:
            raise  # Re-raise token limit errors as-is
        except OpenAIConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:
            error_msg = f"OpenAI API error: {str(e)}. Check your API key and
model configuration."
            logger.error(error_msg)
>           raise OpenAIConnectionError(error_msg)
E           devsynth.application.llm.openai_provider.OpenAIConnectionError:
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:360: OpenAIConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,649 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:42,665 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:30:42,667 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
_
TestOpenRouterProviderInitialization.test_initialization_without_api_key_raises_
error _

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at
0x11ea5c3e0>

    @pytest.mark.fast
    def test_initialization_without_api_key_raises_error(self):
        """Test that initialization fails without API key."""
        config = {"openrouter_model": "google/gemini-flash-1.5"}

>       with pytest.raises(OpenRouterConnectionError) as exc_info:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class
'devsynth.application.llm.openrouter_provider.OpenRouterConnectionError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:64: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,767 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,788 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
_
TestOpenRouterProviderInitialization.test_initialization_with_httpx_unavailable
_

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at
0x11ea5ca70>

    @pytest.mark.fast
    def test_initialization_with_httpx_unavailable(self):
        """Test initialization when httpx is not available."""
        config = {"openrouter_api_key": "test-key"}

        with patch("devsynth.application.llm.openrouter_provider.httpx", None):
            provider = OpenRouterProvider(config)

>       assert provider.sync_client is None
E       assert <httpx.Client object at 0x12fb3ccb0> is None
E        +  where <httpx.Client object at 0x12fb3ccb0> =
<devsynth.application.llm.openrouter_provider.OpenRouterProvider object at
0x12f60e840>.sync_client

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:86: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,840 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,862 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
______ TestOpenRouterProviderErrorHandling.test_invalid_temperature_range ______

self = <test_openrouter_provider.TestOpenRouterProviderErrorHandling object at
0x11ea5d040>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"openrouter_api_key": "test-key"}
        provider = OpenRouterProvider(config)

        with pytest.raises(OpenRouterConnectionError) as exc_info:
            provider.generate("Hello", {"temperature": -0.1})

>       assert "temperature must be between 0 and 2" in str(exc_info.value)
E       AssertionError: assert 'temperature must be between 0 and 2' in
'OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)'
E        +  where 'OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)' = str(OpenRouterConnectionError('OpenRouter API
error: Test timed out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)'))
E        +    where OpenRouterConnectionError('OpenRouter API error: Test timed
out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)') = <ExceptionInfo
OpenRouterConnectionError('OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:428: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:42,926 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:42,943 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:30:42,944 - fallback - WARNING - Retry attempt 1/3 after 2.39s
delay
2025-10-28 10:30:42,945 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.39s)
2025-10-28 10:30:45,346 - fallback - WARNING - Retry attempt 2/3 after 6.04s
delay
2025-10-28 10:30:45,346 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 6.04s)
2025-10-28 10:30:45,916 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.39s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.39s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 6.04s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 6.04s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
_________ TestOpenRouterProviderErrorHandling.test_invalid_max_tokens __________

self = <test_openrouter_provider.TestOpenRouterProviderErrorHandling object at
0x11ea5d460>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"openrouter_api_key": "test-key"}
        provider = OpenRouterProvider(config)

        with pytest.raises(OpenRouterConnectionError) as exc_info:
            provider.generate("Hello", {"max_tokens": 0})

>       assert "max_tokens must be positive" in str(exc_info.value)
E       AssertionError: assert 'max_tokens must be positive' in 'OpenRouter API
error: Test timed out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)'
E        +  where 'OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)' = str(OpenRouterConnectionError('OpenRouter API
error: Test timed out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)'))
E        +    where OpenRouterConnectionError('OpenRouter API error: Test timed
out after 3 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)') = <ExceptionInfo
OpenRouterConnectionError('OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:439: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:45,946 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:45,965 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:30:45,965 - fallback - WARNING - Retry attempt 1/3 after 2.73s
delay
2025-10-28 10:30:45,966 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.73s)
2025-10-28 10:30:48,698 - fallback - WARNING - Retry attempt 2/3 after 4.72s
delay
2025-10-28 10:30:48,698 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 4.72s)
2025-10-28 10:30:48,946 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.73s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.73s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 4.72s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 4.72s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
_____ TestOpenRouterProviderConfiguration.test_configuration_with_defaults _____

self = <test_openrouter_provider.TestOpenRouterProviderConfiguration object at
0x11ea5e2d0>

    @pytest.mark.fast
    def test_configuration_with_defaults(self):
        """Test configuration with default values."""
        config = {"openrouter_api_key": "test-key"}

        provider = OpenRouterProvider(config)

        assert provider.temperature == 0.7  # Default
>       assert provider.max_tokens == 4096  # Default
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 1024 == 4096
E        +  where 1024 =
<devsynth.application.llm.openrouter_provider.OpenRouterProvider object at
0x12fc243e0>.max_tokens

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:482: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:49,007 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:49,027 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
___________ TestOpenRouterProviderHeaders.test_custom_referer_header ___________

self = <test_openrouter_provider.TestOpenRouterProviderHeaders object at
0x11ea71100>

    @pytest.mark.fast
    def test_custom_referer_header(self):
        """Test custom referer header configuration."""
        config = {
            "openrouter_api_key": "test-key",
            "http_referer": "https://custom-app.com",
        }
        provider = OpenRouterProvider(config)

>       assert provider.headers["HTTP-Referer"] == "https://custom-app.com"
E       AssertionError: assert 'https://devsynth.dev' ==
'https://custom-app.com'
E
E         - https://custom-app.com
E         + https://devsynth.dev

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:634: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:49,335 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:30:49,353 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
_________ TestOpenRouterProviderEdgeCases.test_empty_response_handling _________

args = (), kwargs = {}, num_retries = 2, delay = 4.202445108233113
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:209: in _api_call
    response = self.sync_client.post("/chat/completions", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x12fc33980>, method = 'POST'
url = '/chat/completions', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12fc33200>
prompt = 'Hello', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:219:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13eef6050, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at
0x11ea71190>

    @pytest.mark.fast
    def test_empty_response_handling(self):
        """Test handling of empty responses."""
        config = {"openrouter_api_key": "test-key"}

        empty_response = {
            "id": "chatcmpl-empty",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "google/gemini-flash-1.5",
            "choices": [],
        }

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=empty_response,
                status=200,
            )

            provider = OpenRouterProvider(config)

            with pytest.raises(OpenRouterModelError) as exc_info:
>               provider.generate("Hello")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:664:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12fc33200>
prompt = 'Hello', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:223: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:49,367 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:49,382 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:30:49,383 - fallback - WARNING - Retry attempt 1/3 after 1.85s
delay
2025-10-28 10:30:49,383 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.85s)
2025-10-28 10:30:51,237 - fallback - WARNING - Retry attempt 2/3 after 4.20s
delay
2025-10-28 10:30:51,238 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 4.20s)
2025-10-28 10:30:52,371 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.85s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.85s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 4.20s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 4.20s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
_______ TestOpenRouterProviderEdgeCases.test_malformed_response_handling _______

args = (), kwargs = {}, num_retries = 2, delay = 5.630709785896835
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:209: in _api_call
    response = self.sync_client.post("/chat/completions", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x12f634b00>, method = 'POST'
url = '/chat/completions', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12f6512e0>
prompt = 'Hello', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:219:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13ef317e0, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at
0x11ea715e0>

    @pytest.mark.fast
    def test_malformed_response_handling(self):
        """Test handling of malformed responses."""
        config = {"openrouter_api_key": "test-key"}

        malformed_response = {"invalid": "response", "structure": True}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=malformed_response,
                status=200,
            )

            provider = OpenRouterProvider(config)

            with pytest.raises(OpenRouterModelError) as exc_info:
>               provider.generate("Hello")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:686:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12f6512e0>
prompt = 'Hello', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:223: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:52,461 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:52,480 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:30:52,481 - fallback - WARNING - Retry attempt 1/3 after 2.80s
delay
2025-10-28 10:30:52,481 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.80s)
2025-10-28 10:30:55,287 - fallback - WARNING - Retry attempt 2/3 after 5.63s
delay
2025-10-28 10:30:55,288 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 5.63s)
2025-10-28 10:30:55,457 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.80s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.80s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 5.63s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 5.63s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
____________ TestOpenRouterProviderEdgeCases.test_unicode_handling _____________

args = (), kwargs = {}, num_retries = 2, delay = 6.486961330923231
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:209: in _api_call
    response = self.sync_client.post("/chat/completions", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x12fb13650>, method = 'POST'
url = '/chat/completions', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12fb12e70>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:219:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13ee7f7b0, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at
0x11ea71a90>

    @pytest.mark.fast
    def test_unicode_handling(self):
        """Test handling of Unicode content."""
        config = {"openrouter_api_key": "test-key"}

        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "google/gemini-flash-1.5",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! 你好! ¡Hola! 🌟",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10,
"total_tokens": 15},
        }

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=unicode_response,
                status=200,
            )

            provider = OpenRouterProvider(config)
>           response = provider.generate("Say hello in multiple languages")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:722:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12fb12e70>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:223: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:55,551 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:30:55,567 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:30:55,568 - fallback - WARNING - Retry attempt 1/3 after 2.34s
delay
2025-10-28 10:30:55,568 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.34s)
2025-10-28 10:30:57,915 - fallback - WARNING - Retry attempt 2/3 after 6.49s
delay
2025-10-28 10:30:57,915 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 6.49s)
2025-10-28 10:30:58,551 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.34s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.34s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 6.49s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 6.49s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
____________ test_configure_logging_invokes_directory_creation_once ____________

logging_setup_module = <module 'devsynth.logging_setup' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_configure_logging_invokes0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dbe39b0>

    @pytest.mark.fast
    def test_configure_logging_invokes_directory_creation_once(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: LOG-CONF-05A — ensure_log_dir_exists executes only on first
configuration."""

        logging_setup = logging_setup_module
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(tmp_path))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)

        calls: list[str] = []

        def fake_ensure(path: str) -> str:
            calls.append(path)
            return path

        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", fake_ensure)

        logging_setup.configure_logging(log_dir="logs")
        logging_setup.configure_logging(log_dir="logs")

>       assert calls == [str(tmp_path / "logs")]
E       AssertionError: assert ['/private/va...nvokes0/logs'] ==
['/private/va...nvokes0/logs']
E
E         Left contains 3 more items, first extra item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_invokes0/logs'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_configure_logging.py:183: AssertionError
----------------------------- Captured stdout call -----------------------------
WARNING: File logging failed - 2025-10-28 10:30:59,004 - root - WARNING - Failed
to set up file logging: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_invokes0/logs/devsynth.log'
WARNING: File logging failed - 2025-10-28 10:30:59,004 - root - INFO - Logging
configured for console output only (no file logging).
WARNING: File logging failed - 2025-10-28 10:30:59,004 - root - WARNING - Failed
to set up file logging: [Errno 2] No such file or directory:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_invokes0/logs/devsynth.log'
WARNING: File logging failed - 2025-10-28 10:30:59,004 - root - INFO - Logging
configured for console output only (no file logging).
______ test_configure_logging_reenables_file_handler_after_console_toggle ______

logging_setup_module = <module 'devsynth.logging_setup' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_configure_logging_reenabl0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb59790>

    @pytest.mark.fast
    def test_configure_logging_reenables_file_handler_after_console_toggle(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: LOG-CONF-09 — toggling create_dir back on reinstates JSON
handler.

        Issue: issues/coverage-below-threshold.md
        """

        logging_setup = logging_setup_module
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(tmp_path))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)

        logging_setup.configure_logging(log_dir="retention", create_dir=False)

        assert all(
            not isinstance(handler, logging.FileHandler)
            for handler in logging.getLogger().handlers
        ), "Initial console-only run should not attach a file handler."

        calls: list[str] = []

        def track_directory(path: str) -> str:
            calls.append(path)
            Path(path).mkdir(parents=True, exist_ok=True)
            return path

        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists",
track_directory)

        logging_setup.configure_logging(log_dir="retention", create_dir=True)

>       assert calls == [str(tmp_path / "retention")]
E       AssertionError: assert ['/private/va...l0/retention'] ==
['/private/va...l0/retention']
E
E         Left contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_reenabl0/retention'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_configure_logging.py:353: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:59,054 - root - INFO - Logging configured for console output
only (no file logging).
2025-10-28 10:30:59,054 - root - INFO - Logging configured. Log file:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_configure_logging_reenabl0/retention/devsynth.log
_____________ test_configure_logging_retention_matrix[create-dir] ______________

logging_setup_module = <module 'devsynth.logging_setup' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_configure_logging_retenti0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb58b90>
create_dir = True, no_file_env = None, expected_effective = True

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("create_dir", "no_file_env", "expected_effective"),
        [
            pytest.param(True, None, True, id="create-dir"),
            pytest.param(True, "1", False, id="no-file-env"),
            pytest.param(False, None, False, id="create-dir-disabled"),
            pytest.param(False, "1", False,
id="no-file-env-create-dir-disabled"),
        ],
    )
    def test_configure_logging_retention_matrix(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        create_dir: bool,
        no_file_env: Optional[str],
        expected_effective: bool,
    ) -> None:
        """Exercise retention decisions across create_dir and environment
flags."""

        logging_setup = logging_setup_module
        project_dir = tmp_path / "retention_project"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
        if no_file_env is not None:
            monkeypatch.setenv("DEVSYNTH_NO_FILE_LOGGING", no_file_env)

        log_dir_argument = "relative/logs"
        ensure_calls: list[Optional[str]] = []
        real_ensure = logging_setup.ensure_log_dir_exists

        def tracking(log_dir: Optional[str] = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)

        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)

        logging_setup.configure_logging(log_dir=log_dir_argument,
create_dir=create_dir)

        expected_dir = os.path.join(str(project_dir), log_dir_argument)
        expected_file = os.path.join(
            expected_dir,
            os.environ.get("DEVSYNTH_LOG_FILENAME",
logging_setup.DEFAULT_LOG_FILENAME),
        )

        assert logging_setup._configured_log_dir == expected_dir
        assert logging_setup._configured_log_file == expected_file
        assert logging_setup._last_effective_config[0] == expected_dir
        assert logging_setup._last_effective_config[1] == expected_file
        assert logging_setup._last_effective_config[3] is expected_effective

        if expected_effective:
>           assert ensure_calls == [expected_dir]
E           AssertionError: assert ['/private/va...elative/logs'] ==
['/private/va...elative/logs']
E
E             Left contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_retenti0/retention_project/relative/logs'
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:108: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:59,252 - root - INFO - Logging configured. Log file:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_configure_logging_retenti0/retention_project/relative/logs/devsynth.
log
________ test_configure_logging_relocates_absolute_paths[home-absolute] ________

logging_setup_module = <module 'devsynth.logging_setup' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_configure_logging_relocat0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f61ab70>
log_dir_input = PosixPath('/Users/caitlyn/devsynth/logs')
log_file_name = 'home.json'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("log_dir_input", "log_file_name"),
        [
            pytest.param(
                Path.home() / "devsynth" / "logs", "home.json",
id="home-absolute"
            ),
            pytest.param(
                Path("/var/tmp/devsynth/logs"), "system.json",
id="non-home-absolute"
            ),
        ],
    )
    def test_configure_logging_relocates_absolute_paths(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        log_dir_input: Path,
        log_file_name: str,
    ) -> None:
        """Absolute paths are redirected into the sandboxed project
directory."""

        logging_setup = logging_setup_module
        project_dir = tmp_path / "sandbox_relocation"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)

        log_file_input = log_dir_input / log_file_name

        ensure_calls: list[Optional[str]] = []
        real_ensure = logging_setup.ensure_log_dir_exists

        def tracking(log_dir: Optional[str] = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)

        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)

        logging_setup.configure_logging(
            log_dir=str(log_dir_input),
            log_file=str(log_file_input),
            create_dir=True,
        )

        home_prefix = str(Path.home())

        def expected_relative(path: Path) -> str:
            path_str = str(path)
            if path_str.startswith(home_prefix):
                return path_str.replace(home_prefix, "", 1).lstrip("/\\")
            return str(path.relative_to(path.anchor))

        expected_dir = os.path.join(str(project_dir),
expected_relative(log_dir_input))
        expected_file = os.path.join(str(project_dir),
expected_relative(log_file_input))

>       assert ensure_calls == [expected_dir]
E       AssertionError: assert ['/private/va...evsynth/logs'] ==
['/private/va...evsynth/logs']
E
E         Left contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_relocat0/sandbox_relocation/Users/caitlyn/devsynt
h/logs'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:59,304 - root - INFO - Logging configured. Log file:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_configure_logging_relocat0/sandbox_relocation/Users/caitlyn/devsynth
/logs/home.json
______ test_configure_logging_relocates_absolute_paths[non-home-absolute] ______

logging_setup_module = <module 'devsynth.logging_setup' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_configure_logging_relocat1')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6181d0>
log_dir_input = PosixPath('/var/tmp/devsynth/logs')
log_file_name = 'system.json'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("log_dir_input", "log_file_name"),
        [
            pytest.param(
                Path.home() / "devsynth" / "logs", "home.json",
id="home-absolute"
            ),
            pytest.param(
                Path("/var/tmp/devsynth/logs"), "system.json",
id="non-home-absolute"
            ),
        ],
    )
    def test_configure_logging_relocates_absolute_paths(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        log_dir_input: Path,
        log_file_name: str,
    ) -> None:
        """Absolute paths are redirected into the sandboxed project
directory."""

        logging_setup = logging_setup_module
        project_dir = tmp_path / "sandbox_relocation"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)

        log_file_input = log_dir_input / log_file_name

        ensure_calls: list[Optional[str]] = []
        real_ensure = logging_setup.ensure_log_dir_exists

        def tracking(log_dir: Optional[str] = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)

        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)

        logging_setup.configure_logging(
            log_dir=str(log_dir_input),
            log_file=str(log_file_input),
            create_dir=True,
        )

        home_prefix = str(Path.home())

        def expected_relative(path: Path) -> str:
            path_str = str(path)
            if path_str.startswith(home_prefix):
                return path_str.replace(home_prefix, "", 1).lstrip("/\\")
            return str(path.relative_to(path.anchor))

        expected_dir = os.path.join(str(project_dir),
expected_relative(log_dir_input))
        expected_file = os.path.join(str(project_dir),
expected_relative(log_file_input))

>       assert ensure_calls == [expected_dir]
E       AssertionError: assert ['/private/va...evsynth/logs'] ==
['/private/va...evsynth/logs']
E
E         Left contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devsynth/logs
'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:30:59,322 - root - INFO - Logging configured. Log file:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devsynth/logs/
system.json
___________ TestSyncManagerProtocol.test_sync_manager_initialization ___________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb0e3c0>

    def test_sync_manager_initialization(self):
        """Test SyncManager initializes correctly with required stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:43:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x12f6517c0>, 's...MemoryStore object at 0x12f67e030>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
________ TestSyncManagerProtocol.test_sync_manager_write_to_all_stores _________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb0f4d0>

    def test_sync_manager_write_to_all_stores(self):
        """Test write operation propagates to all configured stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x12fba2fc0>, 's...MemoryStore object at 0x13dbdb050>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_______ TestSyncManagerProtocol.test_sync_manager_read_from_first_store ________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb0f980>

    def test_sync_manager_read_from_first_store(self):
        """Test read operation returns from first store containing the key."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:76:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x12f67d040>, 's...MemoryStore object at 0x13dbe2810>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
___ TestSyncManagerProtocol.test_sync_manager_read_fallback_to_second_store ____

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb24320>

    def test_sync_manager_read_fallback_to_second_store(self):
        """Test read operation falls back to second store if first doesn't have
key."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x12fba2f60>, 's...MemoryStore object at 0x12ffea1e0>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_ TestSyncManagerProtocol.test_sync_manager_read_raises_keyerror_if_not_found __

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb247d0>

    def test_sync_manager_read_raises_keyerror_if_not_found(self):
        """Test read operation raises KeyError if key not found in any store."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:102:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x13dbe0770>, 's...MemoryStore object at 0x12fb16540>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_________ TestSyncManagerProtocol.test_sync_manager_transaction_commit _________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb24c80>

    def test_sync_manager_transaction_commit(self):
        """Test transaction commits changes to all stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2},
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:114:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'tinydb':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x13dbd9640>, 's...MemoryStore object at 0x12f67ae70>},
required_stores={'tinydb'}, optional_stores=frozenset({'kuzu', 'duckdb',
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")

        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{',
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_ TestSyncManagerProtocol.test_sync_manager_transaction_rollback_on_exception __

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb25130>

    def test_sync_manager_transaction_rollback_on_exception(self):
        """Test transaction rolls back changes if exception occurs."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()

>       sync_manager = SyncManager(stores={"store1": store1, "store2": store2})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'store1':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x13dbd9580>, 's... object at 0x12fb5bef0>},
required_stores=frozenset({'tinydb'}), optional_stores=frozenset({'kuzu',
'duckdb', 'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
>           raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
E           ValueError: Missing stores: tinydb

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:83: ValueError
_________ TestSyncManagerProtocol.test_sync_manager_with_generic_type __________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol
object at 0x11eb25a90>

    def test_sync_manager_with_generic_type(self):
        """Test SyncManager works with different value types."""
>       sync_manager = SyncManager[str](stores={"mock": MockMemoryStore()})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:162:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/typing.py:1184: in __call__
    result = self.__origin__(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SyncManager(stores={'mock':
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at
0x12fba22d0>}, required_stores=frozenset({'tinydb'}),
optional_stores=frozenset({'kuzu', 'duckdb', 'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
>           raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
E           ValueError: Missing stores: tinydb

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:83: ValueError
_____________________ test_reasoning_loop_records_results ______________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x13d9cb9b0>

    @pytest.mark.fast
    def test_reasoning_loop_records_results(mocker) -> None:
        """It stores results through the coordinator.

        ReqID: DR-1
        """

        coordinator = mocker.create_autospec(EDRRCoordinator, instance=True)
        result = build_dialectical_sequence(status="completed")
>       mocker.patch(
            "devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoni
ng",
            return_value=result,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:21:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13d946810>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> does not have the attribute
'_apply_dialectical_reasoning'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ test_reasoning_loop_logs_consensus_failure __________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x13d9473e0>

    @pytest.mark.fast
    def test_reasoning_loop_logs_consensus_failure(mocker) -> None:
        """It delegates consensus failures to the coordinator.

        ReqID: DR-2
        """

        coordinator = mocker.create_autospec(EDRRCoordinator, instance=True)

        class DummyConsensusError(ConsensusError):
            def __init__(self, message: str):
                Exception.__init__(self, message)

>       mocker.patch(
            "devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoni
ng",
            side_effect=DummyConsensusError("no consensus"),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13d995760>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> does not have the attribute
'_apply_dialectical_reasoning'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ test_reasoning_loop_persists_phase_results __________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x13d9cb4a0>

    @pytest.mark.fast
    def test_reasoning_loop_persists_phase_results(mocker) -> None:
        """It stores results using the memory manager.

        ReqID: DR-3
        """

        memory_manager = mocker.Mock()
        coordinator = EDRRCoordinator(memory_manager)
        result = build_dialectical_sequence(status="completed")
>       mocker.patch(
            "devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoni
ng",
            return_value=result,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:67:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13d9c9b50>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> does not have the attribute
'_apply_dialectical_reasoning'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
___________________ test_reasoning_loop_runs_until_complete ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f679160>

    @pytest.mark.fast
    def test_reasoning_loop_runs_until_complete(monkeypatch):
        """It continues until the reasoning process is complete.

        ReqID: DR-4
        """

        calls = []

        def fake_apply(team, task, critic, memory):
            calls.append(task.get("solution"))
            if len(calls) == 1:
                return {"status": "in_progress", "synthesis": "next"}
            return {"status": "completed", "synthesis": "final"}

>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fake_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:32: AttributeError
__________________ test_reasoning_loop_logs_consensus_failure __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fdc35f0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13dbe0d70>

    @pytest.mark.fast
    def test_reasoning_loop_logs_consensus_failure(monkeypatch, caplog):
        """It logs and swallows consensus failures.

        ReqID: DR-5
        """

        def fail_apply(team, task, critic, memory):
            raise DummyConsensusError("no consensus")

>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fail_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:49: AttributeError
_________________ test_reasoning_loop_respects_max_iterations __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fdc3aa0>

    @pytest.mark.fast
    def test_reasoning_loop_respects_max_iterations(monkeypatch):
        """It stops after reaching the iteration limit.

        ReqID: DR-6
        """

        calls = []

        def fake_apply(team, task, critic, memory):
            calls.append(1)
            return {"status": "in_progress"}

>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fake_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:71: AttributeError
________________ test_reasoning_loop_respects_total_time_budget ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fdc2f00>

    @pytest.mark.fast
    @pytest.mark.unit
    def test_reasoning_loop_respects_total_time_budget(monkeypatch):
        """It stops when the total time budget is exhausted.

        ReqID: DR-6
        """

>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", _slow_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_reasoning_loop_time_budget.py:25: AttributeError
________________________ test_ceremony_mapping_to_phase ________________________

    @pytest.mark.fast
    def test_ceremony_mapping_to_phase():
        """Configured ceremonies map to the correct EDRR phases."""
        config = {
            "settings": {
                "ceremonyMapping": {
                    "planning": "retrospect.iteration_planning",
                    "dailyStandup": "phase_progression_tracking",
                    "review": "refine.outputs_review",
                    "retrospective": "retrospect.process_evaluation",
                }
            }
        }
        adapter = SprintAdapter(config)
        assert adapter.get_ceremony_phase("planning") == Phase.RETROSPECT
        assert adapter.get_ceremony_phase("review") == Phase.REFINE
        assert adapter.get_ceremony_phase("retrospective") == Phase.RETROSPECT
>       assert adapter.get_ceremony_phase("dailyStandup") is None
E       AssertionError: assert <Phase.DIFFERENTIATE: 'differentiate'> is None
E        +  where <Phase.DIFFERENTIATE: 'differentiate'> =
get_ceremony_phase('dailyStandup')
E        +    where get_ceremony_phase =
<devsynth.methodology.sprint.SprintAdapter object at
0x12fd4f9e0>.get_ceremony_phase

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_adapter.py:63: AssertionError
_____________________ test_map_ceremony_to_phase_defaults ______________________

    @pytest.mark.fast
    def test_map_ceremony_to_phase_defaults():
        """Common ceremonies resolve to their default EDRR phases."""
>       assert map_ceremony_to_phase("planning") == Phase.RETROSPECT
E       AssertionError: assert <Phase.EXPAND: 'expand'> == <Phase.RETROSPECT:
'retrospect'>
E        +  where <Phase.EXPAND: 'expand'> = map_ceremony_to_phase('planning')
E        +  and   <Phase.RETROSPECT: 'retrospect'> = Phase.RETROSPECT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_hooks.py:11: AssertionError
_____________________ test_adapter_uses_ceremony_defaults ______________________

    @pytest.mark.fast
    def test_adapter_uses_ceremony_defaults():
        """SprintAdapter falls back to default phase mapping for bare ceremony
names."""
        config = {
            "settings": {
                "ceremonyMapping": {
                    "planning": "planning",
                    "review": "review",
                    "retrospective": "retrospective",
                }
            }
        }
        adapter = SprintAdapter(config)
>       assert adapter.get_ceremony_phase("planning") == Phase.RETROSPECT
E       AssertionError: assert <Phase.EXPAND: 'expand'> == <Phase.RETROSPECT:
'retrospect'>
E        +  where <Phase.EXPAND: 'expand'> = get_ceremony_phase('planning')
E        +    where get_ceremony_phase =
<devsynth.methodology.sprint.SprintAdapter object at
0x13dbe2510>.get_ceremony_phase
E        +  and   <Phase.RETROSPECT: 'retrospect'> = Phase.RETROSPECT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_hooks.py:31: AssertionError
_______________________ test_graph_transitions_complete ________________________

engine =
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine
object at 0x13d9c9640>

    @pytest.mark.fast
    def test_graph_transitions_complete(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1",
agent_type="t")
        step2 = WorkflowStep(id="s2", name="Step 2", description="d2",
agent_type="t")
        wf = engine.add_step(wf, step1)
        wf = engine.add_step(wf, step2)

        # Patch orchestration to be a no-op that appends a message
        class FakeService:
            def process_step(self, state, step):
                state.messages.append({"role": "system", "content": f"ran
{step.id}"})
                return state

        with patch(
            "devsynth.orchestration.step_executor.OrchestrationService",
FakeService
        ):
            result = engine.execute_workflow(wf, context={})

>       assert result.status == WorkflowStatus.COMPLETED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> ==
<WorkflowStatus.COMPLETED: 'completed'>
E        +  where <WorkflowStatus.FAILED: 'failed'> =
Workflow(id='1eb64e29-c012-4286-aab1-49345dfbaa49', name='wf',
description='desc', steps=[WorkflowStep(id='s1',
name='...d_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 948614),
updated_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 948807)).status
E        +  and   <WorkflowStatus.COMPLETED: 'completed'> =
WorkflowStatus.COMPLETED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:00,948 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
_________________ test_retry_branch_succeeds_with_max_retries __________________

engine =
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine
object at 0x13db26a80>

    @pytest.mark.fast
    def test_retry_branch_succeeds_with_max_retries(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1",
agent_type="t")
        wf = engine.add_step(wf, step1)

        calls = {"n": 0}

        class FlakyService:
            def process_step(self, state, step):
                calls["n"] += 1
                if calls["n"] == 1:
                    raise RuntimeError("transient")
                state.messages.append({"role": "system", "content": "ok"})
                return state

        with patch(
            "devsynth.orchestration.step_executor.OrchestrationService",
FlakyService
        ):
            # Allow one retry
            result = engine.execute_workflow(wf, context={"max_retries": 1})

>       assert calls["n"] == 2
E       assert 0 == 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:76: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:00,973 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
________________________ test_streaming_callback_called ________________________

engine =
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine
object at 0x12fde6780>

    @pytest.mark.fast
    def test_streaming_callback_called(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1",
agent_type="t")
        wf = engine.add_step(wf, step1)

        class Service:
            def process_step(self, state, step):
                state.messages.append({"role": "agent", "content": "result"})
                return state

        stream_events = []

        def stream_cb(evt):
            stream_events.append(evt)

        with patch("devsynth.orchestration.step_executor.OrchestrationService",
Service):
            result = engine.execute_workflow(wf, context={"stream_callback":
stream_cb})

>       assert result.status == WorkflowStatus.COMPLETED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> ==
<WorkflowStatus.COMPLETED: 'completed'>
E        +  where <WorkflowStatus.FAILED: 'failed'> =
Workflow(id='06046579-8867-47a2-aafd-b2e8a35e8630', name='wf',
description='desc', steps=[WorkflowStep(id='s1',
name='...d_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 981884),
updated_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 981952)).status
E        +  and   <WorkflowStatus.COMPLETED: 'completed'> =
WorkflowStatus.COMPLETED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:99: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:00,981 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
__________________ test_cancellation_pauses_before_first_step __________________

engine =
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine
object at 0x12f63b5f0>

    @pytest.mark.fast
    def test_cancellation_pauses_before_first_step(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1",
agent_type="t")
        wf = engine.add_step(wf, step1)

        spy = MagicMock()

        class SpyService:
            def process_step(self, state, step):
                spy()
                return state

        with patch("devsynth.orchestration.step_executor.OrchestrationService",
SpyService):
            result = engine.execute_workflow(wf, context={"is_cancelled":
lambda: True})

>       assert result.status == WorkflowStatus.PAUSED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> ==
<WorkflowStatus.PAUSED: 'paused'>
E        +  where <WorkflowStatus.FAILED: 'failed'> =
Workflow(id='f970fae0-bc72-436b-9eb6-a5965af1b00d', name='wf',
description='desc', steps=[WorkflowStep(id='s1',
name='...d_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 992114),
updated_at=datetime.datetime(2025, 10, 28, 10, 31, 0, 992238)).status
E        +  and   <WorkflowStatus.PAUSED: 'paused'> = WorkflowStatus.PAUSED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:122: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:00,992 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
__________________ test_adapter_openai_provider_stub_offline ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f20a630>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_adapter_openai_provider_stub_offline(monkeypatch):
        """
        When DEVSYNTH_OFFLINE=true and normalized stubs are applied (default),
        adapter-level OpenAIProvider should return deterministic responses.
        """
        # Ensure offline and that provider availability flags don't accidentally
enable real backend
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")
        monkeypatch.setenv("DEVSYNTH_PROVIDER", "stub")

        # Import locally to ensure stub application via tests/conftest autouse
fixture has run
        import devsynth.adapters.provider_system as provider_system  # type:
ignore

>       p = provider_system.OpenAIProvider()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: OpenAIProvider.__init__() missing 1 required positional
argument: 'api_key'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/providers/test_p
rovider_stub_offline.py:20: TypeError
________________________ test_generate_arguments_sorted ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x100edce60>

    @pytest.mark.fast
    def test_generate_arguments_sorted(monkeypatch):
        # Reverse order in text, expect sorted by position/content
        text = (
            "Argument 2\n"
            "Position: AGAINST\n"
            "Content: zeta\n"
            "Counterargument: aaa\n\n"
            "Argument 1\n"
            "Position: FOR\n"
            "Content: alpha\n"
            "Counterargument: bbb\n"
        )

        class LL(StubLLM):
            def query(self, prompt: str) -> str:
                if "Arguments" in prompt or "arguments" in prompt:
                    return text
                if "Determine if the following reasoning" in prompt:
                    return "yes"
                return "ok"

        service = make_service(llm=LL())
        change = RequirementChange(requirement_id=uuid4(),
change_type=ChangeType.ADD)
        change.new_state = Requirement(title="T", description="D")
        args = service._generate_arguments(change, "t", "a")
        # After sorting, FOR/alpha should come before AGAINST/zeta
>       assert args[0]["content"].lower() == "alpha"
               ^^^^^^^^^^^^^^^^^^
E       TypeError: 'ParsedDialecticalArgument' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/requirements/tes
t_dialectical_reasoner_determinism.py:185: TypeError
___________ TestRecommendationGeneration.test_calculates_percentages ___________

self =
<tests.unit.scripts.test_analyze_test_dependencies.TestRecommendationGeneration
object at 0x11ecc7890>

    def test_calculates_percentages(self):
        """Test percentage calculations in recommendations."""
        analysis_results = [
            {"has_isolation_marker": True, "safe_for_parallel": True,
"risk_score": 0},
            {"has_isolation_marker": True, "safe_for_parallel": True,
"risk_score": 1},
            {
                "has_isolation_marker": True,
                "safe_for_parallel": False,
                "risk_score": 10,
            },
            {
                "has_isolation_marker": True,
                "safe_for_parallel": False,
                "risk_score": 15,
            },
        ]

>       recommendations = generate_recommendations(analysis_results)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ana
lyze_test_dependencies.py:235:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

analysis_results = [{'has_isolation_marker': True, 'risk_score': 0,
'safe_for_parallel': True}, {'has_isolation_marker': True, 'risk_scor..._score':
10, 'safe_for_parallel': False}, {'has_isolation_marker': True, 'risk_score':
15, 'safe_for_parallel': False}]

    def generate_recommendations(analysis_results: List[Dict[str, Any]]) ->
Dict[str, Any]:
        """Generate recommendations based on analysis results."""
        total_files = len(analysis_results)
        files_with_isolation = sum(
            1 for r in analysis_results if r.get("has_isolation_marker", False)
        )
        safe_for_removal = sum(
            1
            for r in analysis_results
            if r.get("safe_for_parallel", False) and
r.get("has_isolation_marker", False)
        )

        # Categorize by risk level
        low_risk = [
            r
            for r in analysis_results
            if r.get("risk_score", 0) <= 2 and r.get("has_isolation_marker",
False)
        ]
        medium_risk = [
            r
            for r in analysis_results
            if 3 <= r.get("risk_score", 0) <= 8 and
r.get("has_isolation_marker", False)
        ]
        high_risk = [
            r
            for r in analysis_results
            if r.get("risk_score", 0) > 8 and r.get("has_isolation_marker",
False)
        ]

        return {
            "summary": {
                "total_test_files": total_files,
                "files_with_isolation_markers": files_with_isolation,
                "safe_for_removal": safe_for_removal,
                "removal_percentage": round(
                    (
                        (safe_for_removal / files_with_isolation * 100)
                        if files_with_isolation > 0
                        else 0
                    ),
                    1,
                ),
            },
            "risk_categories": {
                "low_risk": {
                    "count": len(low_risk),
>                   "files": [r["relative_path"] for r in low_risk],
                              ^^^^^^^^^^^^^^^^^^
                },
                "medium_risk": {
                    "count": len(medium_risk),
                    "files": [r["relative_path"] for r in medium_risk],
                },
                "high_risk": {
                    "count": len(high_risk),
                    "files": [r["relative_path"] for r in high_risk],
                },
            },
            "recommendations": {
                "immediate_removal": [r["relative_path"] for r in low_risk],
                "careful_review": [r["relative_path"] for r in medium_risk],
                "keep_isolation": [r["relative_path"] for r in high_risk],
            },
        }
E       KeyError: 'relative_path'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/analyze_test_depend
encies.py:313: KeyError
____________ TestTestExecutionBenchmark.test_run_benchmark_success _____________

self =
<tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchmark
object at 0x11ed181d0>
mock_run = <MagicMock name='run' id='5093249664'>

    @patch("subprocess.run")
    def test_run_benchmark_success(self, mock_run):
        """Test successful benchmark execution."""
        # Mock successful subprocess result
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_result.stdout = "===== 5 passed, 0 failed, 2 skipped in 1.23s
====="
        mock_result.stderr = ""
        mock_run.return_value = mock_result

        benchmark = TestExecutionBenchmark()

        with patch("time.time", side_effect=[1000.0, 1001.23]):  # 1.23 second
duration
            result = benchmark.run_benchmark("unit-tests", "fast", 2)

        assert result["target"] == "unit-tests"
        assert result["speed"] == "fast"
        assert result["workers"] == 2
>       assert result["duration"] == 1.23
E       assert 1.2300000000000182 == 1.23

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ben
chmark_test_execution.py:54: AssertionError
----------------------------- Captured stdout call -----------------------------
  Running unit-tests (speed: fast, workers: 2)...
____________ TestTestExecutionBenchmark.test_run_benchmark_failure _____________

self =
<tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchmark
object at 0x11ed18f50>
mock_run = <MagicMock name='run' id='5090591952'>

    @patch("subprocess.run")
    def test_run_benchmark_failure(self, mock_run):
        """Test benchmark failure handling."""
        # Mock subprocess failure
        mock_result = MagicMock()
        mock_result.returncode = 1
        mock_result.stdout = "===== 2 passed, 3 failed in 2.45s ====="
        mock_result.stderr = "Some error"
        mock_run.return_value = mock_result

        benchmark = TestExecutionBenchmark()

        with patch("time.time", side_effect=[1000.0, 1002.45]):
            result = benchmark.run_benchmark("unit-tests", None, 1)

        assert result["success"] == False
>       assert result["passed"] == 2
E       assert 0 == 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ben
chmark_test_execution.py:93: AssertionError
----------------------------- Captured stdout call -----------------------------
  Running unit-tests (speed: all, workers: 1)...
_____________________ test_parametrize_speed_marker_parity _____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_parametrize_speed_marker_0')

    @pytest.mark.fast
    def test_parametrize_speed_marker_parity(tmp_path: Path):
        # Create a synthetic test file that uses only pytest.param speed markers
        test_code = textwrap.dedent(
            """
            import pytest

            @pytest.mark.parametrize(
                "x",
                [
                    pytest.param(1, marks=pytest.mark.medium),
                    pytest.param(2, marks=pytest.mark.medium),
                ],
            )
            def test_derived_speed_from_params(x):
                assert x in (1, 2)
            """
        )
        file_path = tmp_path / "test_sample_param_speed.py"
        file_path.write_text(test_code)

        # Load enhanced_test_parser from scripts
        etp = _import_module_from_path(
            "enhanced_test_parser",
            Path(__file__).parents[3] / "scripts" / "enhanced_test_parser.py",
        )
        vtm = _import_module_from_path(
            "verify_test_markers",
            Path(__file__).parents[3] / "scripts" / "verify_test_markers.py",
        )

        # Parse using enhanced test parser
        parsed = etp.parse_test_file(str(file_path))
        # Find our test
        target = [
>           t for t in parsed["tests"] if t["name"] ==
"test_derived_speed_from_params"
                       ^^^^^^^^^^^^^^^
        ]
E       TypeError: list indices must be integers or slices, not str

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_enh
anced_test_parser_marker_parity.py:54: TypeError
__________________ test_returns_error_when_syntax_is_invalid ___________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_returns_error_when_syntax0')

    def test_returns_error_when_syntax_is_invalid(tmp_path: Path) -> None:
        bad_file = tmp_path / "bad.py"
        bad_file.write_text("def broken(:\n    pass\n", encoding="utf-8")

        result = run_script(tmp_path)

>       assert result.returncode == 1
E       AssertionError: assert 0 == 1
E        +  where 0 =
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.ve
nv/bin/python',
'/Users/caitlyn/Projec...-of-caitlyn/pytest-1440/test_returns_error_when_syntax0
'], returncode=0, stdout='[find_syntax_errors] OK\n', stderr='').returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_fin
d_syntax_errors.py:32: AssertionError
_______________________ test_returns_zero_with_no_errors _______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_returns_zero_with_no_erro0')

    def test_returns_zero_with_no_errors(tmp_path: Path) -> None:
        good_file = tmp_path / "good.py"
        good_file.write_text("print('ok')\n", encoding="utf-8")

        result = run_script(tmp_path)

        assert result.returncode == 0
>       assert "No syntax errors found" in result.stdout
E       AssertionError: assert 'No syntax errors found' in '[find_syntax_errors]
OK\n'
E        +  where '[find_syntax_errors] OK\n' =
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.ve
nv/bin/python',
'/Users/caitlyn/Projec...-of-caitlyn/pytest-1440/test_returns_zero_with_no_erro0
'], returncode=0, stdout='[find_syntax_errors] OK\n', stderr='').stdout

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_fin
d_syntax_errors.py:43: AssertionError
_____ TestQualityReportGenerator.test_quality_score_with_missing_mutation ______

self =
<tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerator
object at 0x11ed59970>

    def test_quality_score_with_missing_mutation(self):
        """Test quality score calculation when mutation testing is skipped."""
        metrics = {
            "coverage": {"line_coverage": 90.0},
            "mutation": {"mutation_score": None},  # Skipped
            "property": {"total_property_tests": 5, "enabled": False},
            "organization": {"marker_compliance": 100.0},
            "performance": {"parallel_speedup": 3.5},
        }

        score = calculate_overall_quality_score(metrics)

        # Should handle None mutation score gracefully
>       assert 60 <= score <= 85
E       assert 60 <= 58.5

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_gen
erate_quality_report.py:124: AssertionError
_______ TestQualityReportGenerator.test_recommendations_for_good_metrics _______

self =
<tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerator
object at 0x11ed59e20>

    def test_recommendations_for_good_metrics(self):
        """Test recommendations when metrics are already good."""
        metrics = {
            "coverage": {"line_coverage": 95.0},
            "mutation": {"skipped": False, "mutation_score": 85.0},
            "property": {"total_property_tests": 20, "enabled": True},
            "organization": {"marker_compliance": 98.0},
            "performance": {"parallel_speedup": 5.0},
        }

        recommendations = generate_quality_recommendations(metrics)

        # Should have fewer recommendations for good metrics
        assert len(recommendations) <= 2
        # Should have positive reinforcement
>       assert any("excellent" in rec.lower() for rec in recommendations)
E       assert False
E        +  where False = any(<generator object
TestQualityReportGenerator.test_recommendations_for_good_metrics.<locals>.<genex
pr> at 0x13db56400>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_gen
erate_quality_report.py:142: AssertionError
__________________ test_verify_test_markers_collection_error ___________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_verify_test_markers_colle0')

    @pytest.mark.fast
    def test_verify_test_markers_collection_error(tmp_path: Path) -> None:
        """Reports collection errors. ReqID: QA-02"""
        test_file = tmp_path / "test_bad.py"
        test_file.write_text(
            "import pytest\nimport nonexistent_module\n\n@pytest.mark.fast\ndef
test_fail():\n    pass\n",
            encoding="utf-8",
        )

        vtm.PERSISTENT_CACHE.clear()
        vtm.FILE_SIGNATURES.clear()

        result = vtm.verify_directory_markers(str(tmp_path))
>       assert result["files_with_issues"] == 1
E       assert 0 == 1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ver
ify_test_markers.py:43: AssertionError
_________________________ test_audit_detects_violation _________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_audit_detects_violation0')

    @pytest.mark.fast
    def test_audit_detects_violation(tmp_path: Path) -> None:
        """A file with forbidden patterns should be reported."""
        config = tmp_path / "unsafe.cfg"
        config.write_text("password=secret")
>       results = policy_audit.audit([config])
                  ^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'policy_audit' has no attribute 'audit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_po
licy_audit.py:18: AttributeError
_________________________ test_audit_passes_clean_file _________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_audit_passes_clean_file0')

    @pytest.mark.fast
    def test_audit_passes_clean_file(tmp_path: Path) -> None:
        """A clean file should yield no findings."""
        config = tmp_path / "safe.cfg"
        config.write_text("value=1")
>       assert policy_audit.audit([config]) == []
               ^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'policy_audit' has no attribute 'audit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_po
licy_audit.py:27: AttributeError
_________________________ test_run_requires_pre_deploy _________________________

mock_policy = <MagicMock name='main' id='5095231792'>
mock_safety = <MagicMock name='run_safety' id='5095883264'>
mock_bandit = <MagicMock name='run_bandit' id='5096789520'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fd944a0>

    @patch("security_audit.audit.run_bandit")
    @patch("security_audit.audit.run_safety")
    @patch("security_audit.verify_security_policy.main", return_value=0)
    @pytest.mark.fast
    def test_run_requires_pre_deploy(
        mock_policy: MagicMock,
        mock_safety: MagicMock,
        mock_bandit: MagicMock,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The audit aborts if pre-deploy approval is missing."""
        monkeypatch.delenv("DEVSYNTH_PRE_DEPLOY_APPROVED", raising=False)
>       with pytest.raises(RuntimeError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'RuntimeError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_se
curity_audit.py:105: Failed
----------------------------- Captured stdout call -----------------------------
[security_audit] Environment: DEVSYNTH_VERSION=0.1.0a1, DEVSYNTH_ENV=alpha
[security] Pre-deploy policy checks not approved, but allowing for alpha release
_________________ test_mvuu_config_schema_and_sample_validate __________________

    @pytest.mark.fast
    def test_mvuu_config_schema_and_sample_validate():
        """Ensure the MVUU config schema is valid and the sample conforms to it.

        This test is intentionally strict but local-only:
        - validates the schema structure using Draft7Validator.check_schema
        - validates the sample instance against the schema using
jsonschema.validate
        """
        repo_root = Path(__file__).resolve().parents[3]
        schema_path = repo_root / "docs/specifications/mvuu_config.schema.json"
        sample_path = repo_root / "docs/specifications/mvuu_config.sample.json"

        assert schema_path.exists(), f"Schema file not found: {schema_path}"
        assert sample_path.exists(), f"Sample config file not found:
{sample_path}"

        schema = json.loads(schema_path.read_text(encoding="utf-8"))
        sample = json.loads(sample_path.read_text(encoding="utf-8"))

        # 1) Validate the schema is itself a valid Draft-07 schema
        Draft7Validator.check_schema(schema)

        # 2) Validate the sample instance against the schema
>       validate(instance=sample, schema=schema)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/specifications/t
est_mvuu_config_schema_validation.py:44:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

instance = {'$schema': './mvuu_config.schema.json', 'issues': {}, 'schema':
'docs/specifications/mvuuschema.json', 'storage': {'format': 'json', 'path':
'docs/specifications/mvuu_database.json'}}
schema = {'$schema': 'http://json-schema.org/draft-07/schema#',
'additionalProperties': False, 'properties': {'issues': {'addit...ype':
'string'}}, 'required': ['path', 'format'], 'type': 'object'}}, 'required':
['schema', 'storage', 'issues'], ...}
cls = <class 'jsonschema.validators.Draft7Validator'>, args = (), kwargs = {}
validator = Draft7Validator(schema={'$schema': 'http://json-...ft-07/schema#',
'additionalProperties': False, 'properties': {'issu...uired': ['path',
'format'], 'type': 'object'}}, 'required': ['schema', 'storage', 'issues'],
...}, format_checker=None)
error = <ValidationError: "Additional properties are not allowed ('$schema' was
unexpected)">

    def validate(instance, schema, cls=None, *args, **kwargs):  # noqa: D417
        """
        Validate an instance under the given schema.

            >>> validate([2, 3, 4], {"maxItems": 2})
            Traceback (most recent call last):
                ...
            ValidationError: [2, 3, 4] is too long

        :func:`~jsonschema.validators.validate` will first verify that the
        provided schema is itself valid, since not doing so can lead to less
        obvious error messages and fail in less obvious or consistent ways.

        If you know you have a valid schema already, especially
        if you intend to validate multiple instances with
        the same schema, you likely would prefer using the
        `jsonschema.protocols.Validator.validate` method directly on a
        specific validator (e.g. ``Draft202012Validator.validate``).


        Arguments:

            instance:

                The instance to validate

            schema:

                The schema to validate with

            cls (jsonschema.protocols.Validator):

                The class that will be used to validate the instance.

        If the ``cls`` argument is not provided, two things will happen
        in accordance with the specification. First, if the schema has a
        :kw:`$schema` keyword containing a known meta-schema [#]_ then the
        proper validator will be used. The specification recommends that
        all schemas contain :kw:`$schema` properties for this reason. If no
        :kw:`$schema` property is found, the default validator class is the
        latest released draft.

        Any other provided positional and keyword arguments will be passed
        on when instantiating the ``cls``.

        Raises:

            `jsonschema.exceptions.ValidationError`:

                if the instance is invalid

            `jsonschema.exceptions.SchemaError`:

                if the schema itself is invalid

        .. rubric:: Footnotes
        .. [#] known by a validator registered with
            `jsonschema.validators.validates`

        """
        if cls is None:
            cls = validator_for(schema)

        cls.check_schema(schema)
        validator = cls(schema, *args, **kwargs)
        error = exceptions.best_match(validator.iter_errors(instance))
        if error is not None:
>           raise error
E           jsonschema.exceptions.ValidationError: Additional properties are not
allowed ('$schema' was unexpected)
E
E           Failed validating 'additionalProperties' in schema:
E               {'$schema': 'http://json-schema.org/draft-07/schema#',
E                'title': 'DevSynth MVUU Config Schema',
E                'type': 'object',
E                'required': ['schema', 'storage', 'issues'],
E                'additionalProperties': False,
E                'properties': {'schema': {'type': 'string',
E                                          'description': 'Path to the MVUU
record '
E                                                         'schema (JSON Schema
file).',
E                                          'default':
'docs/specifications/mvuuschema.json'},
E                               'storage': {'type': 'object',
E                                           'required': ['path', 'format'],
E                                           'additionalProperties': False,
E                                           'properties': {'path': {'type':
'string',
E
'description': 'Location '
E
'for '
E
'storing '
E
'MVUU '
E
'records '
E
'(e.g., '
E
'JSON '
E
'database).',
E                                                                   'default':
'docs/specifications/mvuu_database.json'},
E                                                          'format': {'type':
'string',
E                                                                     'enum':
['json'],
E
'description': 'Storage '
E
'format. '
E
'Currently '
E
'only '
E
"'json' "
E
'is '
E
'supported.',
E                                                                     'default':
'json'}}},
E                               'issues': {'type': 'object',
E                                          'required': [],
E                                          'additionalProperties': False,
E                                          'properties': {'github': {'type':
'object',
E                                                                    'required':
['base_url',
E
'token'],
E
'additionalProperties': False,
E
'properties': {'base_url': {'type': 'string'},
E
'token': {'type': 'string'}}},
E                                                         'jira': {'type':
'object',
E                                                                  'required':
['base_url',
E
'token'],
E
'additionalProperties': False,
E                                                                  'properties':
{'base_url': {'type': 'string'},
E
'token': {'type': 'string'}}}}}}}
E
E           On instance:
E               {'$schema': './mvuu_config.schema.json',
E                'schema': 'docs/specifications/mvuuschema.json',
E                'storage': {'path': 'docs/specifications/mvuu_database.json',
E                            'format': 'json'},
E                'issues': {}}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/jsonschema/validators.py:1332: ValidationError
____________ test_collect_behavior_tests_fallback_when_no_tests_ran ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa71fa0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_behavior_tests_fa0')

    @pytest.mark.fast
    def test_collect_behavior_tests_fallback_when_no_tests_ran(monkeypatch,
tmp_path):
        """ReqID: TR-RT-01 — Behavior/integration fallback when no tests ran."""
        # Simulate the behavior/integration fallback branch when a
speed_category is
        # provided and the initial collection yields "no tests ran". The
function
        # should retry with a relaxed marker expression and return the second
set of
        # collected node ids.

        calls: list[dict[str, Any]] = []

        def fake_run(
            cmd, check=False, capture_output=False, text=False
        ):  # type: ignore[no-untyped-def]
            # Record the call for assertions
            calls.append({"cmd": cmd[:]})
            joined = " ".join(cmd)
            if "--collect-only" in cmd and "-m" in cmd:
                # First path: the pre-check for behavior/integration when
                # speed_category is set
                if "tests/behavior/" in joined or cmd[-1] == ".":
                    # Return a signal equivalent to no tests collected under
this
                    # filter
                    return _CP(stdout="no tests ran\n", returncode=0)
            # The subsequent actual collect_cmd should run with either relaxed
            # marker or same path. Return a couple of synthetic node ids to be
            # sanitized.
            out = (
                "tests/behavior/test_fake.py::test_case\n"
                "tests/behavior/test_other.py::test_ok\n"
            )
            return _CP(stdout=out, returncode=0)

        monkeypatch.setattr("subprocess.run", fake_run)

        # Use a temporary cache dir so we don't affect the real project cache
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        # Force cache directory to tmp by changing CWD since function writes
relative path
        monkeypatch.chdir(tmp_path)

        # Create a minimal behavior tests tree so pruning-by-existence keeps our
ids
        (tmp_path / "tests" / "behavior").mkdir(parents=True, exist_ok=True)
        (tmp_path / "tests" / "behavior" / "test_fake.py").write_text(
            "def test_case():\n    assert True\n"
        )
        (tmp_path / "tests" / "behavior" / "test_other.py").write_text(
            "def test_ok():\n    assert True\n"
        )

>       result = collect_tests_with_cache(target="behavior-tests",
speed_category="fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_behavior_fallback.py:63:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_collect_behavior_tests_fallback_when_no_tests_ran.<locals>.fake_run() got
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:08,452 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=behavior-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=behavior-tests (fast) — collecting via pytest
_________ test_collect_tests_with_cache_prunes_nonexistent_and_caches __________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f702e10>

    @pytest.mark.fast
    def test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path,
monkeypatch):
        # Create a fake tests directory with a simple structure
        tests_dir = tmp_path / "tests"
        unit_dir = tests_dir / "unit"
        unit_dir.mkdir(parents=True)
        # Create some dummy test files
        t1 = unit_dir / "test_a.py"
        t1.write_text("def test_a():\n    assert True\n")
        t2 = unit_dir / "test_b.py"
        t2.write_text("def test_b():\n    assert True\n")

        # Monkeypatch TARGET_PATHS lookup by pretending 'all-tests' maps to our
tmp tests dir
        from devsynth.testing import run_tests as rt

        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tests_dir))

        # Simulate pytest --collect-only -q output lines via subprocess.run
        class FakeProc:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str =
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr

        def fake_run(
            cmd, check=False, capture_output=True, text=True, timeout=None
        ):  # noqa: D401
            # Emit node ids including a non-existent file and a line-number
suffix
            lines = [
                f"{t1}::test_a",
                f"{t2}:42",  # should be sanitized to path only then pruned
check happens on path
                f"{tests_dir}/missing_test.py::test_missing",
            ]
            return FakeProc("\n".join(lines), 0, "")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Ensure os.path.exists behaves normally but returns False for
missing_test.py
        real_exists = os.path.exists

        def fake_exists(path):
            if str(path).endswith("missing_test.py"):
                return False
            return real_exists(path)

        monkeypatch.setattr(rt.os.path, "exists", fake_exists)

        # Point cache directory to tmp so we don't touch repo state
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path /
".cache"))

>       out = collect_tests_with_cache("all-tests")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_cache_sanitize.py:80:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'all-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_____________ test_collect_tests_with_cache_synthesizes_when_empty _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb62ab0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_1')

    def test_collect_tests_with_cache_synthesizes_when_empty(monkeypatch,
tmp_path):
        """ReqID: TR-RT-02 — Synthesize minimal list when collection is empty.

        When both primary and fallback collections return no node ids and no
cache exists,
        collect_tests_with_cache should synthesize a minimal file list by
scanning the
        filesystem under the target path for test_*.py files.
        """
        # Arrange: create an isolated temporary test tree
        test_root = tmp_path / "tests" / "unit"
        test_root.mkdir(parents=True)
        dummy = test_root / "test_dummy.py"
        dummy.write_text("def test_ok():\n    assert True\n")

        # Import module under test
        from devsynth.testing import run_tests as rt

        # Point the target mapping to our isolated test directory
        old_target = rt.TARGET_PATHS.get("unit-tests")
        rt.TARGET_PATHS["unit-tests"] = str(test_root)

        # Ensure cwd is the temp directory so the cache dir is local and empty
        monkeypatch.chdir(tmp_path)

        # Monkeypatch subprocess.run to simulate empty collection outputs for
both
        # the primary and fallback collectors.
        def _fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            return types.SimpleNamespace(returncode=0, stdout="", stderr="")

        monkeypatch.setattr(subprocess, "run", _fake_run)

        # Act: call collect with speed_category=None to exercise the synthesize
path
>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_synthesize_on_empty.py:44:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_collect_tests_with_cache_synthesizes_when_empty.<locals>._fake_run() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:08,554 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (all) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (all) — collecting via pytest
____________________ test_collect_tests_with_cache_bad_json ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_2')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb600b0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_bad_json(tmp_path, monkeypatch):
        """Malformed cache file triggers regeneration.

        ReqID: N/A"""
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path))
        cache = tmp_path / "unit-tests_all_tests.json"
        cache.write_text("{bad json}")

        class Res:
            stdout = "tests/unit/sample_test.py::test_a\n"
            returncode = 0

        monkeypatch.setattr(subprocess, "run", lambda *a, **k: Res())
>       out = rt.collect_tests_with_cache("unit-tests", None)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_bad_json.py:23:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________________ test_cache_invalidation_on_file_change ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cache_invalidation_on_fil0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa70560>

    @pytest.mark.fast
    def test_cache_invalidation_on_file_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        # Arrange: create an isolated tests directory
        tests_dir = tmp_path / "isolated_tests"
        tests_dir.mkdir(parents=True, exist_ok=True)

        test_file = tests_dir / "test_sample.py"
        test_file.write_text(
            """
    import pytest

    @pytest.mark.fast
    def test_example():
        assert 1 + 1 == 2
    """
        )

        # Redirect cache dir to tmp
        cache_dir = tmp_path / ".cache"
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")
        # Patch globals on the module for cache dir and target path
        import devsynth.testing.run_tests as rt

        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tests_dir) + "/")

        outputs = ["test_sample.py::test_example\n"]
        call_index = {"value": 0}

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            idx = min(call_index["value"], len(outputs) - 1)
            call_index["value"] += 1
            return SimpleNamespace(stdout=outputs[idx], stderr="", returncode=0)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Act: initial collection (populates cache)
>       first = collect_tests_with_cache("unit-tests", "fast")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:57:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___________________ test_cache_invalidation_on_marker_change ___________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cache_invalidation_on_mar0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb61850>

    @pytest.mark.fast
    def test_cache_invalidation_on_marker_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        tests_dir = tmp_path / "isolated_tests_marker"
        tests_dir.mkdir(parents=True, exist_ok=True)

        test_file = tests_dir / "test_marker.py"
        test_file.write_text(
            """
    import pytest

    @pytest.mark.fast
    def test_fast_case():
        assert True
    """
        )

        cache_dir = tmp_path / ".cache"
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")
        import devsynth.testing.run_tests as rt

        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tests_dir) + "/")

        outputs = [
            "test_marker.py::test_fast_case\n",
            "test_marker.py::test_medium_case\n",
        ]
        call_index = {"value": 0}

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            idx = min(call_index["value"], len(outputs) - 1)
            call_index["value"] += 1
            return SimpleNamespace(stdout=outputs[idx], stderr="", returncode=0)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Populate cache for fast
>       fast_list = collect_tests_with_cache("unit-tests", "fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
________________ test_cache_invalidation_on_target_path_change _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cache_invalidation_on_tar0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fbf8980>

    @pytest.mark.fast
    def test_cache_invalidation_on_target_path_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        tests_dir_one = tmp_path / "suite_one"
        tests_dir_one.mkdir(parents=True, exist_ok=True)
        (tests_dir_one / "test_alpha.py").write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_alpha():\n    assert
True\n"
        )

        tests_dir_two = tmp_path / "suite_two"
        tests_dir_two.mkdir(parents=True, exist_ok=True)
        (tests_dir_two / "test_beta.py").write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_beta():\n    assert
True\n"
        )

        cache_dir = tmp_path / ".cache"
        import devsynth.testing.run_tests as rt

        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir_one))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999)

        calls = {"count": 0}

        class FakeProc:
            def __init__(self, stdout: str) -> None:
                self.stdout = stdout
                self.stderr = ""
                self.returncode = 0

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):
            assert "--collect-only" in cmd
            calls["count"] += 1
            cwd = os.getcwd()
            if cwd == str(tests_dir_one):
                return FakeProc("test_alpha.py::test_alpha\n")
            if cwd == str(tests_dir_two):
                return FakeProc("test_beta.py::test_beta\n")
            raise AssertionError(f"Unexpected cwd {cwd}")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       first = collect_tests_with_cache("unit-tests", "fast")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:215:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_____________ test_cache_uses_fresh_cache_without_subprocess_call ______________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cache_uses_fresh_cache_wi0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fbf8c20>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_cache_uses_fresh_cache_without_subprocess_call(tmp_path,
monkeypatch):
        """ReqID: CACHE-TTL-1"""
        # Arrange isolated tests dir and cache dir
        tests_dir = tmp_path / "isolated_tests_fresh"
        tests_dir.mkdir(parents=True, exist_ok=True)
        t1 = tests_dir / "test_alpha.py"
        # File existence matters for pruning; create it
        t1.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert
True\n"
        )

        # Redirect module globals
        import devsynth.testing.run_tests as rt

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))

        # Stable mtime for fingerprint match
        monkeypatch.setattr(rt.os.path, "getmtime", lambda p: 123.456)

        # Prepare a fresh cache file that should be reused
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_key = "unit-tests_fast"
        cache_file = cache_dir / f"{cache_key}_tests.json"
        payload = {
            "timestamp": datetime.now().isoformat(),  # fresh
            "tests": [str(t1) + "::test_ok"],
            "fingerprint": {
                "latest_mtime": 123.456,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir) + "/",
                "node_set_hash": 111,
            },
        }
        cache_file.write_text(json.dumps(payload))

        # If subprocess.run is called, fail the test (cache should be used)
        def forbid_run(*args, **kwargs):  # pragma: no cover - should not be hit
            raise AssertionError("subprocess.run should not be called for fresh
cache")

        monkeypatch.setattr(rt.subprocess, "run", forbid_run)

        # Act
>       out = collect_tests_with_cache("unit-tests", "fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_ttl.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________ test_cache_ttl_expired_triggers_subprocess_and_refresh ____________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cache_ttl_expired_trigger0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f23bdd0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_cache_ttl_expired_triggers_subprocess_and_refresh(tmp_path,
monkeypatch):
        """ReqID: CACHE-TTL-2"""
        # Arrange isolated tests dir and cache dir
        tests_dir = tmp_path / "isolated_tests_ttl"
        tests_dir.mkdir(parents=True, exist_ok=True)
        t1 = tests_dir / "test_beta.py"
        t1.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_beta():\n    assert
True\n"
        )

        import devsynth.testing.run_tests as rt

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))

        # Keep fingerprint matching except TTL
        monkeypatch.setattr(rt.os.path, "getmtime", lambda p: 999.0)

        # Expire quickly: TTL = 1 second
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        # Re-import TTL constant by reloading module-level var
        # Note: the module reads TTL at import; our code in run_tests.py guards
        # ValueError and sets default.
        # For simplicity we won't force re-import. collect_tests_with_cache
reads
        # the env only at import time for TTL, but it uses the module-level int
        # COLLECTION_CACHE_TTL_SECONDS.
        # We'll monkeypatch that directly for this test.
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 1)

        # Prepare an old cache file whose timestamp is older than TTL
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_key = "unit-tests_fast"
        cache_file = cache_dir / f"{cache_key}_tests.json"
        old_time = (datetime.now() - timedelta(seconds=5)).isoformat()
        payload = {
            "timestamp": old_time,
            "tests": [str(t1) + "::test_beta"],
            "fingerprint": {
                "latest_mtime": 999.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir) + "/",
                "node_set_hash": 222,
            },
        }
        cache_file.write_text(json.dumps(payload))

        # Track subprocess invocations and emit a predictable collection
        calls = {"count": 0}

        class FakeProc:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str =
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):
            calls["count"] += 1
            lines = [f"{t1}::test_beta"]
            return FakeProc("\n".join(lines), 0, "")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Act: TTL expired so it should call subprocess and refresh cache
>       out = collect_tests_with_cache("unit-tests", "fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_ttl.py:135:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
______________ test_collect_tests_with_cache_respects_ttl_expiry _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f703c50>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_3')

    @pytest.mark.fast
    def test_collect_tests_with_cache_respects_ttl_expiry(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-01 — TTL expiry invalidates stale cache entries."""

        tests_dir = tmp_path / "suite_ttl"
        tests_dir.mkdir()
        node = tests_dir / "test_ttl.py"
        node.write_text("def test_ttl():\n    assert True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_ttl"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt.os.path, "getmtime", lambda path: 42.0)
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 0)

        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        payload = {
            "timestamp": (datetime.now() - timedelta(seconds=5)).isoformat(),
            "tests": [f"{node}::test_ttl"],
            "fingerprint": {
                "latest_mtime": 42.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 1,
            },
        }
        cache_file.write_text(json.dumps(payload))

        calls: list[list[str]] = []

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(
                returncode=0,
                stdout=f"{node}::test_ttl\n",
                stderr="",
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       result = rt.collect_tests_with_cache("unit-tests", "fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
______ test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f701160>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_4')

    @pytest.mark.fast
    def test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-02 — Fingerprint mismatch forces a fresh collection."""

        tests_dir = tmp_path / "suite_fingerprint"
        tests_dir.mkdir()
        node = tests_dir / "test_fp.py"
        node.write_text("def test_fp():\n    assert True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_fp"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        cache_dir.mkdir(parents=True, exist_ok=True)

        monkeypatch.setattr(rt.os.path, "getmtime", lambda path: 321.0)
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 3600)

        cache_file = cache_dir / "unit-tests_fast_tests.json"
        payload = {
            "timestamp": datetime.now().isoformat(),
            "tests": [f"{node}::test_fp"],
            "fingerprint": {
                "latest_mtime": 111.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 5,
            },
        }
        cache_file.write_text(json.dumps(payload))

        calls: list[list[str]] = []

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(
                returncode=0,
                stdout=f"{node}::test_fp\n",
                stderr="",
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       result = rt.collect_tests_with_cache("unit-tests", "fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:124:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___ test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f703dd0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_5')

    @pytest.mark.fast
    def test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-03 — Empty collections trigger fallback to pruned
cache."""

        tests_dir = tmp_path / "suite_fallback"
        tests_dir.mkdir()
        keep = tests_dir / "test_keep.py"
        keep.write_text("def test_keep():\n    assert True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_fb"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 0)
        cache_dir.mkdir(parents=True, exist_ok=True)

        cache_file = cache_dir / "unit-tests_all_tests.json"
        payload = {
            "timestamp": (datetime.now() - timedelta(seconds=10)).isoformat(),
            "tests": [
                f"{keep}::test_ok",
                f"{tests_dir / 'test_missing.py'}::test_missing",
            ],
            "fingerprint": {
                "latest_mtime": 10.0,
                "category_expr": "not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 10,
            },
        }
        cache_file.write_text(json.dumps(payload))

        calls: list[list[str]] = []

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(returncode=0, stdout="", stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        real_exists = rt.os.path.exists

        def fake_exists(path: str) -> bool:
            if path == str(keep):
                return True
            if path == str(tests_dir / "test_missing.py"):
                return False
            if path == str(cache_file):
                return True
            return real_exists(path)

        monkeypatch.setattr(rt.os.path, "exists", fake_exists)

>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:194:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
________ test_collect_tests_with_cache_synthesizes_and_caches_node_ids _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f702810>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_6')

    @pytest.mark.fast
    def test_collect_tests_with_cache_synthesizes_and_caches_node_ids(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-04 — Synthesized node list persists to cache when
empty."""

        tests_dir = tmp_path / "suite_synth"
        tests_dir.mkdir(parents=True)
        file_a = tests_dir / "test_alpha.py"
        file_a.write_text("def test_alpha():\n    assert True\n")
        nested = tests_dir / "nested"
        nested.mkdir()
        file_b = nested / "test_beta.py"
        file_b.write_text("def test_beta():\n    assert True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_synth"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout="", stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:232:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
__________ test_collect_uses_cached_and_prunes_when_collection_empty ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f700c80>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_uses_cached_and_p0')

>   ???

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_fallback.py:69:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_______ test_collect_falls_back_to_unfiltered_and_returns_sanitized_ids ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb6b560>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_falls_back_to_unf0')

>   ???

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_fallback.py:119:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________ test_html_report_artifacts_created_with_stable_naming _____________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_html_report_artifacts_cre0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fd95700>

    @pytest.mark.fast
    def test_html_report_artifacts_created_with_stable_naming(tmp_path,
monkeypatch):
        """
        Verify that --report produces an HTML report under test_reports/ with a
        timestamped folder and target subdirectory, and that the naming is
stable
        enough for tooling (YYYYMMDD_HHMMSS/target/report.html).
        """
        # Arrange: create a minimal passing test in an isolated directory
        test_file = tmp_path / "test_passes.py"
        test_file.write_text(
            """
    import pytest

    @pytest.mark.fast
    def test_will_pass():
        assert True
    """
        )
        # Route the unit-tests target to our isolated directory
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))

        # Act: run tests with HTML report generation enabled
        success, output = run_tests("unit-tests", ["fast"], report=True,
parallel=False)

        # Assert: tests should pass and a report should be created in
test_reports/
>       assert success, f"Expected success, got failure. Output was:\n{output}"
E       AssertionError: Expected success, got failure. Output was:
E         ERROR: file or directory not found: test_passes.py::test_will_pass
E
E         ============================= test session starts
==============================
E         platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
E         benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False
min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10
warmup=False warmup_iterations=100000)
E         rootdir:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_html_report_artifacts_cre0/project
E         plugins: mock-3.15.1, asyncio-1.2.0, anyio-4.11.0, html-4.1.1,
xdist-3.8.0, langsmith-0.4.37, metadata-3.1.1, Faker-37.11.0, benchmark-5.1.0,
hypothesis-6.142.3, bdd-8.1.0, rerunfailures-16.1, cov-7.0.0
E         asyncio: mode=Mode.STRICT, debug=False,
asyncio_default_fixture_loop_scope=None,
asyncio_default_test_loop_scope=function
E         collected 0 items
E
E         ============================ no tests ran in 0.29s
=============================
E
E         Pytest exited with code 4. Command:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest
test_passes.py::test_will_pass -m not memory_intensive and fast and not gui
--cov=src/devsynth --cov-report=json:test_reports/coverage.json
--cov-report=html:htmlcov --cov-append
E         Troubleshooting tips:
E         - Smoke mode: reduce third-party plugin surface to isolate issues:
E           poetry run devsynth run-tests --smoke --speed=fast --no-parallel
--maxfail=1
E         - Marker discipline: default is '-m not memory_intensive'.
E           Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.
E         - Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using
--smoke; plugin options may fail otherwise.
E         - Diagnostics: run 'poetry run devsynth doctor' for a quick
environment check.
E         - Narrow scope: use '-k <expr>' and '-vv' to focus a failure.
E         - Segment large suites to localize failures and flakes:
E           devsynth run-tests --target unit-tests --speed=fast --segment
--segment-size=50
E         - Limit failures early to speed iteration:
E           poetry run devsynth run-tests --target unit-tests --speed=fast
--maxfail=1
E         - Disable parallelism if xdist interaction is suspected:
E           devsynth run-tests --target unit-tests --speed=fast --no-parallel
E         - Generate an HTML report for context (saved under test_reports/):
E           devsynth run-tests --target unit-tests --speed=fast --report
E
E       assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_htm
l_report_artifacts.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:09,152 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:11,385 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
______________________ test_integration_mutation_workflow ______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_integration_mutation_work0')

    @pytest.mark.fast
    def test_integration_mutation_workflow(tmp_path):
        """Test complete mutation testing workflow."""
        # Create a simple source file
        source_dir = tmp_path / "src"
        source_dir.mkdir()
        source_file = source_dir / "calculator.py"
        source_file.write_text(
            """
    def add(x, y):
        return x + y

    def is_positive(x):
        return x > 0

    def is_equal(x, y):
        return x == y
    """
        )

        # Create corresponding tests
        test_dir = tmp_path / "tests"
        test_dir.mkdir()
        test_file = test_dir / "test_calculator.py"
        test_file.write_text(
            """
    import sys
    sys.path.append('../src')
    from calculator import add, is_positive, is_equal

    def test_add():
        assert add(2, 3) == 5
        assert add(-1, 1) == 0

    def test_is_positive():
        assert is_positive(5) == True
        assert is_positive(-5) == False

    def test_is_equal():
        assert is_equal(5, 5) == True
        assert is_equal(5, 6) == False
    """
        )

        # Mock subprocess to avoid actually running tests
        with patch("subprocess.run") as mock_run:
            # Simulate that mutations are caught (tests fail)
            mock_result = MagicMock()
            mock_result.returncode = 1  # Test failed (mutation killed)
            mock_result.stdout = "FAILED"
            mock_result.stderr = ""
            mock_run.return_value = mock_result

            tester = MutationTester(timeout_seconds=5)

            # Run with limited mutations for testing
>           report = tester.run_mutations(str(source_dir), str(test_dir),
max_mutations=5)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_mut
ation_testing.py:425:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/mutati
on_testing.py:452: in run_mutations
    file_key = str(Path(result.file_path).relative_to(Path.cwd()))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_integration_mutation_work0/src/calculator.py')
other =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_integration_mutation_work0/project')
walk_up = False, _deprecated = (), step = 0

    def relative_to(self, other, /, *_deprecated, walk_up=False):
        """Return the relative path to another path identified by the passed
        arguments.  If the operation is not possible (because this is not
        related to the other path), raise ValueError.

        The *walk_up* parameter controls whether `..` may be used to resolve
        the path.
        """
        if _deprecated:
            msg = ("support for supplying more than one positional argument "
                   "to pathlib.PurePath.relative_to() is deprecated and "
                   "scheduled for removal in Python {remove}")
            warnings._deprecated("pathlib.PurePath.relative_to(*args)", msg,
                                 remove=(3, 14))
        other = self.with_segments(other, *_deprecated)
        for step, path in enumerate([other] + list(other.parents)):
            if self.is_relative_to(path):
                break
            elif not walk_up:
>               raise ValueError(f"{str(self)!r} is not in the subpath of
{str(other)!r}")
E               ValueError:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_integration_mutation_work0/src/calculator.py' is not in the subpath
of
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_integration_mutation_work0/project'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:682: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:11,579 - devsynth.testing.mutation_testing - INFO - Starting
mutation testing:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_integration_mutation_work0/src ->
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_integration_mutation_work0/tests
2025-10-28 10:31:11,580 - devsynth.testing.mutation_testing - INFO - Found 1
Python files to mutate
2025-10-28 10:31:11,582 - devsynth.testing.mutation_testing - INFO - Generated 8
total mutations
2025-10-28 10:31:11,582 - devsynth.testing.mutation_testing - INFO - Limited to
5 mutations
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Starting
mutation testing:
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_integration_mutation_work0/src ->
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_integration_mutation_work0/tests
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Found 1 Python
files to mutate
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Generated 8
total mutations
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Limited to 5
mutations
___________________ test_run_tests_keyword_filter_no_matches ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f883410>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_keyword_filter_0')

    @pytest.mark.fast
    def test_run_tests_keyword_filter_no_matches(monkeypatch, tmp_path):
        """ReqID: FR-11.2 — Keyword path handles empty collection gracefully."""
        # Ensure test path exists to satisfy chdir logic
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)

        # Patch target path mapping to our tmp tests dir for unit-tests
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))

        # Mock subprocess.run for collection to return no node ids
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            # Simulate --collect-only output with no matching lines
            return _DummyCompleted(stdout="", stderr="", returncode=0)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )
        assert ok is True
>       assert "No tests matched" in output
E       AssertionError: assert 'No tests matched' in 'Marker fallback
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0
_______________\n\n============================ no tests ran in 0.30s
=============================\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests.py:121: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:11,651 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:11,652 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (medium) — collecting via pytest
2025-10-28 10:31:11,652 - devsynth.testing.run_tests - INFO - marker fallback
triggered for target=unit-tests (speeds=fast,medium)
2025-10-28 10:31:13,355 - devsynth.testing.run_tests - INFO - Coverage data file
detected at .coverage (53248 bytes)
2025-10-28 10:31:13,358 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: no measured files present
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (medium) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback
triggered for target=unit-tests (speeds=fast,medium)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage data file
detected at .coverage (53248 bytes)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: no measured files present
___________ test_collect_tests_with_cache_writes_cache_and_sanitizes ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f882c00>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_7')

    @pytest.mark.fast
    def test_collect_tests_with_cache_writes_cache_and_sanitizes(monkeypatch,
tmp_path):
        """ReqID: FR-11.2 — Collection cache stores sanitized node ids."""
        tests_dir = tmp_path / "tests"
        (tests_dir / "unit").mkdir(parents=True)
        # Create dummy test files to satisfy synthesized fallback if needed
        (tests_dir / "unit" / "test_x.py").write_text("\n")
        monkeypatch.chdir(tmp_path)

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir /
"unit"))

        # First run: provide some noisy output with line numbers and duplicates
        noisy = [
            "tests/unit/test_x.py:10",
            "tests/unit/test_x.py::test_ok",
            "tests/unit/test_x.py:10",  # duplicate
        ]

        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            return _DummyCompleted(stdout="\n".join(noisy), stderr="",
returncode=0)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        out = rt.collect_tests_with_cache("unit-tests", "fast")
        # Sanitized and deduped: line numbers removed when no '::'
>       assert out[0] == "tests/unit/test_x.py"
E       AssertionError: assert 'tests/unit/t...x.py::test_ok' ==
'tests/unit/test_x.py'
E
E         - tests/unit/test_x.py
E         + tests/unit/test_x.py::test_ok
E         ?                     +++++++++

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests.py:215: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,390 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
________________ test_collect_tests_with_cache_handles_timeout _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcfd1f0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_8')

    def test_collect_tests_with_cache_handles_timeout(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Timeouts during collection yield tips but no crash.

        ReqID: coverage-run-tests
        """

        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests")
        (tmp_path / "tests").mkdir()

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(stdout="", stderr="", returncode=-1)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       collected = rt.collect_tests_with_cache("unit-tests",
speed_category="fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_coverage.py:180:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

        result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
        if result.returncode != 0:
            if result.stderr:
                error_message = f"Test collection failed: {result.stderr}"
            else:
                error_message = f"Test collection failed with exit code
{result.returncode}"

            # Log more details for debugging
            logger.warning(
                error_message,
                extra={
                    "event": "test_collection_failed",
                    "target": target,
                    "returncode": result.returncode,
                    "stdout": (
                        result.stdout[:500] if result.stdout else None
                    ),  # First 500 chars
                    "stderr": (
                        result.stderr[:500] if result.stderr else None
                    ),  # First 500 chars
                    "speed_category": category_expr,
                },
            )
>           raise RuntimeError(error_message)
E           RuntimeError: Test collection failed with exit code -1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1367: RuntimeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,475 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:13,475 - devsynth.testing.run_tests - WARNING - Test collection
failed with exit code -1
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection failed
with exit code -1
__________ test_collect_tests_with_cache_handles_subprocess_exception __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff169c0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_9')
caplog = <_pytest.logging.LogCaptureFixture object at 0x12fd971d0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_handles_subprocess_exception(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RT-ERR-01 — Collection errors log tips and return empty list.

        Issue: issues/coverage-below-threshold.md
        """

        tests_dir = tmp_path / "tests_root"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        caplog.set_level(logging.ERROR)

        def boom(*_args, **_kwargs):  # noqa: ANN002
            raise RuntimeError("collection failed")

        monkeypatch.setattr(rt.subprocess, "run", boom)

>       result = rt.collect_tests_with_cache("unit-tests",
speed_category="fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

_args =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_9/tests_root', '--collect-only', '-q', ...],)
_kwargs = {'capture_output': True, 'cwd':
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY'...on.debugpy-2025.14.1-darwin-arm64/bundled/libs/debugpy
', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...}, 'text': True, ...}

    def boom(*_args, **_kwargs):  # noqa: ANN002
>       raise RuntimeError("collection failed")
E       RuntimeError: collection failed

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:27: RuntimeError
______________ test_run_tests_handles_unexpected_execution_error _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff14200>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_handles_unexpec0')

    @pytest.mark.fast
    def test_run_tests_handles_unexpected_execution_error(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RT-ERR-02 — Unexpected execution errors surface
troubleshooting tips.

        Issue: issues/coverage-below-threshold.md
        """

        tests_dir = tmp_path / "tests_exec"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))

        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)

        def failing_popen(*_args, **_kwargs):  # noqa: ANN002
            raise RuntimeError("boom popen")

        monkeypatch.setattr(rt.subprocess, "Popen", failing_popen)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

        assert success is False
        assert "boom popen" in output
>       assert "Troubleshooting tips" in output
E       AssertionError: assert 'Troubleshooting tips' in 'boom popen'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,588 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
__________________ test_run_tests_segment_merges_extra_marker __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff16d50>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_segment_merges_0')

    @pytest.mark.fast
    def test_run_tests_segment_merges_extra_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RT-ERR-03 — Segmented runs combine marker filters with extra
expressions.

        Issue: issues/coverage-below-threshold.md
        """

        tests_dir = tmp_path / "tests_segment"
        tests_dir.mkdir()
        (tests_dir / "test_demo.py").write_text("def test_ok():\n    assert
True\n")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))

        node_ids = "test_demo.py::test_ok\n"

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            marker_index = len(cmd) - 1 - cmd[::-1].index("-m")
            expr = cmd[marker_index + 1]
            assert "fast" in expr and "not memory_intensive" in expr
            assert "not slow" in expr
            return SimpleNamespace(returncode=0, stdout=node_ids, stderr="")

        captured_batches: list[list[str]] = []

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                captured_batches.append(cmd)
                self.returncode = 0

            def communicate(self):
                return ("ok\n", "")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker="not slow",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:122:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cmd = ['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/lbs...23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_r
un_tests_segment_merges_0/tests_segment', '--collect-only', '-q', ...]
check = False, capture_output = True, text = True, timeout = 60.0
cwd = '/Users/caitlyn/Projects/github.com/ravenoak/devsynth'
env = {'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi',
'BUNDLED_DEBUGPY_PATH':
'/Users/caitlyn/.cursor/extensions/ms-python.debugpy-2025.14.1-darwin-arm64/bund
led/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...}

    def fake_run(
        cmd,
        check=False,
        capture_output=True,
        text=True,
        timeout=None,
        cwd=None,
        env=None,
    ):  # noqa: ANN001
        assert "--collect-only" in cmd
        marker_index = len(cmd) - 1 - cmd[::-1].index("-m")
        expr = cmd[marker_index + 1]
        assert "fast" in expr and "not memory_intensive" in expr
>       assert "not slow" in expr
E       AssertionError: assert 'not slow' in 'fast and not memory_intensive'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:104: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,597 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
______________ test_coverage_artifacts_status_detects_empty_html _______________

coverage_paths =
(PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-c
aitlyn/pytest-1440/test_coverage_artif...ar/folders/2v/lbss3by10y51bg7c07nd23rh0
000gn/T/pytest-of-caitlyn/pytest-1440/test_coverage_artifacts_status5/htmlcov'))

    @pytest.mark.fast
    def test_coverage_artifacts_status_detects_empty_html(
        coverage_paths: tuple[Path, Path],
    ) -> None:
        """HTML reports that note missing data should trigger remediation."""

        coverage_json, html_dir = coverage_paths
        coverage_json.parent.mkdir(parents=True, exist_ok=True)
        coverage_json.write_text(json.dumps({"totals": {"percent_covered":
90.0}}))
        html_dir.mkdir(parents=True, exist_ok=True)
        (html_dir / "index.html").write_text("No coverage data available")

        ok, reason = run_tests_module.coverage_artifacts_status()
        assert ok is False
>       assert reason is not None and "No coverage data" in reason
E       AssertionError: assert ('Coverage HTML indicates no recorded data' is
not None and 'No coverage data' in 'Coverage HTML indicates no recorded data')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_artifacts.py:264: AssertionError
__________________ test_failure_tips_includes_command_context __________________

    @pytest.mark.fast
    def test_failure_tips_includes_command_context() -> None:
        """Failure tips prefix the command and retain segmentation guidance."""

        cmd = ["python", "-m", "pytest", "tests/unit"]
        tips = run_tests_module._failure_tips(2, cmd)

        assert tips.startswith(
            "\nPytest exited with code 2. Command: python -m pytest tests/unit"
        )
        assert "Troubleshooting tips:" in tips
        assert "Segment large suites" in tips
>       assert "Re-run failing segments" in tips
E       AssertionError: assert 'Re-run failing segments' in '\nPytest exited
with code 2. Command: python -m pytest tests/unit\nTroubleshooting tips:\n-
Smoke mode: reduce third-...HTML report for context (saved under
test_reports/):\n  devsynth run-tests --target unit-tests --speed=fast
--report\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_artifacts.py:294: AssertionError
____________ test_segmented_run_treats_benchmark_warning_as_success ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9ea120>

    @pytest.mark.fast
    def test_segmented_run_treats_benchmark_warning_as_success(monkeypatch):
        """
        ReqID: FR-11.2
        When running in segmented mode, if a batch returns a nonzero exit code
but
        stderr contains PytestBenchmarkWarning, the batch should be treated as
        successful. This test simulates that path deterministically.
        """

        # Simulate collection returning two node ids
        class DummyCompleted:
            def __init__(self, stdout: str = "", stderr: str = "") -> None:
                self.stdout = stdout
                self.stderr = stderr
                self.returncode = 0

        def fake_run(
            cmd: list[str], check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            # Return minimal collect-only output resembling pytest's -q
--collect-only
            # Use python path style entries so _sanitize_node_ids accepts them
            out = "\n".join(
                [
                    "tests/unit/sample_test.py::test_one",
                    "tests/unit/sample_test.py::test_two",
                ]
            )
            return DummyCompleted(stdout=out, stderr="")

        monkeypatch.setattr("subprocess.run", fake_run)

        # Simulate pytest execution batches: nonzero returncode
        # but with a benchmark warning in stderr
        class DummyPopen:
            def __init__(
                self,
                cmd: list[str],
                stdout: Any,
                stderr: Any,
                text: bool,
                env: dict[str, str],
            ):  # noqa: D401
                # store for potential assertions/debug
                self.cmd = cmd
                # nonzero return code; handled as success due to warning in
stderr
                self._returncode = 1

            def communicate(self):
                stdout = ""
                stderr = "PytestBenchmarkWarning: benchmark plugin present\n"
                return stdout, stderr

            @property
            def returncode(self) -> int:
                return self._returncode

        monkeypatch.setattr("subprocess.Popen", DummyPopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            segment=True,
            segment_size=1,  # force two batches
            parallel=False,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_benchmark_warning.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_segmented_run_treats_benchmark_warning_as_success.<locals>.fake_run() got
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,762 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_________ test_collect_tests_with_cache_prunes_nonexistent_and_caches __________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_10')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9ea480>

    @pytest.mark.fast
    def test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path,
monkeypatch):
        # Direct cache into a temp directory
        import devsynth.testing.run_tests as rt

        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path / "cache"))

        # Simulate pytest --collect-only output with one non-existent and one
existent file
        existing = "tests/unit/synthetic_test_file.py::test_ok"
        missing = "tests/unit/missing_test_file.py::test_missing"

        stdout = f"{missing}\n{existing}\n"

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout=stdout, stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Make os.path.exists return True only for the existing path part
        real_exists = os.path.exists

        def fake_exists(path):  # noqa: ANN001
            if isinstance(path, str) and path.startswith(
                "tests/unit/synthetic_test_file.py"
            ):
                return True
            if isinstance(path, str) and
path.startswith("tests/unit/missing_test_file.py"):
                return False
            return real_exists(path)

        monkeypatch.setattr(rt.os.path, "exists", fake_exists)

        results = collect_tests_with_cache(target="unit-tests",
speed_category=None)
        assert existing in results
>       assert all(missing not in r for r in results)
E       assert False
E        +  where False = all(<generator object
test_collect_tests_with_cache_prunes_nonexistent_and_caches.<locals>.<genexpr>
at 0x12f8b4860>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cache_prune_and_tips.py:65: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,824 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (all) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (all) — collecting via pytest
_________________ test_prunes_nonexistent_paths_and_uses_cache _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_prunes_nonexistent_paths_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12facdc70>

    @pytest.mark.fast
    def test_prunes_nonexistent_paths_and_uses_cache(tmp_path, monkeypatch):
        """ReqID: CACHE-PRUNE-1"""
        # Prepare a fake cache with a nonexistent test path and an existent one
        os.makedirs(COLLECTION_CACHE_DIR, exist_ok=True)
        cache_file = os.path.join(COLLECTION_CACHE_DIR,
"unit-tests_all_tests.json")
        existent = "tests/unit/test_example.py::test_ok"
        nonexistent = "tests/unit/test_deleted.py::test_gone"
        # Ensure the existent path exists on filesystem for the pruning check
        exist_dir = Path("tests/unit")
        exist_dir.mkdir(parents=True, exist_ok=True)
        exist_file = exist_dir / "test_example.py"
        if not exist_file.exists():
            exist_file.write_text(
                "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert
True\n"
            )

        cache_payload = {
            "timestamp": "2099-01-01T00:00:00",
            "tests": [existent, nonexistent],
            "fingerprint": {
                "latest_mtime": 0.0,
                "category_expr": "not memory_intensive",
                "test_path": "tests/",
                "node_set_hash": 123,
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cache_payload, f)

        # Monkeypatch TTL to be huge so cache would be used if fingerprint
matches
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            """Return the cached node ids without invoking pytest."""

            assert "--collect-only" in cmd, cmd
            return SimpleNamespace(
                returncode=0,
                stdout="\n".join([existent, nonexistent]),
                stderr="",
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Force fingerprint mismatch by changing latest_mtime via
        # monkeypatching os.path.getmtime
        original_getmtime = os.path.getmtime

        def fake_getmtime(path):
            return 1.0

        monkeypatch.setattr(os.path, "getmtime", fake_getmtime)

        # Now call collection; it should regenerate and prune the nonexistent
path
        out = collect_tests_with_cache(target="all-tests", speed_category=None)
        assert existent in out
>       assert all(nonexistent != nid for nid in out)
E       assert False
E        +  where False = all(<generator object
test_prunes_nonexistent_paths_and_uses_cache.<locals>.<genexpr> at 0x13da4b850>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cache_pruning.py:77: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,837 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=all-tests (all) — decomposing all-tests into dependent
targets
2025-10-28 10:31:13,837 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (all) — collecting via pytest
2025-10-28 10:31:13,838 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=integration-tests (all) — collecting via pytest
2025-10-28 10:31:13,838 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=behavior-tests (all) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=all-tests (all) — decomposing all-tests into dependent targets
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (all) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=integration-tests (all) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=behavior-tests (all) — collecting via pytest
____________ test_segmented_batch_exception_emits_tips_and_plugins _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12facd8b0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_batch_exception0')

    @pytest.mark.fast
    def test_segmented_batch_exception_emits_tips_and_plugins(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENT-CLI-2 — Exceptions surface tips and preserve
plugins."""

        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_segment.py"
        test_file.write_text("def test_fail():\n    assert True\n")

        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None,
**kwargs
        ):  # noqa: ANN001
            return SimpleNamespace(
                returncode=0, stdout=f"{test_file}::test_fail\n", stderr=""
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)

        captured_envs: list[dict[str, str]] = []

        def exploding_batch(
            cmd, stdout=None, stderr=None, text=True, env=None
        ):  # noqa: ANN001
            captured_envs.append(dict(env or {}))
            raise RuntimeError("segmented batch crashed")

        monkeypatch.setattr(rt.subprocess, "Popen", exploding_batch)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=3,
            extra_marker=None,
        )

        assert success is False
        assert "segmented batch crashed" in output
>       assert output.count("Troubleshooting tips:") == 2
E       AssertionError: assert 0 == 2
E        +  where 0 = <built-in method count of str object at
0x13d873030>('Troubleshooting tips:')
E        +    where <built-in method count of str object at 0x13d873030> =
'Failed to run tests: segmented batch crashed'.count

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:186: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,858 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:13,858 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:13,858 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-28 10:31:13,858 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:13,859 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 1 for target=unit-tests
2025-10-28 10:31:13,859 - devsynth.testing.run_tests - INFO - Running segment
1/1 (1 tests)
2025-10-28 10:31:13,859 - devsynth.testing.run_tests - ERROR - Failed to run
tests: segmented batch crashed
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1
tests)
ERROR    devsynth.testing.run_tests:logging_setup.py:615 Failed to run tests:
segmented batch crashed
_______________ test_segmented_batches_reinject_when_env_mutates _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f86b620>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_batches_reinjec0')

    @pytest.mark.fast
    def test_segmented_batches_reinject_when_env_mutates(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """Segments reapply plugin directives even if previous runs stripped
them."""

        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        first = tmp_path / "test_first.py"
        second = tmp_path / "test_second.py"
        first.write_text("def test_one():\n    assert True\n")
        second.write_text("def test_two():\n    assert True\n")

        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None,
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout="\n".join([f"{first}::test_one", f"{second}::test_two"]),
                stderr="",
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)

        popen_envs: list[dict[str, str]] = []

        class MutatingPopen:
            call_index = 0

            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=True,
                env=None,
            ):  # noqa: ANN001
                MutatingPopen.call_index += 1
                env_map = dict(env or {})
                popen_envs.append(env_map)
                _assert_plugins_in_env(env_map)

                if env is not None:
                    tokens = env.get("PYTEST_ADDOPTS", "").split()
                    filtered = [
                        token
                        for token in tokens
                        if token not in {"-p", "pytest_cov",
"pytest_bdd.plugin"}
                    ]
                    env["PYTEST_ADDOPTS"] = " ".join(filtered)

                self.returncode = 0
                self._stdout = f"segment {MutatingPopen.call_index} ok"
                self._stderr = ""

            def communicate(self):  # noqa: D401 - subprocess API emulation
                """Return the stubbed stdout/stderr pair."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "Popen", MutatingPopen)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )

        assert success is True
        assert "segment 1 ok" in output
        assert "segment 2 ok" in output
        assert len(popen_envs) == 2
        for env_snapshot in popen_envs:
            _assert_plugins_in_env(env_snapshot)
>       _assert_plugins_in_env(dict(os.environ))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:280:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:39: in _assert_plugins_in_env
    _assert_plugins_in_addopts(env.get("PYTEST_ADDOPTS", ""))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

addopts = '-q'

    def _assert_plugins_in_addopts(addopts: str) -> None:
        """Assert pytest-cov and pytest-bdd plugins are present in addopts."""

        normalized = addopts.strip()
        assert normalized, "PYTEST_ADDOPTS should not be empty when plugins are
injected"
>       assert (
            "-p pytest_cov" in normalized or "-ppytest_cov" in normalized
        ), f"pytest-cov plugin missing: {normalized}"
E       AssertionError: pytest-cov plugin missing: -q
E       assert ('-p pytest_cov' in '-q' or '-ppytest_cov' in '-q')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:28: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,872 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:13,872 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:13,872 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-28 10:31:13,872 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:13,873 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-28 10:31:13,873 - devsynth.testing.run_tests - INFO - Running segment
1/2 (1 tests)
2025-10-28 10:31:13,873 - devsynth.testing.run_tests - INFO - Running segment
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1
tests)
_________ test_run_tests_env_var_propagation_retains_existing_addopts __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f86ba70>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_env_var_propaga0')

    @pytest.mark.fast
    def test_run_tests_env_var_propagation_retains_existing_addopts(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-ENV-1 — CLI helper preserves existing
PYTEST_ADDOPTS."""

        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_env.py"
        test_file.write_text("def test_env():\n    assert True\n")

        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None,
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(returncode=0,
stdout=f"{test_file}::test_env", stderr="")

        recorded: list[tuple[list[str], dict[str, str]]] = []

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                recorded.append((list(cmd), dict(env or {})))
                self.returncode = 0
                self._stdout = "pass"
                self._stderr = ""

            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            maxfail=1,
            extra_marker=None,
        )

        assert success is True
        assert output == "pass"
        assert recorded, "Expected subprocess invocation to be recorded"

        process_addopts = os.environ.get("PYTEST_ADDOPTS", "")
        assert "-q" in process_addopts
>       _assert_plugins_in_addopts(process_addopts)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:340:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

addopts = '-q'

    def _assert_plugins_in_addopts(addopts: str) -> None:
        """Assert pytest-cov and pytest-bdd plugins are present in addopts."""

        normalized = addopts.strip()
        assert normalized, "PYTEST_ADDOPTS should not be empty when plugins are
injected"
>       assert (
            "-p pytest_cov" in normalized or "-ppytest_cov" in normalized
        ), f"pytest-cov plugin missing: {normalized}"
E       AssertionError: pytest-cov plugin missing: -q
E       assert ('-p pytest_cov' in '-q' or '-ppytest_cov' in '-q')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:28: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,899 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:13,899 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:13,899 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-28 10:31:13,900 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_____________ test_run_tests_option_wiring_includes_expected_flags _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f868470>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_option_wiring_i0')

    @pytest.mark.fast
    def test_run_tests_option_wiring_includes_expected_flags(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-PYTEST-OPTS-1 — Command wiring emits coverage/report
args."""

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_opts.py"
        test_file.write_text("def test_opts():\n    assert True\n")

        class FakeDT:
            @staticmethod
            def now(tz=None) -> SimpleNamespace:  # type: ignore[no-untyped-def]
                return SimpleNamespace(
                    isoformat=lambda: "2025-01-02T00:00:00",
                    strftime=lambda fmt: "20250102_000000",
                )

        monkeypatch.setattr(rt, "datetime", FakeDT)

        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None,
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0, stdout=f"{test_file}::test_opts", stderr=""
            )

        recorded: list[list[str]] = []

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                recorded.append(list(cmd))
                self.returncode = 0
                self._stdout = "opts"
                self._stderr = ""

            def communicate(self):
                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=True,
            report=True,
            parallel=False,
            segment=False,
            maxfail=3,
            extra_marker=None,
        )

        assert success is True
        assert "opts" in output  # The main output should contain "opts"
        assert recorded, "Expected pytest command to be recorded"

        cmd = recorded[0]
>       assert "--maxfail=3" in cmd
E       AssertionError: assert '--maxfail=3' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lbs...-1440/test_run_tests_option_wiring_i0/test_opts.p
y::test_opts', '-m', 'not memory_intensive and fast and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:412: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,917 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:13,918 - devsynth.testing.run_tests - WARNING - Skipping
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph
publication: Coverage JSON missing at test_reports/coverage.json
_______________ test_cli_marker_expression_includes_extra_marker _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9e86b0>

    @pytest.mark.fast
    def test_cli_marker_expression_includes_extra_marker(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-1 — CLI marker flag augments default
filters."""

        commands: list[list[str]] = []

        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                commands.append(cmd)
                self.returncode = 0

            def communicate(self) -> tuple[str, str]:
                return ("collected 1 item", "")

        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="custom_marker",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_cli_marker_expression_includes_extra_marker.<locals>.DummyProcess.__init__(
) got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:13,952 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
__________________ test_cli_failure_surfaces_actionable_tips ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9eac30>

    @pytest.mark.fast
    def test_cli_failure_surfaces_actionable_tips(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ERROR-1 — CLI surfaces troubleshooting tips."""

        commands: list[list[str]] = []

        class FailingProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                commands.append(cmd)
                raise RuntimeError("simulated failure")

        monkeypatch.setattr(rt.subprocess, "Popen", FailingProcess)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:107:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_cli_failure_surfaces_actionable_tips.<locals>.FailingProcess.__init__() got
an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,053 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
________________ test_cli_segment_failure_emits_aggregate_tips _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa84830>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_segment_failure_emits0')

    @pytest.mark.fast
    def test_cli_segment_failure_emits_aggregate_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-SEGMENT-2 — failing batch surfaces aggregate
tips."""

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_segmented.py"
        test_file.write_text(
            "def test_batch_one():\n    assert True\n\n"
            "def test_batch_two():\n    assert True\n"
        )

        node_ids = [
            f"{test_file}::test_batch_one",
            f"{test_file}::test_batch_two",
        ]

        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            return SimpleNamespace(stdout="\n".join(node_ids), stderr="",
returncode=0)

        batch_commands: list[list[str]] = []
        responses = [
            ("batch one ok", "", 0),
            ("batch two fail", "collected errors", 2),
        ]
        tips_record: list[tuple[int, tuple[str, ...], str]] = []

        def fake_failure_tips(returncode: int, cmd: list[str]) -> str:
            tip = f"[tip {returncode} #{len(tips_record)}]"
            tips_record.append((returncode, tuple(cmd), tip))
            return tip

        call_index = {"value": 0}

        class DummyBatchProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                idx = call_index["value"]
                batch_commands.append(cmd)
                stdout_payload, stderr_payload, returncode = responses[idx]
                self._stdout = stdout_payload
                self._stderr = stderr_payload
                self.returncode = returncode
                call_index["value"] += 1

            def communicate(self) -> tuple[str, str]:
                return (self._stdout, self._stderr)

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyBatchProcess)
        monkeypatch.setattr(rt, "_failure_tips", fake_failure_tips)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
        )

        assert success is False
        assert len(batch_commands) == 2, "Segmented execution should spawn two
batches"
        assert tips_record, "Expected failure tips to be generated"

        failing_tip = tips_record[0]
>       aggregate_tip = tips_record[1]
                        ^^^^^^^^^^^^^^
E       IndexError: list index out of range

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:281: IndexError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,155 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:14,156 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-28 10:31:14,156 - devsynth.testing.run_tests - INFO - Running segment
1/2 (1 tests)
2025-10-28 10:31:14,156 - devsynth.testing.run_tests - INFO - Running segment
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1
tests)
__________________ test_cli_marker_filters_merge_extra_marker __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ff15ee0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_marker_filters_merge_0')

    @pytest.mark.fast
    def test_cli_marker_filters_merge_extra_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-3 — speed markers combine with extra
filter."""

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_filters.py"
        test_file.write_text("def test_placeholder():\n    assert True\n")

        collect_invocations: list[list[str]] = []

        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            collect_invocations.append(cmd)
            return SimpleNamespace(
                stdout=f"{test_file}::test_placeholder\n", stderr="",
returncode=0
            )

        run_commands: list[list[str]] = []

        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                run_commands.append(cmd)
                self.returncode = 0

            def communicate(self) -> tuple[str, str]:
                return ("ok", "")

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast", "slow"],
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="custom_marker",
        )

        assert success is True
        assert run_commands, "Expected run commands for each speed"
        assert collect_invocations, "Expected collection to occur"
        assert output.count("ok") == len(run_commands)

        collect_strings = [" ".join(cmd) for cmd in collect_invocations]
>       assert any(
            "(fast and not memory_intensive) and (custom_marker)" in cmd
            for cmd in collect_strings
        )
E       assert False
E        +  where False = any(<generator object
test_cli_marker_filters_merge_extra_marker.<locals>.<genexpr> at 0x13d8e1560>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:437: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,193 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:14,194 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (slow) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (slow) — collecting via pytest
___________________ test_cli_report_mode_adds_html_argument ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa87c20>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_report_mode_adds_html0')

    @pytest.mark.fast
    def test_cli_report_mode_adds_html_argument(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-REPORT-1 — report flag appends HTML output
options."""

        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        test_file = tmp_path / "test_report.py"
        test_file.write_text("def test_report():\n    assert True\n")

        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            return SimpleNamespace(
                stdout=f"{test_file.name}::test_report\n", stderr="",
returncode=0
            )

        run_commands: list[list[str]] = []

        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                run_commands.append(cmd)
                self.returncode = 0

            def communicate(self) -> tuple[str, str]:
                return ("report ok", "")

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=True,
            parallel=False,
        )

        assert success is True
        assert "report ok" in output
        assert run_commands, "Expected report-mode command to run"

        html_args = [arg for arg in run_commands[0] if
arg.startswith("--html=")]
>       assert html_args, run_commands[0]
E       AssertionError:
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 'test_report.py::test_report', '-m', 'not memory_intensive and (fast
or medium) and not gui', ...]
E       assert []

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:508: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,214 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:14,216 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (medium) — collecting via pytest
2025-10-28 10:31:14,219 - devsynth.testing.run_tests - WARNING - Skipping
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (medium) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph
publication: Coverage JSON missing at test_reports/coverage.json
___________ test_cli_keyword_filter_returns_success_when_no_matches ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9e8b60>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_cli_keyword_filter_return0')

    @pytest.mark.fast
    def test_cli_keyword_filter_returns_success_when_no_matches(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-4 — keyword fallback exits cleanly when
empty."""

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))

        lifecycle: list[str] = []

        def fake_reset() -> None:
            lifecycle.append("reset")

        def fake_ensure() -> None:
            lifecycle.append("ensure")

        monkeypatch.setattr(rt, "_reset_coverage_artifacts", fake_reset)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", fake_ensure)

        collect_commands: list[list[str]] = []

        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            collect_commands.append(cmd)
            return SimpleNamespace(stdout="", stderr="", returncode=0)

        def fail_popen(*args, **kwargs):
            raise AssertionError("Popen should not be invoked when no tests
match")

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", fail_popen)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="requires_resource('lmstudio')",
        )

        assert collect_commands, "Expected keyword-filter collection to execute"
        collect_tokens = " ".join(collect_commands[0])
        assert "-k lmstudio" in collect_tokens

>       assert success is True
E       assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:722: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,282 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:14,283 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (medium) — collecting via pytest
2025-10-28 10:31:14,283 - devsynth.testing.run_tests - INFO - marker fallback
triggered for target=unit-tests (speeds=fast,medium)
2025-10-28 10:31:14,283 - devsynth.testing.run_tests - ERROR - Failed to run
tests: Popen should not be invoked when no tests match
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (medium) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback
triggered for target=unit-tests (speeds=fast,medium)
ERROR    devsynth.testing.run_tests:logging_setup.py:615 Failed to run tests:
Popen should not be invoked when no tests match
____________ test_run_tests_generates_artifacts_for_normal_profile _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2ffb0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_generates_artif0')

    @pytest.mark.fast
    def test_run_tests_generates_artifacts_for_normal_profile(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Normal run writes `.coverage`, JSON, and HTML artifacts via the
harness."""

        monkeypatch.chdir(tmp_path)

        coverage_json = tmp_path / "reports" / "coverage.json"
        html_dir = tmp_path / "htmlcov"
        monkeypatch.setattr(rt, "COVERAGE_JSON_PATH", coverage_json)
        monkeypatch.setattr(rt, "COVERAGE_HTML_DIR", html_dir)

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests/unit")
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", "tests")

        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda *_:
["tests/unit/test_ok.py::test_one"]
        )

        lifecycle: list[str] = []
        monkeypatch.setattr(
            rt, "_reset_coverage_artifacts", lambda: lifecycle.append("reset")
        )
        monkeypatch.setattr(
            rt, "_ensure_coverage_artifacts", lambda: lifecycle.append("ensure")
        )

        popen_envs: list[dict[str, str]] = []

        def fake_single_batch(
            config: rt.SingleBatchRequest,
        ) -> rt.BatchExecutionResult:
            popen_envs.append(dict(config.env))
            tmp_path.joinpath(".coverage").write_text("data")
            html_dir.mkdir(parents=True, exist_ok=True)
            (html_dir / "index.html").write_text("<html>ok</html>")
            coverage_json.parent.mkdir(parents=True, exist_ok=True)
            coverage_json.write_text(json.dumps({"totals": {"percent_covered":
98.7}}))
            return True, "batch ok", build_batch_metadata("batch-cli-artifacts")

        monkeypatch.setattr(rt, "_run_single_test_batch", fake_single_batch)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=True,
            parallel=False,
        )

        assert success is True
>       assert output == "batch ok"
E       AssertionError: assert 'batch ok\n[k...hold=70.00%\n' == 'batch ok'
E
E         - batch ok
E         + batch ok
E         ?         +
E         + [knowledge-graph] coverage gate pass → QualityGate
729ee718-77a5-49e9-81a0-32dfd134940e (new), TestRun
1118da7d-6f32-4b74-818f-9e1279ac09fc (new), Evidence
[37a72613-cc94-4a4c-94fd-efc628783f70 (new),
b9e15367-fc8b-4c20-b1c7-516943d2ca6e (new)] via networkx; coverage=98.70%
threshold=70.00%

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:779: AssertionError
__________ test_run_tests_generates_artifacts_with_autoload_disabled ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2d0d0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_generates_artif1')

    @pytest.mark.fast
    def test_run_tests_generates_artifacts_with_autoload_disabled(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Smoke-style environments still create coverage artifacts with plugin
injection."""

        monkeypatch.chdir(tmp_path)

        coverage_json = tmp_path / "reports" / "coverage.json"
        html_dir = tmp_path / "htmlcov"
        monkeypatch.setattr(rt, "COVERAGE_JSON_PATH", coverage_json)
        monkeypatch.setattr(rt, "COVERAGE_HTML_DIR", html_dir)

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests/unit")
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", "tests")

        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda *_:
["tests/unit/test_ok.py::test_one"]
        )

        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        captured_envs: list[dict[str, str]] = []

        def fake_single_batch(
            config: rt.SingleBatchRequest,
        ) -> rt.BatchExecutionResult:
            captured_envs.append(dict(config.env))
            tmp_path.joinpath(".coverage").write_text("data")
            html_dir.mkdir(parents=True, exist_ok=True)
            (html_dir / "index.html").write_text("<html>smoke</html>")
            coverage_json.parent.mkdir(parents=True, exist_ok=True)
            coverage_json.write_text(json.dumps({"totals": {"percent_covered":
94.2}}))
            return True, "smoke ok", build_batch_metadata("batch-cli-smoke")

        monkeypatch.setattr(rt, "_run_single_test_batch", fake_single_batch)

        env = {"PYTEST_DISABLE_PLUGIN_AUTOLOAD": "1"}
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=True,
            report=True,
            parallel=True,
            env=env,
        )

        assert success is True
>       assert output == "smoke ok"
E       AssertionError: assert 'smoke ok\n[k...hold=70.00%\n' == 'smoke ok'
E
E         - smoke ok
E         + smoke ok
E         ?         +
E         + [knowledge-graph] coverage gate pass → QualityGate
4a4f8584-25f9-4660-85d0-b51414df6edd (new), TestRun
cabe22f9-81fd-4a76-b342-04a40abfe829 (new), Evidence
[44869652-3b48-45af-ad48-e603cf57fb05 (new),
969a6e90-5d02-4ca6-a15b-41a76cdae7b7 (new)] via networkx; coverage=94.20%
threshold=70.00%

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:838: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,324 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:14,324 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:14,324 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
_________ test_ensure_coverage_artifacts_skips_when_module_unavailable _________

coverage_test_environment =
namespace(coverage_json=PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd2
3rh0000gn/T/pytest-of-caitlyn/pytest-1...olders/2v/lbss3by10y51bg7c07nd23rh0000g
n/T/pytest-of-caitlyn/pytest-1440/test_ensure_coverage_artifacts4/legacy/html'))
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12faf5910>
caplog = <_pytest.logging.LogCaptureFixture object at 0x12fa710d0>

    @pytest.mark.fast
    def test_ensure_coverage_artifacts_skips_when_module_unavailable(
        coverage_test_environment: SimpleNamespace,
        monkeypatch: pytest.MonkeyPatch,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """ReqID: COV-ART-02A — Missing coverage module logs debug message and
exits."""

        monkeypatch.delitem(sys.modules, "coverage", raising=False)

        original_import = builtins.__import__

        def fake_import(name, *args, **kwargs):  # noqa: ANN001
            if name == "coverage":
                raise ImportError("coverage module unavailable")
            return original_import(name, *args, **kwargs)

        monkeypatch.setattr(builtins, "__import__", fake_import)

        with caplog.at_level(logging.DEBUG,
logger="devsynth.testing.run_tests"):
            rt._ensure_coverage_artifacts()

>       assert any("coverage library unavailable" in message for message in
caplog.messages)
E       assert False
E        +  where False = any(<generator object
test_ensure_coverage_artifacts_skips_when_module_unavailable.<locals>.<genexpr>
at 0x12f2eb850>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_coverage_artifacts.py:218: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,407 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
________________ test_keyword_filter_no_matches_returns_success ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcfe8d0>

    @pytest.mark.fast
    def test_keyword_filter_no_matches_returns_success(monkeypatch) -> None:
        """ReqID: RT-01 — keyword filter with no matches returns success and
message."""

        # Simulate collect-only returning no matching node ids under keyword
filter
        def fake_run(
            cmd,  # noqa: ANN001
            check: bool = False,
            capture_output: bool = False,
            text: bool = False,
        ):  # type: ignore[no-redef]
            class R:
                def __init__(self) -> None:
                    self.returncode = 0
                    # nothing that matches the node id regex
                    self.stdout = "\n"
                    self.stderr = ""

            return R()

        # Ensure Popen is not invoked if there are no node ids
        def fail_popen(*args, **kwargs):  # type: ignore[no-redef]
            pytest.fail("Popen should not be called when no node ids match")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", fail_popen)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_keyword_filter_no_matches_returns_success.<locals>.fake_run() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,549 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_________________ test_failure_tips_appended_on_nonzero_return _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13d983710>

    @pytest.mark.fast
    def test_failure_tips_appended_on_nonzero_return(monkeypatch) -> None:
        """ReqID: RT-02 — non-zero exit appends troubleshooting tips."""

        # Make the simple '-m not memory_intensive' path run and return a
non-zero code
        class DummyProc:
            def __init__(self) -> None:
                self.returncode = 2

            def communicate(self) -> tuple[str, str]:
                return ("", "boom")

        def fake_popen(
            args,  # noqa: ANN001
            stdout=None,  # noqa: ANN001
            stderr=None,  # noqa: ANN001
            text=None,  # noqa: ANN001
            env=None,  # noqa: ANN001
        ):  # type: ignore[no-redef]
            return DummyProc()

        monkeypatch.setattr(rt.subprocess, "Popen", fake_popen)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_failure_tips_appended_on_nonzero_return.<locals>.fake_popen() got an
unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,606 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
___________ test_keyword_filter_lmstudio_no_matches_returns_success ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2db20>

    @pytest.mark.fast
    def test_keyword_filter_lmstudio_no_matches_returns_success(monkeypatch):
        """ReqID: TR-RT-08 — Keyword 'lmstudio' no-match returns success.

        When extra_marker includes requires_resource('lmstudio') and collection
        yields no matching node ids, run_tests should return success=True with a
        helpful message without attempting to execute pytest on any node ids.
        """

        class DummyRunResult:
            def __init__(self):
                self.stdout = ""  # no node ids collected
                self.stderr = ""
                self.returncode = 0

        calls = {
            "run": [],
            "popen": [],
        }

        def fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            calls["run"].append(cmd)
            return DummyRunResult()

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-redef]
                calls["popen"].append(cmd)
                # This path should not be reached because no node ids => early
return
                self._stdout = ""
                self._stderr = ""
                self.returncode = 0

            def communicate(self):
                return self._stdout, self._stderr

        import subprocess

        monkeypatch.setattr(subprocess, "run", fake_run)
        monkeypatch.setattr(subprocess, "Popen", FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker.py:50:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_keyword_filter_lmstudio_no_matches_returns_success.<locals>.fake_run() got
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,714 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
__________________ test_extra_marker_merges_into_m_expression __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2fb60>

    @pytest.mark.fast
    def test_extra_marker_merges_into_m_expression(monkeypatch):
        """ReqID: TR-RT-09 — Non-keyword extra_marker merges into -m
expression."""
        """
        When extra_marker does not invoke the keyword fallback, ensure it is
merged
        into the -m expression and subprocess.Popen is called once; run_tests
returns
        success if the process returncode is 0.
        """

        class FakePopen:
            last_cmd = None

            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-redef]
                FakePopen.last_cmd = cmd
                self.returncode = 0
                self._stdout = "pytest ok\n"
                self._stderr = ""

            def communicate(self):
                return self._stdout, self._stderr

        import subprocess

        def fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            # Not used in this path; ensure it's not called unnecessarily
            raise AssertionError("subprocess.run should not be called for
non-keyword path")

        monkeypatch.setattr(subprocess, "Popen", FakePopen)
        monkeypatch.setattr(subprocess, "run", fake_run)

        extra = "slow or (requires_resource('codebase'))"
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=extra,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker.py:112:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_extra_marker_merges_into_m_expression.<locals>.fake_run() got an unexpected
keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,788 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_________ test_run_tests_merges_extra_marker_into_category_expression __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2db50>

    @pytest.mark.fast
    def
test_run_tests_merges_extra_marker_into_category_expression(monkeypatch):
        """ReqID: TR-RT-09 — Merge extra_marker into -m expression.

        When speed_categories is None and extra_marker is a normal expression
        (not a requires_resource('lmstudio') keyword case), run_tests should
pass
        a combined -m expression that includes both the base filter and the
extra
        marker.
        """
        import devsynth.testing.run_tests as rt

        captured_cmd = {}

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # The command should include '-m' with the merged expression
                assert "-m" in cmd, f"-m missing in: {cmd}"
                # There are two '-m' flags: Python module and pytest marker; use
the last
                idx = len(cmd) - 1 - cmd[::-1].index("-m")
                expr = cmd[idx + 1]
                assert "not memory_intensive" in expr
                assert "(not slow)" in expr or "not slow" in expr
                captured_cmd["cmd"] = cmd
                self.returncode = 0

            def communicate(self):
                return ("ok\n", "")

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="not slow",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker_passthrough.py:38:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_run_tests_merges_extra_marker_into_category_expression.<locals>.FakePopen._
_init__() got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:14,849 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_______________ test_collect_fallback_on_behavior_speed_no_tests _______________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_fallback_on_behav0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2c740>

    @pytest.mark.fast
    def test_collect_fallback_on_behavior_speed_no_tests(tmp_path, monkeypatch):
        """When behavior-tests with a speed filter yields no tests, fallback to
marker_expr.

        We simulate the preliminary check returning a 'no tests ran' message and
assert that
        the second collection call (fallback) is invoked and its items are
returned.
        ReqID: C3 (coverage of fallback branch)
        """
        # Arrange
        from devsynth.testing import run_tests as rt

        # Redirect cache dir and target path to isolated temp locations
        cache_dir = tmp_path / ".cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "behavior-tests", str(tmp_path))
        # Create a real file that the pruning check will accept
        real_file = tmp_path / "test_example.py"
        real_file.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert
True\n"
        )

        # Prepare fake subprocess.run that returns two different responses:
        calls = {"invocations": []}

        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str =
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls["invocations"].append(cmd)
            # First invocation is the preliminary check (with same
category_expr)
            if len(calls["invocations"]) == 1:
                return FakeCompleted(stdout="no tests ran\n", returncode=0)
            # Second invocation should be the fallback collection using
marker_expr only
            # Return a couple of node ids
            return FakeCompleted(stdout=f"{real_file}::test_ok\n")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Act
>       out = rt.collect_tests_with_cache(target="behavior-tests",
speed_category="fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'behavior-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___________________ test_collect_malformed_cache_regenerates ___________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_malformed_cache_r0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2d340>

    @pytest.mark.fast
    def test_collect_malformed_cache_regenerates(tmp_path, monkeypatch):
        """Malformed JSON cache should be ignored and collection regenerated.

        ReqID: C3 (coverage of malformed cache read path)
        """
        from devsynth.testing import run_tests as rt

        # Point cache dir and target path
        cache_dir = tmp_path / ".cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        # Create a real file for existence pruning
        real2 = tmp_path / "test_a.py"
        real2.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_b():\n    assert
True\n"
        )

        # Write malformed JSON to the expected cache file for key all-tests_all
        cache_file = cache_dir / "all-tests_all_tests.json"
        cache_file.write_text("{ not-json }")

        # Fake subprocess.run to return one id
        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str =
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr

        monkeypatch.setattr(
            rt.subprocess,
            "run",
            lambda *a, **k: FakeCompleted(stdout=f"{real2}::test_b\n"),
        )

>       out = rt.collect_tests_with_cache(target="all-tests",
speed_category=None)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'all-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
__________ test_run_tests_lmstudio_extra_marker_keyword_early_success __________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_lmstudio_extra_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13dc2e6c0>

    @pytest.mark.fast
    def test_run_tests_lmstudio_extra_marker_keyword_early_success(tmp_path,
monkeypatch):
        """With extra_marker requires_resource('lmstudio') and no matches,
run_tests should
        perform keyword-based collection and return success with a friendly
message.

        ReqID: C3 (coverage of extra_marker keyword path with early success)
        """
        from devsynth.testing import run_tests as rt

        # Point the unit-tests target to an isolated path (not used directly
here)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))

        # Fake subprocess.run for the collect-only path to return no matching
node IDs
        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str =
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            # Simulate collect-only returning nothing for '-k lmstudio'
            return FakeCompleted(stdout="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            parallel=False,
            extra_marker="requires_resource('lmstudio')",
        )

        assert success is True
>       assert "No tests matched" in output
E       AssertionError: assert 'No tests matched' in 'Marker fallback
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0
_______________\n\n============================ no tests ran in 0.30s
=============================\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:144: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:15,016 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:15,017 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (medium) — collecting via pytest
2025-10-28 10:31:15,018 - devsynth.testing.run_tests - INFO - marker fallback
triggered for target=unit-tests (speeds=fast,medium)
2025-10-28 10:31:16,711 - devsynth.testing.run_tests - INFO - Coverage data file
detected at .coverage (53248 bytes)
2025-10-28 10:31:16,718 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: no measured files present
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (medium) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback
triggered for target=unit-tests (speeds=fast,medium)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage data file
detected at .coverage (53248 bytes)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: no measured files present
____________________ test_failure_tips_include_common_flags ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_failure_tips_include_comm0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12faf64e0>

    @pytest.mark.fast
    def test_failure_tips_include_common_flags(tmp_path, monkeypatch):
        """ReqID: FR-59 Ensure failure tips include common flags examples.

        We create a minimal failing test and assert that the output contains
        actionable hints for --smoke, --segment/--segment-size, --maxfail,
        --no-parallel, and --report.
        """
        test_file = tmp_path / "test_fails.py"
        test_file.write_text(
            """
    import pytest

    @pytest.mark.fast
    def test_will_fail():
        assert False
    """
        )
        # Point unit-tests target to our tmp_path
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))

        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        def fake_collect(cmd, check=False, capture_output=True, text=True):  #
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_will_fail",
                stderr="",
            )

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                self.cmd = list(cmd)
                self.returncode = 1
                self._stdout = ""
                self._stderr = "FAIL Required test coverage of 90% not reached."

            def communicate(self):  # noqa: D401 - mimic subprocess signature
                """Return predetermined stdout/stderr."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests("unit-tests", ["fast"], parallel=False)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_failure_tips.py:58:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_failure_tips_include_common_flags.<locals>.fake_collect() got an unexpected
keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:16,740 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
____________ test_keyword_filter_no_matches_returns_success_message ____________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_keyword_filter_no_matches1')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f139190>

    @pytest.mark.fast
    def test_keyword_filter_no_matches_returns_success_message(tmp_path,
monkeypatch):
        """ReqID: FR-11.2 — Keyword filter path returns success when no matches.

        Triggers the lmstudio keyword path and expects a friendly message.
        """
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        def fake_collect(cmd, check=False, capture_output=True, text=True):  #
noqa: ANN001
            assert "--collect-only" in cmd
            assert "-k" in cmd and "lmstudio" in cmd
            return SimpleNamespace(returncode=0, stdout="", stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)

        # Use a very specific marker expression to trigger keyword path.
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter.py:29:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_keyword_filter_no_matches_returns_success_message.<locals>.fake_collect()
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:16,798 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
________ test_keyword_filter_honors_report_flag_and_creates_report_dir _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db401a0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_keyword_filter_honors_rep0')

    @pytest.mark.fast
    def test_keyword_filter_honors_report_flag_and_creates_report_dir(
        monkeypatch, tmp_path
    ):
        """ReqID: FR-11.2 — Report flag creates deterministic report directory.

        Use keyword path with report=True and patch datetime to assert directory
path.
        """

        class FakeDT:
            @staticmethod
            def now():
                # Fixed timestamp for stable directory path
                class _DT:
                    def strftime(self, fmt: str) -> str:
                        return "20250101_000000"

                return _DT()

        monkeypatch.setattr(rt, "datetime", FakeDT)

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        test_file = tmp_path / "test_lmstudio.py"
        test_file.write_text("def test_placeholder():\n    assert True\n")

        def fake_collect(cmd, check=False, capture_output=True, text=True):  #
noqa: ANN001
            assert "--collect-only" in cmd
            assert "-k" in cmd and "lmstudio" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_placeholder",
                stderr="",
            )

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                self.cmd = list(cmd)
                self.returncode = 0
                self._stdout = "ok"
                self._stderr = ""

            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=True,
            parallel=False,
            segment=False,
            extra_marker='requires_resource("lmstudio")',
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter.py:97:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_keyword_filter_honors_report_flag_and_creates_report_dir.<locals>.fake_coll
ect() got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:16,870 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
____ test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fa71070>

    @pytest.mark.fast
    def
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success(monkeypat
ch):
        """
        ReqID: C3-05
        When extra_marker requires_resource('lmstudio') is provided and the
keyword-filtered
        collection yields no tests, run_tests should short-circuit and return
success=True
        with a friendly message instead of attempting to invoke pytest with
empty args.
        """

        # Simulate `pytest --collect-only -q -m <expr> -k lmstudio` returning no
node IDs.
        class DummyCompleted:
            def __init__(self, stdout: str = "", returncode: int = 0):
                self.stdout = stdout
                self.stderr = ""
                self.returncode = returncode

        def fake_run(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
        ):  # type: ignore[no-redef]
            # Ensure we're calling a python -m pytest command with
'--collect-only'
            assert cmd[:3] == [sys.executable, "-m", "pytest"], cmd
            assert "--collect-only" in cmd
            # Return empty stdout to indicate no matched tests
            return DummyCompleted(stdout="", returncode=0)

        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")  # keep
hermetic and fast
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")

        monkeypatch.setattr("subprocess.run", fake_run)

        # Call run_tests with an lmstudio resource marker expression
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,  # triggers the single-pass branch
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter_empty.py:42:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success.<locals>.
fake_run() got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:16,939 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:16,940 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:16,940 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-28 10:31:16,941 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
___________________ test_collect_tests_with_cache_uses_cache ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f138380>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_11')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_uses_cache(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-1 — collect_tests_with_cache uses the cache."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 1.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)

        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
>           tests = rt.collect_tests_with_cache("unit-tests", "fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:77:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

        result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
        if result.returncode != 0:
            if result.stderr:
                error_message = f"Test collection failed: {result.stderr}"
            else:
                error_message = f"Test collection failed with exit code
{result.returncode}"

            # Log more details for debugging
            logger.warning(
                error_message,
                extra={
                    "event": "test_collection_failed",
                    "target": target,
                    "returncode": result.returncode,
                    "stdout": (
                        result.stdout[:500] if result.stdout else None
                    ),  # First 500 chars
                    "stderr": (
                        result.stderr[:500] if result.stderr else None
                    ),  # First 500 chars
                    "speed_category": category_expr,
                },
            )
>           raise RuntimeError(error_message)
E           RuntimeError: Test collection failed: <MagicMock name='run().stderr'
id='5092400624'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1367: RuntimeError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - WARNING - Test collection
failed: <MagicMock name='run().stderr' id='5092400624'>
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection failed:
<MagicMock name='run().stderr' id='5092400624'>
____________ test_collect_tests_with_cache_regenerates_when_expired ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12faf4dd0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_12')

    @typed_freeze_time("2025-01-02")
    @pytest.mark.fast
    def test_collect_tests_with_cache_regenerates_when_expired(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-2 — collect_tests_with_cache regenerates when
expired."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 0.5,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)

        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            tests = rt.collect_tests_with_cache("unit-tests", "fast")

>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E
E         Right contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_with_cache_12/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:118: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-01-01 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
______________________ test_collect_tests_with_cache_miss ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12faf6d80>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_13')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_miss(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-3 — collect_tests_with_cache handles a cache
miss."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")

        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "test_file.py") + "\n",
                returncode=0,
                stderr="",
            )
            tests = rt.collect_tests_with_cache("unit-tests", "fast")

>       assert tests == [os.path.join(tmp_path, "test_file.py")]
E       AssertionError: assert [] == ['/private/va...test_file.py']
E
E         Right contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_with_cache_13/test_file.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:143: AssertionError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
______________ test_collect_tests_with_cache_invalidated_by_mtime ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f112e40>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_14')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_invalidated_by_mtime(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-4 — collect_tests_with_cache is invalidated by
mtime."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 0.5,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)

        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            with freeze_time("2025-01-02"):
                tests = rt.collect_tests_with_cache("unit-tests", "fast")

>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E
E         Right contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_with_cache_14/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:183: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-01-01 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_____________ test_collect_tests_with_cache_invalidated_by_marker ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1126c0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_15')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_invalidated_by_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-5 — collect_tests_with_cache is invalidated by
marker."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 1.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)

        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            with freeze_time("2025-01-01"):
                tests = rt.collect_tests_with_cache("unit-tests", "slow")

>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E
E         Right contains one more item:
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_collect_tests_with_cache_15/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:223: AssertionError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (slow) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (slow) — collecting via pytest
______________________ test_run_tests_verbose_and_report _______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_verbose_and_rep0')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6995e0>

    @pytest.mark.fast
    def test_run_tests_verbose_and_report(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with verbose and report flags."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
        monkeypatch.setattr(
            rt, "_ensure_coverage_artifacts", lambda: None
        )  # Prevent actual artifact generation
        monkeypatch.setattr(
            rt, "enforce_coverage_threshold", lambda *args, **kwargs: 90.0
        )  # Mock coverage enforcement
        custom_env = {"PYTEST_DISABLE_PLUGIN_AUTOLOAD": "1"}  # Enable plugin
injection
        success, output = rt.run_tests(
            target="unit-tests", verbose=True, report=True, env=custom_env
        )

        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "-v" in cmd
        assert f"--cov={rt.COVERAGE_TARGET}" in cmd
        assert f"--cov-report=json:{rt.COVERAGE_JSON_PATH}" in cmd
        assert f"--cov-report=html:{rt.COVERAGE_HTML_DIR}" in cmd
        assert "--cov-append" in cmd
        assert "PYTEST_ADDOPTS" in env  # Should contain plugin injection
>       del os.environ["PYTEST_DISABLE_PLUGIN_AUTOLOAD"]  # Clean up env
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:228:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = environ({'LOCAL_SHARE': '/Users/caitlyn/.local/share', 'PWD':
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth',
...n/T/pytest-of-caitlyn/pytest-1440/test_run_tests_verbose_and_rep0/home/.local
/share', 'DEVSYNTH_NO_FILE_LOGGING': '1'})
key = 'PYTEST_DISABLE_PLUGIN_AUTOLOAD'

>   ???
E   KeyError: 'PYTEST_DISABLE_PLUGIN_AUTOLOAD'

<frozen os>:730: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:17,787 - devsynth.testing.run_tests - INFO - Injected -p
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-28 10:31:17,787 - devsynth.testing.run_tests - INFO - Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-28 10:31:17,787 - devsynth.testing.run_tests - INFO - Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-28 10:31:17,788 - devsynth.testing.run_tests - WARNING - Skipping
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph
publication: Coverage JSON missing at test_reports/coverage.json
________________ test_run_tests_with_markers_and_keyword_filter ________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_with_markers_an0')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f69af90>

    @pytest.mark.fast
    def test_run_tests_with_markers_and_keyword_filter(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with extra_marker and keyword_filter."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

        success, output = rt.run_tests(
            target="unit-tests", extra_marker="slow", keyword_filter="example"
        )

        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "-m" in cmd
>       assert "not memory_intensive and (slow)" in cmd
E       AssertionError: assert 'not memory_intensive and (slow)' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lbs...markers_an0/test_example.py::test_pass', '-m',
'not memory_intensive and (fast or medium) and (slow) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:257: AssertionError
_______________ test_run_tests_collection_failure_returns_false ________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_collection_fail0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f876810>

    @pytest.mark.fast
    def test_run_tests_collection_failure_returns_false(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests returns False on test collection failure."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))

        def failing_collect(target, speed_category):
            raise RuntimeError("Collection failed")

        monkeypatch.setattr(rt, "collect_tests_with_cache", failing_collect)

        success, output = rt.run_tests(target="unit-tests")

        assert success is False
>       assert output == "Test collection failed"
E       AssertionError: assert 'Collection failed' == 'Test collection failed'
E
E         - Test collection failed
E         ? ^^^^^^
E         + Collection failed
E         ? ^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:341: AssertionError
_________ test_run_tests_no_tests_collected_returns_true_with_message __________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_no_tests_collec0')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74d8b0>

    @pytest.mark.fast
    def test_run_tests_no_tests_collected_returns_true_with_message(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests returns True and message when no tests are
collected."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))

        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda target, speed_category: []
        )

        success, output = rt.run_tests(target="unit-tests")

        assert success is True
>       assert "No tests collected" in output
E       AssertionError: assert 'No tests collected' in 'Marker fallback
executed.\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:362: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:17,891 - devsynth.testing.run_tests - INFO - marker fallback
triggered for target=unit-tests (speeds=fast,medium)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback
triggered for target=unit-tests (speeds=fast,medium)
______________________ test_run_tests_segmented_execution ______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_segmented_execu1')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74dc10>

    @pytest.mark.fast
    def test_run_tests_segmented_execution(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with segmented execution."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_a = tmp_path / "test_a.py"
        test_b = tmp_path / "test_b.py"
        test_c = tmp_path / "test_c.py"
        test_a.write_text("def test_a(): pass")
        test_b.write_text("def test_b(): pass")
        test_c.write_text("def test_c(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [
                f"{test_a}::test_a",
                f"{test_b}::test_b",
                f"{test_c}::test_c",
            ],
        )

        # Mock Popen to return success for all batches
        class FakePopenSuccess(rt.subprocess.Popen):
            def __init__(self, *args: Any, **kwargs: Any) -> None:
                # Append to the fixture's recorded_calls
                mock_subprocess_popen.append((args[0], kwargs.get("env", {})))
                super().__init__(*args, **kwargs)
                self.returncode = 0
                self._stdout = "ok"
                self._stderr = ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopenSuccess)

        success, output = rt.run_tests(target="unit-tests", segment=True,
segment_size=1)

        assert success is True
>       assert len(mock_subprocess_popen) == 3  # Three batches
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 6 == 3
E        +  where 6 =
len([(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:439: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:17,929 - devsynth.testing.run_tests - INFO - Running 3 tests in
3 segments of size 1 for target=unit-tests
2025-10-28 10:31:17,929 - devsynth.testing.run_tests - INFO - Running segment
1/3 (1 tests)
2025-10-28 10:31:17,930 - devsynth.testing.run_tests - INFO - Running segment
2/3 (1 tests)
2025-10-28 10:31:17,930 - devsynth.testing.run_tests - INFO - Running segment
3/3 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 3 tests in 3
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/3 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/3 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 3/3 (1
tests)
_______________ test_run_tests_segmented_execution_with_failure ________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_segmented_execu2')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f73af30>

    @pytest.mark.fast
    def test_run_tests_segmented_execution_with_failure(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with segmented execution where one batch fails."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_a = tmp_path / "test_a.py"
        test_b = tmp_path / "test_b.py"
        test_a.write_text("def test_a(): pass")
        test_b.write_text("def test_b(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [
                f"{test_a}::test_a",
                f"{test_b}::test_b",
            ],
        )

        # Mock Popen to simulate failure in the first batch
        class FakePopenMixed(rt.subprocess.Popen):
            call_count = 0

            def __init__(self, *args: Any, **kwargs: Any) -> None:
                # Append to the fixture's recorded_calls
                mock_subprocess_popen.append((args[0], kwargs.get("env", {})))
                super().__init__(*args, **kwargs)
                FakePopenMixed.call_count += 1
                if FakePopenMixed.call_count == 1:
                    self.returncode = 1  # Fail first batch
                    self._stdout = "fail"
                    self._stderr = "error"
                else:
                    self.returncode = 0
                    self._stdout = "ok"
                    self._stderr = ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopenMixed)

        success, output = rt.run_tests(target="unit-tests", segment=True,
segment_size=1)

        assert success is False
        assert "Troubleshooting tips" in output  # Should include failure tips
>       assert len(mock_subprocess_popen) == 2  # Both batches should run
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 4 == 2
E        +  where 4 =
len([(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:492: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:17,961 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-28 10:31:17,961 - devsynth.testing.run_tests - INFO - Running segment
1/2 (1 tests)
2025-10-28 10:31:17,961 - devsynth.testing.run_tests - INFO - Running segment
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1
tests)
______________________ test_run_tests_parallel_execution _______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_parallel_execut0')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f739eb0>

    @pytest.mark.fast
    def test_run_tests_parallel_execution(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with parallel execution."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

        success, output = rt.run_tests(target="unit-tests", parallel=True)

        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
>       assert "-n auto" in cmd  # Should add parallel flag
        ^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert '-n auto' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lbs...tests_parallel_execut0/test_example.py::test_pass
', '-m', 'not memory_intensive and (fast or medium) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:520: AssertionError
____________ test_run_tests_parallel_execution_disabled_by_segment _____________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_parallel_execut1')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f738830>

    @pytest.mark.fast
    def test_run_tests_parallel_execution_disabled_by_segment(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test parallel execution is disabled when segment is True."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

        success, output = rt.run_tests(target="unit-tests", parallel=True,
segment=True)

        assert success is True
        assert len(mock_subprocess_popen) == 1  # Only one batch due to
segment=True
        cmd, env = mock_subprocess_popen[0]
        assert "-n auto" not in cmd  # Parallel should be disabled
>       assert [f"{test_file}::test_pass"] == cmd[
            3:
        ]  # Check if node_ids are correctly passed
E       AssertionError: assert ['/private/va...y::test_pass'] ==
['/private/va...htmlcov', ...]
E
E         Right contains 8 more items, first extra item: '-m'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:548: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:17,999 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 50 for target=unit-tests
2025-10-28 10:31:18,000 - devsynth.testing.run_tests - INFO - Running segment
1/1 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1
segments of size 50 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1
tests)
___________________ test_run_tests_with_env_var_propagation ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_with_env_var_pr0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f73bb00>

    @pytest.mark.fast
    def test_run_tests_with_env_var_propagation(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        # Set some initial environment variables
        os.environ["EXISTING_VAR"] = "initial_value"
        os.environ["PYTEST_ADDOPTS"] = "-q"

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

        custom_env = {"NEW_VAR": "new_value", "PYTEST_ADDOPTS":
"--strict-markers"}

>       success, output = rt.run_tests(target="unit-tests", env=custom_env)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:572:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests
    nodes = collect_callable(target, category)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

>       lambda target, speed_category: [f"{test_file}::test_pass"],
                                           ^^^^^^^^^
    )
E   NameError: name 'test_file' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:567: NameError
_____________ test_run_tests_with_empty_speed_categories_uses_all ______________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_with_empty_spee0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f739910>

    @pytest.mark.fast
    def test_run_tests_with_empty_speed_categories_uses_all(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

>       success, output = rt.run_tests(target="unit-tests", speed_categories=[])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:617:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests
    nodes = collect_callable(target, category)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

>       lambda target, speed_category: [f"{test_file}::test_pass"],
                                           ^^^^^^^^^
    )
E   NameError: name 'test_file' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:614: NameError
________________ test_run_tests_with_specific_speed_categories _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_with_specific_s0')
mock_subprocess_run = []
mock_subprocess_popen =
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-darwin-arm64
/bundled/libs/debugpy', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f73a6c0>

    @pytest.mark.fast
    def test_run_tests_with_specific_speed_categories(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test that specific speed_categories are correctly applied."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")

        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )

        success, output = rt.run_tests(
            target="unit-tests", speed_categories=["fast", "medium"]
        )

        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "-m" in cmd
>       assert "not memory_intensive and (fast or medium)" in cmd
E       AssertionError: assert 'not memory_intensive and (fast or medium)' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lbs...tests_with_specific_s0/test_example.py::test_pass
', '-m', 'not memory_intensive and (fast or medium) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:654: AssertionError
__________ test_collect_tests_with_cache_uses_cache_and_respects_ttl ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f795010>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_19')

    @pytest.mark.fast
    def test_collect_tests_with_cache_uses_cache_and_respects_ttl(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-02 — collect_tests_with_cache caches and reuses results
        respecting TTL and fingerprint."""
        # Point TARGET_PATHS to tmp tests dir
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        (tests_dir / "test_sample.py").write_text("def test_ok():\n    assert
True\n")

        # Monkeypatch TARGET_PATHS and subprocess to avoid invoking real pytest
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests":
str(tests_dir)}
        )

        class DummyProc:
            def __init__(self, out: str):
                self.stdout = out
                self.returncode = 0

        def fake_run(
            cmd: list[str], check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            assert "--collect-only" in cmd
            # emulate -q output: one per line
            return DummyProc(out=f"{tests_dir}/test_sample.py\n")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        # Speed None path
>       ids1 = rt.collect_tests_with_cache("unit-tests", speed_category=None)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:76:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_collect_tests_with_cache_uses_cache_and_respects_ttl.<locals>.fake_run()
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,244 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (all) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (all) — collecting via pytest
___________ test_run_tests_translates_args_and_handles_return_codes ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f784860>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_translates_args0')

    @pytest.mark.fast
    def test_run_tests_translates_args_and_handles_return_codes(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-03 — run_tests translates args, treats code 0/5 as
success,
        and omits -n when parallel=False."""
        # Arrange base to avoid plugin interactions and filesystem writes
        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        (tests_dir / "test_x.py").write_text("def test_one():\n    assert
True\n")
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests":
str(tests_dir)}
        )

        # Capture the command built for the run path (no speed_categories
provided)
        captured = {"cmd": None, "env": None}

        class CollectResult:
            def __init__(self, out: str) -> None:
                self.stdout = out
                self.stderr = ""
                self.returncode = 0

        def fake_run(
            cmd: list[str],
            check: bool,
            capture_output: bool,
            text: bool,
        ) -> CollectResult:  # type: ignore[override]
            assert "--collect-only" in cmd
            return CollectResult(f"{tests_dir}/test_x.py::test_one\n")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        class P:
            def __init__(self, cmd: list[str], code: int, out: str = "ok\n",
err: str = ""):
                self.args = cmd
                self._code = code
                self._out = out
                self._err = err

            def communicate(self, *_args: Any, **_kwargs: Any):
                return self._out, self._err

            @property
            def returncode(self) -> int:
                return self._code

            def __enter__(self) -> "P":
                return self

            def __exit__(
                self,
                _exc_type: Any,
                _exc: Any,
                _tb: Any,
            ) -> bool:
                return False

            def kill(self) -> None:  # pragma: no cover - subprocess.run
compatibility
                pass

            def wait(self) -> int:  # pragma: no cover - subprocess.run
compatibility
                return self._code

            def poll(self) -> int:  # pragma: no cover - subprocess.run
compatibility
                return self._code

        def fake_popen(
            cmd: list[str],
            stdout,
            stderr,
            text: bool,
            env: dict[str, str] | None = None,
        ):  # type: ignore[override]
            captured["cmd"] = cmd
            captured["env"] = env
            # Succeed with code 0 first
            return P(cmd, 0)

        monkeypatch.setattr(rt.subprocess, "Popen", fake_popen)

>       ok, output = rt.run_tests(
            target="unit-tests", speed_categories=["fast"], parallel=False
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:174:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_run_tests_translates_args_and_handles_return_codes.<locals>.fake_run() got
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,310 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
___________ test_run_tests_keyword_filter_for_extra_marker_lmstudio ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f786ae0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_keyword_filter_1')

    @pytest.mark.fast
    def test_run_tests_keyword_filter_for_extra_marker_lmstudio(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-04 — extra_marker 'requires_resource("lmstudio")' uses
        keyword filter and early success on no matches."""
        # Arrange: ensure keyword narrowing path is exercised with no matches ->
        # early success
        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests":
str(tests_dir)}
        )

        class Dummy:
            def __init__(self, stdout: str, returncode: int = 0):
                self.stdout = stdout
                self.returncode = returncode

        def fake_run(
            cmd, check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            # '--collect-only' path with '-k lmstudio' produces no items
            assert "--collect-only" in cmd
            return Dummy(stdout="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

>       ok, msg = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:236:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_run_tests_keyword_filter_for_extra_marker_lmstudio.<locals>.fake_run() got
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,378 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_________ test_run_tests_handles_popen_exception_without_speed_filters _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f784920>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_handles_popen_e0')

    @pytest.mark.fast
    def test_run_tests_handles_popen_exception_without_speed_filters(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RTM-05 — run_tests surfaces subprocess errors with
guidance."""

        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests":
str(tests_dir)}
        )

        # ``run_tests`` should not perform collection when no speed categories
are
        # provided. Guard against unexpected subprocess.run usage.
        def fail_run(*_args: Any, **_kwargs: Any) -> None:  # pragma: no cover -
safety
            raise AssertionError("subprocess.run should not be invoked in this
branch")

        monkeypatch.setattr(rt.subprocess, "run", fail_run)

        captured: dict[str, list[str]] = {}

        def boom_popen(
            cmd: list[str],
            stdout: Any = None,
            stderr: Any = None,
            text: bool = True,
            env: dict[str, str] | None = None,
        ) -> Any:  # pragma: no cover - behavior exercised via exception path
            captured["cmd"] = cmd
            raise RuntimeError("intentional popen failure")

        monkeypatch.setattr(rt.subprocess, "Popen", boom_popen)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:278:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

_args =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/t
est_run_tests_handles_popen_e0/tests', '--collect-only', '-q', ...],)
_kwargs = {'capture_output': True, 'cwd':
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY'...on.debugpy-2025.14.1-darwin-arm64/bundled/libs/debugpy
', 'CLICOLOR': '1', 'COLORFGBG': '15;0', ...}, 'text': True, ...}

    def fail_run(*_args: Any, **_kwargs: Any) -> None:  # pragma: no cover -
safety
>       raise AssertionError("subprocess.run should not be invoked in this
branch")
E       AssertionError: subprocess.run should not be invoked in this branch

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:260: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,459 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_______________ test_collect_unknown_target_uses_all_tests_path ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7851c0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_unknown_target_us0')

    @pytest.mark.fast
    def test_collect_unknown_target_uses_all_tests_path(monkeypatch, tmp_path):
        """ReqID: RTM-06 — Unknown target falls back to all-tests mapping."""

        tests_dir = tmp_path / "some_tests"
        tests_dir.mkdir()
        cache_dir = tmp_path / "cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt, "TARGET_PATHS", {"all-tests": str(tests_dir)})

        observed: list[list[str]] = []

        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            observed.append(cmd[:])
            return SimpleNamespace(
                stdout="test_sample.py::test_ok\n",
                stderr="",
                returncode=0,
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        original_isdir = rt.os.path.isdir

        def fake_isdir(path: str) -> bool:
            if path == str(tests_dir):
                return False
            return original_isdir(path)

        monkeypatch.setattr(rt.os.path, "isdir", fake_isdir)

        original_exists = rt.os.path.exists

        def fake_exists(path: str) -> bool:
            if path == "test_sample.py":
                return True
            return original_exists(path)

        monkeypatch.setattr(rt.os.path, "exists", fake_exists)

>       result = rt.collect_tests_with_cache("custom-target",
speed_category=None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:346:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'custom-target', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_______________ test_enforce_coverage_threshold_exit_and_return ________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_enforce_coverage_threshol2')

    @pytest.mark.fast
    def test_enforce_coverage_threshold_exit_and_return(tmp_path):
        """ReqID: RTM-05 — Coverage helper returns percent and exits on
failure."""

        cov_file = tmp_path / "coverage.json"
        cov_file.write_text(json.dumps({"totals": {"percent_covered": 95.25}}))
        percent = rt.enforce_coverage_threshold(
            coverage_file=cov_file, exit_on_failure=False
        )
        assert percent == pytest.approx(95.25)

        cov_file.write_text(json.dumps({"totals": {"percent_covered": 81.7}}))
>       with pytest.raises(RuntimeError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'RuntimeError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:365: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,561 - devsynth.testing.run_tests - INFO - Coverage 95.25%
meets the 70.00% threshold.
2025-10-28 10:31:18,561 - devsynth.testing.run_tests - INFO - Coverage 81.70%
meets the 70.00% threshold.
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage 95.25% meets
the 70.00% threshold.
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage 81.70% meets
the 70.00% threshold.
_______________ test_run_tests_segment_appends_aggregation_tips ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f787c50>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_segment_appends0')

    @pytest.mark.fast
    def test_run_tests_segment_appends_aggregation_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RTM-07 — Segmented failures append aggregate troubleshooting
tips."""

        tests_dir = tmp_path / "segmented"
        tests_dir.mkdir()
        (tests_dir / "test_one.py").write_text("def test_one():\n    assert
True\n")
        (tests_dir / "test_two.py").write_text("def test_two():\n    assert
True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        class CollectProc:
            def __init__(self, out: str) -> None:
                self.stdout = out
                self.stderr = ""
                self.returncode = 0

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            stdout = "\n".join(["test_one.py::test_one",
"test_two.py::test_two"])
            return CollectProc(stdout)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        batch_calls: list[list[str]] = []

        class FakeBatchProcess:
            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=False,
                env=None,
            ) -> None:  # noqa: ANN001
                batch_calls.append(cmd)
                self.args = cmd
                index = len(batch_calls) - 1
                if index == 0:
                    self._stdout = "batch-1\n"
                    self._stderr = ""
                    self._returncode = 0
                else:
                    self._stdout = "batch-2\n"
                    self._stderr = "boom"
                    self._returncode = 1

            def communicate(self):  # noqa: D401 - simple stub
                return self._stdout, self._stderr

            @property
            def returncode(self) -> int:
                return self._returncode

        monkeypatch.setattr(rt.subprocess, "Popen", FakeBatchProcess)

        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )

        assert success is False
        assert len(batch_calls) == 2

        expected_agg_cmd = [
            rt.sys.executable,
            "-m",
            "pytest",
            f"--cov={rt.COVERAGE_TARGET}",
            "--cov-report=term-missing",
            f"--cov-report=json:{rt.COVERAGE_JSON_PATH}",
            f"--cov-report=html:{rt.COVERAGE_HTML_DIR}",
            "--cov-append",
            str(tests_dir),
        ]
        aggregate_tip = rt._failure_tips(1, expected_agg_cmd)

>       assert aggregate_tip in output
E       AssertionError: assert '\nPytest exited with code 1. Command:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m
pytest...HTML report for context (saved under test_reports/):\n  devsynth
run-tests --target unit-tests --speed=fast --report\n' in
'batch-1\n\nbatch-2\n\nPytest exited with code 1. Command:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/...HTML report for
context (saved under test_reports/):\n  devsynth run-tests --target unit-tests
--speed=fast --report\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:484: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,592 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:18,592 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-28 10:31:18,592 - devsynth.testing.run_tests - INFO - Running segment
1/2 (1 tests)
2025-10-28 10:31:18,592 - devsynth.testing.run_tests - INFO - Running segment
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1
tests)
______________ test_run_tests_completes_without_xdist_assertions _______________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_completes_witho0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fd95a00>

    @pytest.mark.fast
    def test_run_tests_completes_without_xdist_assertions(tmp_path,
monkeypatch):
        """run_tests completes without INTERNALERROR when run in parallel.
ReqID: FR-22"""
        test_file = tmp_path / "test_dummy.py"
        test_file.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert
True\n"
        )
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        def fake_collect(cmd, check=False, capture_output=True, text=True):  #
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_ok",
                stderr="",
            )

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                assert "-n" in cmd and "auto" in cmd
                self.returncode = 0
                self._stdout = "passed"
                self._stderr = ""

            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests("unit-tests", ["fast"], parallel=True)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_no_xdist_assertions.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_run_tests_completes_without_xdist_assertions.<locals>.fake_collect() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,655 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_________________ test_report_flag_adds_html_report_to_command _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f107ce0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_report_flag_adds_html_rep0')

    @pytest.mark.fast
    def test_report_flag_adds_html_report_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-2 — report=True adds --html to the pytest
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")

        recorded_cmds: list[list[str]] = []

        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0

            def communicate(self):
                return "ok", ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        with (
            patch.object(rt, "collect_tests_with_cache",
return_value=["test_file.py"]),
            patch.object(rt, "datetime") as mock_dt,
        ):
            mock_dt.now.return_value.strftime.return_value = "20250101_000000"
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=True,
                parallel=False,
                segment=False,
                maxfail=None,
                extra_marker=None,
            )

        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert any(arg.startswith("--html=") for arg in pytest_cmd)
E       assert False
E        +  where False = any(<generator object
test_report_flag_adds_html_report_to_command.<locals>.<genexpr> at 0x13f9369b0>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:101: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,790 - devsynth.testing.run_tests - WARNING - Skipping
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph
publication: Coverage JSON missing at test_reports/coverage.json
___________________ test_no_parallel_flag_adds_n0_to_command ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f104a40>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_no_parallel_flag_adds_n0_0')

    @pytest.mark.fast
    def test_no_parallel_flag_adds_n0_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-3 — parallel=False adds -n0 to the pytest
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")

        recorded_cmds: list[list[str]] = []

        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0

            def communicate(self):
                return "ok", ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        with patch.object(rt, "collect_tests_with_cache",
return_value=["test_file.py"]):
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=False,
                maxfail=None,
                extra_marker=None,
            )

        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert "-n0" in pytest_cmd
E       AssertionError: assert '-n0' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 'test_file.py', '-m', 'not memory_intensive and fast and not gui',
...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:138: AssertionError
__________________ test_maxfail_flag_adds_maxfail_to_command ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7615b0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_maxfail_flag_adds_maxfail0')

    @pytest.mark.fast
    def test_maxfail_flag_adds_maxfail_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-4 — maxfail=N adds --maxfail=N to the pytest
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")

        recorded_cmds: list[list[str]] = []

        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0

            def communicate(self):
                return "ok", ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        with patch.object(rt, "collect_tests_with_cache",
return_value=["test_file.py"]):
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=False,
                maxfail=5,
                extra_marker=None,
            )

        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert "--maxfail=5" in pytest_cmd
E       AssertionError: assert '--maxfail=5' in
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 'test_file.py', '-m', 'not memory_intensive and fast and not gui',
...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:175: AssertionError
___________________ test_segment_flags_trigger_segmented_run ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1046e0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segment_flags_trigger_seg0')

    @pytest.mark.fast
    def test_segment_flags_trigger_segmented_run(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-5 — segment=True triggers a segmented run."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")

        recorded_cmds: list[list[str]] = []

        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0

            def communicate(self):
                return "ok", ""

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        with patch.object(rt.subprocess, "run") as mock_run:
            mock_run.return_value.stdout = "test_file.py\ntest_file2.py"
            mock_run.return_value.returncode = 0
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=True,
                segment_size=1,
                maxfail=None,
                extra_marker=None,
            )

>       assert len(recorded_cmds) == 2, "Expected two Popen calls for a
segmented run"
E       AssertionError: Expected two Popen calls for a segmented run
E       assert 1 == 2
E        +  where 1 =
len([['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python',
'-m', 'pytest',
'/private/var/folders/2v/lb...ytest-of-caitlyn/pytest-1440/test_segment_flags_tr
igger_seg0', '-m', 'not memory_intensive and fast and not gui', ...]])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:213: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,850 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:18,850 - devsynth.testing.run_tests - INFO - marker fallback
triggered for target=unit-tests (speeds=fast)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback
triggered for target=unit-tests (speeds=fast)
_______________ test_run_tests_parallel_includes_cov_and_n_auto ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74c980>

    @pytest.mark.fast
    def test_run_tests_parallel_includes_cov_and_n_auto(monkeypatch):
        """ReqID: RUN-TESTS-PARALLEL-1

        When parallel=True and no explicit node ids are collected (single-pass
branch),
        run_tests should include '-n auto' and explicit coverage instrumentation
in the
        pytest command.
        """

        import devsynth.testing.run_tests as rt

        # We won't validate the collection step here; the single-pass branch
does not
        # pre-collect node ids when speed_categories is None.

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Assert parallel-related flags are present
                assert "-n" in cmd and "auto" in cmd, f"parallel flags missing
in: {cmd}"
                cov_flag = f"--cov={rt.COVERAGE_TARGET}"
                json_flag = f"--cov-report=json:{rt.COVERAGE_JSON_PATH}"
                html_flag = f"--cov-report=html:{rt.COVERAGE_HTML_DIR}"
                assert cov_flag in cmd, f"{cov_flag} missing in: {cmd}"
                assert json_flag in cmd, f"{json_flag} missing in: {cmd}"
                assert html_flag in cmd, f"{html_flag} missing in: {cmd}"
                assert "--cov-append" in cmd, f"--cov-append missing in: {cmd}"
                self.returncode = 0

            def communicate(self):
                return ("ok\n", "")

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,  # triggers non-collection single-pass branch
            verbose=False,
            report=False,
            parallel=True,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_parallel_flags.py:40:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_run_tests_parallel_includes_cov_and_n_auto.<locals>.FakePopen.__init__()
got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,883 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_______________ test_parallel_injects_cov_reports_and_xdist_auto _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74d0a0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_parallel_injects_cov_repo0')

    @pytest.mark.fast
    def test_parallel_injects_cov_reports_and_xdist_auto(monkeypatch, tmp_path:
Path):
        """ReqID: TR-RT-11 — Parallel path injects -n auto with coverage
reports.

        Verify that when parallel=True, run_tests injects xdist flags and
preserves
        coverage instrumentation so JSON/HTML artifacts are generated.
        """

        called = {}

        class FakeCompleted:
            def __init__(self, stdout: str = "", stderr: str = "", returncode:
int = 0):
                self.stdout = stdout
                self.stderr = stderr
                self.returncode = returncode

        test_a = tmp_path / "test_alpha.py"
        test_b = tmp_path / "test_beta.py"
        test_a.write_text("def test_one():\n    assert True\n")
        test_b.write_text("def test_two():\n    assert True\n")

        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)

        def fake_run(
            cmd, check=False, capture_output=False, text=False
        ):  # type: ignore[no-untyped-def]
            # Simulate collection with two node ids; pattern
            # ".*\\.py(::|$)" will match them.
            stdout = "\n".join(
                [
                    f"{test_a}::test_one",
                    f"{test_b}::test_two",
                ]
            )
            return FakeCompleted(stdout=stdout, stderr="", returncode=0)

        # pragma: no cover - communicate() path is asserted via effects
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-untyped-def]
                called["cmd"] = cmd
                self.returncode = 0

            def communicate(self):  # type: ignore[no-untyped-def]
                return ("", "")

        # Patch subprocess in module under test
        monkeypatch.setattr("devsynth.testing.run_tests.subprocess.run",
fake_run)
        monkeypatch.setattr("devsynth.testing.run_tests.subprocess.Popen",
FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=True,
            segment=False,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_parallel_no_cov.py:63:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_parallel_injects_cov_reports_and_xdist_auto.<locals>.fake_run() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:18,965 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
___________ test_collect_tests_with_cache_handles_subprocess_timeout ___________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_collect_tests_with_cache_20')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f7430b0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x12f6989b0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_handles_subprocess_timeout(
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """Timeouts during collection surface a warning and yield no tests."""

        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", tmp_path / ".cache")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "sample_test.py").write_text("def test_sample():\n    assert
True\n")

        def fake_run(*_args: object, **_kwargs: object) ->
subprocess.CompletedProcess[str]:
            raise subprocess.TimeoutExpired(
                cmd=["pytest"], timeout=rt.DEFAULT_COLLECTION_TIMEOUT_SECONDS
            )

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        with caplog.at_level(logging.WARNING,
logger="devsynth.testing.run_tests"):
            collected = rt.collect_tests_with_cache("unit-tests")

        assert collected == []
>       assert "Test collection failed" in caplog.text
E       AssertionError: assert 'Test collection failed' in 'WARNING
devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout for
target=unit-tests (all); falling back to path\n'
E        +  where 'WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test
collection timeout for target=unit-tests (all); falling back to path\n' =
<_pytest.logging.LogCaptureFixture object at 0x12f6989b0>.text

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_plugin_timeouts.py:35: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,064 - devsynth.testing.run_tests - WARNING - Test collection
timeout for target=unit-tests (all); falling back to path
------------------------------ Captured log call -------------------------------
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout
for target=unit-tests (all); falling back to path
________________ test_pytest_plugins_registers_pytest_bdd_once _________________

    @pytest.mark.fast
    def test_pytest_plugins_registers_pytest_bdd_once() -> None:
        """Ensure the centralized helper exports pytest-bdd exactly once."""

        import importlib

        registry = importlib.import_module("tests.pytest_plugin_registry")
        plugin_list = list(registry.PYTEST_PLUGINS)

>       assert plugin_list.count("pytest_bdd.plugin") == 1
E       AssertionError: assert 0 == 1
E        +  where 0 = <built-in method count of list object at
0x13fd2ea80>('pytest_bdd.plugin')
E        +    where <built-in method count of list object at 0x13fd2ea80> =
[].count

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_pytest_plugins_bdd.py:107: AssertionError
___________ test_run_tests_report_injects_html_args_and_creates_dir ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f1127e0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_report_injects_0')

    @pytest.mark.fast
    def test_run_tests_report_injects_html_args_and_creates_dir(monkeypatch,
tmp_path):
        """
        ReqID: TR-RT-12 — Report HTML generation and directory creation.

        Validate that when report=True, run_tests:
        - adds --html=<test_reports/.../target>/report.html and
--self-contained-html
        - creates the report directory path
        - executes pytest with node ids (non-parallel path)
        """

        # Arrange a tmp tests dir and map unit-tests target to it
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))

        # Collection returns a couple of node ids
        collected = [
            "tests/unit/test_alpha.py::test_a",
            "tests/unit/test_beta.py::test_b",
        ]

        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            if "--collect-only" in cmd:
                return SimpleNamespace(stdout="\n".join(collected), stderr="",
returncode=0)
            return SimpleNamespace(stdout="", stderr="", returncode=0)

        seen_cmds: list[list[str]] = []

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                seen_cmds.append(cmd)
                self.returncode = 0

            def communicate(self):  # noqa: D401
                return ("", "")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        # Act
        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=[
                "fast"
            ],  # go through segmented-speed path without segmentation
            verbose=False,
            report=True,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

        # Assert
        assert ok is True
>       assert output == ""
E       AssertionError: assert '\n[knowledge...verage.json\n' == ''
E
E         +
E         + [knowledge-graph] coverage ingestion skipped: Coverage JSON missing
at test_reports/coverage.json

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_report.py:78: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,220 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:19,220 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
2025-10-28 10:31:19,220 - devsynth.testing.run_tests - WARNING - Skipping
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph
publication: Coverage JSON missing at test_reports/coverage.json
_____________ test_single_pass_non_keyword_returncode_5_is_success _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f112ae0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_single_pass_non_keyword_returncode_5_is_success(monkeypatch) ->
None:
        """ReqID: TR-RT-10 — Return code 5 is success in single-pass non-keyword
path.

        In the single-pass, non-keyword path (no speed_categories), pytest
return
        code 5 (no tests collected) should be treated as success. This exercises
        the branch where we do not pre-collect node ids and simply pass a
category
        expression to pytest via '-m'.
        """

        # Force the branch: speed_categories=None, no extra_marker or keyword
filter,
        # parallel=False to avoid xdist flags.

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Ensure the '-m' category expression is present and no '-k'
keyword filter
                assert "-m" in cmd, f"expected -m category expression in cmd:
{cmd}"
                assert "-k" not in cmd, f"did not expect -k in cmd: {cmd}"
                # Simulate pytest exit code 5 (no tests collected)
                self.returncode = 5

            def communicate(self):
                return ("", "")

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_returncode5_success.py:36:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = True, timeout = 60.0, check = False
popenargs =
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest',
'/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env':
{'BRAVE_SEARCH_API_KEY':
'BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy', 'CLICOLOR':
'1', 'COLORFGBG': '15;0', ...}, 'stderr': -1, 'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError:
test_single_pass_non_keyword_returncode_5_is_success.<locals>.FakePopen.__init__
() got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,231 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
_______ test_segmented_batches_surface_plugin_fallbacks_and_failure_tips _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f79de80>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_batches_surface0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x12f1126f0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segmented_batches_surface_plugin_fallbacks_and_failure_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog:
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENTATION-1 — Segmented failures emit rich
diagnostics.

        This test simulates a segmented execution where the first batch fails
with a
        coverage gate error. It verifies that fallback plugin injection occurs
for the
        subprocess environment, coverage warnings propagate to stdout/stderr,
and the
        aggregated failure guidance from :func:`_failure_tips` is appended
exactly once.
        """

        caplog.set_level(logging.INFO)

        tests_dir = tmp_path / "segmented"
        tests_dir.mkdir()
        test_one = tests_dir / "test_one.py"
        test_two = tests_dir / "test_two.py"
        test_one.write_text("def test_one():\n    assert True\n")
        test_two.write_text("def test_two():\n    assert True\n")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path / "cache"))

        # Avoid mutating real coverage artifacts while exercising segmentation
logic.
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        coverage_calls: list[str] = []
        monkeypatch.setattr(
            rt, "_ensure_coverage_artifacts", lambda:
coverage_calls.append("ensured")
        )

        ensure_calls: list[tuple[str, bool, str]] = []

        def fake_cov(env: dict[str, str]) -> bool:
            ensure_calls.append(("cov", env is os.environ,
env.get("PYTEST_ADDOPTS", "")))
            if env is os.environ:
                # Simulate a no-op at the process level so the subprocess copy
applies the fix.
                return False
            env["PYTEST_ADDOPTS"] = (
                env.get("PYTEST_ADDOPTS", "") + " -p pytest_cov"
            ).strip()
            return True

        def fake_bdd(env: dict[str, str]) -> bool:
            ensure_calls.append(("bdd", env is os.environ,
env.get("PYTEST_ADDOPTS", "")))
            if env is os.environ:
                return False
            env["PYTEST_ADDOPTS"] = (
                env.get("PYTEST_ADDOPTS", "") + " -p pytest_bdd.plugin"
            ).strip()
            return True

        monkeypatch.setattr(rt, "ensure_pytest_cov_plugin_env", fake_cov)
        monkeypatch.setattr(rt, "ensure_pytest_bdd_plugin_env", fake_bdd)

        collect_output = "\n".join(
            [
                f"{test_one}::test_one",
                f"{test_two}::test_two",
            ]
        )

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd, "collection command expected"
            return SimpleNamespace(returncode=0, stdout=collect_output,
stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_run)

        batch_plan = iter(
            [
                {
                    "returncode": 1,
                    "stdout": "batch-one\n",
                    "stderr": "FAIL Required test coverage of 90% not
reached.\n",
                },
                {
                    "returncode": 0,
                    "stdout": "batch-two\n",
                    "stderr": "",
                },
            ]
        )
        popen_calls: list[dict[str, object]] = []

        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                popen_calls.append({"cmd": list(cmd), "env": dict(env or {})})
                try:
                    result = next(batch_plan)
                except StopIteration as exc:  # pragma: no cover - guards test
integrity
                    raise AssertionError("Unexpected extra Popen invocation")
from exc
                self.returncode = result["returncode"]
                self._stdout = result["stdout"]
                self._stderr = result["stderr"]

            def communicate(self):  # noqa: D401 - signature mirrors subprocess
API
                """Return the stubbed stdout/stderr pair."""

                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmentation.py:131:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.

        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during
collection.

        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])

        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"

        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
______________ test_segmented_failure_appends_aggregate_tips_once ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f784d40>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_segmented_failure_appends0')

    @pytest.mark.fast
    def test_segmented_failure_appends_aggregate_tips_once(monkeypatch,
tmp_path):
        """
        ReqID: RT-11 — Aggregated troubleshooting tips appended once after
segments.
        """
        # Arrange a fake tests directory and map unit-tests to it
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))

        # Collected node ids => 2 batches with size=2
        collected = [
            "tests/unit/test_a.py::test_1",
            "tests/unit/test_a.py::test_2",
            "tests/unit/test_b.py::test_3",
            "tests/unit/test_b.py::test_4",
        ]

        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            if "--collect-only" in cmd:
                return _DummyCompleted(stdout="\n".join(collected), stderr="",
returncode=0)
            return _DummyCompleted(stdout="", stderr="", returncode=0)

        # First batch fails (rc=1), second succeeds (rc=0)
        dummy_popen = _DummyPopen(rc_sequence=[1, 0])

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", dummy_popen)

        # Act
        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=2,
            maxfail=1,
            extra_marker=None,
        )

        # Assert: overall not ok due to failed batch
        assert ok is False
        # The troubleshooting tips block should appear once per failed batch (1)
plus
        # one aggregated block at the end => total 2 occurrences.
>       assert output.count("Troubleshooting tips:") == 2
E       AssertionError: assert 1 == 2
E        +  where 1 = <built-in method count of str object at
0x12b2b1a00>('Troubleshooting tips:')
E        +    where <built-in method count of str object at 0x12b2b1a00> =
'\nPytest exited with code 1. Command:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m
pytest...HTML report for context (saved under test_reports/):\n  devsynth
run-tests --target unit-tests --speed=fast --report\n'.count

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_aggregate_fail_tips_once.py:104: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,578 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:19,579 - devsynth.testing.run_tests - INFO - Running 4 tests in
2 segments of size 2 for target=unit-tests
2025-10-28 10:31:19,579 - devsynth.testing.run_tests - INFO - Running segment
1/2 (2 tests)
2025-10-28 10:31:19,580 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 4 tests in 2
segments of size 2 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (2
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
____________ test_segmented_aggregate_tips_command_includes_maxfail ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f787050>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segmented_aggregate_tips_command_includes_maxfail(monkeypatch) ->
None:
        """
        ReqID: RT-11 — When segmented mode runs and any batch fails, the
aggregated
        troubleshooting tips are generated using a command that includes
--maxfail
        if maxfail was provided.
        """

        collected_ids = "tests/unit/sample_test.py::test_a\n"

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            # collection phase returns one node id (ensures one batch)
            return SimpleNamespace(returncode=0, stdout=collected_ids,
stderr="")

        class FailingBatch:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Simulate a failing batch
                self.returncode = 1

            def communicate(self) -> tuple[str, str]:
                return ("", "boom")

        captured = {}

        def fake_failure_tips(returncode, cmd):  # noqa: ANN001
            # Capture the command used to generate tips for later assertion
            captured["cmd"] = cmd
            return "\nTroubleshooting tips: ...\n"

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FailingBatch)
        monkeypatch.setattr(rt, "_failure_tips", fake_failure_tips)

        success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=5,
            maxfail=2,
            extra_marker=None,
        )

        assert success is False
        assert "Troubleshooting tips" in output
        # Ensure --maxfail=2 was included in the aggregate cmd, not just batch
cmd
>       assert any(
            isinstance(arg, str) and arg.startswith("--maxfail=") and
arg.endswith("2")
            for arg in captured.get("cmd", [])
        ), f"--maxfail not propagated in aggregate cmd: {captured}"
E       AssertionError: --maxfail not propagated in aggregate cmd: {'cmd':
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 'tests/unit/sample_test.py::test_a', '-m', 'not memory_intensive and
fast and not gui', '--cov=src/devsynth',
'--cov-report=json:test_reports/coverage.json', '--cov-report=html:htmlcov',
'--cov-append', '--maxfail', '2']}
E       assert False
E        +  where False = any(<generator object
test_segmented_aggregate_tips_command_includes_maxfail.<locals>.<genexpr> at
0x13fd3cc80>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_aggregate_maxfail.py:68: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,592 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:19,592 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 5 for target=unit-tests
2025-10-28 10:31:19,592 - devsynth.testing.run_tests - INFO - Running segment
1/1 (1 tests)
2025-10-28 10:31:19,593 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1
segments of size 5 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
___________ test_run_tests_segmented_falls_back_on_empty_collection ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f74cc50>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_run_tests_segmented_falls0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x12f6b1a90>

    @pytest.mark.fast
    def test_run_tests_segmented_falls_back_on_empty_collection(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENTED-5 — Fallback run executes when no node ids
exist."""

        tests_dir = tmp_path / "segmented-empty"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "ensure_pytest_cov_plugin_env", lambda _env:
False)
        monkeypatch.setattr(rt, "ensure_pytest_bdd_plugin_env", lambda _env:
False)

        def fake_collect(cmd, check=False, capture_output=True, text=True):  #
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(returncode=0, stdout="", stderr="")

        monkeypatch.setattr(rt.subprocess, "run", fake_collect)

        popen_calls: list[list[str]] = []

        class FakePopen:
            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=True,
                env=None,
            ):  # noqa: ANN001
                popen_calls.append(cmd[:])
                self.returncode = 0
                self._stdout = "ok\n"
                self._stderr = ""

            def communicate(self):  # noqa: D401 - simple stub
                return self._stdout, self._stderr

        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)

        caplog.set_level(logging.WARNING)

>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=10,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_empty_node_ids.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""

        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]

        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])

        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent

        # Inherit the full environment but override specific variables for
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"

>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError:
test_run_tests_segmented_falls_back_on_empty_collection.<locals>.fake_collect()
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
_____________ test_segment_batch_benchmark_warning_forces_success ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f784e30>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segment_batch_benchmark_warning_forces_success(monkeypatch) ->
None:
        """ReqID: RT-09 — PytestBenchmarkWarning in stderr forces success for
the batch."""

        collected_ids = (
            "tests/unit/mod_x_test.py::test_x1\n"
"tests/unit/mod_y_test.py::test_y1\n"
        )

        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout=collected_ids,
stderr="")

        class WarnBatch:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                self._stderr = "PytestBenchmarkWarning: calibration"
                self.returncode = 1  # would fail without the special-case
handling

            def communicate(self) -> tuple[str, str]:
                return ("ok\n", self._stderr)

        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", WarnBatch)

        success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )

>       assert success is True
E       assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_failure_paths.py:105: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:19,681 - devsynth.testing.run_tests - INFO - test collection
cache miss for target=unit-tests (fast) — collecting via pytest
2025-10-28 10:31:19,681 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-28 10:31:19,681 - devsynth.testing.run_tests - INFO - Running segment
1/2 (1 tests)
2025-10-28 10:31:19,681 - devsynth.testing.run_tests - INFO - Running segment
2/2 (1 tests)
2025-10-28 10:31:19,681 - devsynth.testing.run_tests - WARNING - Coverage
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache
miss for target=unit-tests (fast) — collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing
__________ test_api_health_and_metrics_startup_without_binding_ports ___________

    @pytest.mark.no_network
    @pytest.mark.fast
    def test_api_health_and_metrics_startup_without_binding_ports():
        """Metrics endpoint returns consistent counters without binding ports.
ReqID: N/A"""
        pytest.importorskip("fastapi")
        TestClient = pytest.importorskip("fastapi.testclient").TestClient

        # Import the app from the lightweight API module that includes /health
and /metrics
        api_mod = __import__("devsynth.api", fromlist=["app"])  # type:
ignore[assignment]
        app = getattr(api_mod, "app")

        client = TestClient(app)

        r_health = client.get("/health")
        assert r_health.status_code == 200
        assert r_health.json().get("status") == "ok"

        # Metrics should be available and consistent
        client.get("/metrics")  # prime counter for /metrics
        r_metrics = client.get("/metrics")
        assert r_metrics.status_code == 200
        metrics_text = r_metrics.text
        assert isinstance(metrics_text, str)

>       assert 'request_count_total{endpoint="/health"} 1.0' in metrics_text
E       assert 'request_count_total{endpoint="/health"} 1.0' in '# HELP
python_gc_objects_collected_total Objects collected during gc\n# TYPE
python_gc_objects_collected_total
counte...4003e+09\ndevsynth_cli_run_tests_invocations_created{smoke="false",targ
et="not-a-real-target"} 1.761672583839073e+09\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/api/test_
api_startup.py:28: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:23,208 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 200 OK"
2025-10-28 10:31:23,216 - httpx - INFO - HTTP Request: GET
http://testserver/metrics "HTTP/1.1 200 OK"
2025-10-28 10:31:23,229 - httpx - INFO - HTTP Request: GET
http://testserver/metrics "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 200 OK"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics
"HTTP/1.1 200 OK"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics
"HTTP/1.1 200 OK"
____________________ test_prometheus_exporter_refuses_root _____________________

scripts_dir =
PosixPath('/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/deployme
nt')

    @pytest.mark.fast
    def test_prometheus_exporter_refuses_root(scripts_dir):
        try:
>           result = subprocess.run(
                ["python", str(scripts_dir / "prometheus_exporter.py")],
                capture_output=True,
                text=True,
                timeout=5,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/deploymen
t/test_deployment_scripts.py:59:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:550: in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:1209: in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:2115: in _communicate
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/selectors.py:415: in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13fdbd640, file
'/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/selectors.py', line 416, code select>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
_________________________ test_collection_performance __________________________

context = {}

    @then("test collection should complete within performance targets")
    def test_collection_performance(context):
        """Verify test collection completes within performance targets."""
>       assert "performance_results" in context
E       AssertionError: assert 'performance_results' in {}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_e
nhanced_test_infrastructure_steps.py:979: AssertionError
__________________________ test_analysis_performance ___________________________

context = {}

    @then("test analysis should complete within performance targets")
    def test_analysis_performance(context):
        """Verify test analysis completes within performance targets."""
>       assert context["performance_results"]["analysis_time"] <=
context["performance_targets"]["analysis_time"]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'performance_results'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_e
nhanced_test_infrastructure_steps.py:986: KeyError
_______________________________ test_docs_build ________________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_docs_build0')

    @pytest.mark.fast
    def test_docs_build(tmp_path):
        """Docs build without errors using MkDocs."""
        pytest.importorskip("mkdocs")
        pytest.importorskip("material")
        if shutil.which("mkdocs") is None:
            pytest.skip("mkdocs command not found")

        repo_root = pathlib.Path(__file__).resolve().parents[2]
        env = os.environ.copy()
        env["PYTHONPATH"] = str(repo_root / "src")
        tmp_config = tmp_path / "mkdocs.yml"
        tmp_config.write_text(
            "\n".join(
                [
                    f"INHERIT: {repo_root / 'mkdocs.yml'}",
                    f"docs_dir: {repo_root / 'docs'}",
                    f"site_dir: {tmp_path / 'site'}",
                    "plugins:",
                    "  - search",
                    "  - literate-nav:",
                    "      nav_file: SUMMARY.md",
                    "  - gen-files:",
                    "      scripts:",
                    f"        - {repo_root / 'scripts/gen_ref_pages.py'}",
                    "  - include-markdown",
                ]
            )
        )
>       result = subprocess.run(
            [
                "poetry",
                "run",
                "mkdocs",
                "build",
                "--strict",
                "--quiet",
                "-f",
                str(tmp_config),
            ],
            cwd=repo_root,
            env=env,
            capture_output=True,
            text=True,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/test_documen
tation_generation.py:40:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:550: in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:1209: in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:2115: in _communicate
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/selectors.py:415: in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13fdbeb40, file
'/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/selectors.py', line 416, code select>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 3 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
_____________ test_warn_if_features_disabled_all_disabled_succeeds _____________

self = <MagicMock name='echo' id='5081915824'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'echo' to have been called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

mock_echo = <MagicMock name='echo' id='5081915824'>
mock_load_config = <MagicMock name='load_config' id='5088282816'>

    @patch("devsynth.adapters.cli.typer_adapter.load_config")
    @patch("devsynth.adapters.cli.typer_adapter.typer.echo")
    @pytest.mark.medium
    def test_warn_if_features_disabled_all_disabled_succeeds(mock_echo,
mock_load_config):
        """Test that _warn_if_features_disabled shows a warning when all
features are disabled.

        ReqID: N/A"""
        mock_config = MagicMock()
        mock_config.features = {"feature1": False, "feature2": False}
        mock_load_config.return_value = mock_config
        _warn_if_features_disabled()
>       mock_echo.assert_called_once()
E       AssertionError: Expected 'echo' to have been called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:63: AssertionError
___________________________ test_show_help_succeeds ____________________________

mock_build_app = <MagicMock name='build_app' id='5080398688'>
mock_console = <MagicMock name='Console' id='5072147600'>

    @patch("devsynth.adapters.cli.typer_adapter.Console")
    @patch("devsynth.adapters.cli.typer_adapter.build_app")
    @pytest.mark.medium
    def test_show_help_succeeds(mock_build_app, mock_console):
        """Test that show_help displays the CLI help message.

        ReqID: N/A"""
        mock_app = MagicMock()
        mock_app.info = MagicMock()
        mock_app.info.help = "Test help text"
        mock_app.registered_commands = []
        mock_app.registered_groups = []
        mock_build_app.return_value = mock_app
        mock_console_instance = MagicMock()
        mock_console.return_value = mock_console_instance
        show_help()
>       assert mock_console_instance.print.call_count > 0
E       AssertionError: assert 0 > 0
E        +  where 0 = <MagicMock name='Console().print'
id='5075435440'>.call_count
E        +    where <MagicMock name='Console().print' id='5075435440'> =
<MagicMock name='Console()' id='5080656032'>.print

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:109: AssertionError
----------------------------- Captured stdout call -----------------------------
╭──────────────────────────────── DevSynth CLI ────────────────────────────────╮
│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │
│ ┃    DevSynth CLI - automate iterative 'Expand, Differentiate, Refine,     ┃ │
│ ┃                           Retrace' workflows.                            ┃ │
│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │
│                                                                              │
│ DevSynth is a tool for automating software development workflows using the   │
│ EDRR (Expand, Differentiate, Refine, Retrace) methodology. It helps you      │
│ generate specifications from requirements, tests from specifications, and    │
│ code from tests, all while maintaining traceability and alignment.           │
│                                                                              │
│                                                                              │
│                                   Examples                                   │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
Commands                                        ╭─────────── Notes ────────────╮
├── align                                       │ • Only the embedded ChromaDB │
├── alignment-metrics                           │ backend is currently         │
├── atomic-rewrite                              │ supported.                   │
├── check                                       │ • Use &#x27;devsynth         │
├── code                                        │ &lt;command&gt; --help&#x27; │
├── completion                                  │ for more information on a    │
├── completion – Show or install shell          │ specific command.            │
│   completion scripts (see                     │ • Configuration can be       │
│   scripts/completions).                       ╰──────────────────────────────╯
Use 'devsynth [COMMAND] --help' for more information on a command.
2025-10-28 10:31:31,438 - devsynth.adapters.cli.typer_adapter - INFO - Commands:
2025-10-28 10:31:31,438 - devsynth.adapters.cli.typer_adapter - INFO - Run
'devsynth [COMMAND] --help' for more information on a command.
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.cli.typer_adapter:logging_setup.py:615 Commands:
INFO     devsynth.adapters.cli.typer_adapter:logging_setup.py:615 Run 'devsynth
[COMMAND] --help' for more information on a command.
__________________________ test_show_help_table_mode ___________________________

mock_build_app = <MagicMock name='build_app' id='5080397488'>
mock_console = <MagicMock name='Console' id='5075781440'>

    @patch("devsynth.adapters.cli.typer_adapter.Console")
    @patch("devsynth.adapters.cli.typer_adapter.build_app")
    @pytest.mark.medium
    def test_show_help_table_mode(mock_build_app, mock_console):
        """Table render mode should render commands without error.

        ReqID: N/A"""

        command = SimpleNamespace(name="init", help="Init projects")
        mock_app = SimpleNamespace(
            info=SimpleNamespace(help="Test"),
            registered_commands=[command],
            registered_groups=[],
        )
        mock_build_app.return_value = mock_app
        mock_console_instance = MagicMock()
        mock_console.return_value = mock_console_instance

        show_help(render_mode="table", group_filter=["config"])

>       assert mock_console_instance.print.call_count > 0
E       AssertionError: assert 0 > 0
E        +  where 0 = <MagicMock name='Console().print'
id='5081635424'>.call_count
E        +    where <MagicMock name='Console().print' id='5081635424'> =
<MagicMock name='Console()' id='5049935360'>.print

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:133: AssertionError
----------------------------- Captured stdout call -----------------------------
╭──────────────────────────────── DevSynth CLI ────────────────────────────────╮
│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │
│ ┃    DevSynth CLI - automate iterative 'Expand, Differentiate, Refine,     ┃ │
│ ┃                           Retrace' workflows.                            ┃ │
│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │
│                                                                              │
│ DevSynth is a tool for automating software development workflows using the   │
│ EDRR (Expand, Differentiate, Refine, Retrace) methodology. It helps you      │
│ generate specifications from requirements, tests from specifications, and    │
│ code from tests, all while maintaining traceability and alignment.           │
│                                                                              │
│                                                                              │
│                                   Examples                                   │
│                                                                              │
│  • devsynth init                                                             │
│     • Initialize a new project in the current directory                      │
│  • devsynth spec --requirements-file requirements.md                         │
│     • Generate specifications from requirements                              │
│  • devsynth test --spec-file specs.md                                        │
│     • Generate tests from specifications                                     │
│  • devsynth code                                                             │
│     • Generate code from tests                                               │
│  • devsynth tui                                                              │
│     • Launch the Textual TUI for guided wizards                              │
│  • devsynth run-pipeline --target unit-tests                                 │
│     • Execute the generated code                                             │
│                                                                              │
│                                                                              │
│                                    Notes                                     │
│                                                                              │
│  • Only the embedded ChromaDB backend is currently supported.                │
│  • Use 'devsynth <command> --help' for more information on a specific        │
│    command.                                                                  │
│  • Configuration can be managed with 'devsynth config' commands.             │
│  • Shell completion is available via '--install-completion' or the           │
│    'completion' command.                                                     │
│  • Long-running commands display progress indicators for better feedback.    │
│  • Dashboard metric hooks can be registered with '--dashboard-hook           │
│    module:function'.                                                         │
│  • Enable the colorblind-friendly palette with '--colorblind' or the         │
│    DEVSYNTH_CLI_COLORBLIND environment variable.                             │
╰──────────────────────────────────────────────────────────────────────────────╯
                               Available Commands
╭─────────────────────┬────────────────────────────────────────────────────────╮
│ Command             │ Description                                            │
├─────────────────────┼────────────────────────────────────────────────────────┤
│ align               │                                                        │
│ alignment-metrics   │                                                        │
│ atomic-rewrite      │                                                        │
│ check               │                                                        │
│ code                │                                                        │
│ completion          │                                                        │
│ completion          │ Show or install shell completion scripts (see          │
│                     │ scripts/completions).                                  │
│ dbschema            │                                                        │
│ doctor              │                                                        │
│ dpg                 │                                                        │
│ edrr-cycle          │                                                        │
│ gather              │                                                        │
│ generate-docs       │                                                        │
│ ingest              │                                                        │
│ init                │                                                        │
│ inspect             │                                                        │
│ inspect-config      │                                                        │
│ mvuu-dashboard      │                                                        │
│ refactor            │                                                        │
│ reprioritize-issues │                                                        │
│ run                 │                                                        │
│ run-pipeline        │                                                        │
│ run-tests           │                                                        │
│ security-audit      │                                                        │
│ serve               │                                                        │
│ spec                │                                                        │
│ test                │                                                        │
│ test-metrics        │                                                        │
│ tui                 │ Launch the Textual interface for DevSynth wizards.     │
│ validate-manifest   │                                                        │
│ validate-metadata   │                                                        │
│ webapp              │                                                        │
│ webui               │                                                        │
│ config              │ Manage configuration settings                          │
╰─────────────────────┴────────────────────────────────────────────────────────╯
╭─────────────────────────────────── Notes ────────────────────────────────────╮
│ • Only the embedded ChromaDB backend is currently supported.                 │
│ • Use &#x27;devsynth &lt;command&gt; --help&#x27; for more information on a  │
│ specific command.                                                            │
│ • Configuration can be managed with &#x27;devsynth config&#x27; commands.    │
│ • Shell completion is available via &#x27;--install-completion&#x27; or the  │
│ &#x27;completion&#x27; command.                                              │
│ • Long-running commands display progress indicators for better feedback.     │
│ • Dashboard metric hooks can be registered with &#x27;--dashboard-hook       │
│ module:function&#x27;.                                                       │
│ • Enable the colorblind-friendly palette with &#x27;--colorblind&#x27; or    │
│ the DEVSYNTH_CLI_COLORBLIND environment variable.                            │
╰──────────────────────────────────────────────────────────────────────────────╯
Use 'devsynth [COMMAND] --help' for more information on a command.
2025-10-28 10:31:31,468 - devsynth.adapters.cli.typer_adapter - INFO - Commands:
2025-10-28 10:31:31,468 - devsynth.adapters.cli.typer_adapter - INFO - Run
'devsynth [COMMAND] --help' for more information on a command.
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.cli.typer_adapter:logging_setup.py:615 Commands:
INFO     devsynth.adapters.cli.typer_adapter:logging_setup.py:615 Run 'devsynth
[COMMAND] --help' for more information on a command.
_________________________ test_parse_args_has_expected _________________________

self = <TyperGroup >, args = ['init', '--path', './test']
prog_name = 'python -m pytest', complete_var = None, standalone_mode = True
windows_expand_args = True
rich_markup_mode = <typer.models.DefaultPlaceholder object at 0x107dc5d00>
extra = {}, ctx = <click.core.Context object at 0x12ee3be00>

    def _main(
        self: click.Command,
        *,
        args: Optional[Sequence[str]] = None,
        prog_name: Optional[str] = None,
        complete_var: Optional[str] = None,
        standalone_mode: bool = True,
        windows_expand_args: bool = True,
        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,
        **extra: Any,
    ) -> Any:
        # Typer override, duplicated from click.main() to handle custom rich
exceptions
        # Verify that the environment is configured correctly, or reject
        # further execution to avoid a broken script.
        if args is None:
            args = sys.argv[1:]

            # Covered in Click tests
            if os.name == "nt" and windows_expand_args:  # pragma: no cover
                args = click.utils._expand_args(args)
        else:
            args = list(args)

        if prog_name is None:
            prog_name = click.utils._detect_program_name()

        # Process shell completion requests and exit early.
        self._main_shell_completion(extra, prog_name, complete_var)

        try:
            try:
                with self.make_context(prog_name, args, **extra) as ctx:
>                   rv = self.invoke(ctx)
                         ^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:193:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1828: in invoke
    sub_ctx = cmd.make_context(cmd_name, args, parent=ctx)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1186: in make_context
    self.parse_args(ctx, args)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1194: in parse_args
    opts, args, param_order = parser.parse_args(args=args)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/parser.py:307: in parse_args
    self._process_args_for_options(state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/parser.py:334: in _process_args_for_options
    self._process_opts(arg, state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/parser.py:484: in _process_opts
    self._match_long_opt(norm_long_opt, explicit_value, state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <click.parser._OptionParser object at 0x12e8f9ac0>, opt = '--path'
explicit_value = None
state = <click.parser._ParsingState object at 0x12e8f91c0>

    def _match_long_opt(
        self, opt: str, explicit_value: str | None, state: _ParsingState
    ) -> None:
        if opt not in self._long_opt:
            from difflib import get_close_matches

            possibilities = get_close_matches(opt, self._long_opt)
>           raise NoSuchOption(opt, possibilities=possibilities, ctx=self.ctx)
E           click.exceptions.NoSuchOption: No such option: --path

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/parser.py:368: NoSuchOption

During handling of the above exception, another exception occurred:

mock_build_app = <MagicMock name='build_app' id='5080392736'>
mock_warn = <MagicMock name='_warn_if_features_disabled' id='5081635664'>

    @patch("devsynth.adapters.cli.typer_adapter._warn_if_features_disabled")
    @patch("devsynth.adapters.cli.typer_adapter.build_app")
    @pytest.mark.medium
    def test_parse_args_has_expected(mock_build_app, mock_warn):
        """Test that parse_args calls the app with the provided arguments.

        ReqID: N/A"""
        mock_app = MagicMock()
        mock_build_app.return_value = mock_app
>       parse_args(["init", "--path", "./test"])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:156:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/cli/t
yper_adapter.py:878: in parse_args
    build_app()(args)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:313: in __call__
    return get_command(self)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1442: in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:765: in main
    return _main(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <TyperGroup >, args = ['init', '--path', './test']
prog_name = 'python -m pytest', complete_var = None, standalone_mode = True
windows_expand_args = True
rich_markup_mode = <typer.models.DefaultPlaceholder object at 0x107dc5d00>
extra = {}, ctx = <click.core.Context object at 0x12ee3be00>

    def _main(
        self: click.Command,
        *,
        args: Optional[Sequence[str]] = None,
        prog_name: Optional[str] = None,
        complete_var: Optional[str] = None,
        standalone_mode: bool = True,
        windows_expand_args: bool = True,
        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,
        **extra: Any,
    ) -> Any:
        # Typer override, duplicated from click.main() to handle custom rich
exceptions
        # Verify that the environment is configured correctly, or reject
        # further execution to avoid a broken script.
        if args is None:
            args = sys.argv[1:]

            # Covered in Click tests
            if os.name == "nt" and windows_expand_args:  # pragma: no cover
                args = click.utils._expand_args(args)
        else:
            args = list(args)

        if prog_name is None:
            prog_name = click.utils._detect_program_name()

        # Process shell completion requests and exit early.
        self._main_shell_completion(extra, prog_name, complete_var)

        try:
            try:
                with self.make_context(prog_name, args, **extra) as ctx:
                    rv = self.invoke(ctx)
                    if not standalone_mode:
                        return rv
                    # it's not safe to `ctx.exit(rv)` here!
                    # note that `rv` may actually contain data like "1" which
                    # has obvious effects
                    # more subtle case: `rv=[None, None]` can come out of
                    # chained commands which all returned `None` -- so it's not
                    # even always obvious that `rv` indicates success/failure
                    # by its truthiness/falsiness
                    ctx.exit()
            except EOFError as e:
                click.echo(file=sys.stderr)
                raise click.Abort() from e
            except KeyboardInterrupt as e:
                raise click.exceptions.Exit(130) from e
            except click.ClickException as e:
                if not standalone_mode:
                    raise
                # Typer override
                if rich and rich_markup_mode is not None:
                    from . import rich_utils

                    rich_utils.rich_format_error(e)
                else:
                    e.show()
                # Typer override end
>               sys.exit(e.exit_code)
E               SystemExit: 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:220: SystemExit
----------------------------- Captured stderr call -----------------------------
Usage: python -m pytest init [OPTIONS]
Try 'python -m pytest init --help' for help.
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│ No such option: --path                                                       │
╰──────────────────────────────────────────────────────────────────────────────╯
____________________________ test_run_cli_succeeds _____________________________

self = <TyperGroup >
args =
['tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_rai
ses',
'tests/unit/adapters/cli/test_ty...ub_adapter.py::test_fetch_github_issue',
'tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue', ...]
prog_name = 'python -m pytest', complete_var = None, standalone_mode = True
windows_expand_args = True
rich_markup_mode = <typer.models.DefaultPlaceholder object at 0x107dc5d00>
extra = {}, ctx = <click.core.Context object at 0x12e240e30>

    def _main(
        self: click.Command,
        *,
        args: Optional[Sequence[str]] = None,
        prog_name: Optional[str] = None,
        complete_var: Optional[str] = None,
        standalone_mode: bool = True,
        windows_expand_args: bool = True,
        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,
        **extra: Any,
    ) -> Any:
        # Typer override, duplicated from click.main() to handle custom rich
exceptions
        # Verify that the environment is configured correctly, or reject
        # further execution to avoid a broken script.
        if args is None:
            args = sys.argv[1:]

            # Covered in Click tests
            if os.name == "nt" and windows_expand_args:  # pragma: no cover
                args = click.utils._expand_args(args)
        else:
            args = list(args)

        if prog_name is None:
            prog_name = click.utils._detect_program_name()

        # Process shell completion requests and exit early.
        self._main_shell_completion(extra, prog_name, complete_var)

        try:
            try:
                with self.make_context(prog_name, args, **extra) as ctx:
>                   rv = self.invoke(ctx)
                         ^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:193:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1824: in invoke
    cmd_name, cmd, args = self.resolve_command(ctx, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1888: in resolve_command
    ctx.fail(_("No such command {name!r}.").format(name=original_cmd_name))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <click.core.Context object at 0x12e240e30>
message = "No such command
'tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_rais
es'."

    def fail(self, message: str) -> t.NoReturn:
        """Aborts the execution of the program with a specific error
        message.

        :param message: the error message to fail with.
        """
>       raise UsageError(message, self)
E       click.exceptions.UsageError: No such command
'tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_rais
es'.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:704: UsageError

During handling of the above exception, another exception occurred:

mock_build_app = <MagicMock name='build_app' id='5080397296'>
mock_warn = <MagicMock name='_warn_if_features_disabled' id='4511311504'>

    @patch("devsynth.adapters.cli.typer_adapter._warn_if_features_disabled")
    @patch("devsynth.adapters.cli.typer_adapter.build_app")
    @pytest.mark.medium
    def test_run_cli_succeeds(mock_build_app, mock_warn):
        """Test that run_cli calls the app.

        ReqID: N/A"""
        mock_app = MagicMock()
        mock_build_app.return_value = mock_app
>       run_cli()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:170:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/cli/t
yper_adapter.py:900: in run_cli
    build_app()()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:313: in __call__
    return get_command(self)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/click/core.py:1442: in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:765: in main
    return _main(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <TyperGroup >
args =
['tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_rai
ses',
'tests/unit/adapters/cli/test_ty...ub_adapter.py::test_fetch_github_issue',
'tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue', ...]
prog_name = 'python -m pytest', complete_var = None, standalone_mode = True
windows_expand_args = True
rich_markup_mode = <typer.models.DefaultPlaceholder object at 0x107dc5d00>
extra = {}, ctx = <click.core.Context object at 0x12e240e30>

    def _main(
        self: click.Command,
        *,
        args: Optional[Sequence[str]] = None,
        prog_name: Optional[str] = None,
        complete_var: Optional[str] = None,
        standalone_mode: bool = True,
        windows_expand_args: bool = True,
        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,
        **extra: Any,
    ) -> Any:
        # Typer override, duplicated from click.main() to handle custom rich
exceptions
        # Verify that the environment is configured correctly, or reject
        # further execution to avoid a broken script.
        if args is None:
            args = sys.argv[1:]

            # Covered in Click tests
            if os.name == "nt" and windows_expand_args:  # pragma: no cover
                args = click.utils._expand_args(args)
        else:
            args = list(args)

        if prog_name is None:
            prog_name = click.utils._detect_program_name()

        # Process shell completion requests and exit early.
        self._main_shell_completion(extra, prog_name, complete_var)

        try:
            try:
                with self.make_context(prog_name, args, **extra) as ctx:
                    rv = self.invoke(ctx)
                    if not standalone_mode:
                        return rv
                    # it's not safe to `ctx.exit(rv)` here!
                    # note that `rv` may actually contain data like "1" which
                    # has obvious effects
                    # more subtle case: `rv=[None, None]` can come out of
                    # chained commands which all returned `None` -- so it's not
                    # even always obvious that `rv` indicates success/failure
                    # by its truthiness/falsiness
                    ctx.exit()
            except EOFError as e:
                click.echo(file=sys.stderr)
                raise click.Abort() from e
            except KeyboardInterrupt as e:
                raise click.exceptions.Exit(130) from e
            except click.ClickException as e:
                if not standalone_mode:
                    raise
                # Typer override
                if rich and rich_markup_mode is not None:
                    from . import rich_utils

                    rich_utils.rich_format_error(e)
                else:
                    e.show()
                # Typer override end
>               sys.exit(e.exit_code)
E               SystemExit: 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/core.py:220: SystemExit
----------------------------- Captured stderr call -----------------------------
Usage: python -m pytest [OPTIONS] COMMAND [ARGS]...
Try 'python -m pytest --help' for help.
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│ No such command                                                              │
│ 'tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_ │
│ raises'.                                                                     │
╰──────────────────────────────────────────────────────────────────────────────╯
_____________________ test_dashboard_hook_option_registers _____________________

self = <MagicMock name='register_dashboard_hook' id='5069675008'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'register_dashboard_hook' to have been
called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

mock_reg = <MagicMock name='register_dashboard_hook' id='5069675008'>

    @patch("devsynth.adapters.cli.typer_adapter.register_dashboard_hook")
    @pytest.mark.medium
    def test_dashboard_hook_option_registers(mock_reg):
        """Global --dashboard-hook registers provided callback."""
        runner = CliRunner()
        path = "tests.unit.adapters.cli.test_typer_adapter:dummy_hook"
        result = runner.invoke(build_app(), ["--dashboard-hook", path])
        assert result.exit_code == 0
>       mock_reg.assert_called_once()
E       AssertionError: Expected 'register_dashboard_hook' to have been called
once. Called 0 times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli/tes
t_typer_adapter.py:200: AssertionError
________________________ test_store_and_search_succeeds ________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_store_and_search_succeeds0')
KuzuMemoryStoreClass = <class
'devsynth.adapters.kuzu_memory_store.KuzuMemoryStore'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fbd0bf0>

    @pytest.mark.medium
    def test_store_and_search_succeeds(tmp_path, KuzuMemoryStoreClass,
monkeypatch):
        """Test that store and search succeeds.

        ReqID: N/A"""
>       store = KuzuMemoryStoreClass(
            persist_directory=str(tmp_path), use_provider_system=True
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_ku
zu_memory_store.py:75:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.kuzu_memory_store.KuzuMemoryStore object at
0x12f9e86e0>
persist_directory =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_store_and_search_succeeds0'
use_provider_system = True, provider_type = None
collection_name = 'devsynth_artifacts'

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        use_provider_system: bool = True,
        provider_type: Optional[str] = None,
        collection_name: str = "devsynth_artifacts",
    ) -> None:
        """Initialize a KuzuMemoryStore with the given parameters.

        Args:
            persist_directory: Directory to store data in. If ``None``, uses
                the default path.
            use_provider_system: Whether to use the provider system for
                embeddings.
            provider_type: Type of provider to use for embeddings.
            collection_name: Name of the collection to use for vectors.
        """
        self._temp_dir: Optional[str] = None

        # Refresh settings to ensure environment overrides are respected
        current_settings = settings_module.get_settings()

        # Determine the base directory path with proper fallbacks
        base_directory = (
            persist_directory
            or getattr(current_settings, "kuzu_db_path",
settings_module.kuzu_db_path)
            or os.path.join(os.getcwd(), ".devsynth", "kuzu_store")
        )

        # Normalize, expand and apply any test isolation redirections
        normalized_path = os.path.abspath(os.path.expanduser(base_directory))
        redirected_path = settings_module.ensure_path_exists(normalized_path)

        # Determine embedded mode from configuration.  Some older versions of
        # the settings module may not expose ``kuzu_embedded`` directly, so use
        # ``getattr`` with a sensible default.
        use_embedded = getattr(
            current_settings,
            "kuzu_embedded",
            getattr(
                settings_module,
                "kuzu_embedded",
>               settings_module.DEFAULT_KUZU_EMBEDDED,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ),
        )
E       AttributeError: module 'devsynth.config.settings' has no attribute
'DEFAULT_KUZU_EMBEDDED'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:86: AttributeError
________________________ test_create_ephemeral_fallback ________________________

KuzuMemoryStoreClass = <class
'devsynth.adapters.kuzu_memory_store.KuzuMemoryStore'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb358e0>

    @pytest.mark.medium
    def test_create_ephemeral_fallback(KuzuMemoryStoreClass, monkeypatch):
        monkeypatch.setitem(sys.modules, "kuzu", None)
>       store = KuzuMemoryStoreClass.create_ephemeral()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_ku
zu_memory_store.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:389: in create_ephemeral
    store = cls(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.kuzu_memory_store.KuzuMemoryStore object at
0x12e70f560>
persist_directory =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/kuzu_fcaw4hcu'
use_provider_system = True, provider_type = None
collection_name = 'devsynth_artifacts'

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        use_provider_system: bool = True,
        provider_type: Optional[str] = None,
        collection_name: str = "devsynth_artifacts",
    ) -> None:
        """Initialize a KuzuMemoryStore with the given parameters.

        Args:
            persist_directory: Directory to store data in. If ``None``, uses
                the default path.
            use_provider_system: Whether to use the provider system for
                embeddings.
            provider_type: Type of provider to use for embeddings.
            collection_name: Name of the collection to use for vectors.
        """
        self._temp_dir: Optional[str] = None

        # Refresh settings to ensure environment overrides are respected
        current_settings = settings_module.get_settings()

        # Determine the base directory path with proper fallbacks
        base_directory = (
            persist_directory
            or getattr(current_settings, "kuzu_db_path",
settings_module.kuzu_db_path)
            or os.path.join(os.getcwd(), ".devsynth", "kuzu_store")
        )

        # Normalize, expand and apply any test isolation redirections
        normalized_path = os.path.abspath(os.path.expanduser(base_directory))
        redirected_path = settings_module.ensure_path_exists(normalized_path)

        # Determine embedded mode from configuration.  Some older versions of
        # the settings module may not expose ``kuzu_embedded`` directly, so use
        # ``getattr`` with a sensible default.
        use_embedded = getattr(
            current_settings,
            "kuzu_embedded",
            getattr(
                settings_module,
                "kuzu_embedded",
>               settings_module.DEFAULT_KUZU_EMBEDDED,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ),
        )
E       AttributeError: module 'devsynth.config.settings' has no attribute
'DEFAULT_KUZU_EMBEDDED'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:86: AttributeError
________________________ test_create_ephemeral_embedded ________________________

KuzuMemoryStoreClass = <class
'devsynth.adapters.kuzu_memory_store.KuzuMemoryStore'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fbd3140>

    @pytest.mark.medium
    def test_create_ephemeral_embedded(KuzuMemoryStoreClass, monkeypatch):
        class Database:
            def __init__(self, path):
                pass

        class Connection:
            def __init__(self, db):
                pass

            def execute(self, *args, **kwargs):
                return None

        fake = ModuleType("kuzu")
        fake.Database = Database
        fake.Connection = Connection
        monkeypatch.setitem(sys.modules, "kuzu", fake)
>       store = KuzuMemoryStoreClass.create_ephemeral()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_ku
zu_memory_store.py:115:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:389: in create_ephemeral
    store = cls(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.kuzu_memory_store.KuzuMemoryStore object at
0x12e2ccfe0>
persist_directory =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/kuzu_trbgx3kd'
use_provider_system = True, provider_type = None
collection_name = 'devsynth_artifacts'

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        use_provider_system: bool = True,
        provider_type: Optional[str] = None,
        collection_name: str = "devsynth_artifacts",
    ) -> None:
        """Initialize a KuzuMemoryStore with the given parameters.

        Args:
            persist_directory: Directory to store data in. If ``None``, uses
                the default path.
            use_provider_system: Whether to use the provider system for
                embeddings.
            provider_type: Type of provider to use for embeddings.
            collection_name: Name of the collection to use for vectors.
        """
        self._temp_dir: Optional[str] = None

        # Refresh settings to ensure environment overrides are respected
        current_settings = settings_module.get_settings()

        # Determine the base directory path with proper fallbacks
        base_directory = (
            persist_directory
            or getattr(current_settings, "kuzu_db_path",
settings_module.kuzu_db_path)
            or os.path.join(os.getcwd(), ".devsynth", "kuzu_store")
        )

        # Normalize, expand and apply any test isolation redirections
        normalized_path = os.path.abspath(os.path.expanduser(base_directory))
        redirected_path = settings_module.ensure_path_exists(normalized_path)

        # Determine embedded mode from configuration.  Some older versions of
        # the settings module may not expose ``kuzu_embedded`` directly, so use
        # ``getattr`` with a sensible default.
        use_embedded = getattr(
            current_settings,
            "kuzu_embedded",
            getattr(
                settings_module,
                "kuzu_embedded",
>               settings_module.DEFAULT_KUZU_EMBEDDED,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ),
        )
E       AttributeError: module 'devsynth.config.settings' has no attribute
'DEFAULT_KUZU_EMBEDDED'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:86: AttributeError
_________________ test_store_failure_raises_memory_store_error _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_store_failure_raises_memo0')
KuzuMemoryStoreClass = <class
'devsynth.adapters.kuzu_memory_store.KuzuMemoryStore'>

    @pytest.mark.medium
    def test_store_failure_raises_memory_store_error(tmp_path,
KuzuMemoryStoreClass):
>       store = KuzuMemoryStoreClass(
            persist_directory=str(tmp_path), use_provider_system=False
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_ku
zu_memory_store.py:124:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.kuzu_memory_store.KuzuMemoryStore object at
0x12f796600>
persist_directory =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_store_failure_raises_memo0'
use_provider_system = False, provider_type = None
collection_name = 'devsynth_artifacts'

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        use_provider_system: bool = True,
        provider_type: Optional[str] = None,
        collection_name: str = "devsynth_artifacts",
    ) -> None:
        """Initialize a KuzuMemoryStore with the given parameters.

        Args:
            persist_directory: Directory to store data in. If ``None``, uses
                the default path.
            use_provider_system: Whether to use the provider system for
                embeddings.
            provider_type: Type of provider to use for embeddings.
            collection_name: Name of the collection to use for vectors.
        """
        self._temp_dir: Optional[str] = None

        # Refresh settings to ensure environment overrides are respected
        current_settings = settings_module.get_settings()

        # Determine the base directory path with proper fallbacks
        base_directory = (
            persist_directory
            or getattr(current_settings, "kuzu_db_path",
settings_module.kuzu_db_path)
            or os.path.join(os.getcwd(), ".devsynth", "kuzu_store")
        )

        # Normalize, expand and apply any test isolation redirections
        normalized_path = os.path.abspath(os.path.expanduser(base_directory))
        redirected_path = settings_module.ensure_path_exists(normalized_path)

        # Determine embedded mode from configuration.  Some older versions of
        # the settings module may not expose ``kuzu_embedded`` directly, so use
        # ``getattr`` with a sensible default.
        use_embedded = getattr(
            current_settings,
            "kuzu_embedded",
            getattr(
                settings_module,
                "kuzu_embedded",
>               settings_module.DEFAULT_KUZU_EMBEDDED,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ),
        )
E       AttributeError: module 'devsynth.config.settings' has no attribute
'DEFAULT_KUZU_EMBEDDED'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:86: AttributeError
______________________ test_delete_returns_false_on_error ______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_delete_returns_false_on_e0')
KuzuMemoryStoreClass = <class
'devsynth.adapters.kuzu_memory_store.KuzuMemoryStore'>

    @pytest.mark.medium
    def test_delete_returns_false_on_error(tmp_path, KuzuMemoryStoreClass):
>       store = KuzuMemoryStoreClass(
            persist_directory=str(tmp_path), use_provider_system=False
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_ku
zu_memory_store.py:145:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.kuzu_memory_store.KuzuMemoryStore object at
0x12fd30980>
persist_directory =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_delete_returns_false_on_e0'
use_provider_system = False, provider_type = None
collection_name = 'devsynth_artifacts'

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        use_provider_system: bool = True,
        provider_type: Optional[str] = None,
        collection_name: str = "devsynth_artifacts",
    ) -> None:
        """Initialize a KuzuMemoryStore with the given parameters.

        Args:
            persist_directory: Directory to store data in. If ``None``, uses
                the default path.
            use_provider_system: Whether to use the provider system for
                embeddings.
            provider_type: Type of provider to use for embeddings.
            collection_name: Name of the collection to use for vectors.
        """
        self._temp_dir: Optional[str] = None

        # Refresh settings to ensure environment overrides are respected
        current_settings = settings_module.get_settings()

        # Determine the base directory path with proper fallbacks
        base_directory = (
            persist_directory
            or getattr(current_settings, "kuzu_db_path",
settings_module.kuzu_db_path)
            or os.path.join(os.getcwd(), ".devsynth", "kuzu_store")
        )

        # Normalize, expand and apply any test isolation redirections
        normalized_path = os.path.abspath(os.path.expanduser(base_directory))
        redirected_path = settings_module.ensure_path_exists(normalized_path)

        # Determine embedded mode from configuration.  Some older versions of
        # the settings module may not expose ``kuzu_embedded`` directly, so use
        # ``getattr`` with a sensible default.
        use_embedded = getattr(
            current_settings,
            "kuzu_embedded",
            getattr(
                settings_module,
                "kuzu_embedded",
>               settings_module.DEFAULT_KUZU_EMBEDDED,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ),
        )
E       AttributeError: module 'devsynth.config.settings' has no attribute
'DEFAULT_KUZU_EMBEDDED'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/kuzu_
memory_store.py:86: AttributeError
________________ test_create_provider_env_fallback_has_expected ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fbd1ee0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x12f6747d0>

    @pytest.mark.medium
    @pytest.mark.requires_resource("lmstudio")
    def test_create_provider_env_fallback_has_expected(monkeypatch, caplog):
        """ProviderFactory should fall back to LMStudio when OPENAI_API_KEY is
missing.

        ReqID: N/A"""
        caplog.set_level(logging.WARNING)
        get_provider_config.cache_clear()
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)
        monkeypatch.setenv("LM_STUDIO_ENDPOINT", "http://localhost:9999")
        provider = ProviderFactory.create_provider()
>       assert isinstance(provider, LMStudioProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x12f169be0>, LMStudioProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_factory.py:25: AssertionError
________________ test_explicit_openai_missing_key_raises_error _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f2a9400>

    @pytest.mark.medium
    @pytest.mark.requires_resource("openai")
    def test_explicit_openai_missing_key_raises_error(monkeypatch):
        """Explicitly requesting OpenAI without an API key should raise an
error.

        ReqID: N/A"""
        get_provider_config.cache_clear()
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)
        monkeypatch.setenv("LM_STUDIO_ENDPOINT", "http://localhost:9999")

        def _raise(*_args, **_kwargs):
            raise RuntimeError("boom")

        monkeypatch.setattr(
            "devsynth.adapters.provider_system.LMStudioProvider.__init__",
_raise
        )
>       with pytest.raises(ProviderError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'devsynth.exceptions.ProviderError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_factory.py:45: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:35,698 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
______________________ test_env_provider_openai_succeeds _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f2ab770>

    @pytest.mark.medium
    @pytest.mark.requires_resource("openai")
    def test_env_provider_openai_succeeds(monkeypatch):
        """Test that env provider openai succeeds.

        ReqID: N/A"""
        get_provider_config.cache_clear()
        monkeypatch.setenv("DEVSYNTH_PROVIDER", "openai")
        monkeypatch.setenv("OPENAI_API_KEY", "test-key")
        provider = ProviderFactory.create_provider()
>       assert isinstance(provider, OpenAIProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x12e0b6300>, OpenAIProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_factory_env_vars.py:21: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:35,708 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_____________________ test_env_provider_lmstudio_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f2a8110>

    @pytest.mark.medium
    @pytest.mark.requires_resource("lmstudio")
    def test_env_provider_lmstudio_succeeds(monkeypatch):
        """Test that env provider lmstudio succeeds.

        ReqID: N/A"""
        get_provider_config.cache_clear()
        monkeypatch.setenv("DEVSYNTH_PROVIDER", "lmstudio")
        monkeypatch.setenv("LM_STUDIO_ENDPOINT", "http://localhost:8888")
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)
        provider = ProviderFactory.create_provider()
>       assert isinstance(provider, LMStudioProvider)
E       assert False
E        +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x12fbd0ad0>, LMStudioProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_factory_env_vars.py:35: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:35,718 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_____________ test_display_command_help_markdown_renders_markdown ______________

recording_console = <console width=120 None>

    @pytest.mark.medium
    def test_display_command_help_markdown_renders_markdown(
        recording_console: RecordingConsole,
    ) -> None:
        """Markdown help should emit a Rich Markdown renderable."""

        command = next(iter(COMMANDS))
        display_command_help_markdown(command, recording_console)

        assert recording_console.rendered, "Console did not record any
renderables"
        first_renderable = recording_console.rendered[0]
        assert isinstance(first_renderable, Markdown)
>       assert command in first_renderable.source
                          ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'Markdown' object has no attribute 'source'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_help_rendering.py:96: AttributeError
----------------------------- Captured stdout call -----------------------------
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                                                         init
┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛


                                                      Description

Initialize a new project


                                                        Examples


 devsynth init



 devsynth init --wizard

______________________ test_completion_cmd_outputs_script ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f674ad0>

    @pytest.mark.medium
    def test_completion_cmd_outputs_script(monkeypatch):
        """completion_cmd outputs a completion script when not installing."""
        bridge = MagicMock()
        monkeypatch.setattr(
            "devsynth.adapters.cli.typer_adapter.typer_completion.get_completion
_script",
            lambda **_: "script",
        )
>       typer_adapter.completion_cmd(shell="bash", bridge=bridge)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.adapters.cli.typer_adapter' has no
attribute 'completion_cmd'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_completion_cmd.py:18: AttributeError
_____________________ test_completion_cmd_installs_script ______________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_completion_cmd_installs_s0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e29a840>

    @pytest.mark.medium
    def test_completion_cmd_installs_script(tmp_path, monkeypatch):
        """completion_cmd writes script to path when install=True."""
        bridge = MagicMock()
        target = tmp_path / "comp.sh"
        monkeypatch.setattr(
            "devsynth.adapters.cli.typer_adapter.typer_completion.get_completion
_script",
            lambda **_: "script",
        )
>       typer_adapter.completion_cmd(shell="bash", install=True, path=target,
bridge=bridge)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.adapters.cli.typer_adapter' has no
attribute 'completion_cmd'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_completion_cmd.py:31: AttributeError
________________________ test_config_warnings_succeeds _________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_config_warnings_succeeds0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f16cad0>

    @pytest.mark.medium
    def test_config_warnings_succeeds(tmp_path, monkeypatch):
        """Doctor command reports configuration issues when validation fails."""

        monkeypatch.setattr(doctor_cmd.sys, "version_info", (3, 11, 0))
        monkeypatch.setenv("OPENAI_API_KEY", "1")
        monkeypatch.setenv("ANTHROPIC_API_KEY", "1")

        config_dir = tmp_path / "cfg"
        config_dir.mkdir()
        (config_dir / "default.yml").write_text(
            "application: {name: App, version: '1.0'}\n"
        )

        real_spec = doctor_cmd.importlib.util.spec_from_file_location

        def fake_spec(name, location, *args, **kwargs):
            path = Path(__file__).parents[4] / "scripts" / "validate_config.py"
            return real_spec(name, path, *args, **kwargs)

        with (
            patch.object(
                doctor_cmd.importlib.util,
                "spec_from_file_location",
                side_effect=fake_spec,
            ),
>           patch.object(doctor_cmd, "load_config"),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(doctor_cmd.bridge, "print") as mock_print,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_config_validation.py:40:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12e898830>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module
'devsynth.application.cli.commands.doctor_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/doctor_cmd.py'> does not have the attribute 'load_config'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________________________ test_config_success_succeeds _________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_config_success_succeeds0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e2591c0>

    @pytest.mark.medium
    def test_config_success_succeeds(tmp_path, monkeypatch):
        """Doctor command reports success when all configs are valid."""

        monkeypatch.setattr(doctor_cmd.sys, "version_info", (3, 11, 0))
        monkeypatch.setenv("OPENAI_API_KEY", "1")
        monkeypatch.setenv("ANTHROPIC_API_KEY", "1")
        monkeypatch.chdir(tmp_path)

        config_dir = tmp_path / "cfg"
        config_dir.mkdir()
        for env in ["default", "development", "testing", "staging",
"production"]:
            (config_dir / f"{env}.yml").write_text(VALID_CONFIG)

        # Create expected project directories to avoid warnings
        for name in ("src", "tests", "docs"):
            (tmp_path / name).mkdir()

        real_spec = doctor_cmd.importlib.util.spec_from_file_location

        def fake_spec(name, location, *args, **kwargs):
            path = Path(__file__).parents[4] / "scripts" / "validate_config.py"
            return real_spec(name, path, *args, **kwargs)

        with (
            patch.object(
                doctor_cmd.importlib.util,
                "spec_from_file_location",
                side_effect=fake_spec,
            ),
>           patch.object(
                doctor_cmd,
                "load_config",
                return_value=SimpleNamespace(
                    features={"wsde_collaboration": False},
memory_store_type="memory"
                ),
            ),
            patch.object(doctor_cmd, "_find_project_config",
return_value=tmp_path),
            patch.object(doctor_cmd.bridge, "print") as mock_print,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_config_validation.py:114:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12fa86810>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module
'devsynth.application.cli.commands.doctor_cmd' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/doctor_cmd.py'> does not have the attribute 'load_config'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
________________________ test_retrospect_phase_succeeds ________________________

sample_project = ({'metadata': {'name': 'sample'}, 'structure': {'directories':
{'source': ['src'], 'tests': ['tests']}, 'type':
'singl...rivate/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitly
n/pytest-1440/test_retrospect_phase_succeeds0'))
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f169190>
mock_bridge = <MagicMock id='5087451456'>
mock_memory_manager = <MagicMock name='MemoryManager()' id='5074319264'>

    @pytest.mark.medium
    def test_retrospect_phase_succeeds(
        sample_project, monkeypatch, mock_bridge, mock_memory_manager
    ):
        """Test that retrospect phase succeeds.

        ReqID: N/A"""
        manifest, root = sample_project
        monkeypatch.chdir(root)
        expand_res = expand_phase(manifest)
        diff_res = differentiate_phase(manifest, expand_res)
        refine_res: RefinePhaseResult = refine_phase(manifest, diff_res)
        mock_memory_manager.reset_mock()
>       result: RetrospectPhaseResult = retrospect_phase(manifest, refine_res)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_ingest_phases.py:136:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

manifest = {'metadata': {'name': 'sample'}, 'structure': {'directories':
{'source': ['src'], 'tests': ['tests']}, 'type': 'single_package'}}
refine_results = {'detailed_plan': [], 'duration_seconds': 0,
'implementation_plan': [], 'optimized_plan': [], ...}
verbose = False

    def retrospect_phase(
        manifest: ManifestModel,
        refine_results: RefinePhaseResult,
        verbose: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
        memory_manager: Union[MemoryManager, None] = None,
        code_analyzer: Union[CodeAnalyzer, None] = None,
        wsde_team: Union[WSDETeam, None] = None,
    ) -> RetrospectPhaseResult:
        """Summarize results and suggest improvements."""

        start = time.perf_counter()

        bridge = bridge or DEFAULT_BRIDGE
        memory_manager = memory_manager or MemoryManager(
            adapters={"tinydb": TinyDBMemoryAdapter()}
        )
        analyzer = code_analyzer or CodeAnalyzer()
        wsde_team = wsde_team or WSDETeam(name="IngestCmdTeam")

        if verbose:
            bridge.print("  Generating retrospective report...")

        improvements = refine_results.get("relationships_created", 0)
        gaps = refine_results.get("outdated_items_archived", 0)

        learnings = cast(JSONValue, wsde_team.extract_learnings(refine_results,
True))
        patterns = cast(
            JSONValue,
            wsde_team.recognize_patterns(
                learnings,
                historical_context=memory_manager.retrieve_historical_patterns()
,
                code_analyzer=analyzer,
            ),
        )
        integrated = cast(
            JSONValue,
>           wsde_team.integrate_knowledge(learnings, patterns, memory_manager),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       TypeError: _integrate_knowledge() takes 3 positional arguments but 4
were given

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/ingest_cmd.py:654: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:37,846 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:31:37,846 - devsynth.domain.models.wsde_decision_making - INFO -
Generating diverse ideas for task: unknown
2025-10-28 10:31:37,846 - devsynth.domain.models.wsde_decision_making - WARNING
- No ideas generated by any agent
2025-10-28 10:31:37,847 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Generating diverse ideas for task: unknown
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - WARNING
- No ideas generated by any agent
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Creating comparison matrix for ideas
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Evaluating options
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Analyzing trade-offs between options
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Formulating decision criteria
2025-10-28 10:31:37,847 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Selecting best option
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Elaborating details for selected option
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Creating implementation plan
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Optimizing implementation plan
2025-10-28 10:31:37,847 - devsynth.domain.models.wsde_decision_making - INFO -
Performing quality assurance on implementation plan
2025-10-28 10:31:37,847 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Generating diverse ideas for task: unknown
WARNING  devsynth.domain.models.wsde_decision_making:logging_setup.py:615 No
ideas generated by any agent
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Generating diverse ideas for task: unknown
WARNING  devsynth.domain.models.wsde_decision_making:logging_setup.py:615 No
ideas generated by any agent
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Creating comparison matrix for ideas
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Evaluating options
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Analyzing trade-offs between options
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Formulating decision criteria
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Selecting best option
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Elaborating details for selected option
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Creating implementation plan
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Optimizing implementation plan
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615
Performing quality assurance on implementation plan
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
____________________ test_init_cmd_creates_config_succeeds _____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_init_cmd_creates_config_s0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f16a720>

    @pytest.mark.medium
    def test_init_cmd_creates_config_succeeds(tmp_path, monkeypatch):
        """Init command creates configuration when none exists."""

        printed = _run_init(tmp_path, monkeypatch)

        cfg_file = tmp_path / ".devsynth" / "project.yaml"
        if not cfg_file.exists():
            cfg_file = tmp_path / "pyproject.toml"

        data = _load_config(cfg_file)

        assert data["project_root"] == str(tmp_path)
        assert data["language"] == "python"
>       assert data["goals"] == "do stuff"
E       AssertionError: assert '' == 'do stuff'
E
E         - do stuff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_init_cmd.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
Project root directory [/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_init_cmd_creates_config_s0]/private/var/fol
                                                                               d
ers/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_init
                                                                               _
cmd_creates_config_s0                     Project root directory
[/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_init_cmd_creates_config_s0]/private/var/fol
                                                                               d
ers/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_init
                                                                               _
cmd_creates_config_s0
Primary language [python]python                               Primary language
[python]python
Project goals             Project goals
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘

























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘


























































Memory backend [memory]memory                             Memory backend
[memory]memory
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘





























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘






























































2025-10-28 10:31:38,230 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
------------------------------ Captured log call -------------------------------
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
_______________________ test_cli_rejects_invalid_target ________________________

    @pytest.mark.medium
    def test_cli_rejects_invalid_target() -> None:
        """run_tests_cmd target validation prints guidance before exiting (lines
214-229)."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "_configure_optional_providers",
return_value=None),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^
            patch.object(module, "increment_counter", return_value=None),
            patch.object(module, "run_tests") as mock_run,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:86:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13da44950>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute '_configure_optional_providers'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
________________________ test_cli_rejects_invalid_speed ________________________

    @pytest.mark.medium
    def test_cli_rejects_invalid_speed() -> None:
        """Invalid --speed entries surface remediation text
(run_tests_cmd.py:231-244)."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "_configure_optional_providers",
return_value=None),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^
            patch.object(module, "increment_counter", return_value=None),
            patch.object(module, "run_tests") as mock_run,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:105:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13da46bd0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute '_configure_optional_providers'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_cli_reports_disabled_coverage ______________________

    @pytest.mark.medium
    def test_cli_reports_disabled_coverage() -> None:
        """Disabled coverage flows emit remediation and exit 1
(run_tests_cmd.py:312-356)."""

        runner = CliRunner()
        app = build_app()

        with (
>           patch.object(module, "_configure_optional_providers",
return_value=None),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^
            patch.object(module, "increment_counter", return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages",
return_value=None),
            patch.object(module, "run_tests", return_value=(True, "Done")) as
mock_run,
            patch.object(
                module,
                "_coverage_instrumentation_status",
                return_value=(
                    False,
                    "pytest plugin autoload disabled without -p pytest_cov",
                ),
            ),
            patch.object(module, "coverage_artifacts_status") as mock_artifacts,
            patch.object(module, "enforce_coverage_threshold") as mock_enforce,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:124:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13d8b7ec0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10ac6ad40> does not
have the attribute '_configure_optional_providers'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ TestCollaborativeWSDETeam.test_initialization_succeeds ____________

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team.TestCollabora
tiveWSDETeam object at 0x11f4f83e0>

    @pytest.mark.medium
    def test_initialization_succeeds(self):
        """Test that the CollaborativeWSDETeam initializes correctly.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TestTeam")
        assert hasattr(team, "tracked_decisions")
>       assert hasattr(team, "subtasks")
E       AssertionError: assert False
E        +  where False =
hasattr(<devsynth.application.collaboration.wsde_team_extended.CollaborativeWSDE
Team object at 0x13d8d8620>, 'subtasks')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team.py:90: AssertionError
_____ TestCollaborativeWSDETeam.test_build_consensus_no_conflicts_succeeds _____

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team.TestCollabora
tiveWSDETeam object at 0x11f4f8470>
mock_agent_with_expertise = <function
mock_agent_with_expertise.<locals>._create_agent at 0x13da8c040>

    @pytest.mark.medium
    def test_build_consensus_no_conflicts_succeeds(self,
mock_agent_with_expertise):
        """Test building consensus when there are no conflicts.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="ConsensusTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["python", "backend"])
        agent2 = mock_agent_with_expertise("Agent2", ["javascript", "frontend"])
        agent3 = mock_agent_with_expertise("Agent3", ["security", "devops"])
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        task = {
            "id": "task1",
            "type": "decision_task",
            "description": "Choose a database technology",
            "options": [
                {"id": "option1", "name": "PostgreSQL"},
                {"id": "option2", "name": "MongoDB"},
                {"id": "option3", "name": "MySQL"},
            ],
        }

        def mock_get_messages(agent=None, filters=None):
            if agent == "Agent1" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent1",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer PostgreSQL",
                            "rationale": "It's robust and reliable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent2" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent2",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer PostgreSQL",
                            "rationale": "It has good performance",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent3" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent3",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer PostgreSQL",
                            "rationale": "It's secure and stable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            return []

        with patch.object(team, "get_messages", side_effect=mock_get_messages):
            with patch.object(team, "_identify_conflicts", return_value=[]):
>               consensus_result = team.build_consensus(task)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team.py:162:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:915: in build_consensus
    metadata_update = self._build_consensus_metadata(task_id, task,
base_outcome)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.collaboration.wsde_team_extended.CollaborativeWSDETeam
object at 0x12f787890>
task_id = 'task1'
task = {'description': 'Choose a database technology', 'id': 'task1', 'options':
[{'id': 'option1', 'name': 'PostgreSQL'}, {'id': 'option2', 'name': 'MongoDB'},
{'id': 'option3', 'name': 'MySQL'}], 'type': 'decision_task'}
outcome = {'explanation': "Partial consensus on option 'option1' with 0.0%
support after 3 rounds of discussion.", 'final_prefer...2': {'option1': 0.0,
'option2': 0.0, 'option3': 0.0}, 'Agent3': {'option1': 0.0, 'option2': 0.0,
'option3': 0.0}}, ...}

    def _build_consensus_metadata(
        self,
        task_id: str,
        task: Mapping[str, Any],
        outcome: ConsensusOutcome,
    ) -> Dict[str, Any]:
        """Create deterministic metadata annotations for a consensus outcome."""

>       opinions: Sequence[AgentOpinionRecord] = outcome.agent_opinions
                                                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'agent_opinions'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:980: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:39,801 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team ConsensusTeam
2025-10-28 10:31:39,801 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team ConsensusTeam
2025-10-28 10:31:39,801 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team ConsensusTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team ConsensusTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team ConsensusTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team ConsensusTeam
____ TestCollaborativeWSDETeam.test_build_consensus_with_conflicts_succeeds ____

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team.TestCollabora
tiveWSDETeam object at 0x11f4f8e00>
mock_agent_with_expertise = <function
mock_agent_with_expertise.<locals>._create_agent at 0x16cb6c180>

    @pytest.mark.medium
    def test_build_consensus_with_conflicts_succeeds(self,
mock_agent_with_expertise):
        """Test building consensus when there are conflicts.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="ConflictTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["python", "backend"], 8)
        agent2 = mock_agent_with_expertise("Agent2", ["javascript", "frontend"],
7)
        agent3 = mock_agent_with_expertise("Agent3", ["security", "devops"], 9)
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        task = {
            "id": "task1",
            "type": "decision_task",
            "description": "Choose a database technology",
            "options": [
                {"id": "option1", "name": "PostgreSQL"},
                {"id": "option2", "name": "MongoDB"},
                {"id": "option3", "name": "MySQL"},
            ],
        }

        def mock_get_messages(agent=None, filters=None):
            if agent == "Agent1" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent1",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer PostgreSQL",
                            "rationale": "It's robust and reliable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent2" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent2",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer MongoDB",
                            "rationale": "It's flexible and scalable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent3" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent3",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer PostgreSQL",
                            "rationale": "It's secure and stable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            return []

        conflicts = [
            ConflictRecord(
                conflict_id="conflict-0",
                task_id=task["id"],
                agent_a="Agent1",
                agent_b="Agent2",
                opinion_a="I prefer PostgreSQL",
                opinion_b="I prefer MongoDB",
                rationale_a="It's robust and reliable",
                rationale_b="It's flexible and scalable",
                severity_label="high",
                severity_score=0.8,
            )
        ]
        synthesis = SynthesisArtifact(
            text=(
                "After considering all perspectives, PostgreSQL is recommended
for "
                "its robustness, security, and stability."
            ),
            key_points=(
                "PostgreSQL is robust",
                "PostgreSQL is secure",
                "PostgreSQL is stable",
            ),
            readability_score={"flesch_reading_ease": 75.0},
        )
        with patch.object(team, "get_messages", side_effect=mock_get_messages):
            with patch.object(team, "_identify_conflicts",
return_value=conflicts):
                with patch.object(
                    team,
                    "_generate_conflict_resolution_synthesis",
                    return_value=synthesis,
                ):
>                   consensus_result = team.build_consensus(task)
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team.py:335:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:915: in build_consensus
    metadata_update = self._build_consensus_metadata(task_id, task,
base_outcome)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.collaboration.wsde_team_extended.CollaborativeWSDETeam
object at 0x12ff0ade0>
task_id = 'task1'
task = {'description': 'Choose a database technology', 'id': 'task1', 'options':
[{'id': 'option1', 'name': 'PostgreSQL'}, {'id': 'option2', 'name': 'MongoDB'},
{'id': 'option3', 'name': 'MySQL'}], 'type': 'decision_task'}
outcome = {'explanation': "Partial consensus on option 'option1' with 0.0%
support after 3 rounds of discussion.", 'final_prefer...2': {'option1': 0.0,
'option2': 0.0, 'option3': 0.0}, 'Agent3': {'option1': 0.0, 'option2': 0.0,
'option3': 0.0}}, ...}

    def _build_consensus_metadata(
        self,
        task_id: str,
        task: Mapping[str, Any],
        outcome: ConsensusOutcome,
    ) -> Dict[str, Any]:
        """Create deterministic metadata annotations for a consensus outcome."""

>       opinions: Sequence[AgentOpinionRecord] = outcome.agent_opinions
                                                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'agent_opinions'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:980: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:39,882 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team ConflictTeam
2025-10-28 10:31:39,882 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team ConflictTeam
2025-10-28 10:31:39,882 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team ConflictTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team ConflictTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team ConflictTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team ConflictTeam
_______ TestCollaborativeWSDETeam.test_tie_breaking_strategies_succeeds ________

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team.TestCollabora
tiveWSDETeam object at 0x11f4f97c0>
mock_agent_with_expertise = <function
mock_agent_with_expertise.<locals>._create_agent at 0x13ff0a660>

    @pytest.mark.medium
    def test_tie_breaking_strategies_succeeds(self, mock_agent_with_expertise):
        """Test tie-breaking strategies in voting.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TieBreakingTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["python"], 7)
        agent2 = mock_agent_with_expertise("Agent2", ["javascript"], 7)
        agent3 = mock_agent_with_expertise("Agent3", ["security"], 7)
        agent4 = mock_agent_with_expertise("Agent4", ["devops"], 7)
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        team.add_agent(agent4)
        task = {
            "id": "tie_decision",
            "type": "critical_decision",
            "description": "Choose a technology",
            "options": [
                {"id": "option1", "name": "Option 1"},
                {"id": "option2", "name": "Option 2"},
            ],
        }
        tie_result = team.force_voting_tie(task)
>       assert "task_id" in tie_result
               ^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: argument of type 'NoneType' is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team.py:442: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:39,957 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team TieBreakingTeam
2025-10-28 10:31:39,957 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team TieBreakingTeam
2025-10-28 10:31:39,957 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team TieBreakingTeam
2025-10-28 10:31:39,957 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent4 to team TieBreakingTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team TieBreakingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team TieBreakingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team TieBreakingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent4 to team TieBreakingTeam
__ TestCollaborativeWSDETeam.test_decision_tracking_and_explanation_succeeds ___

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team.TestCollabora
tiveWSDETeam object at 0x11f4f9ca0>
mock_agent_with_expertise = <function
mock_agent_with_expertise.<locals>._create_agent at 0x16cea5f80>

    @pytest.mark.medium
    def test_decision_tracking_and_explanation_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test decision tracking and explanation.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TrackingTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["python", "backend"], 8)
        agent2 = mock_agent_with_expertise("Agent2", ["javascript", "frontend"],
7)
        agent3 = mock_agent_with_expertise("Agent3", ["security", "devops"], 9)
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        task = {
            "id": "tracked_decision",
            "type": "decision_task",
            "description": "Choose a technology stack",
            "criticality": "high",
            "options": [
                {"id": "stack1", "name": "Stack 1"},
                {"id": "stack2", "name": "Stack 2"},
                {"id": "stack3", "name": "Stack 3"},
            ],
        }

        def mock_get_messages(agent=None, filters=None):
            if agent == "Agent1" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent1",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer Stack 1",
                            "rationale": "It's robust and reliable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent2" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent2",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer Stack 2",
                            "rationale": "It's flexible and scalable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            elif agent == "Agent3" and filters.get("type") == "opinion":
                return [
                    {
                        "sender": "Agent3",
                        "type": "opinion",
                        "content": {
                            "opinion": "I prefer Stack 1",
                            "rationale": "It's secure and stable",
                        },
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"task_id": task["id"]},
                    }
                ]
            return []

        with patch.object(team, "get_messages", side_effect=mock_get_messages):
            with patch.object(team, "_identify_conflicts", return_value=[]):
>               consensus_result = team.build_consensus(task)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team.py:553:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:915: in build_consensus
    metadata_update = self._build_consensus_metadata(task_id, task,
base_outcome)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.collaboration.wsde_team_extended.CollaborativeWSDETeam
object at 0x12f1a3d10>
task_id = 'tracked_decision'
task = {'criticality': 'high', 'description': 'Choose a technology stack', 'id':
'tracked_decision', 'options': [{'id': 'stack1', 'name': 'Stack 1'}, {'id':
'stack2', 'name': 'Stack 2'}, {'id': 'stack3', 'name': 'Stack 3'}], ...}
outcome = {'explanation': "Partial consensus on option 'stack1' with 0.0%
support after 3 rounds of discussion.", 'final_prefere...'Agent2': {'stack1':
0.0, 'stack2': 0.0, 'stack3': 0.0}, 'Agent3': {'stack1': 0.0, 'stack2': 0.0,
'stack3': 0.0}}, ...}

    def _build_consensus_metadata(
        self,
        task_id: str,
        task: Mapping[str, Any],
        outcome: ConsensusOutcome,
    ) -> Dict[str, Any]:
        """Create deterministic metadata annotations for a consensus outcome."""

>       opinions: Sequence[AgentOpinionRecord] = outcome.agent_opinions
                                                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'agent_opinions'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_extended.py:980: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:39,980 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent1 to team TrackingTeam
2025-10-28 10:31:39,980 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent2 to team TrackingTeam
2025-10-28 10:31:39,980 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent3 to team TrackingTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent1 to team TrackingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent2 to team TrackingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Agent3 to team TrackingTeam
___ TestCollaborativeWSDETeamTaskManagement.test_associate_subtasks_succeeds ___

self =
<tests.unit.application.collaboration.test_collaborative_wsde_team_task_manageme
nt.TestCollaborativeWSDETeamTaskManagement object at 0x118b5baa0>
mock_agent_with_expertise = <function
mock_agent_with_expertise.<locals>._create_agent at 0x16cb6d3a0>

    @pytest.mark.medium
    def test_associate_subtasks_succeeds(self, mock_agent_with_expertise):
        """Test associating subtasks with a main task.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TestTeam")
        main_task = {
            "id": "main_task",
            "type": "implementation_task",
            "title": "Implement a feature",
            "description": "Implement a feature",
        }
        subtasks = [
            {
                "id": "subtask1",
                "description": "Implement backend",
                "primary_expertise": "python",
            },
            {
                "id": "subtask2",
                "description": "Implement frontend",
                "primary_expertise": "javascript",
            },
        ]
>       team.associate_subtasks(main_task, subtasks)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_collaborative_wsde_team_task_management.py:296:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.collaboration.collaborative_wsde_team.CollaborativeWSDETea
m object at 0x13d8cf680>
main_task = {'description': 'Implement a feature', 'id': 'main_task', 'title':
'Implement a feature', 'type': 'implementation_task'}
subtasks = [{'description': 'Implement backend', 'id': 'subtask1',
'primary_expertise': 'python'}, {'description': 'Implement frontend', 'id':
'subtask2', 'primary_expertise': 'javascript'}]

    def associate_subtasks(
        self, main_task: TaskSpec, subtasks: Sequence[SubtaskSpec]
    ) -> None:
        """Associate subtasks with a main task."""

        context = cast(TaskManagementContext, self)
>       task_id = main_task.id
                  ^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_task_management.py:119: AttributeError
_
TestAgentCoordinatorPrimusSelection.test_primus_selection_and_consensus_fields_s
ucceeds _

self =
<tests.unit.application.collaboration.test_coordinator.TestAgentCoordinatorPrimu
sSelection object at 0x11f4faae0>

    @pytest.mark.medium
    def test_primus_selection_and_consensus_fields_succeeds(self):
        """Test that primus selection and consensus fields succeeds.

        ReqID: N/A"""
        consensus = ConsensusOutcome(
            consensus_id="consensus-1",
            task_id="task",
            method="consensus_synthesis",
            participants=("python", "javascript", "docs"),
            synthesis=SynthesisArtifact(text="final"),
        )
        with patch.object(
            self.coordinator.team, "build_consensus", return_value=consensus
        ):
            with patch.object(
                self.coordinator.team,
                "select_primus_by_expertise",
                wraps=self.coordinator.team.select_primus_by_expertise,
            ) as spy:
                task = {"team_task": True, "language": "python", "type":
"coding"}
                result = self.coordinator.delegate_task(task)
                spy.assert_called_once_with(task)
>       assert self.coordinator.team.get_primus() == self.python_agent
E       AssertionError: assert <MagicMock spec='Agent' id='5096968016'> ==
<MagicMock spec='Agent' id='5099273824'>
E        +  where <MagicMock spec='Agent' id='5096968016'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam
object at 0x12ff0a180>.get_primus
E        +      where <devsynth.domain.models.wsde_facade.WSDETeam object at
0x12ff0a180> =
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x12ff0b560>.team
E        +        where
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x12ff0b560> =
<tests.unit.application.collaboration.test_coordinator.TestAgentCoordinatorPrimu
sSelection object at 0x11f4faae0>.coordinator
E        +  and   <MagicMock spec='Agent' id='5099273824'> =
<tests.unit.application.collaboration.test_coordinator.TestAgentCoordinatorPrimu
sSelection object at 0x11f4faae0>.python_agent

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_coordinator.py:65: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:40,147 - devsynth.domain.models.wsde_core - INFO - Added agent
python to team AgentCoordinatorTeam
2025-10-28 10:31:40,147 - devsynth.domain.models.wsde_core - INFO - Added agent
javascript to team AgentCoordinatorTeam
2025-10-28 10:31:40,147 - devsynth.domain.models.wsde_core - INFO - Added agent
docs to team AgentCoordinatorTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
javascript to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent docs
to team AgentCoordinatorTeam
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:40,148 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'python', 'worker':
'javascript', 'supervisor': 'docs', 'designer': None, 'evaluator': None}
2025-10-28 10:31:40,148 - devsynth.domain.models.wsde_roles - INFO - Selected
javascript as primus based on expertise
2025-10-28 10:31:40,148 - devsynth.application.collaboration.coordinator - INFO
- Selected javascript as Primus based on task expertise
2025-10-28 10:31:40,148 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 23a39048-2f67-4dc8-b3af-7a80800373f0
2025-10-28 10:31:40,148 - devsynth.application.collaboration.coordinator - INFO
- Agent python (Primus) proposed a solution
2025-10-28 10:31:40,148 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 23a39048-2f67-4dc8-b3af-7a80800373f0
2025-10-28 10:31:40,148 - devsynth.application.collaboration.coordinator - INFO
- Agent javascript (Primus) proposed a solution
2025-10-28 10:31:40,148 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 23a39048-2f67-4dc8-b3af-7a80800373f0
2025-10-28 10:31:40,148 - devsynth.application.collaboration.coordinator - INFO
- Agent docs (Supervisor) proposed a solution
2025-10-28 10:31:40,149 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 3 solutions (method=consensus_synthesis)
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'python', 'worker': 'javascript',
'supervisor': 'docs', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
javascript as primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected javascript as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 23a39048-2f67-4dc8-b3af-7a80800373f0
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent python (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 23a39048-2f67-4dc8-b3af-7a80800373f0
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent javascript (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 23a39048-2f67-4dc8-b3af-7a80800373f0
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent docs (Supervisor) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 3 solutions (method=consensus_synthesis)
____________________ test_flush_memory_queue_waits_for_sync ____________________

    @pytest.mark.medium
    def test_flush_memory_queue_waits_for_sync():
        """Flushing memory queue should await async sync if available."""

        adapter = MagicMock(flush=MagicMock())
        mm = MemoryManager(adapters={"default": adapter})
        mm.wait_for_sync = MagicMock(return_value=asyncio.sleep(0))

        item = MemoryItem(id="m1", content={},
memory_type=MemoryType.TEAM_STATE)
        mm.queue_update("default", item)
>       flushed = flush_memory_queue(mm)
                  ^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_wsde_phase_transition_and_memory_flush.py:49:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x12e2c6240>

    def flush_memory_queue(memory_manager: Any) -> List[MemoryQueueEntry]:
        """Flush queued memory updates and return flushed items
deterministically.

        The sync manager maintains an internal queue protected by
``_queue_lock``.
        This helper snapshots the queue under that lock, performs a flush, waits
for
        any asynchronous synchronization to finish, and *does not* clear the
queue
        again afterwards. Clearing is handled by the sync manager itself during
the
        flush to avoid races with new updates being queued concurrently.

        Args:
            memory_manager: The memory manager instance coordinating updates.

        Returns:
            List of ``(store, MemoryItem)`` tuples that were flushed.
        """

        if not memory_manager or not hasattr(memory_manager, "sync_manager"):
            return []

        sync_manager = memory_manager.sync_manager
        lock = getattr(sync_manager, "_queue_lock", None)
        if lock:
            with lock:
                queue_snapshot: List[tuple[str, MemoryItem]] = list(
                    getattr(sync_manager, "_queue", [])
                )
        else:
            queue_snapshot = list(getattr(sync_manager, "_queue", []))

        normalized_queue: List[MemoryQueueEntry] = []
>       for store, item in queue_snapshot:
            ^^^^^^^^^^^
E       ValueError: too many values to unpack (expected 2)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/collaboration_memory_utils.py:86: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:40,419 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:31:40,419 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:31:40,420 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
___________ TestEDRRCoordinatorPhaseExecution.test_start_cycle_basic ___________

self = <test_core.TestEDRRCoordinatorPhaseExecution object at 0x118c070e0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpre8x7kd_')

    @pytest.mark.medium
    def test_start_cycle_basic(self, tmp_project_dir):
        """Test basic cycle execution."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:114: TypeError
____ TestEDRRCoordinatorPhaseExecution.test_start_cycle_with_error_handling ____

self = <test_core.TestEDRRCoordinatorPhaseExecution object at 0x118c07500>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmptciw_4di')

    @pytest.mark.medium
    def test_start_cycle_with_error_handling(self, tmp_project_dir):
        """Test cycle execution with error handling."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:149: TypeError
____________ TestEDRRCoordinatorMicroCycles.test_create_micro_cycle ____________

self = <test_core.TestEDRRCoordinatorMicroCycles object at 0x118c1d880>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpxpasz997')

    @pytest.mark.medium
    def test_create_micro_cycle(self, tmp_project_dir):
        """Test micro-cycle creation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:285: TypeError
_______ TestEDRRCoordinatorIntegration.test_edrr_cycle_with_micro_cycles _______

self = <test_core.TestEDRRCoordinatorIntegration object at 0x118c34cb0>
tmp_project_dir =
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpq_5mubzx')

    @pytest.mark.medium
    def test_edrr_cycle_with_micro_cycles(self, tmp_project_dir):
        """Test EDRR cycle with micro-cycle creation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer',
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:548: TypeError
_____________________ test_progress_to_phase_runs_succeeds _____________________

self = <MagicMock name='_execute_expand_phase' id='5069625856'>, args = ({},)
kwargs = {}
msg = "Expected '_execute_expand_phase' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected '_execute_expand_phase' to be called once.
Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16cc475c0>

    @pytest.mark.medium
    def test_progress_to_phase_runs_succeeds(coordinator):
        """Test that progress to phase runs succeeds.

        ReqID: N/A"""
        coordinator.task = {"description": "demo"}
        coordinator.cycle_id = "cid"
        coordinator._historical_data = []
        with patch.object(
            coordinator, "_execute_expand_phase", return_value={"done": True}
        ) as ex:
            coordinator.progress_to_phase(Phase.EXPAND)
>       ex.assert_called_once_with({})
E       AssertionError: Expected '_execute_expand_phase' to be called once.
Called 0 times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_phases_simple.py:56: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,381 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,382 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,382 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,382 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,382 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,383 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:41,388 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: demo
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: demo
______________________ test_execute_expand_phase_succeeds ______________________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x13dc2ca40>

    @pytest.mark.medium
    def test_execute_expand_phase_succeeds(coordinator):
        """Test that execute expand phase succeeds.

        ReqID: N/A"""
        coordinator.task = {"description": "demo"}
        coordinator.cycle_id = "cid"
        coordinator.memory_manager.retrieve_relevant_knowledge.return_value =
["k"]
        coordinator.wsde_team.generate_diverse_ideas.return_value = ["idea"]
        coordinator.code_analyzer.analyze_project_structure.return_value = {"f":
1}
        res = coordinator._execute_expand_phase({})
>       assert res["ideas"] == ["idea"]
               ^^^^^^^^^^^^
E       KeyError: 'ideas'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_phases_simple.py:72: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,440 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,440 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,440 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,440 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,441 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,441 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:41,441 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:31:41,442 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 1 ideas generated (recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 1 ideas generated (recursion depth: 0)
_______________ TestEDRRCoordinator.test_initialization_succeeds _______________

self = <tests.unit.application.edrr.test_edrr_coordinator.TestEDRRCoordinator
object at 0x11f59c380>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16cff7e60>

    @pytest.mark.medium
    def test_initialization_succeeds(self, coordinator):
        """Test that the coordinator is initialized correctly.

        ReqID: N/A"""
        assert coordinator.memory_manager is not None
        assert coordinator.wsde_team is not None
        assert coordinator.code_analyzer is not None
        assert coordinator.ast_transformer is not None
        assert coordinator.prompt_manager is not None
        assert coordinator.documentation_manager is not None
        assert coordinator._enable_enhanced_logging is True
>       assert coordinator._execution_traces == {}
E       AssertionError: assert {'phases': {}} == {}
E
E         Left contains 1 more item:
E         {'phases': {}}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_edrr_coordinator.py:160: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,511 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,511 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,511 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,511 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,511 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,511 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
_________ TestEDRRCoordinator.test_expand_phase_execution_has_expected _________

self = <tests.unit.application.edrr.test_edrr_coordinator.TestEDRRCoordinator
object at 0x11f59ce90>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16cf73fe0>
memory_manager = <MagicMock spec='MemoryManager' id='4357705456'>
wsde_team = <MagicMock spec='WSDETeam' id='5328515392'>
code_analyzer = <MagicMock spec='CodeAnalyzer' id='6123103216'>

    @pytest.mark.medium
    def test_expand_phase_execution_has_expected(
        self, coordinator, memory_manager, wsde_team, code_analyzer
    ):
        """Test executing the Expand phase.

        ReqID: N/A"""
        coordinator.task = {"description": "Test Task"}
        coordinator.cycle_id = "test-cycle-id"
        coordinator.current_phase = Phase.EXPAND
        results = coordinator._execute_expand_phase({})
        assert "wsde_brainstorm" in results
        assert "documentation" in results
        assert "completed" in results
        assert wsde_team.generate_diverse_ideas.call_count == 1
>       assert memory_manager.retrieve_relevant_knowledge.call_count == 1
E       AssertionError: assert 0 == 1
E        +  where 0 = <MagicMock name='mock.retrieve_relevant_knowledge'
id='5097451360'>.call_count
E        +    where <MagicMock name='mock.retrieve_relevant_knowledge'
id='5097451360'> = <MagicMock spec='MemoryManager'
id='4357705456'>.retrieve_relevant_knowledge

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_edrr_coordinator.py:189: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,559 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,559 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,559 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,559 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,559 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,559 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:41,559 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:31:41,560 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
___________ TestEDRRCoordinator.test_progress_to_phase_has_expected ____________

self = <tests.unit.application.edrr.test_edrr_coordinator.TestEDRRCoordinator
object at 0x11f59f170>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16cfef470>
memory_manager = <MagicMock spec='MemoryManager' id='6123247984'>

    @pytest.mark.medium
    def test_progress_to_phase_has_expected(self, coordinator, memory_manager):
        """Test progressing to a phase.

        ReqID: N/A"""
        coordinator.task = {"description": "Test Task"}
        coordinator.cycle_id = "test-cycle-id"
        coordinator._historical_data = []
        with patch.object(coordinator, "_execute_expand_phase"):
            coordinator.progress_to_phase(Phase.EXPAND)
            assert coordinator.current_phase == Phase.EXPAND
            assert memory_manager.store_with_edrr_phase.call_count >= 1
        coordinator.manifest = {}
        coordinator.manifest_parser = MagicMock()
        coordinator.manifest_parser.check_phase_dependencies.return_value = True
        with patch.object(coordinator, "_execute_differentiate_phase"):
            coordinator.progress_to_phase(Phase.DIFFERENTIATE)
            assert coordinator.current_phase == Phase.DIFFERENTIATE
            assert coordinator.manifest_parser.start_phase.call_count == 1
>           assert coordinator.manifest_parser.complete_phase.call_count == 1
E           AssertionError: assert 0 == 1
E            +  where 0 = <MagicMock name='mock.complete_phase'
id='6108136656'>.call_count
E            +    where <MagicMock name='mock.complete_phase' id='6108136656'> =
<MagicMock id='6123615968'>.complete_phase
E            +      where <MagicMock id='6123615968'> =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x16cfef470>.manifest_parser

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_edrr_coordinator.py:399: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,682 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,682 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,683 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,683 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,684 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,684 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:41,686 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Test Task
2025-10-28 10:31:41,687 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: Test Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Test Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: Test Task
_________________ TestEDRRCoordinator.test_full_cycle_succeeds _________________

self = <tests.unit.application.edrr.test_edrr_coordinator.TestEDRRCoordinator
object at 0x11f59d550>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16c139fd0>

    @pytest.mark.medium
    def test_full_cycle_succeeds(self, coordinator):
        """Test a full EDRR cycle.

        ReqID: N/A"""
        task = {"description": "Test Task", "requirements": ["Req 1", "Req 2"]}
        with (
            patch.object(
                coordinator, "_execute_expand_phase", return_value={"ideas": []}
            ),
            patch.object(
                coordinator,
                "_execute_differentiate_phase",
                return_value={"evaluated_options": []},
            ),
            patch.object(
                coordinator,
                "_execute_refine_phase",
                return_value={"implementation_plan": []},
            ),
            patch.object(
                coordinator, "_execute_retrospect_phase",
return_value={"learnings": []}
            ),
        ):
            coordinator.start_cycle(task)
            assert coordinator.current_phase == Phase.EXPAND
            coordinator.progress_to_phase(Phase.DIFFERENTIATE)
            assert coordinator.current_phase == Phase.DIFFERENTIATE
            coordinator.progress_to_phase(Phase.REFINE)
            assert coordinator.current_phase == Phase.REFINE
            coordinator.progress_to_phase(Phase.RETROSPECT)
            assert coordinator.current_phase == Phase.RETROSPECT
>           assert "EXPAND" in coordinator.results
E           AssertionError: assert 'EXPAND' in {}
E            +  where {} =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x16c139fd0>.results

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_edrr_coordinator.py:453: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:41,728 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:41,728 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:41,728 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:41,728 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:41,728 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:41,728 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:41,730 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Test Task
2025-10-28 10:31:41,730 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Test Task
2025-10-28 10:31:41,730 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: Test Task
2025-10-28 10:31:41,730 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to refine phase for task: Test Task
2025-10-28 10:31:41,730 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to retrospect phase for task: Test Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Test Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Test Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: Test Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to refine phase for task: Test Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to retrospect phase for task: Test Task
_______ TestEnhancedEDRRCoordinator.test_progress_to_phase_has_expected ________

self = <MagicMock name='_safe_store_with_edrr_phase' id='6123252160'>
args = ({'completeness': 0.8, 'consistency': 0.75, 'duration': 0.000476,
'quality': 0.85}, <MemoryType.EPISODIC: 'episodic'>, 'differentiate',
{'cycle_id': 'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type': 'PHASE_METRICS'})
kwargs = {}
expected = call({'duration': 0.000476, 'quality': 0.85, 'completeness': 0.8,
'consistency': 0.75}, <MemoryType.EPISODIC: 'episodic'>, 'differentiate',
{'cycle_id': 'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type': 'PHASE_METRICS'})
actual = call({'from': 'expand', 'to': 'differentiate'}, <MemoryType.EPISODIC:
'episodic'>, 'differentiate', {'cycle_id':
'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type': 'PHASE_TRANSITION'})
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16cbf7f60>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: _safe_store_with_edrr_phase({'duration': 0.000476,
'quality': 0.85, 'completeness': 0.8, 'consistency': 0.75},
<MemoryType.EPISODIC: 'episodic'>, 'differentiate', {'cycle_id':
'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type': 'PHASE_METRICS'})
E             Actual: _safe_store_with_edrr_phase({'from': 'expand', 'to':
'differentiate'}, <MemoryType.EPISODIC: 'episodic'>, 'differentiate',
{'cycle_id': 'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type':
'PHASE_TRANSITION'})

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self =
<tests.unit.application.edrr.test_edrr_coordinator_enhanced.TestEnhancedEDRRCoor
dinator object at 0x11f5ab9e0>
enhanced_coordinator =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x16cfa2ff0>
memory_manager = <MagicMock spec='MemoryManager' id='6123067136'>

    @pytest.mark.medium
    def test_progress_to_phase_has_expected(self, enhanced_coordinator,
memory_manager):
        """Test enhanced progress to phase with metrics collection.

        ReqID: N/A"""
        task = {
            "name": "Test Task",
            "description": "This is a test task",
            "requirements": ["req1", "req2"],
        }
        enhanced_coordinator.start_cycle(task)
        with patch.object(
            enhanced_coordinator, "_safe_store_with_edrr_phase"
        ) as mock_store:
            with patch(
                "devsynth.application.edrr.edrr_coordinator_enhanced."
                "collect_phase_metrics"
            ) as mock_collect_metrics:
                mock_collect_metrics.return_value = {
                    "duration": 10.5,
                    "quality": 0.85,
                    "completeness": 0.8,
                    "consistency": 0.75,
                }
                enhanced_coordinator.progress_to_phase(Phase.DIFFERENTIATE)
                assert enhanced_coordinator.current_phase == Phase.DIFFERENTIATE
                mock_collect_metrics.assert_called_once()
>               mock_store.assert_called_with(
                    mock_collect_metrics.return_value,
                    MemoryType.EPISODIC,
                    Phase.DIFFERENTIATE.value,
                    {
                        "cycle_id": enhanced_coordinator.cycle_id,
                        "type": "PHASE_METRICS",
                    },
                )
E               AssertionError: expected call not found.
E               Expected: _safe_store_with_edrr_phase({'duration': 0.000476,
'quality': 0.85, 'completeness': 0.8, 'consistency': 0.75},
<MemoryType.EPISODIC: 'episodic'>, 'differentiate', {'cycle_id':
'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type': 'PHASE_METRICS'})
E                 Actual: _safe_store_with_edrr_phase({'from': 'expand', 'to':
'differentiate'}, <MemoryType.EPISODIC: 'episodic'>, 'differentiate',
{'cycle_id': 'a17c9432-302d-4709-8a3a-d21298fa3b3c', 'type':
'PHASE_TRANSITION'})
E
E               pytest introspection follows:
E
E               Args:
E               assert ({'from': 'ex..._TRANSITION'}) ==
({'completene...ASE_METRICS'})
E
E                 At index 0 diff: {'from': 'expand', 'to': 'differentiate'} !=
{'duration': 0.000476, 'quality': 0.85, 'completeness': 0.8, 'consistency':
0.75}
E                 Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_edrr_coordinator_enhanced.py:218: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:42,351 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:42,352 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:42,352 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:42,352 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:42,352 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:42,352 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:42,354 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: This is a test task
2025-10-28 10:31:42,354 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: This is a test task
2025-10-28 10:31:42,354 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: This is a test task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: This is a test task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: This is a test task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: This is a test task
_________ test_execute_single_agent_task_stores_result_and_calls_agent _________

coordinator =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x16c197ce0>

    @pytest.mark.medium
    def
test_execute_single_agent_task_stores_result_and_calls_agent(coordinator):
        with patch.object(
            coordinator, "_get_llm_response", return_value="done"
        ) as mock_llm:
            result = coordinator.execute_single_agent_task(
                task={"foo": "bar"},
                agent_name="agent",
                phase=Phase.ANALYSIS,
                llm_prompt="hi",
            )
        mock_llm.assert_called_once()
>       coordinator.wsde_team.get_agent.assert_called_once_with("agent")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'function' object has no attribute
'assert_called_once_with'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_execute_single_agent_task.py:49: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:42,972 - devsynth.domain.models.wsde_core - INFO - Added agent
<MagicMock name='mock.name' id='6123642112'> to team team
2025-10-28 10:31:42,974 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:42,974 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:42,974 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:42,974 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:42,974 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:42,974 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
<MagicMock name='mock.name' id='6123642112'> to team team
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:42,975 - devsynth.application.edrr.edrr_phase_transitions -
WARNING - CoreValues is not iterable, using default values
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.edrr.edrr_phase_transitions:logging_setup.py:615
CoreValues is not iterable, using default values
_____________ TestManifestParser.test_complete_execution_succeeds ______________

self = <tests.unit.application.edrr.test_manifest_parser.TestManifestParser
object at 0x11f635280>
manifest_parser_with_manifest =
<devsynth.application.edrr.manifest_parser.ManifestParser object at 0x13dc82db0>

    @pytest.mark.medium
    def test_complete_execution_succeeds(self, manifest_parser_with_manifest):
        """Test completing execution tracking.

        ReqID: N/A"""
        manifest_parser_with_manifest.start_execution()
        manifest_parser_with_manifest.start_phase(Phase.EXPAND)
        manifest_parser_with_manifest.complete_phase(Phase.EXPAND)
        manifest_parser_with_manifest.start_phase(Phase.DIFFERENTIATE)
        manifest_parser_with_manifest.complete_phase(Phase.DIFFERENTIATE)
        manifest_parser_with_manifest.start_phase(Phase.REFINE)
        manifest_parser_with_manifest.complete_phase(Phase.REFINE)
        manifest_parser_with_manifest.start_phase(Phase.RETROSPECT)
        manifest_parser_with_manifest.complete_phase(Phase.RETROSPECT)
        trace = manifest_parser_with_manifest.complete_execution()
        assert trace["end_time"] is not None
        assert trace["duration"] is not None
>       assert trace["completed"] is True
E       assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_manifest_parser.py:552: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:43,353 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:43,354 - devsynth.application.edrr.manifest_parser - INFO -
Started execution tracking for manifest: test-manifest
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Started phase expand
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Dependencies now met for phase differentiate
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Completed phase expand in 0.00 seconds
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO - All
dependencies met for phase differentiate
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Started phase differentiate
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Dependencies now met for phase refine
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Completed phase differentiate in 0.00 seconds
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO - All
dependencies met for phase refine
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Started phase refine
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Dependencies now met for phase retrospect
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Completed phase refine in 0.00 seconds
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO - All
dependencies met for phase retrospect
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Started phase retrospect
2025-10-28 10:31:43,355 - devsynth.application.edrr.manifest_parser - INFO -
Completed phase retrospect in 0.00 seconds
2025-10-28 10:31:43,356 - devsynth.application.edrr.manifest_parser - WARNING -
Phase analysis not completed
2025-10-28 10:31:43,356 - devsynth.application.edrr.manifest_parser - WARNING -
Phase implementation not completed
2025-10-28 10:31:43,356 - devsynth.application.edrr.manifest_parser - WARNING -
Phase refinement not completed
2025-10-28 10:31:43,356 - devsynth.application.edrr.manifest_parser - INFO -
Completed execution in 0.00 seconds, all phases completed: False
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Started
execution tracking for manifest: test-manifest
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Started
phase expand
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Dependencies now met for phase differentiate
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Completed phase expand in 0.00 seconds
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 All
dependencies met for phase differentiate
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Started
phase differentiate
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Dependencies now met for phase refine
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Completed phase differentiate in 0.00 seconds
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 All
dependencies met for phase refine
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Started
phase refine
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Dependencies now met for phase retrospect
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Completed phase refine in 0.00 seconds
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 All
dependencies met for phase retrospect
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Started
phase retrospect
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Completed phase retrospect in 0.00 seconds
WARNING  devsynth.application.edrr.manifest_parser:logging_setup.py:615 Phase
analysis not completed
WARNING  devsynth.application.edrr.manifest_parser:logging_setup.py:615 Phase
implementation not completed
WARNING  devsynth.application.edrr.manifest_parser:logging_setup.py:615 Phase
refinement not completed
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615
Completed execution in 0.00 seconds, all phases completed: False
_____________________ test_auto_phase_progression_succeeds _____________________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x13da62e40>

    @pytest.mark.medium
    def test_auto_phase_progression_succeeds(coordinator):
        """Test that auto phase progression succeeds.

        ReqID: N/A"""
        with (
            patch.object(
                coordinator, "_execute_expand_phase",
return_value={"phase_complete": True}
            ),
            patch.object(
                coordinator,
                "_execute_differentiate_phase",
                return_value={"phase_complete": True},
            ),
            patch.object(
                coordinator, "_execute_refine_phase",
return_value={"phase_complete": True}
            ),
            patch.object(
                coordinator,
                "_execute_retrospect_phase",
                return_value={"phase_complete": True},
            ),
        ):
            coordinator.start_cycle({"description": "Task"})
>           assert coordinator.current_phase == Phase.RETROSPECT
E           AssertionError: assert <Phase.EXPAND: 'expand'> ==
<Phase.RETROSPECT: 'retrospect'>
E            +  where <Phase.EXPAND: 'expand'> =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x13da62e40>.current_phase
E            +  and   <Phase.RETROSPECT: 'retrospect'> = Phase.RETROSPECT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_progression.py:80: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:43,681 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:43,682 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:43,682 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:43,682 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:43,682 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:43,682 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:43,683 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Task
2025-10-28 10:31:43,683 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Task
____________ test_result_aggregation_after_full_cycle_has_expected _____________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x13db6cf20>

    @pytest.mark.medium
    def test_result_aggregation_after_full_cycle_has_expected(coordinator):
        """All phase results should be aggregated after auto progression.

        ReqID: N/A"""
        with (
            patch.object(
                coordinator,
                "_execute_expand_phase",
                return_value={"expand": True, "phase_complete": True},
            ),
            patch.object(
                coordinator,
                "_execute_differentiate_phase",
                return_value={"differentiate": True, "phase_complete": True},
            ),
            patch.object(
                coordinator,
                "_execute_refine_phase",
                return_value={"refine": True, "phase_complete": True},
            ),
            patch.object(
                coordinator,
                "_execute_retrospect_phase",
                return_value={"retrospect": True, "phase_complete": True},
            ),
        ):
            coordinator.start_cycle({"description": "Task"})
        aggregated = coordinator.results.get("AGGREGATED", {})
>       assert aggregated["expand"]["expand"] is True
               ^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'expand'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_progression.py:134: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:43,717 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:43,717 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:43,717 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:43,718 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:43,718 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:43,718 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:43,726 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Task
2025-10-28 10:31:43,726 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Task
________________ test_progress_to_phase_auto_recursion_succeeds ________________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12ff00da0>

    @pytest.mark.medium
    def test_progress_to_phase_auto_recursion_succeeds(coordinator):
        """Test that progress to phase auto recursion succeeds.

        ReqID: N/A"""
        coordinator.task = {"description": "t"}
        coordinator.cycle_id = "cid"
        coordinator.auto_phase_transitions = True
        phase_iter = iter([Phase.DIFFERENTIATE, Phase.REFINE, Phase.RETROSPECT])
        with (
            patch.object(
                coordinator, "_execute_expand_phase",
return_value={"phase_complete": True}
            ) as ex,
            patch.object(
                coordinator,
                "_execute_differentiate_phase",
                return_value={"phase_complete": True},
            ) as diff,
            patch.object(
                coordinator, "_execute_refine_phase",
return_value={"phase_complete": True}
            ) as ref,
            patch.object(
                coordinator,
                "_execute_retrospect_phase",
                return_value={"phase_complete": True},
            ) as ret,
            patch.object(
                coordinator,
                "_decide_next_phase",
                side_effect=lambda: next(phase_iter, None),
            ),
        ):
            coordinator.progress_to_phase(Phase.EXPAND)
        assert coordinator.current_phase == Phase.RETROSPECT
>       assert ex.called and diff.called and ref.called and ret.called
E       AssertionError: assert (False)
E        +  where False = <MagicMock name='_execute_expand_phase'
id='5099228288'>.called

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_progress_recursion.py:70: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:43,777 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:43,777 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:43,777 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:43,777 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:43,778 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:43,778 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:43,783 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: t
2025-10-28 10:31:43,784 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: t
2025-10-28 10:31:43,784 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to refine phase for task: t
2025-10-28 10:31:43,785 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to retrospect phase for task: t
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: t
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: t
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to refine phase for task: t
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to retrospect phase for task: t
_ TestRecursiveEDRRCoordinator.test_micro_edrr_within_expand_phase_has_expected
_

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6b9760>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16c05ec60>

    @pytest.mark.medium
    def test_micro_edrr_within_expand_phase_has_expected(self, coordinator):
        """Test micro-EDRR cycles within the Expand phase.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Macro Task"})
        original_execute = coordinator._execute_expand_phase

        def mock_execute_expand(context=None):
            coordinator.create_micro_cycle(
                {"description": "Brainstorm Ideas"}, Phase.EXPAND
            )
            coordinator.create_micro_cycle(
                {"description": "Research Existing Solutions"}, Phase.EXPAND
            )
            coordinator.create_micro_cycle(
                {"description": "Analyze Requirements"}, Phase.EXPAND
            )
            return original_execute(context)

        coordinator._execute_expand_phase = mock_execute_expand
        coordinator.execute_current_phase()
>       assert len(coordinator.child_cycles) == 3
E       assert 6 == 3
E        +  where 6 =
len([<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12f65a150>, <devsynth.application.edrr.coordi...Coordinator object at
0x12f6ab3b0>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x11f484110>])
E        +    where [<devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12f65a150>, <devsynth.application.edrr.coordi...Coordinator object
at 0x12f6ab3b0>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x11f484110>] =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x16c05ec60>.child_cycles

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:226: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,394 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,394 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,394 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,394 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,394 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,394 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,395 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Macro Task
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Macro Task
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,396 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,396 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,396 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,396 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,396 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Brainstorm Ideas
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Brainstorm Ideas
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 40e84a51-a400-4c81-81cc-639c7c625690 at
recursion depth 1
2025-10-28 10:31:44,396 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,397 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,397 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,397 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,397 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,397 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Research Existing Solutions
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Research Existing Solutions
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 067df188-b67e-40ca-b38a-bd6ac4e3b966 at
recursion depth 1
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,397 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,398 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,398 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,398 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,398 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,398 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,398 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,398 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Analyze Requirements
2025-10-28 10:31:44,398 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Analyze Requirements
2025-10-28 10:31:44,398 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 65e205e4-1fa9-47f9-a1cf-adbb395b4a56 at
recursion depth 1
2025-10-28 10:31:44,398 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase expand: Mock object has no attribute 'agents'
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.EXPAND
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,399 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,399 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,399 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,399 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,399 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,399 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,400 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Brainstorm Ideas
2025-10-28 10:31:44,400 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Brainstorm Ideas
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID e8788296-fab8-4fe2-8c6c-49889960be76 at
recursion depth 1
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,401 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,401 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,401 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,401 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,401 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,401 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,402 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Research Existing Solutions
2025-10-28 10:31:44,402 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Research Existing Solutions
2025-10-28 10:31:44,404 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID abbe6cea-03c4-4bf3-a386-f4d51eaec597 at
recursion depth 1
2025-10-28 10:31:44,404 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,405 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,405 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,405 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,405 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,406 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,406 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,406 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,406 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Analyze Requirements
2025-10-28 10:31:44,406 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Analyze Requirements
2025-10-28 10:31:44,410 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID ac27a000-2d6e-4b2d-a8c9-8d39f560b516 at
recursion depth 1
2025-10-28 10:31:44,410 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:31:44,411 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Macro Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Macro Task
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Brainstorm Ideas
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Brainstorm Ideas
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 40e84a51-a400-4c81-81cc-639c7c625690 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Research Existing Solutions
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Research Existing Solutions
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 067df188-b67e-40ca-b38a-bd6ac4e3b966 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Analyze Requirements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Analyze Requirements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 65e205e4-1fa9-47f9-a1cf-adbb395b4a56 at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase expand: Mock object has no attribute 'agents'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.EXPAND
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Brainstorm Ideas
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Brainstorm Ideas
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID e8788296-fab8-4fe2-8c6c-49889960be76 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Research Existing Solutions
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Research Existing Solutions
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID abbe6cea-03c4-4bf3-a386-f4d51eaec597 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Analyze Requirements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Analyze Requirements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID ac27a000-2d6e-4b2d-a8c9-8d39f560b516 at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
_
TestRecursiveEDRRCoordinator.test_micro_edrr_within_differentiate_phase_has_expe
cted _

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6b9d30>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12ee6aed0>

    @pytest.mark.medium
    def test_micro_edrr_within_differentiate_phase_has_expected(self,
coordinator):
        """Test micro-EDRR cycles within the Differentiate phase.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Macro Task"})
        coordinator.progress_to_phase(Phase.DIFFERENTIATE)
        original_execute = coordinator._execute_differentiate_phase

        def mock_execute_differentiate(context=None):
            coordinator.create_micro_cycle(
                {"description": "Compare Approaches"}, Phase.DIFFERENTIATE
            )
            coordinator.create_micro_cycle(
                {"description": "Evaluate Trade-offs"}, Phase.DIFFERENTIATE
            )
            coordinator.create_micro_cycle(
                {"description": "Select Best Approach"}, Phase.DIFFERENTIATE
            )
            return original_execute(context)

        coordinator._execute_differentiate_phase = mock_execute_differentiate
        coordinator.execute_current_phase()
>       assert len(coordinator.child_cycles) == 3
E       assert 6 == 3
E        +  where 6 =
len([<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12f6c7b30>, <devsynth.application.edrr.coordi...Coordinator object at
0x13dae6240>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12f1d3080>])
E        +    where [<devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12f6c7b30>, <devsynth.application.edrr.coordi...Coordinator object
at 0x13dae6240>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12f1d3080>] =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12ee6aed0>.child_cycles

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:258: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,438 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,438 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,438 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,438 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,438 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,438 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,440 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Macro Task
2025-10-28 10:31:44,440 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Macro Task
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: Macro Task
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,441 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,442 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,442 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,442 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,442 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,442 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,442 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Compare Approaches
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Compare Approaches
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID b50a2472-7954-4be2-b840-ed3f3152a277 at
recursion depth 1
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,443 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,444 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,444 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,444 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,444 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,444 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,444 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,444 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,444 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,444 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,444 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,445 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Evaluate Trade-offs
2025-10-28 10:31:44,445 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Evaluate Trade-offs
2025-10-28 10:31:44,445 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 62e9982d-924a-420b-a674-1f9df737a967 at
recursion depth 1
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,446 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,446 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,446 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,446 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,447 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,447 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,447 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,447 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Select Best Approach
2025-10-28 10:31:44,448 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Select Best Approach
2025-10-28 10:31:44,448 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 0f182b83-0426-4015-a601-a77e69596696 at
recursion depth 1
2025-10-28 10:31:44,448 - devsynth.application.edrr.coordinator.core - INFO -
Executing Differentiate phase (recursion depth: 0)
2025-10-28 10:31:44,449 - devsynth.application.edrr.coordinator.core - INFO -
Differentiate phase completed with 2 options evaluated (recursion depth: 0)
2025-10-28 10:31:44,449 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase differentiate: Mock object has no attribute 'agents'
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.DIFFERENTIATE
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,450 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,450 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,450 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,450 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,451 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,451 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,451 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,451 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Compare Approaches
2025-10-28 10:31:44,451 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Compare Approaches
2025-10-28 10:31:44,456 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID ec133b88-b284-45e5-a8d0-4e54ef43cd8f at
recursion depth 1
2025-10-28 10:31:44,456 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,457 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,457 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,457 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,458 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,458 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,458 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,458 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,458 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Evaluate Trade-offs
2025-10-28 10:31:44,459 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Evaluate Trade-offs
2025-10-28 10:31:44,461 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 7cdca152-6bf6-4d3f-aba7-76c9999a3147 at
recursion depth 1
2025-10-28 10:31:44,461 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,461 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,461 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,461 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,462 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,462 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,462 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,462 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,462 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,462 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,462 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,462 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,462 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,463 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Select Best Approach
2025-10-28 10:31:44,463 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Select Best Approach
2025-10-28 10:31:44,467 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 4bff5178-480c-4961-812f-d37385d18aa1 at
recursion depth 1
2025-10-28 10:31:44,468 - devsynth.application.edrr.coordinator.core - INFO -
Executing Differentiate phase (recursion depth: 0)
2025-10-28 10:31:44,468 - devsynth.application.edrr.coordinator.core - INFO -
Differentiate phase completed with 2 options evaluated (recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Macro Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Macro Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: Macro Task
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Compare Approaches
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Compare Approaches
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID b50a2472-7954-4be2-b840-ed3f3152a277 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Evaluate Trade-offs
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Evaluate Trade-offs
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 62e9982d-924a-420b-a674-1f9df737a967 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Select Best Approach
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Select Best Approach
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 0f182b83-0426-4015-a601-a77e69596696 at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Differentiate phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Differentiate phase completed with 2 options evaluated (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase differentiate: Mock object has no attribute 'agents'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.DIFFERENTIATE
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Compare Approaches
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Compare Approaches
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID ec133b88-b284-45e5-a8d0-4e54ef43cd8f at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Evaluate Trade-offs
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Evaluate Trade-offs
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 7cdca152-6bf6-4d3f-aba7-76c9999a3147 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Select Best Approach
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Select Best Approach
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 4bff5178-480c-4961-812f-d37385d18aa1 at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Differentiate phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Differentiate phase completed with 2 options evaluated (recursion depth: 0)
_ TestRecursiveEDRRCoordinator.test_micro_edrr_within_refine_phase_has_expected
_

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6ba300>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12efe53d0>

    @pytest.mark.medium
    def test_micro_edrr_within_refine_phase_has_expected(self, coordinator):
        """Test micro-EDRR cycles within the Refine phase.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Macro Task"})
        coordinator.progress_to_phase(Phase.REFINE)
        original_execute = coordinator._execute_refine_phase

        def mock_execute_refine(context=None):
            coordinator.create_micro_cycle(
                {"description": "Implement Solution"}, Phase.REFINE
            )
            coordinator.create_micro_cycle(
                {"description": "Optimize Performance"}, Phase.REFINE
            )
            coordinator.create_micro_cycle(
                {"description": "Ensure Quality"}, Phase.REFINE
            )
            return original_execute(context)

        coordinator._execute_refine_phase = mock_execute_refine
        coordinator.execute_current_phase()
>       assert len(coordinator.child_cycles) == 3
E       assert 6 == 3
E        +  where 6 =
len([<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12f606d20>, <devsynth.application.edrr.coordi...Coordinator object at
0x10cca9280>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12e09ede0>])
E        +    where [<devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12f606d20>, <devsynth.application.edrr.coordi...Coordinator object
at 0x10cca9280>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12e09ede0>] =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12efe53d0>.child_cycles

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:290: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,496 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,496 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,497 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,497 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,497 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,497 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Macro Task
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Macro Task
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to refine phase for task: Macro Task
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,498 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Implement Solution
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Implement Solution
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 6bcd793f-3650-4196-a12a-49d8816c276d at
recursion depth 1
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,499 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,499 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Optimize Performance
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Optimize Performance
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 892e9405-d4ae-470a-a4c3-b7b46041441f at
recursion depth 1
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,500 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,500 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,500 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,500 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,500 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Ensure Quality
2025-10-28 10:31:44,500 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Ensure Quality
2025-10-28 10:31:44,501 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID dec5915d-15d5-4e3f-9591-8189c82a3eac at
recursion depth 1
2025-10-28 10:31:44,501 - devsynth.application.edrr.coordinator.core - INFO -
Executing Refine phase (recursion depth: 0)
2025-10-28 10:31:44,501 - devsynth.application.edrr.coordinator.core - INFO -
Refine phase completed with implementation plan created (recursion depth: 0)
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase refine: Mock object has no attribute 'agents'
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.REFINE
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,502 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,503 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,503 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,503 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,503 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,503 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,503 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,503 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Implement Solution
2025-10-28 10:31:44,503 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Implement Solution
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 5ed59078-6fae-4339-bff0-b1628c7bfe4e at
recursion depth 1
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,504 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,504 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,504 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,504 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,504 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,504 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,505 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Optimize Performance
2025-10-28 10:31:44,505 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Optimize Performance
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID d1f4c70b-f6a5-4155-b2a1-580852a1c234 at
recursion depth 1
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,506 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,506 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,506 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,506 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,506 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Ensure Quality
2025-10-28 10:31:44,506 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Ensure Quality
2025-10-28 10:31:44,507 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 30487912-f7fd-4c35-8d10-11db1c28882e at
recursion depth 1
2025-10-28 10:31:44,507 - devsynth.application.edrr.coordinator.core - INFO -
Executing Refine phase (recursion depth: 0)
2025-10-28 10:31:44,508 - devsynth.application.edrr.coordinator.core - INFO -
Refine phase completed with implementation plan created (recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Macro Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Macro Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to refine phase for task: Macro Task
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Implement Solution
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Implement Solution
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 6bcd793f-3650-4196-a12a-49d8816c276d at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Optimize Performance
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Optimize Performance
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 892e9405-d4ae-470a-a4c3-b7b46041441f at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Ensure Quality
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Ensure Quality
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID dec5915d-15d5-4e3f-9591-8189c82a3eac at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Refine phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Refine
phase completed with implementation plan created (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase refine: Mock object has no attribute 'agents'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.REFINE
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Implement Solution
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Implement Solution
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 5ed59078-6fae-4339-bff0-b1628c7bfe4e at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Optimize Performance
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Optimize Performance
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID d1f4c70b-f6a5-4155-b2a1-580852a1c234 at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Ensure Quality
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Ensure Quality
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 30487912-f7fd-4c35-8d10-11db1c28882e at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Refine phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Refine
phase completed with implementation plan created (recursion depth: 0)
_
TestRecursiveEDRRCoordinator.test_micro_edrr_within_retrospect_phase_has_expecte
d _

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6ba900>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12f79c9b0>

    @pytest.mark.medium
    def test_micro_edrr_within_retrospect_phase_has_expected(self, coordinator):
        """Test micro-EDRR cycles within the Retrospect phase.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Macro Task"})
        coordinator.progress_to_phase(Phase.RETROSPECT)
        original_execute = coordinator._execute_retrospect_phase

        def mock_execute_retrospect(context=None):
            coordinator.create_micro_cycle(
                {"description": "Extract Learnings"}, Phase.RETROSPECT
            )
            coordinator.create_micro_cycle(
                {"description": "Identify Patterns"}, Phase.RETROSPECT
            )
            coordinator.create_micro_cycle(
                {"description": "Generate Improvements"}, Phase.RETROSPECT
            )
            return original_execute(context)

        coordinator._execute_retrospect_phase = mock_execute_retrospect
        coordinator.execute_current_phase()
>       assert len(coordinator.child_cycles) == 3
E       assert 6 == 3
E        +  where 6 =
len([<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12faf5490>, <devsynth.application.edrr.coordi...Coordinator object at
0x12facb2f0>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12fac9790>])
E        +    where [<devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12faf5490>, <devsynth.application.edrr.coordi...Coordinator object
at 0x12facb2f0>, <devsynth.application.edrr.coordinator.core.EDRRCoordinator
object at 0x12fac9790>] =
<devsynth.application.edrr.coordinator.core.EDRRCoordinator object at
0x12f79c9b0>.child_cycles

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:322: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,536 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,536 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,537 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,537 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,537 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,537 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,540 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Macro Task
2025-10-28 10:31:44,542 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Macro Task
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to retrospect phase for task: Macro Task
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,543 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,544 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,544 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,544 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,544 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,544 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,544 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Extract Learnings
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Extract Learnings
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID e2121887-1018-4593-890d-de924ed6dacd at
recursion depth 1
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,545 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,546 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,546 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,546 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,546 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,546 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,546 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Identify Patterns
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Identify Patterns
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID ce68c181-6959-48ea-b9e8-a024984c933c at
recursion depth 1
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,547 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,548 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,548 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,548 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,548 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,548 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,548 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,548 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,549 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,549 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,549 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Generate Improvements
2025-10-28 10:31:44,549 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Generate Improvements
2025-10-28 10:31:44,550 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID d50381e6-9eb0-47a6-8a39-855f1c60948b at
recursion depth 1
2025-10-28 10:31:44,550 - devsynth.application.edrr.coordinator.core - INFO -
Executing Retrospect phase (recursion depth: 0)
2025-10-28 10:31:44,550 - devsynth.application.edrr.coordinator.core - INFO -
Generating final report for EDRR cycle (recursion depth: 0)
2025-10-28 10:31:44,551 - devsynth.application.edrr.coordinator.core - INFO -
Final report generated for cycle 037f1f04-43f1-428b-8c54-2d038e9a5f0d (recursion
depth: 0)
2025-10-28 10:31:44,551 - devsynth.application.edrr.coordinator.core - INFO -
Retrospect phase completed with learnings extracted and final report generated
(recursion depth: 0)
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase retrospect: Mock object has no attribute 'agents'
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.RETROSPECT
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,553 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,553 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,553 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,553 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,553 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,553 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,554 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Extract Learnings
2025-10-28 10:31:44,554 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Extract Learnings
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 59b27508-a8cc-466f-bae9-4bcd196dff4d at
recursion depth 1
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,555 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,555 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,555 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,555 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,555 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Identify Patterns
2025-10-28 10:31:44,555 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Identify Patterns
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID bef18719-9101-4265-99c1-2a66865b428f at
recursion depth 1
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,556 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,557 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,557 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,557 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,557 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,558 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,558 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,558 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,558 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Generate Improvements
2025-10-28 10:31:44,559 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Generate Improvements
2025-10-28 10:31:44,561 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID 1af422f7-d716-4a3c-b7af-34e200118f03 at
recursion depth 1
2025-10-28 10:31:44,561 - devsynth.application.edrr.coordinator.core - INFO -
Executing Retrospect phase (recursion depth: 0)
2025-10-28 10:31:44,562 - devsynth.application.edrr.coordinator.core - INFO -
Generating final report for EDRR cycle (recursion depth: 0)
2025-10-28 10:31:44,562 - devsynth.application.edrr.coordinator.core - INFO -
Final report generated for cycle 037f1f04-43f1-428b-8c54-2d038e9a5f0d (recursion
depth: 0)
2025-10-28 10:31:44,563 - devsynth.application.edrr.coordinator.core - INFO -
Retrospect phase completed with learnings extracted and final report generated
(recursion depth: 0)
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Macro Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Macro Task
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to retrospect phase for task: Macro Task
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Extract Learnings
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Extract Learnings
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID e2121887-1018-4593-890d-de924ed6dacd at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Identify Patterns
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Identify Patterns
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID ce68c181-6959-48ea-b9e8-a024984c933c at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Generate Improvements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Generate Improvements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID d50381e6-9eb0-47a6-8a39-855f1c60948b at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Retrospect phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Generating final report for EDRR cycle (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Final
report generated for cycle 037f1f04-43f1-428b-8c54-2d038e9a5f0d (recursion
depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Retrospect phase completed with learnings extracted and final report generated
(recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase retrospect: Mock object has no attribute 'agents'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from Mock object has no attribute 'agents' in phase
Phase.RETROSPECT
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Extract Learnings
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Extract Learnings
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 59b27508-a8cc-466f-bae9-4bcd196dff4d at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Identify Patterns
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Identify Patterns
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID bef18719-9101-4265-99c1-2a66865b428f at recursion depth
1
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Generate Improvements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Generate Improvements
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID 1af422f7-d716-4a3c-b7af-34e200118f03 at recursion depth
1
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Retrospect phase (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Generating final report for EDRR cycle (recursion depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Final
report generated for cycle 037f1f04-43f1-428b-8c54-2d038e9a5f0d (recursion
depth: 0)
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Retrospect phase completed with learnings extracted and final report generated
(recursion depth: 0)
_
TestRecursiveEDRRCoordinator.test_decide_next_phase_phase_complete_has_expected
_

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6c2780>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16cf29940>

    @pytest.mark.medium
    def test_decide_next_phase_phase_complete_has_expected(self, coordinator):
        """_decide_next_phase should transition when phase is complete.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Task"})
>       coordinator.results[Phase.EXPAND.name]["phase_complete"] = True
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'EXPAND'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:507: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,792 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,792 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,792 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,792 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,792 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,792 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,794 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Task
2025-10-28 10:31:44,794 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Task
___ TestRecursiveEDRRCoordinator.test_decide_next_phase_timeout_has_expected ___

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6c2d80>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x12ed0b1d0>

    @pytest.mark.medium
    def test_decide_next_phase_timeout_has_expected(self, coordinator):
        """_decide_next_phase should transition after timeout.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Task"})
>       coordinator.results[Phase.EXPAND.name].pop("phase_complete", None)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'EXPAND'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:517: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,816 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,816 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,816 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,816 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,816 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,816 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,817 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Task
2025-10-28 10:31:44,817 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Task
_
TestRecursiveEDRRCoordinator.test_decide_next_phase_no_transition_returns_expect
ed_result _

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6c3380>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16d0a5940>

    @pytest.mark.medium
    def test_decide_next_phase_no_transition_returns_expected_result(self,
coordinator):
        """_decide_next_phase should return None when conditions not met.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "Task"})
>       coordinator.results[Phase.EXPAND.name].pop("phase_complete", None)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'EXPAND'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:531: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,842 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,842 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,842 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,842 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,843 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,843 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,845 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Task
2025-10-28 10:31:44,845 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Task
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Task
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Task
_ TestRecursiveEDRRCoordinator.test_micro_cycle_updates_parent_results_succeeds
_

self =
<tests.unit.application.edrr.test_recursive_edrr_coordinator.TestRecursiveEDRRCo
ordinator object at 0x11f6ccbf0>
coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x16ca322a0>

    @pytest.mark.medium
    def test_micro_cycle_updates_parent_results_succeeds(self, coordinator):
        """Child cycle aggregation should refresh parent data.

        ReqID: N/A"""
        coordinator.start_cycle({"description": "macro"})
        micro = coordinator.create_micro_cycle({"description": "child"},
Phase.EXPAND)
        with patch.object(
            micro, "_execute_differentiate_phase", return_value={"analysis":
"ok"}
        ):
            micro.progress_to_phase(Phase.DIFFERENTIATE)
        entry = coordinator.results[Phase.EXPAND.name]["micro_cycle_results"][
            micro.cycle_id
        ]
>       assert entry[Phase.DIFFERENTIATE.name] == {"analysis": "ok"}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'DIFFERENTIATE'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_recursive_edrr_coordinator.py:622: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:44,940 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,940 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,940 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,940 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,940 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,940 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: macro
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: macro
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.2
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.5
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.9
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.8
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.7
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.85
2025-10-28 10:31:44,941 - devsynth.application.edrr.coordinator.core - WARNING -
Invalid threshold 'None'; using default 0.3
2025-10-28 10:31:44,941 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:31:44,941 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:31:44,941 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:31:44,942 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:31:44,942 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:31:44,942 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 1)
2025-10-28 10:31:44,942 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: child
2025-10-28 10:31:44,942 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: child
2025-10-28 10:31:44,942 - devsynth.application.edrr.coordinator.core - INFO -
Created micro-EDRR cycle with ID fa3a4062-62b1-48ef-ae8e-ed1a93467a79 at
recursion depth 1
2025-10-28 10:31:44,942 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to differentiate phase for task: child
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: macro
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: macro
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.2
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.5
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.9
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.8
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.7
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.85
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 Invalid
threshold 'None'; using default 0.3
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 1)
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: child
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: child
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Created
micro-EDRR cycle with ID fa3a4062-62b1-48ef-ae8e-ed1a93467a79 at recursion depth
1
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to differentiate phase for task: child
___________ test_import_lmstudio_provider_without_lmstudio_succeeds ____________

    @pytest.mark.medium
    def test_import_lmstudio_provider_without_lmstudio_succeeds():
        """Importing providers should succeed without lmstudio installed.

        ReqID: LMSTUDIO-2"""
        # Remove modules from sys.modules to force reimport
        sys.modules.pop("devsynth.application.llm.providers", None)
        sys.modules.pop("devsynth.application.llm.lmstudio_provider", None)
        sys.modules.pop("lmstudio", None)

        # Set lmstudio module to None to simulate it not being available
        sys.modules["lmstudio"] = None

        providers =
importlib.import_module("devsynth.application.llm.providers")
>       assert providers.LMStudioProvider is None
E       AssertionError: assert <class
'devsynth.application.llm.lmstudio_provider.LMStudioProvider'> is None
E        +  where <class
'devsynth.application.llm.lmstudio_provider.LMStudioProvider'> = <module
'devsynth.application.llm.providers' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'>.LMStudioProvider

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_import_without_lmstudio.py:21: AssertionError
__________ test_factory_missing_lmstudio_provider_raises_clear_error ___________

    @pytest.mark.medium
    def test_factory_missing_lmstudio_provider_raises_clear_error():
        """Requesting LM Studio provider without SDK yields a helpful error.

        ReqID: LMSTUDIO-3"""
        # Remove modules from sys.modules to force reimport
        sys.modules.pop("devsynth.application.llm.providers", None)
        sys.modules.pop("devsynth.application.llm.lmstudio_provider", None)
        sys.modules.pop("lmstudio", None)

        # Set lmstudio module to None to simulate it not being available
        sys.modules["lmstudio"] = None

        providers =
importlib.import_module("devsynth.application.llm.providers")
>       with pytest.raises(
            providers.ValidationError, match="LMStudio provider is unavailable"
        ):
E       Failed: DID NOT RAISE <class
'devsynth.application.llm.providers.ValidationError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_import_without_lmstudio.py:39: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:45,226 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: LMStudio support requires the
'lmstudio' package. Install it with 'pip install lmstudio' or use 'poetry
install --extras "llm"'.
2025-10-28 10:31:45,249 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:31:45,251 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:31:45,251 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:31:45,253 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:31:45,254 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: LMStudio support requires the
'lmstudio' package. Install it with 'pip install lmstudio' or use 'poetry
install --extras "llm"'.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______ TestGraphMemoryAdapter.test_research_artifact_traversal_and_reload ______

self =
<tests.unit.application.memory.test_graph_memory_adapter.TestGraphMemoryAdapter
object at 0x1192cdc10>
temp_dir = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw'

    @pytest.mark.medium
    def test_research_artifact_traversal_and_reload(self, temp_dir):
        """Research artefacts persist and participate in traversals."""

        adapter = EnhancedGraphMemoryAdapter(base_path=temp_dir,
use_rdflib_store=True)

        item_one = MemoryItem(
            id="node1",
            content="Baseline requirement",
            memory_type=MemoryType.DOCUMENTATION,
            metadata={},
        )
        item_two = MemoryItem(
            id="node2",
            content="Derived implementation",
            memory_type=MemoryType.CODE,
            metadata={"related_to": "node1"},
        )

        adapter.store(item_one)
        adapter.store(item_two)

        artifact_path = Path(temp_dir) / "research.txt"
        artifact_path.write_text("Graph traversal with research nodes")

        evidence_hash = adapter.compute_evidence_hash(artifact_path)
        summary = adapter.summarize_artifact(artifact_path)
        assert summary.startswith("Graph traversal")

        artifact = ResearchArtifact(
            title="Traversal Paper",
            summary=summary,
            citation_url="file://" + str(artifact_path),
            evidence_hash=evidence_hash,
            published_at=datetime.datetime.now(datetime.timezone.utc),
            supports=[item_two.id],
            derived_from=[item_one.id],
            archive_path=str(artifact_path),
            metadata={"roles": ("Research Lead",)},
        )

        artifact_id = adapter.store_research_artifact(artifact)

        without_research = adapter.traverse_graph(item_one.id, 2)
>       assert without_research == {item_two.id}
E       AssertionError: assert set() == {'node2'}
E
E         Extra items in the right set:
E         'node2'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_graph_memory_adapter.py:207: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:45,702 - devsynth.application.memory.rdflib_store - WARNING -
Failed to initialize tokenizer: Network access disabled during tests. Token
counting will be approximate.
2025-10-28 10:31:45,703 - devsynth.application.memory.rdflib_store - INFO - No
existing RDF graph found at
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/memory.ttl
2025-10-28 10:31:45,703 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Graph Memory
Adapter initialized with RDFLibStore integration
2025-10-28 10:31:45,705 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
2025-10-28 10:31:45,707 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
2025-10-28 10:31:45,707 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Stored memory
item with ID node1 in Graph Memory Adapter
2025-10-28 10:31:45,710 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
2025-10-28 10:31:45,710 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Stored memory
item with ID node2 in Graph Memory Adapter
2025-10-28 10:31:45,713 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
2025-10-28 10:31:45,713 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Traversed
graph from node1 to depth 2 and found 0 nodes
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.memory.rdflib_store:logging_setup.py:615 Failed to
initialize tokenizer: Network access disabled during tests. Token counting will
be approximate.
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 No
existing RDF graph found at
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Graph Memory Adapter initialized with RDFLibStore integration
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Stored memory item with ID node1 in Graph Memory Adapter
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Stored memory item with ID node2 in Graph Memory Adapter
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwl8sh4zw/graph_memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Traversed graph from node1 to depth 2 and found 0 nodes
___________ TestGraphMemoryAdapter.test_context_aware_query_succeeds ___________

self =
<tests.unit.application.memory.test_graph_memory_adapter.TestGraphMemoryAdapter
object at 0x1192d6570>
populated_router = <devsynth.application.memory.query_router.QueryRouter object
at 0x16c298590>

    @pytest.mark.medium
    def test_context_aware_query_succeeds(self, populated_router):
        """Context-aware query should incorporate context into search.

        ReqID: N/A"""
        manager = populated_router.memory_manager
        manager.adapters["tinydb"].store(
            MemoryItem(
                id="ctx", content="apple location:home",
memory_type=MemoryType.CODE
            )
        )
        manager.adapters["document"].store(
            MemoryItem(
                id="ctx2", content="apple location:home",
memory_type=MemoryType.CODE
            )
        )
        results = populated_router.context_aware_query("apple", {"location":
"home"})
>       assert len(results["tinydb"]) == 1
                   ^^^^^^^^^^^^^^^^^
E       KeyError: 'tinydb'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_graph_memory_adapter.py:722: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:45,960 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:31:45,960 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb, document, vector
2025-10-28 10:31:45,960 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:31:45,961 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID vec in Vector Memory Adapter
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb, document, vector
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID vec in Vector Memory Adapter
___________ TestGraphMemoryAdapter.test_query_router_route_succeeds ____________

self =
<tests.unit.application.memory.test_graph_memory_adapter.TestGraphMemoryAdapter
object at 0x1192d6ae0>
populated_router = <devsynth.application.memory.query_router.QueryRouter object
at 0x16c298e90>

    @pytest.mark.medium
    def test_query_router_route_succeeds(self, populated_router):
        """Exercise the router.route method for various strategies.

        ReqID: N/A"""
        direct = populated_router.route("apple", store="tinydb",
strategy="direct")
>       assert len(direct) == 1
E       AssertionError: assert 2 == 1
E        +  where 2 = len({'records': [MemoryRecord(item=MemoryItem(id='tiny',
content='apple tinydb', memory_type=<MemoryType.CODE: 'code'>,
me...tetime.datetime(2025, 10, 28, 10, 31, 45, 991137)), similarity=None,
source='tinydb', metadata={})], 'store': 'tinydb'})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_graph_memory_adapter.py:738: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:45,990 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:31:45,991 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb, document, vector
2025-10-28 10:31:45,991 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:31:45,991 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID vec in Vector Memory Adapter
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb, document, vector
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID vec in Vector Memory Adapter
_ TestGraphMemoryAdapter.test_store_and_retrieve_with_edrr_phase_has_expected __

self =
<tests.unit.application.memory.test_graph_memory_adapter.TestGraphMemoryAdapter
object at 0x1192d7050>
basic_adapter =
<devsynth.application.memory.adapters.graph_memory_adapter.GraphMemoryAdapter
object at 0x16c29b920>

    @pytest.mark.medium
    def test_store_and_retrieve_with_edrr_phase_has_expected(self,
basic_adapter):
        """Test storing and retrieving items with EDRR phase.

        ReqID: N/A"""
        # Create test items with EDRR phase metadata
        item1 = MemoryItem(
            id="",
            content={"key": "value1"},
            memory_type=MemoryType.CODE,
            metadata={"edrr_phase": "EXPAND", "source": "test1"},
        )
        item2 = MemoryItem(
            id="",
            content={"key": "value2"},
            memory_type=MemoryType.CODE,
            metadata={"edrr_phase": "DIFFERENTIATE", "source": "test2"},
        )

        # Store the items
        item1_id = basic_adapter.store(item1)
        item2_id = basic_adapter.store(item2)

        # Retrieve the items to verify they were stored correctly
        retrieved_item1 = basic_adapter.retrieve(item1_id)
        retrieved_item2 = basic_adapter.retrieve(item2_id)

        # Verify the retrieved items match the original items
        assert retrieved_item1 is not None
        assert retrieved_item2 is not None
        assert retrieved_item1.content == {"key": "value1"}
        assert retrieved_item2.content == {"key": "value2"}
        assert retrieved_item1.metadata.get("edrr_phase") == "EXPAND"
        assert retrieved_item2.metadata.get("edrr_phase") == "DIFFERENTIATE"

        # Collect all items in the graph
        all_items = []
        for s, p, o in basic_adapter.graph.triples(
            (None, RDF.type, DEVSYNTH.MemoryItem)
        ):
            item = basic_adapter._triples_to_memory_item(s)
            if item:
                all_items.append(item)

        # Find items matching specific criteria
        matching_items1 = []
        for item in all_items:
            if (
                (
                    hasattr(item.memory_type, "value")
                    and item.memory_type.value == "CODE"
                    or str(item.memory_type) == "CODE"
                )
                and item.metadata.get("edrr_phase") == "EXPAND"
                and (item.metadata.get("source") == "test1")
            ):
                matching_items1.append(item)

        matching_items2 = []
        for item in all_items:
            if (
                (
                    hasattr(item.memory_type, "value")
                    and item.memory_type.value == "CODE"
                    or str(item.memory_type) == "CODE"
                )
                and item.metadata.get("edrr_phase") == "DIFFERENTIATE"
                and (item.metadata.get("source") == "test2")
            ):
                matching_items2.append(item)

        # Test the retrieve_with_edrr_phase method
        result1 = basic_adapter.retrieve_with_edrr_phase(
            item_type="CODE", edrr_phase="EXPAND", metadata={"source": "test1"}
        )
        result2 = basic_adapter.retrieve_with_edrr_phase(
            item_type="CODE", edrr_phase="DIFFERENTIATE", metadata={"source":
"test2"}
        )

        # Verify the results match the expected values
>       assert result1 == {"key": "value1"}
E       AssertionError: assert {} == {'key': 'value1'}
E
E         Right contains 1 more item:
E         {'key': 'value1'}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_graph_memory_adapter.py:823: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:46,017 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Graph Memory
Adapter initialized with basic RDFLib
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Graph Memory Adapter initialized with basic RDFLib
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:46,018 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbx_4h9k8/graph_memory.ttl
2025-10-28 10:31:46,018 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Stored memory
item with ID graph_d48ace49-53fc-4719-aa90-5f0de7a4e05c in Graph Memory Adapter
2025-10-28 10:31:46,019 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Saved RDF
graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbx_4h9k8/graph_memory.ttl
2025-10-28 10:31:46,019 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Stored memory
item with ID graph_3ff0caa2-5f01-4c17-ac53-b34f6d50f556 in Graph Memory Adapter
2025-10-28 10:31:46,019 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Retrieved
memory item with ID graph_d48ace49-53fc-4719-aa90-5f0de7a4e05c from Graph Memory
Adapter
2025-10-28 10:31:46,019 -
devsynth.application.memory.adapters.graph_memory_adapter - INFO - Retrieved
memory item with ID graph_3ff0caa2-5f01-4c17-ac53-b34f6d50f556 from Graph Memory
Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbx_4h9k8/graph_memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Stored memory item with ID graph_d48ace49-53fc-4719-aa90-5f0de7a4e05c in Graph
Memory Adapter
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Saved RDF graph to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbx_4h9k8/graph_memory.ttl
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Stored memory item with ID graph_3ff0caa2-5f01-4c17-ac53-b34f6d50f556 in Graph
Memory Adapter
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Retrieved memory item with ID graph_d48ace49-53fc-4719-aa90-5f0de7a4e05c from
Graph Memory Adapter
INFO
devsynth.application.memory.adapters.graph_memory_adapter:logging_setup.py:615
Retrieved memory item with ID graph_3ff0caa2-5f01-4c17-ac53-b34f6d50f556 from
Graph Memory Adapter
____________________ test_tinydb_serializes_unhandled_types ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_tinydb_serializes_unhandl0')

    @pytest.mark.medium
    def test_tinydb_serializes_unhandled_types(tmp_path):
        """ReqID: FR-44

        TinyDB adapter should gracefully handle metadata with non-JSON
values."""

        from datetime import datetime

        adapter = TinyDBMemoryAdapter(db_path=str(tmp_path / "db.json"))
        item = MemoryItem(
            id="meta1",
            content="content",
            memory_type=MemoryType.KNOWLEDGE,
            metadata={"tags": {"a", "b"}, "timestamp": datetime(2024, 1, 1)},
        )

        item_id = adapter.store(item)
        retrieved = adapter.retrieve(item_id)
        assert retrieved is not None
        # sets should round-trip as lists
        assert set(retrieved.metadata["tags"]) == {"a", "b"}
        # datetimes should round-trip as ISO strings
>       assert retrieved.metadata["timestamp"] == datetime(2024, 1,
1).isoformat()
E       AssertionError: assert datetime.datetime(2024, 1, 1, 0, 0) ==
'2024-01-01T00:00:00'
E        +  where '2024-01-01T00:00:00' = <built-in method isoformat of
datetime.datetime object at 0x16c4b9830>()
E        +    where <built-in method isoformat of datetime.datetime object at
0x16c4b9830> = datetime.datetime(2024, 1, 1, 0, 0).isoformat
E        +      where datetime.datetime(2024, 1, 1, 0, 0) = <class
'datetime.datetime'>(2024, 1, 1)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_memory_adapters_regression.py:169: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:46,914 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:31:46,915 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID meta1 in TinyDB Memory Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID meta1 in TinyDB Memory Adapter
______ TestMemoryManagerStore.test_store_prefers_graph_for_edrr_succeeds _______

self = <tests.unit.application.memory.test_memory_manager.TestMemoryManagerStore
object at 0x11f8a4d70>
graph_adapters = {'graph':
<tests.unit.application.memory.test_memory_manager.DummyGraphStore object at
0x16c708950>, 'tinydb': <MagicMock id='6114281856'>}

    @pytest.mark.medium
    def test_store_prefers_graph_for_edrr_succeeds(self, graph_adapters):
        """Test that store prefers graph for edrr succeeds.

        ReqID: N/A"""
        manager = MemoryManager(adapters=graph_adapters)
        manager.store_with_edrr_phase("x", MemoryType.CODE, "EXPAND")
>       assert len(graph_adapters["graph"].stored) == 1
E       AssertionError: assert 2 == 1
E        +  where 2 = len([MemoryItem(id='36898795-8f90-4aa8-ab0b-6e2c581b9963',
content='x', memory_type=<MemoryType.CODE: 'code'>,
metadata={'...ory_type=<MemoryType.CONTEXT: 'context'>, metadata={},
created_at=datetime.datetime(2025, 10, 28, 10, 31, 46, 929112))])
E        +    where [MemoryItem(id='36898795-8f90-4aa8-ab0b-6e2c581b9963',
content='x', memory_type=<MemoryType.CODE: 'code'>,
metadata={'...ory_type=<MemoryType.CONTEXT: 'context'>, metadata={},
created_at=datetime.datetime(2025, 10, 28, 10, 31, 46, 929112))] =
<tests.unit.application.memory.test_memory_manager.DummyGraphStore object at
0x16c708950>.stored

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_memory_manager.py:142: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:46,928 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: graph, tinydb
2025-10-28 10:31:46,928 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:31:46,929 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: graph, tinydb
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
_______ TestMemoryManagerRetrieve.test_retrieve_with_edrr_phase_succeeds _______

self =
<tests.unit.application.memory.test_memory_manager.TestMemoryManagerRetrieve
object at 0x11f8a6360>
manager_with_graph = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x16c740c20>
graph_adapter =
<tests.unit.application.memory.test_memory_manager.DummyGraphStore object at
0x16c740d70>

    @pytest.mark.medium
    def test_retrieve_with_edrr_phase_succeeds(self, manager_with_graph,
graph_adapter):
        """Test that retrieve with edrr phase succeeds.

        ReqID: N/A"""
        test_item = MemoryItem(
            id="CODE_EXPAND",
            content={"key": "value"},
            memory_type=MemoryType.CODE,
            metadata={"edrr_phase": "EXPAND"},
        )
        graph_adapter.edrr_items[test_item.id] = test_item
        result = manager_with_graph.retrieve_with_edrr_phase("CODE", "EXPAND")
>       assert isinstance(result, MemoryRecord)
E       assert False
E        +  where False = isinstance(None, MemoryRecord)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_memory_manager.py:268: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:46,976 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: graph
2025-10-28 10:31:46,976 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: graph
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
_ TestMemoryManagerRetrieve.test_retrieve_with_edrr_phase_with_metadata_succeeds
_

self =
<tests.unit.application.memory.test_memory_manager.TestMemoryManagerRetrieve
object at 0x11f8a6390>
manager_with_graph = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x16c72c590>
graph_adapter =
<tests.unit.application.memory.test_memory_manager.DummyGraphStore object at
0x16c72e1e0>

    @pytest.mark.medium
    def test_retrieve_with_edrr_phase_with_metadata_succeeds(
        self, manager_with_graph, graph_adapter
    ):
        """Test that retrieve with edrr phase with metadata succeeds.

        ReqID: N/A"""
        test_item = MemoryItem(
            id="CODE_EXPAND",
            content={"key": "value"},
            memory_type=MemoryType.CODE,
            metadata={"edrr_phase": "EXPAND"},
        )
        graph_adapter.edrr_items[test_item.id] = test_item
        result = manager_with_graph.retrieve_with_edrr_phase(
            "CODE", "EXPAND", {"cycle_id": "123"}
        )
>       assert isinstance(result, MemoryRecord)
E       assert False
E        +  where False = isinstance(None, MemoryRecord)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_memory_manager.py:297: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:46,995 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: graph
2025-10-28 10:31:46,995 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: graph
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
__________________ TestRDFLibStore.test_store_vector_succeeds __________________

self = <tests.unit.application.memory.test_rdflib_store.TestRDFLibStore object
at 0x11f9012e0>
store = <devsynth.application.memory.rdflib_store.RDFLibStore object at
0x16c50d130>

    @pytest.mark.medium
    def test_store_vector_succeeds(self, store):
        """Test storing and retrieving a vector.

        ReqID: N/A"""
        vector = MemoryVector(
            id="",
            content="Test vector content",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"key": "value"},
            created_at=datetime.now(),
        )
        vector_id = store.store_vector(vector)
        assert vector_id
        assert vector.id == vector_id
        retrieved_vector = store.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector_id
        assert retrieved_vector.content == "Test vector content"
>       assert len(retrieved_vector.item.embedding) == 5
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryItem' object has no attribute 'embedding'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_rdflib_store.py:233: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:47,458 - devsynth.application.memory.rdflib_store - WARNING -
Failed to initialize tokenizer: Network access disabled during tests. Token
counting will be approximate.
2025-10-28 10:31:47,458 - devsynth.application.memory.rdflib_store - INFO - No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds0/memory.ttl
------------------------------ Captured log setup ------------------------------
WARNING  devsynth.application.memory.rdflib_store:logging_setup.py:615 Failed to
initialize tokenizer: Network access disabled during tests. Token counting will
be approximate.
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds0/memory.ttl
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,460 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds0/memory.ttl
2025-10-28 10:31:47,460 - devsynth.application.memory.rdflib_store - INFO -
Stored vector with ID 18632028-691d-4b8c-a197-3031154efbc3 in RDFLib graph
2025-10-28 10:31:47,461 - devsynth.application.memory.rdflib_store - INFO -
Retrieved vector with ID 18632028-691d-4b8c-a197-3031154efbc3 from RDFLib graph
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds0/memory.ttl
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Stored
vector with ID 18632028-691d-4b8c-a197-3031154efbc3 in RDFLib graph
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Retrieved
vector with ID 18632028-691d-4b8c-a197-3031154efbc3 from RDFLib graph
______________ TestRDFLibStore.test_get_collection_stats_succeeds ______________

self = <tests.unit.application.memory.test_rdflib_store.TestRDFLibStore object
at 0x11f8fbd40>
store = <devsynth.application.memory.rdflib_store.RDFLibStore object at
0x16c6f2de0>

    @pytest.mark.medium
    def test_get_collection_stats_succeeds(self, store):
        """Test getting collection statistics.

        ReqID: N/A"""
        stats = store.get_collection_stats()
        assert stats["vector_count"] == 0
>       assert stats["num_triples"] == 0
               ^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'num_triples'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_rdflib_store.py:305: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:47,527 - devsynth.application.memory.rdflib_store - WARNING -
Failed to initialize tokenizer: Network access disabled during tests. Token
counting will be approximate.
2025-10-28 10:31:47,527 - devsynth.application.memory.rdflib_store - INFO - No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_get_collection_stats_succ1/memory.ttl
------------------------------ Captured log setup ------------------------------
WARNING  devsynth.application.memory.rdflib_store:logging_setup.py:615 Failed to
initialize tokenizer: Network access disabled during tests. Token counting will
be approximate.
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_get_collection_stats_succ1/memory.ttl
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,536 - devsynth.application.memory.rdflib_store - INFO -
Retrieved collection statistics: {'collection_name': 'memory.ttl',
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ1', 'metadata': {'graph_file':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ1/memory.ttl', 'triple_count': 0}}
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Retrieved
collection statistics: {'collection_name': 'memory.ttl', 'vector_count': 0,
'embedding_dimensions': 0, 'persist_directory':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ1', 'metadata': {'graph_file':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ1/memory.ttl', 'triple_count': 0}}
_________________________ TestOperationLog.test_replay _________________________

self = <devsynth.application.memory.recovery.OperationLog object at 0x16c730830>
store = <MagicMock id='6114444976'>, start_time = None, end_time = None

    def replay(
        self,
        store: Any,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> int:
        """
        Replay operations on a memory store.

        Args:
            store: Memory store to replay operations on
            start_time: Optional start time for replay
            end_time: Optional end time for replay

        Returns:
            Number of operations replayed

        Raises:
            RecoveryError: If replay fails
        """
        # Filter operations by time range
        filtered_operations = self.operations

        if start_time is not None:
            start_time_str = start_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] >=
start_time_str
            ]

        if end_time is not None:
            end_time_str = end_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] <=
end_time_str
            ]

        # Replay operations
        replayed_count = 0

        for operation in filtered_operations:
            try:
                operation_type = operation["type"]
                operation_data = operation["data"]

                if operation_type == "store":
                    serialized = cast(StoreOperationPayload, operation_data)
>                   record = _deserialize_record(serialized["record"])
                                                 ^^^^^^^^^^^^^^^^^^^^
E                   KeyError: 'record'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:430: KeyError

The above exception was the direct cause of the following exception:

self = <tests.unit.application.memory.test_recovery.TestOperationLog object at
0x11f922810>
memory_item = MemoryItem(id='test-item-1', content='This is a test item',
memory_type=<MemoryType.WORKING: 'working'>, metadata={'test': 'value'},
created_at=datetime.datetime(2025, 10, 28, 10, 31, 47, 661512))

    @pytest.mark.medium
    def test_replay(self, memory_item):
        """Test replaying operations on a memory store."""
        log = OperationLog(store_id="test-store")
        log.log_operation(
            operation_type="store", operation_data={"item":
memory_item.to_dict()}
        )
        log.log_operation(
            operation_type="delete", operation_data={"item_id": "test-item-2"}
        )
        mock_store = MagicMock()
>       replayed_count = log.replay(mock_store)
                         ^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_recovery.py:194:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.memory.recovery.OperationLog object at 0x16c730830>
store = <MagicMock id='6114444976'>, start_time = None, end_time = None

    def replay(
        self,
        store: Any,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> int:
        """
        Replay operations on a memory store.

        Args:
            store: Memory store to replay operations on
            start_time: Optional start time for replay
            end_time: Optional end time for replay

        Returns:
            Number of operations replayed

        Raises:
            RecoveryError: If replay fails
        """
        # Filter operations by time range
        filtered_operations = self.operations

        if start_time is not None:
            start_time_str = start_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] >=
start_time_str
            ]

        if end_time is not None:
            end_time_str = end_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] <=
end_time_str
            ]

        # Replay operations
        replayed_count = 0

        for operation in filtered_operations:
            try:
                operation_type = operation["type"]
                operation_data = operation["data"]

                if operation_type == "store":
                    serialized = cast(StoreOperationPayload, operation_data)
                    record = _deserialize_record(serialized["record"])
                    store.store(record.item)

                elif operation_type == "delete":
                    delete_payload = cast(DeleteOperationPayload,
operation_data)
                    item_id = delete_payload["item_id"]
                    store.delete(item_id)

                # Add more operation types as needed

                replayed_count += 1

            except Exception as e:
                error_message = f"Failed to replay {operation_type} operation:
{e}"
                self.logger.error(error_message)
>               raise RecoveryError(error_message) from e
E               devsynth.application.memory.recovery.RecoveryError: Failed to
replay store operation: 'record'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:445: RecoveryError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,661 - devsynth.application.memory.recovery - ERROR - Failed
to replay store operation: 'record'
------------------------------ Captured log call -------------------------------
ERROR    devsynth.application.memory.recovery:logging_setup.py:615 Failed to
replay store operation: 'record'
_________________ TestOperationLog.test_replay_with_time_range _________________

self = <devsynth.application.memory.recovery.OperationLog object at 0x16c732cf0>
store = <MagicMock id='6114455296'>
start_time = datetime.datetime(2025, 1, 1, 18, 0)
end_time = datetime.datetime(2025, 1, 2, 18, 0)

    def replay(
        self,
        store: Any,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> int:
        """
        Replay operations on a memory store.

        Args:
            store: Memory store to replay operations on
            start_time: Optional start time for replay
            end_time: Optional end time for replay

        Returns:
            Number of operations replayed

        Raises:
            RecoveryError: If replay fails
        """
        # Filter operations by time range
        filtered_operations = self.operations

        if start_time is not None:
            start_time_str = start_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] >=
start_time_str
            ]

        if end_time is not None:
            end_time_str = end_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] <=
end_time_str
            ]

        # Replay operations
        replayed_count = 0

        for operation in filtered_operations:
            try:
                operation_type = operation["type"]
                operation_data = operation["data"]

                if operation_type == "store":
                    serialized = cast(StoreOperationPayload, operation_data)
>                   record = _deserialize_record(serialized["record"])
                                                 ^^^^^^^^^^^^^^^^^^^^
E                   KeyError: 'record'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:430: KeyError

The above exception was the direct cause of the following exception:

self = <tests.unit.application.memory.test_recovery.TestOperationLog object at
0x11f922cf0>

    @pytest.mark.medium
    def test_replay_with_time_range(self):
        """Test replaying operations within a time range."""
        log = OperationLog(store_id="test-store")
        timestamp1 = datetime(2025, 1, 1, 12, 0, 0)
        timestamp2 = datetime(2025, 1, 2, 12, 0, 0)
        timestamp3 = datetime(2025, 1, 3, 12, 0, 0)
        log.log_operation(
            operation_type="store",
            operation_data={"item_id": "test-item-1"},
            timestamp=timestamp1,
        )
        log.log_operation(
            operation_type="store",
            operation_data={"item_id": "test-item-2"},
            timestamp=timestamp2,
        )
        log.log_operation(
            operation_type="store",
            operation_data={"item_id": "test-item-3"},
            timestamp=timestamp3,
        )
        mock_store = MagicMock()
        start_time = datetime(2025, 1, 1, 18, 0, 0)
        end_time = datetime(2025, 1, 2, 18, 0, 0)
>       replayed_count = log.replay(
            mock_store, start_time=start_time, end_time=end_time
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_recovery.py:224:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.memory.recovery.OperationLog object at 0x16c732cf0>
store = <MagicMock id='6114455296'>
start_time = datetime.datetime(2025, 1, 1, 18, 0)
end_time = datetime.datetime(2025, 1, 2, 18, 0)

    def replay(
        self,
        store: Any,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> int:
        """
        Replay operations on a memory store.

        Args:
            store: Memory store to replay operations on
            start_time: Optional start time for replay
            end_time: Optional end time for replay

        Returns:
            Number of operations replayed

        Raises:
            RecoveryError: If replay fails
        """
        # Filter operations by time range
        filtered_operations = self.operations

        if start_time is not None:
            start_time_str = start_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] >=
start_time_str
            ]

        if end_time is not None:
            end_time_str = end_time.isoformat()
            filtered_operations = [
                op for op in filtered_operations if op["timestamp"] <=
end_time_str
            ]

        # Replay operations
        replayed_count = 0

        for operation in filtered_operations:
            try:
                operation_type = operation["type"]
                operation_data = operation["data"]

                if operation_type == "store":
                    serialized = cast(StoreOperationPayload, operation_data)
                    record = _deserialize_record(serialized["record"])
                    store.store(record.item)

                elif operation_type == "delete":
                    delete_payload = cast(DeleteOperationPayload,
operation_data)
                    item_id = delete_payload["item_id"]
                    store.delete(item_id)

                # Add more operation types as needed

                replayed_count += 1

            except Exception as e:
                error_message = f"Failed to replay {operation_type} operation:
{e}"
                self.logger.error(error_message)
>               raise RecoveryError(error_message) from e
E               devsynth.application.memory.recovery.RecoveryError: Failed to
replay store operation: 'record'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:445: RecoveryError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,687 - devsynth.application.memory.recovery - ERROR - Failed
to replay store operation: 'record'
------------------------------ Captured log call -------------------------------
ERROR    devsynth.application.memory.recovery:logging_setup.py:615 Failed to
replay store operation: 'record'
____________________ TestRecoveryManager.test_recover_store ____________________

self = <tests.unit.application.memory.test_recovery.TestRecoveryManager object
at 0x11f9318e0>
memory_item = MemoryItem(id='test-item-1', content='This is a test item',
memory_type=<MemoryType.WORKING: 'working'>, metadata={'test': 'value'},
created_at=datetime.datetime(2025, 10, 28, 10, 31, 47, 791416))

    @pytest.mark.medium
    def test_recover_store(self, memory_item):
        """Test recovering a memory store using snapshot and operation log."""
        manager = RecoveryManager()
>       snapshot = MemorySnapshot(
            store_id="test-store",
            items=[memory_item],
            created_at=datetime(2025, 1, 1, 12, 0, 0),
        )
E       TypeError: MemorySnapshot.__init__() got an unexpected keyword argument
'created_at'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_recovery.py:333: TypeError
__________________ TestWithRecovery.test_successful_execution __________________

self = <tests.unit.application.memory.test_recovery.TestWithRecovery object at
0x11f931e80>
memory_item = MemoryItem(id='test-item-1', content='This is a test item',
memory_type=<MemoryType.WORKING: 'working'>, metadata={'test': 'value'},
created_at=datetime.datetime(2025, 10, 28, 10, 31, 47, 808916))

    @pytest.mark.medium
    def test_successful_execution(self, memory_item):
        """Test that the decorator returns the result when the function
succeeds."""
        mock_store = MagicMock()
        mock_store.get_all_items.return_value = [memory_item]

        @with_recovery("test-store")
        def decorated_func(self, store):
            return "success"

>       result = decorated_func(mock_store)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_recovery.py:375:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<MagicMock id='6115233328'>,), kwargs = {}
recovery_manager = <devsynth.application.memory.recovery.RecoveryManager object
at 0x1192ac080>
store = <MagicMock id='6115233328'>

    def wrapper(*args: Any, **kwargs: Any) -> T:
        # Get the recovery manager
        recovery_manager = get_recovery_manager()

        # Get the store from args or kwargs
        store = None
        if args and hasattr(args[0], "get_all_items"):
            store = args[0]
        elif "store" in kwargs and hasattr(kwargs["store"], "get_all_items"):
            store = kwargs["store"]

        # Create snapshot if requested and store is available
        if create_snapshot and store is not None:
            recovery_manager.create_snapshot(store_id, store)

        try:
            # Execute the function
>           result = func(*args, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^
E           TypeError:
TestWithRecovery.test_successful_execution.<locals>.decorated_func() missing 1
required positional argument: 'store'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:681: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,809 - devsynth.application.memory.recovery - INFO - Saved
memory snapshot to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/devsynth_recovery/memory_snapsh
ot_test-store_1761672707_6115250096.json
2025-10-28 10:31:47,809 - devsynth.application.memory.recovery - INFO - Created
snapshot for store test-store
2025-10-28 10:31:47,809 - devsynth.application.memory.recovery - ERROR - Error
in function decorated_func:
TestWithRecovery.test_successful_execution.<locals>.decorated_func() missing 1
required positional argument: 'store'
2025-10-28 10:31:47,809 - devsynth.application.memory.recovery - INFO -
Attempting recovery for store test-store
2025-10-28 10:31:47,810 - devsynth.application.memory.recovery - INFO - Restored
store test-store from snapshot test-store_1761672707_6115250096
2025-10-28 10:31:47,810 - devsynth.application.memory.recovery - INFO -
Recovered store test-store
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Saved memory
snapshot to
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/devsynth_recovery/memory_snapsh
ot_test-store_1761672707_6115250096.json
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Created
snapshot for store test-store
ERROR    devsynth.application.memory.recovery:logging_setup.py:615 Error in
function decorated_func:
TestWithRecovery.test_successful_execution.<locals>.decorated_func() missing 1
required positional argument: 'store'
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Attempting
recovery for store test-store
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Restored
store test-store from snapshot test-store_1761672707_6115250096
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Recovered
store test-store
__________________ TestWithRecovery.test_no_snapshot_creation __________________

self = <tests.unit.application.memory.test_recovery.TestWithRecovery object at
0x11f9327e0>

    @pytest.mark.medium
    def test_no_snapshot_creation(self):
        """Test that the decorator doesn't create a snapshot when requested."""
        mock_store = MagicMock()

        @with_recovery("test-store", create_snapshot=False)
        def decorated_func(self, store):
            return "success"

>       result = decorated_func(mock_store)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_recovery.py:403:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<MagicMock id='6115560960'>,), kwargs = {}
recovery_manager = <devsynth.application.memory.recovery.RecoveryManager object
at 0x1192ac080>
store = <MagicMock id='6115560960'>

    def wrapper(*args: Any, **kwargs: Any) -> T:
        # Get the recovery manager
        recovery_manager = get_recovery_manager()

        # Get the store from args or kwargs
        store = None
        if args and hasattr(args[0], "get_all_items"):
            store = args[0]
        elif "store" in kwargs and hasattr(kwargs["store"], "get_all_items"):
            store = kwargs["store"]

        # Create snapshot if requested and store is available
        if create_snapshot and store is not None:
            recovery_manager.create_snapshot(store_id, store)

        try:
            # Execute the function
>           result = func(*args, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^
E           TypeError:
TestWithRecovery.test_no_snapshot_creation.<locals>.decorated_func() missing 1
required positional argument: 'store'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/recovery.py:681: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:47,840 - devsynth.application.memory.recovery - ERROR - Error
in function decorated_func:
TestWithRecovery.test_no_snapshot_creation.<locals>.decorated_func() missing 1
required positional argument: 'store'
2025-10-28 10:31:47,840 - devsynth.application.memory.recovery - INFO -
Attempting recovery for store test-store
2025-10-28 10:31:47,841 - devsynth.application.memory.recovery - INFO - Restored
store test-store from snapshot test-store_1761672707_6115735264
2025-10-28 10:31:47,841 - devsynth.application.memory.recovery - INFO -
Recovered store test-store
------------------------------ Captured log call -------------------------------
ERROR    devsynth.application.memory.recovery:logging_setup.py:615 Error in
function decorated_func:
TestWithRecovery.test_no_snapshot_creation.<locals>.decorated_func() missing 1
required positional argument: 'store'
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Attempting
recovery for store test-store
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Restored
store test-store from snapshot test-store_1761672707_6115735264
INFO     devsynth.application.memory.recovery:logging_setup.py:615 Recovered
store test-store
__________________ test_cross_store_query_and_update_wrappers __________________

    @pytest.mark.medium
    def test_cross_store_query_and_update_wrappers():
        manager = _manager()
        item_a = MemoryItem(id="x", content="foo", memory_type=MemoryType.CODE)
        manager.adapters["a"].store(item_a)
        item_b = MemoryItem(id="y", content="foo", memory_type=MemoryType.CODE)
        manager.adapters["b"].store(item_b)
        results = manager.cross_store_query("foo")
>       assert set(results.keys()) == {"a", "b"}
E       AssertionError: assert {'by_store', ...ned', 'query'} == {'a', 'b'}
E
E         Extra items in the left set:
E         'query'
E         'by_store'
E         'combined'
E         Extra items in the right set:
E         'a'
E         'b'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_sync_wrappers.py:21: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:48,272 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: a, b
2025-10-28 10:31:48,273 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: a, b
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
________________ test_generate_does_not_call_external_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16c87db20>

    @pytest.mark.medium
    def test_generate_does_not_call_external_succeeds(monkeypatch):
        """Test that generate does not call external succeeds.

            This test needs to be run in isolation due to interactions with
other tests.

        ReqID: N/A"""
        monkeypatch.setattr("devsynth.core.config_loader.load_config",
_mock_config)
        monkeypatch.setattr("devsynth.config.get_llm_settings", _llm_settings)
        llm = importlib.import_module("devsynth.application.llm")
        providers =
importlib.import_module("devsynth.application.llm.providers")
        importlib.reload(providers)
        importlib.reload(llm)
        OfflineProvider = importlib.import_module(
            "devsynth.application.llm.offline_provider"
        ).OfflineProvider

        def mock_request(*args, **kwargs):
            pytest.fail("External HTTP request was made when it shouldn't have
been")

        monkeypatch.setattr(httpx, "request", mock_request)
        provider = llm.get_llm_provider()
>       assert isinstance(
            provider, OfflineProvider
        ), f"Expected OfflineProvider but got {type(provider)}"
E       AssertionError: Expected OfflineProvider but got <class
'devsynth.application.llm.offline_provider.OfflineProvider'>
E       assert False
E        +  where False =
isinstance(<devsynth.application.llm.offline_provider.OfflineProvider object at
0x109db5370>, <class
'devsynth.application.llm.offline_provider.OfflineProvider'>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
_offline_provider_cli.py:40: AssertionError
_________ TestPromptAutoTuner.test_auto_tuner_initialization_succeeds __________

self = <tests.unit.application.test_prompt_auto_tuning.TestPromptAutoTuner
object at 0x11f9aa0c0>
auto_tuner = <devsynth.application.prompts.auto_tuning.PromptAutoTuner object at
0x16c7f7470>

    @pytest.mark.medium
    def test_auto_tuner_initialization_succeeds(self, auto_tuner):
        """Test initialization of a PromptAutoTuner.

        ReqID: FR-56"""
>       assert auto_tuner.prompt_variants == {}
E       assert PromptVariant...y_template={}) == {}
E
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
_prompt_auto_tuning.py:140: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:48,781 - devsynth.application.prompts.auto_tuning - INFO -
Prompt Auto-Tuner initialized
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.prompts.auto_tuning:logging_setup.py:615 Prompt
Auto-Tuner initialized
____________________ test_run_consensus_falls_back_to_build ____________________

    @pytest.mark.medium
    def test_run_consensus_falls_back_to_build():
        wsde = WSDE(name="team")
        wsde.consensus_vote = MagicMock(return_value={"status": "incomplete"})
        wsde.build_consensus = MagicMock(return_value={"decision": "x"})
        task = {"id": "2"}
>       result = wsde.run_consensus(task)
                 ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/wsde
/test_wsde_utils.py:24:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.collaboration.WSDE.WSDE object at 0x16c8061b0>
task = {'id': '2'}

    def run_consensus(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Perform a consensus vote and fallback to consensus building."""
        result = self.consensus_vote(task)
        decision = result.get("decision") or result.get("result")

        consensus_outcome: Optional[ConsensusOutcome] = None
        existing_consensus = result.get("consensus")
        if isinstance(existing_consensus, ConsensusOutcome):
            consensus_outcome = existing_consensus
        elif isinstance(existing_consensus, Mapping):
            consensus_outcome = ConsensusOutcome.from_dict(existing_consensus)

        if not decision or result.get("status") != "completed":
            error = RuntimeError("Consensus vote failed")
            log_consensus_failure(self.logger, error, extra={"task_id":
task.get("id")})
            consensus_outcome = self.build_consensus(task)

        if consensus_outcome is not None:
            result["consensus_outcome"] = consensus_outcome
>           result["consensus"] = consensus_outcome.to_dict()
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'dict' object has no attribute 'to_dict'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/WSDE.py:39: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:48,900 - devsynth.domain.models.wsde_core - ERROR - Consensus
failure
RuntimeError: Consensus vote failed
------------------------------ Captured log call -------------------------------
ERROR    devsynth.domain.models.wsde_core:logging_setup.py:615 Consensus failure
RuntimeError: Consensus vote failed
______________________ test_summarize_and_store_consensus ______________________

    @pytest.mark.medium
    def test_summarize_and_store_consensus():
        team = CollaborativeWSDETeam()
        team.memory_manager = DummyMemoryManager()
        task = {"id": "t1"}
        consensus = {"method": "synthesis", "synthesis": {"text": "do x"}}

        result = team._summarize_and_store_consensus(task, consensus)

>       assert "summary" in result
E       AssertionError: assert 'summary' in {'method': 'synthesis', 'synthesis':
{'text': 'do x'}}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior/test_ws
de_team_extended.py:25: AssertionError
___________________ test_load_config_normalizes_mvuu_section ___________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_load_config_normalizes_mv3')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16c836b40>

    @pytest.mark.medium
    def test_load_config_normalizes_mvuu_section(tmp_path, monkeypatch):
        """MVUU configuration is normalized even when provider data is
incomplete."""

        home = tmp_path / "home"
        monkeypatch.setattr(
            os.path,
            "expanduser",
            lambda p: str(home) if p == "~" else os.path.expanduser(p),
        )
        (tmp_path / "pyproject.toml").write_text(
            """
            [tool.devsynth]
            language = "python"

            [tool.devsynth.mvuu.issues.github]
            token = 123
            """
        )
        cfg = load_config(tmp_path)
>       assert cfg.mvuu == {}
               ^^^^^^^^
E       AttributeError: 'ConfigModel' object has no attribute 'mvuu'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_config
_loader.py:127: AttributeError
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_expertise_score_exact_match
_matches_expected _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f8e00>

    @pytest.mark.medium
    def
test_enhanced_calculate_expertise_score_exact_match_matches_expected(self):
        """Test enhanced expertise scoring with exact keyword matches.

        ReqID: N/A"""
        task = {"type": "code_generation", "language": "python"}
>       score = self.team.enhanced_calculate_expertise_score(self.python_agent,
task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_expertise_score'. Did you mean:
'_calculate_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:73: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,531 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,531 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,531 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,531 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_expertise_score_partial_mat
ch_matches_expected _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f9250>

    @pytest.mark.medium
    def
test_enhanced_calculate_expertise_score_partial_match_matches_expected(self):
        """Test enhanced expertise scoring with partial keyword matches.

        ReqID: N/A"""
        task = {"description": "Create Python code for API integration"}
>       score = self.team.enhanced_calculate_expertise_score(self.python_agent,
task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_expertise_score'. Did you mean:
'_calculate_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:85: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,557 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,557 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,557 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,557 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_expertise_score_experience_
level_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f9730>

    @pytest.mark.medium
    def test_enhanced_calculate_expertise_score_experience_level_succeeds(self):
        """Test that experience level affects expertise scoring.

        ReqID: N/A"""
        task = {"type": "documentation", "description": "Write technical
documentation"}
        junior_doc = SimpleAgent(
            name="junior_doc",
            expertise=["documentation", "technical_writing"],
            experience_level=2,
        )
        senior_doc = SimpleAgent(
            name="senior_doc",
            expertise=["documentation", "technical_writing"],
            experience_level=9,
        )
>       junior_score = self.team.enhanced_calculate_expertise_score(junior_doc,
task)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_expertise_score'. Did you mean:
'_calculate_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:107: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,579 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,579 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,579 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,579 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_expertise_score_performance
_history_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f9c10>

    @pytest.mark.medium
    def
test_enhanced_calculate_expertise_score_performance_history_succeeds(self):
        """Test that past performance affects expertise scoring.

        ReqID: N/A"""
        task = {"type": "testing", "description": "Create automated tests"}
        good_tester = SimpleAgent(
            name="good_tester",
            expertise=["testing", "test_automation"],
            performance_history={"testing": 0.9},
        )
        poor_tester = SimpleAgent(
            name="poor_tester",
            expertise=["testing", "test_automation"],
            performance_history={"testing": 0.5},
        )
>       good_score = self.team.enhanced_calculate_expertise_score(good_tester,
task)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_expertise_score'. Did you mean:
'_calculate_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:129: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,617 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,617 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,617 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,617 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_expertise_score_nested_task
_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f3920>

    @pytest.mark.medium
    def test_enhanced_calculate_expertise_score_nested_task_succeeds(self):
        """Test enhanced expertise scoring with nested task structure.

        ReqID: N/A"""
        task = {
            "type": "security",
            "details": {
                "requirements": [
                    {"description": "Implement authentication system"},
                    {"description": "Use encryption for sensitive data"},
                ]
            },
        }
>       score =
self.team.enhanced_calculate_expertise_score(self.security_agent, task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_expertise_score'. Did you mean:
'_calculate_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:149: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,656 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,656 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,656 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,657 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedExpertiseScoring.test_enhanced_calculate_phase_expertise_score_has_e
xpected _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedExpert
iseScoring object at 0x11f9f8e60>

    @pytest.mark.medium
    def test_enhanced_calculate_phase_expertise_score_has_expected(self):
        """Test enhanced phase-specific expertise scoring.

        ReqID: N/A"""
        task = {"type": "code_generation", "language": "python"}
        expand_agent = SimpleAgent(
            name="expand_expert",
            expertise=["python", "brainstorming", "idea_generation",
"exploration"],
        )
        differentiate_agent = SimpleAgent(
            name="differentiate_expert",
            expertise=["python", "analysis", "comparison", "critical_thinking"],
        )
        expand_phase_keywords = [
            "exploration",
            "brainstorming",
            "divergent thinking",
            "idea generation",
        ]
>       expand_score = self.team.enhanced_calculate_phase_expertise_score(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            expand_agent, task, expand_phase_keywords
        )
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_calculate_phase_expertise_score'. Did you mean:
'_calculate_phase_expertise_score'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:181: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,688 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,688 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,688 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,688 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedPrimusSelection.test_enhanced_select_primus_by_expertise_code_task_s
ucceeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedPrimus
Selection object at 0x11f9fa870>

    @pytest.mark.medium
    def test_enhanced_select_primus_by_expertise_code_task_succeeds(self):
        """Test enhanced primus selection for a code-related task.

        ReqID: N/A"""
        task = {"type": "code_generation", "language": "python"}
>       selected_primus = self.team.enhanced_select_primus_by_expertise(task)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_select_primus_by_expertise'. Did you mean:
'select_primus_by_expertise'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:236: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,719 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,719 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,719 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,719 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedPrimusSelection.test_enhanced_select_primus_by_expertise_doc_task_su
cceeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedPrimus
Selection object at 0x11f9facc0>

    @pytest.mark.medium
    def test_enhanced_select_primus_by_expertise_doc_task_succeeds(self):
        """Test enhanced primus selection for a documentation task.

        ReqID: N/A"""
        task = {"type": "documentation", "description": "Write technical
documentation"}
>       selected_primus = self.team.enhanced_select_primus_by_expertise(task)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_select_primus_by_expertise'. Did you mean:
'select_primus_by_expertise'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:247: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,754 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,754 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,755 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,755 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedPrimusSelection.test_enhanced_select_primus_by_expertise_security_ta
sk_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedPrimus
Selection object at 0x11f9fb1a0>

    @pytest.mark.medium
    def test_enhanced_select_primus_by_expertise_security_task_succeeds(self):
        """Test enhanced primus selection for a security task.

        ReqID: N/A"""
        task = {"type": "security", "description": "Implement authentication
system"}
>       selected_primus = self.team.enhanced_select_primus_by_expertise(task)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_select_primus_by_expertise'. Did you mean:
'select_primus_by_expertise'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:258: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,793 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,793 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,793 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,793 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedPrimusSelection.test_enhanced_select_primus_by_expertise_rotation_su
cceeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedPrimus
Selection object at 0x11f9fb680>

    @pytest.mark.medium
    def test_enhanced_select_primus_by_expertise_rotation_succeeds(self):
        """Test that primus selection rotates after all agents have been primus.

        ReqID: N/A"""
        for agent in self.team.agents:
            agent.has_been_primus = True
        task = {"type": "code_generation", "language": "python"}
>       selected_primus = self.team.enhanced_select_primus_by_expertise(task)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_select_primus_by_expertise'. Did you mean:
'select_primus_by_expertise'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:271: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,836 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,836 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,836 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,836 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestEnhancedPrimusSelection.test_enhanced_select_primus_by_expertise_unused_prio
rity_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestEnhancedPrimus
Selection object at 0x11fa20080>

    @pytest.mark.medium
    def test_enhanced_select_primus_by_expertise_unused_priority_succeeds(self):
        """Test that unused agents are prioritized for primus selection.

        ReqID: N/A"""
        self.python_agent.has_been_primus = True
        task = {"type": "code_generation", "language": "python"}
>       selected_primus = self.team.enhanced_select_primus_by_expertise(task)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'enhanced_select_primus_by_expertise'. Did you mean:
'select_primus_by_expertise'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:284: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,875 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,875 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,875 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,875 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestDynamicRoleReassignment.test_dynamic_role_reassignment_enhanced_code_task_su
cceeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestDynamicRoleRea
ssignment object at 0x11fa202c0>

    @pytest.mark.medium
    def test_dynamic_role_reassignment_enhanced_code_task_succeeds(self):
        """Test enhanced dynamic role reassignment for a code-related task.

        ReqID: N/A"""
        task = {"type": "code_generation", "language": "python"}
>       roles = self.team.dynamic_role_reassignment_enhanced(task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'dynamic_role_reassignment_enhanced'. Did you mean: 'dynamic_role_reassignment'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:322: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,908 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,908 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,908 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,908 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestDynamicRoleReassignment.test_dynamic_role_reassignment_enhanced_doc_task_suc
ceeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestDynamicRoleRea
ssignment object at 0x11fa20710>

    @pytest.mark.medium
    def test_dynamic_role_reassignment_enhanced_doc_task_succeeds(self):
        """Test enhanced dynamic role reassignment for a documentation task.

        ReqID: N/A"""
        task = {"type": "documentation", "description": "Write technical
documentation"}
>       roles = self.team.dynamic_role_reassignment_enhanced(task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'dynamic_role_reassignment_enhanced'. Did you mean: 'dynamic_role_reassignment'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:339: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,945 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,945 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,945 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,945 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestDynamicRoleReassignment.test_dynamic_role_reassignment_enhanced_testing_task
_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestDynamicRoleRea
ssignment object at 0x11fa20bf0>

    @pytest.mark.medium
    def test_dynamic_role_reassignment_enhanced_testing_task_succeeds(self):
        """Test enhanced dynamic role reassignment for a testing task.

        ReqID: N/A"""
        task = {"type": "testing", "description": "Create automated tests"}
>       roles = self.team.dynamic_role_reassignment_enhanced(task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'dynamic_role_reassignment_enhanced'. Did you mean: 'dynamic_role_reassignment'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:358: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:49,979 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:49,979 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:49,979 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:49,979 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_
TestDynamicRoleReassignment.test_dynamic_role_reassignment_enhanced_security_tas
k_succeeds _

self =
<tests.unit.domain.models.test_wsde_context_driven_leadership.TestDynamicRoleRea
ssignment object at 0x11fa210d0>

    @pytest.mark.medium
    def test_dynamic_role_reassignment_enhanced_security_task_succeeds(self):
        """Test enhanced dynamic role reassignment for a security task.

        ReqID: N/A"""
        task = {"type": "security", "description": "Implement authentication
system"}
>       roles = self.team.dynamic_role_reassignment_enhanced(task)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute
'dynamic_role_reassignment_enhanced'. Did you mean: 'dynamic_role_reassignment'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_context_driven_leadership.py:377: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:50,016 - devsynth.domain.models.wsde_core - INFO - Added agent
python_dev to team test_team
2025-10-28 10:31:50,016 - devsynth.domain.models.wsde_core - INFO - Added agent
doc_writer to team test_team
2025-10-28 10:31:50,016 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team test_team
2025-10-28 10:31:50,016 - devsynth.domain.models.wsde_core - INFO - Added agent
security_expert to team test_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
python_dev to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
doc_writer to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
security_expert to team test_team
_____ TestWSDEDialecticalReasoning.test_identify_domain_conflicts_succeeds _____

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa20c80>

    @pytest.mark.medium
    def test_identify_domain_conflicts_succeeds(self):
        """Test identifying conflicts between different domain perspectives.

        ReqID: N/A"""
        domain_critiques = {
            "security": [
                "Security issue: Need more validation checks",
                "Security issue: Use encryption for sensitive data",
            ],
            "performance": [
                "Performance issue: Validation is too slow",
                "Performance issue: Encryption adds overhead",
            ],
            "maintainability": [
                "Maintainability issue: Code is too complex",
                "Maintainability issue: Need better organization",
            ],
        }
        conflicts = self.team._identify_domain_conflicts(domain_critiques)
        assert len(conflicts) > 0
        security_performance_conflict = None
        for conflict in conflicts:
>           if set(conflict["domains"]) == set(["security", "performance"]):
                   ^^^^^^^^^^^^^^^^^^^
E           KeyError: 'domains'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:86: KeyError
_______ TestWSDEDialecticalReasoning.test_prioritize_critiques_succeeds ________

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa22300>

    @pytest.mark.medium
    def test_prioritize_critiques_succeeds(self):
        """Test prioritizing critique points based on severity and relevance.

        ReqID: N/A"""
        critiques = [
            "Critical security vulnerability: SQL injection possible",
            "Minor UI issue: Button color is inconsistent",
            "Important: No input validation",
            "Consider adding more comments for clarity",
            "Major performance bottleneck in database queries",
        ]
        prioritized = self.team._prioritize_critiques(critiques)
        assert len(prioritized) == 5
        for i in range(len(prioritized) - 1):
            assert (
>               prioritized[i]["priority_score"] >= prioritized[i +
1]["priority_score"]
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            )
E           TypeError: string indices must be integers, not 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:111: TypeError
_ TestWSDEDialecticalReasoning.test_resolve_code_improvement_conflict_succeeds _

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa227e0>

    @pytest.mark.medium
    def test_resolve_code_improvement_conflict_succeeds(self):
        """Test resolving conflicts between code improvements from different
domains.

        ReqID: N/A"""
        conflict = {
            "domains": ["security", "performance"],
            "description": "Security measures may impact performance",
            "critiques": {
                "security": ["Need more validation checks"],
                "performance": ["Validation is too slow"],
            },
        }
        security_improvements = [
            "Added input validation",
            "Implemented secure password handling",
        ]
        performance_improvements = [
            "Optimized validation algorithm",
            "Reduced database queries",
        ]
>       resolution = self.team._resolve_code_improvement_conflict(
            conflict, security_improvements, performance_improvements
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:143:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.domain.models.wsde_facade.WSDETeam object at 0x16c8fb470>
conflict = {'critiques': {'performance': ['Validation is too slow'], 'security':
['Need more validation checks']}, 'description': 'Security measures may impact
performance', 'domains': ['security', 'performance']}
improvements1 = ['Added input validation', 'Implemented secure password
handling']
improvements2 = ['Optimized validation algorithm', 'Reduced database queries']

    def _resolve_code_improvement_conflict(
        self: WSDETeam,
        conflict: dict[str, Any],
        improvements1: list[str],
        improvements2: list[str],
    ):
        """
        Resolve conflicts between code improvements.

        Args:
            conflict: Dictionary describing the conflict
            improvements1: List of improvements for the first domain
            improvements2: List of improvements for the second domain

        Returns:
            Dictionary containing the resolution details
        """
>       domain1 = conflict["domain1"]
                  ^^^^^^^^^^^^^^^^^^^
E       KeyError: 'domain1'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_dialectical.py:1152: KeyError
_
TestWSDEDialecticalReasoning.test_resolve_content_improvement_conflict_succeeds
_

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa22cc0>

    @pytest.mark.medium
    def test_resolve_content_improvement_conflict_succeeds(self):
        """Test resolving conflicts between content improvements from different
domains.

        ReqID: N/A"""
        conflict = {
            "domains": ["security", "usability"],
            "description": "Security requirements may affect usability",
            "critiques": {
                "security": ["Need to explain security measures"],
                "usability": ["Documentation is too technical"],
            },
        }
        security_improvements = [
            "Added security warnings",
            "Detailed security requirements",
        ]
        usability_improvements = ["Simplified language", "Added user-friendly
examples"]
>       resolution = self.team._resolve_content_improvement_conflict(
            conflict, security_improvements, usability_improvements
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:172:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.domain.models.wsde_facade.WSDETeam object at 0x16c975490>
conflict = {'critiques': {'security': ['Need to explain security measures'],
'usability': ['Documentation is too technical']}, 'description': 'Security
requirements may affect usability', 'domains': ['security', 'usability']}
improvements1 = ['Added security warnings', 'Detailed security requirements']
improvements2 = ['Simplified language', 'Added user-friendly examples']

    def _resolve_content_improvement_conflict(
        self: WSDETeam,
        conflict: dict[str, Any],
        improvements1: list[str],
        improvements2: list[str],
    ):
        """
        Resolve conflicts between content improvements.

        Args:
            conflict: Dictionary describing the conflict
            improvements1: List of improvements for the first domain
            improvements2: List of improvements for the second domain

        Returns:
            Dictionary containing the resolution details
        """
>       domain1 = conflict["domain1"]
                  ^^^^^^^^^^^^^^^^^^^
E       KeyError: 'domain1'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_dialectical.py:1199: KeyError
__ TestWSDEDialecticalReasoning.test_check_code_standards_compliance_succeeds __

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa231a0>

        @pytest.mark.medium
        def test_check_code_standards_compliance_succeeds(self):
            """Test checking if code complies with standards and best practices.

            ReqID: N/A"""
            compliant_code = """
    def authenticate(username, password):
        ""\"
        Authenticate a user with username and password.

        Args:
            username: The username to authenticate
            password: The password to authenticate

        Returns:
            True if authentication succeeds, False otherwise
        ""\"
        # Validate inputs
        if not username or not password:
            return False

        try:
            # In a real system, this would use a secure password hashing
algorithm
            import hashlib
            hashed_password = hashlib.sha256(password.encode()).hexdigest()
            stored_hash = get_stored_hash(username)

            return hashed_password == stored_hash
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False

    def get_stored_hash(username):
        ""\"Get the stored password hash for a user.""\"
        # This would normally query a database
        return
"5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\"
    """
            non_compliant_code = """
    def authenticate(username, password):
        return username == "admin" and password == "password\"
    """
            compliant_result =
self.team._check_code_standards_compliance(compliant_code)
            non_compliant_result = self.team._check_code_standards_compliance(
                non_compliant_code
            )
>           assert "details" in compliant_result
E           AssertionError: assert 'details' in {'overall_compliance': 'high',
'pep8': {'compliance_level': 'high', 'issues': [], 'suggestions': []},
'security': {'compliance_level': 'high', 'issues': [], 'suggestions': []}}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:225: AssertionError
_ TestWSDEDialecticalReasoning.test_check_content_standards_compliance_succeeds
_

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa23680>

        @pytest.mark.medium
        def test_check_content_standards_compliance_succeeds(self):
            """Test checking if content complies with standards and best
practices.

            ReqID: N/A"""
            compliant_content = """
    # User Authentication System

    This document describes the user authentication system.

    ## Overview

    The system uses secure password hashing and multi-factor authentication.

    ## Example

    For example, to authenticate a user:

    ```python
    result = authenticate(username, password)
    ```

    ## Security Considerations

    Always use HTTPS and implement rate limiting.
    """
            non_compliant_content = (
                "The system authenticates users with username and password."
            )
            compliant_result = self.team._check_content_standards_compliance(
                compliant_content
            )
            non_compliant_result =
self.team._check_content_standards_compliance(
                non_compliant_content
            )
>           assert "details" in compliant_result
E           AssertionError: assert 'details' in {'clarity': True,
'compliance_level': 'high', 'examples': True, 'structure': True, ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:270: AssertionError
_______ TestWSDEDialecticalReasoning.test_check_pep8_compliance_succeeds _______

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa30080>

        @pytest.mark.medium
        def test_check_pep8_compliance_succeeds(self):
            """Test checking if code complies with PEP 8 style guide.

            ReqID: N/A"""
            compliant_code = """
    def authenticate(username, password):
        ""\"Authenticate a user with username and password.""\"
        if not username or not password:
            return False

        try:
            import hashlib
            hashed_password = hashlib.sha256(password.encode()).hexdigest()
            stored_hash = get_stored_hash(username)

            return hashed_password == stored_hash
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False
    """
            non_compliant_code = """
    def authenticate( username,password ):
        if not username or not password: return False
        try:
            import hashlib
            hashed_password=hashlib.sha256(password.encode()).hexdigest()
            stored_hash=get_stored_hash(username)
            return hashed_password==stored_hash
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False
    """
            compliant_result = self.team._check_pep8_compliance(compliant_code)
            non_compliant_result =
self.team._check_pep8_compliance(non_compliant_code)
>           assert compliant_result == True
E           AssertionError: assert {'compliance_level': 'high', 'issues': [],
'suggestions': []} == True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:314: AssertionError
___ TestWSDEDialecticalReasoning.test_check_security_best_practices_succeeds ___

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa30560>

        @pytest.mark.medium
        def test_check_security_best_practices_succeeds(self):
            """Test checking if code follows security best practices.

            ReqID: N/A"""
            secure_code = """
    def authenticate(username, password):
        ""\"Authenticate a user with username and password.""\"
        if not username or not password:
            return False

        try:
            import hashlib
            import hmac
            import os

            hashed_password = hashlib.sha256(password.encode()).hexdigest()
            stored_hash = get_stored_hash(username)

            return hmac.compare_digest(hashed_password, stored_hash)
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False
    """
            insecure_code = """
    def authenticate(username, password):
        password = "hardcoded_password"
        token = "secret_token"

        if username == "admin" and password == "password":
            return True

        # Potential command injection
        import subprocess
        subprocess.call(f"echo {username}", shell=True)

        # Potential code injection
        eval(f"print('{username}')")

        return False
    """
            secure_result =
self.team._check_security_best_practices(secure_code)
            insecure_result =
self.team._check_security_best_practices(insecure_code)
>           assert secure_result == True
E           AssertionError: assert {'compliance_level': 'high', 'issues': [],
'suggestions': []} == True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:360: AssertionError
_ TestWSDEDialecticalReasoning.test_balance_security_and_performance_succeeds __

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa30a40>

    @pytest.mark.medium
    def test_balance_security_and_performance_succeeds(self):
        """Test balancing security and performance in code.

        ReqID: N/A"""
        code = "def authenticate(username, password):\n    return True"
        balanced_code = self.team._balance_security_and_performance(code)
>       assert "Security and performance balance" in balanced_code
E       AssertionError: assert 'Security and performance balance' in
{'implementation': ['Use strong encryption for sensitive data, even with
performance cost', 'Implement caching for fre... where possible'],
'recommendation': 'Prioritize security for critical operations while optimizing
non-critical paths'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:370: AssertionError
__ TestWSDEDialecticalReasoning.test_balance_security_and_usability_succeeds ___

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa30f20>

    @pytest.mark.medium
    def test_balance_security_and_usability_succeeds(self):
        """Test balancing security and usability in code.

        ReqID: N/A"""
        code = "def authenticate(username, password):\n    return True"
        balanced_code = self.team._balance_security_and_usability(code)
>       assert "Security and usability balance" in balanced_code
E       AssertionError: assert 'Security and usability balance' in
{'implementation': ['Use progressive security that scales with sensitivity of
operations', 'Implement secure defaults ...ar feedback for security-related
actions'], 'recommendation': 'Implement security measures with minimal user
friction'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:382: AssertionError
_
TestWSDEDialecticalReasoning.test_balance_performance_and_maintainability_succee
ds _

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa31400>

    @pytest.mark.medium
    def test_balance_performance_and_maintainability_succeeds(self):
        """Test balancing performance and maintainability in code.

        ReqID: N/A"""
        code = "def authenticate(username, password):\n    return True"
        balanced_code = self.team._balance_performance_and_maintainability(code)
>       assert "Performance and maintainability balance" in balanced_code
E       AssertionError: assert 'Performance and maintainability balance' in
{'implementation': ['Use clear variable names and add comments for complex
optimizations', 'Extract complex optimizati...ptimization on measured
bottlenecks only'], 'recommendation': 'Optimize critical paths while maintaining
code clarity'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:394: AssertionError
_
TestWSDEDialecticalReasoning.test_generate_detailed_synthesis_reasoning_succeeds
_

self =
<tests.unit.domain.models.test_wsde_dialectical_reasoning.TestWSDEDialecticalRea
soning object at 0x11fa318e0>

    @pytest.mark.medium
    def test_generate_detailed_synthesis_reasoning_succeeds(self):
        """Test generating detailed reasoning about the synthesis process.

        ReqID: N/A"""
        domain_critiques = {
            "security": ["Security issue: Hardcoded credentials"],
            "performance": ["Performance issue: Inefficient algorithm"],
            "maintainability": ["Maintainability issue: No documentation"],
        }
        domain_improvements = {
            "security": ["Removed hardcoded credentials"],
            "performance": ["Optimized algorithm"],
            "maintainability": ["Added documentation"],
        }
        domain_conflicts = [
            {
                "domains": ["security", "performance"],
                "description": "Security measures may impact performance",
            }
        ]
        resolved_conflicts = [
            {
                "resolution": "Prioritized security over performance",
                "reasoning": "Security is critical for protecting user data",
            }
        ]
        standards_compliance = {
            "code": {
                "details": {
                    "pep8": True,
                    "security_best_practices": True,
                    "error_handling": True,
                    "input_validation": True,
                    "documentation": True,
                },
                "score": 0.8,
                "level": "high",
            }
        }
>       reasoning = self.team._generate_detailed_synthesis_reasoning(
            domain_critiques,
            domain_improvements,
            domain_conflicts,
            resolved_conflicts,
            standards_compliance,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_reasoning.py:445:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.domain.models.wsde_facade.WSDETeam object at 0x16c723110>
domain_critiques = {'maintainability': ['Maintainability issue: No
documentation'], 'performance': ['Performance issue: Inefficient algorithm'],
'security': ['Security issue: Hardcoded credentials']}
domain_improvements = {'maintainability': ['Added documentation'],
'performance': ['Optimized algorithm'], 'security': ['Removed hardcoded
credentials']}
domain_conflicts = [{'description': 'Security measures may impact performance',
'domains': ['security', 'performance']}]
resolved_conflicts = [{'reasoning': 'Security is critical for protecting user
data', 'resolution': 'Prioritized security over performance'}]
standards_compliance = {'code': {'details': {'documentation': True,
'error_handling': True, 'input_validation': True, 'pep8': True, ...}, 'level':
'high', 'score': 0.8}}

    def _generate_detailed_synthesis_reasoning(
        self: WSDETeam,
        domain_critiques: dict[str, list[str]],
        domain_improvements: dict[str, list[str]],
        domain_conflicts: list[dict[str, Any]],
        resolved_conflicts: list[dict[str, Any]],
        standards_compliance: dict[str, Any],
    ):
        """
        Generate detailed reasoning for the synthesis.

        Args:
            domain_critiques: Dictionary mapping domains to lists of critiques
            domain_improvements: Dictionary mapping domains to lists of
improvements
            domain_conflicts: List of dictionaries describing domain conflicts
            resolved_conflicts: List of dictionaries describing resolved
conflicts
            standards_compliance: Dictionary containing standards compliance
results

        Returns:
            String containing detailed reasoning
        """
        reasoning_parts = []

        # Summarize critiques by domain
        reasoning_parts.append("## Critique Analysis")
        for domain, critiques in domain_critiques.items():
            reasoning_parts.append(f"\n### {domain.capitalize()} Critiques")
            for critique in critiques:
                reasoning_parts.append(f"- {critique}")

        # Summarize improvements by domain
        reasoning_parts.append("\n## Improvements Applied")
        for domain, improvements in domain_improvements.items():
            if improvements:
                reasoning_parts.append(f"\n### {domain.capitalize()}
Improvements")
                for improvement in improvements:
                    reasoning_parts.append(f"- {improvement}")

        # Describe domain conflicts and resolutions
        if domain_conflicts:
            reasoning_parts.append("\n## Domain Conflicts")
            for i, conflict in enumerate(domain_conflicts):
                reasoning_parts.append(
>                   f"\n### Conflict {i+1}: {conflict['domain1'].capitalize()}
vs {conflict['domain2'].capitalize()}"
                                             ^^^^^^^^^^^^^^^^^^^
                )
E               KeyError: 'domain1'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_dialectical.py:1518: KeyError
_____________ test_documentation_task_prefers_doc_agents_succeeds ______________

    @pytest.mark.medium
    def test_documentation_task_prefers_doc_agents_succeeds():
        """Test that documentation task prefers doc agents succeeds.

        ReqID: N/A"""
        team = WSDETeam(name="TestPrimusSelectionEdgeCasesTeam")
        coder = _agent("coder", ["python"])
        writer = _agent("writer", ["documentation"])
        doc = _agent("doc", ["documentation", "markdown"])
        team.add_agents([coder, writer, doc])
        team.select_primus_by_expertise({"type": "documentation"})
        primus = team.get_primus()
>       assert primus in (writer, doc)
E       AssertionError: assert <MagicMock id='6117157936'> in (<MagicMock
id='6117152272'>, <MagicMock id='6113678192'>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_prim
us_selection_edge_cases.py:47: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:51,045 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,046 - devsynth.domain.models.wsde_core - INFO - Added agent
writer to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,046 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,046 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
writer to team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
_______________________ test_edge_case_coverage_succeeds _______________________

    @pytest.mark.medium
    def test_edge_case_coverage_succeeds():
        """Test that edge case coverage succeeds.

        ReqID: N/A"""
        import devsynth.domain.models.wsde_facade as wsde

        cov = coverage.Coverage()
        cov.start()
        empty = wsde.WSDETeam(name="TestPrimusSelectionEdgeCasesTeam")
        empty.select_primus_by_expertise({})
        team = wsde.WSDETeam(name="TestPrimusSelectionEdgeCasesTeam")
        team.add_agents(
            [_agent("a"), _agent("b", ["documentation"]), _agent("c",
["python"])]
        )
        team.select_primus_by_expertise({"type": "documentation", "extra": {"n":
1}})
        team.select_primus_by_expertise({"language": "python"})
        team.select_primus_by_expertise({})
        team.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"context": {"info": [{"language":
"python"}]}})
        cov.stop()
        path = wsde.__file__
        lines, start =
inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)
        executable = []
        skip = False
        for i, line in enumerate(lines, start):
            stripped = line.strip()
            if stripped.startswith('"""'):
                if (
                    stripped.count('"""') == 2
                    and stripped.endswith('""')
                    and stripped != '"""'
                ):
                    continue
                skip = not skip
                continue
            if skip:
                if stripped.endswith('"""'):
                    skip = False
                continue
            if stripped:
                executable.append(i)
>       executed = set(cov.get_data().lines(path))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_prim
us_selection_edge_cases.py:111: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:51,096 - devsynth.domain.models.wsde_roles - WARNING - Cannot
select primus: no agents in team
2025-10-28 10:31:51,097 - devsynth.domain.models.wsde_core - INFO - Added agent
a to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,097 - devsynth.domain.models.wsde_core - INFO - Added agent
b to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,097 - devsynth.domain.models.wsde_core - INFO - Added agent
c to team TestPrimusSelectionEdgeCasesTeam
2025-10-28 10:31:51,097 - devsynth.domain.models.wsde_roles - INFO - Selected a
as primus based on expertise
2025-10-28 10:31:51,098 - devsynth.domain.models.wsde_roles - INFO - Selected b
as primus based on expertise
2025-10-28 10:31:51,098 - devsynth.domain.models.wsde_roles - INFO - Selected c
as primus based on expertise
2025-10-28 10:31:51,098 - devsynth.domain.models.wsde_roles - INFO - Selected a
as primus based on expertise
2025-10-28 10:31:51,098 - devsynth.domain.models.wsde_roles - INFO - Selected b
as primus based on expertise
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select
primus: no agents in team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a to
team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent b to
team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent c to
team TestPrimusSelectionEdgeCasesTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected b as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected c as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected b as
primus based on expertise
_____________________ test_build_consensus_produces_result _____________________

    @pytest.mark.medium
    def test_build_consensus_produces_result():
        team = WSDETeam(name="t")
        a1 = DummyAgent("a1", ["x"])
        a2 = DummyAgent("a2", ["y"])
        team.add_agents([a1, a2])
        task = {"id": "t1", "options": ["x", "y"]}
        team.solutions = {"t1": []}
>       team.add_solution(task, {"agent": "a1", "content": "x"})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_core_methods.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

team = <devsynth.domain.models.wsde_facade.WSDETeam object at 0x16c080ef0>
task = {'id': 't1', 'options': ['x', 'y']}
solution = {'agent': 'a1', 'content': 'x'}

    def add_solution(
        team: Any, task: TaskPayload, solution: SolutionRecord
    ) -> SolutionRecord:
        """Add a solution to the team and trigger dialectical hooks."""
        task_id = task.get("id")
        if not task_id:
            task_id = str(uuid4())
            task["id"] = task_id
        else:
            # Normalise the identifier on the task payload so future calls reuse
it.
            task["id"] = task_id

>       team.solutions.add(task_id, solution)
        ^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'add'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_utils.py:236: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:51,215 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team t
2025-10-28 10:31:51,215 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team t
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team t
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team t
____________________ test_memory_retry_metrics_and_callback ____________________

    @pytest.mark.medium
    def test_memory_retry_metrics_and_callback():
        reset_memory_retry_metrics()
        cb_called = False

        def cond_cb(exc: Exception, attempt: int) -> bool:
            nonlocal cb_called
            cb_called = True
            return True

        func = Mock(side_effect=[Exception("err"), "ok"])
        func.__name__ = "mem_func"

        wrapped = retry_with_backoff(
            max_retries=2,
            initial_backoff=0,
            condition_callbacks=[cond_cb],
        )(func)

        result = wrapped()

        assert result == "ok"
        assert cb_called is True
>       assert memory_retry_event_counter.labels(status="attempt")._value.get()
== 1
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: '_CounterWrapper' object has no attribute '_value'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fallback/test_co
ndition_callbacks_prometheus.py:83: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:51,278 - devsynth.application.memory.retry - WARNING - Attempt
1 failed with Exception: err. Retrying in 0.00 seconds...
2025-10-28 10:31:51,278 - devsynth.application.memory.retry - INFO - Retry
attempt 1/2 for mem_func
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.memory.retry:logging_setup.py:615 Attempt 1 failed
with Exception: err. Retrying in 0.00 seconds...
INFO     devsynth.application.memory.retry:logging_setup.py:615 Retry attempt
1/2 for mem_func
________________ test_memory_condition_callback_records_metrics ________________

    @pytest.mark.medium
    def test_memory_condition_callback_records_metrics():
        reset_memory_retry_metrics()

        def cb(exc: Exception, attempt: int) -> bool:
            return False

        func = Mock(side_effect=Exception("boom"))
        func.__name__ = "mem_func"

        wrapped = retry_with_backoff(
            max_retries=2,
            initial_backoff=0,
            condition_callbacks=[cb],
        )(func)

        with pytest.raises(Exception):
            wrapped()

        assert (
            memory_retry_condition_counter.labels(
                condition=f"{cb.__name__}:suppress"
>           )._value.get()
              ^^^^^^
            == 1
        )
E       AttributeError: '_CounterWrapper' object has no attribute '_value'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fallback/test_co
ndition_callbacks_prometheus.py:134: AttributeError
_________________ test_memory_retry_condition_records_metrics __________________

    @pytest.mark.medium
    def test_memory_retry_condition_records_metrics() -> None:
        reset_memory_retry_metrics()
        func = Mock(side_effect=Exception("boom"))
        func.__name__ = "mem_func"

        wrapped = retry_with_backoff(
            max_retries=2,
            initial_backoff=0,
            retry_conditions={"needs_retry": "retry"},
        )(func)

        with pytest.raises(Exception):
            wrapped()

        assert (
            memory_retry_condition_counter.labels(
                condition="needs_retry:suppress"
>           )._value.get()
              ^^^^^^
            == 1
        )
E       AttributeError: '_CounterWrapper' object has no attribute '_value'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fallback/test_co
ndition_callbacks_prometheus.py:157: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:51,315 - devsynth.application.memory.retry - WARNING - Not
retrying mem_func due to retry_conditions policy (needs_retry): boom
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.memory.retry:logging_setup.py:615 Not retrying
mem_func due to retry_conditions policy (needs_retry): boom
_____________________ TestTyperCLI.test_cli_init_succeeds ______________________

self = <MagicMock name='init_cmd' spec='function' id='6122339984'>, args = ()
kwargs = {'bridge': None, 'wizard': False}
msg = "Expected 'init_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'init_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadcaa0>
mock_init_cmd = <function init_cmd at 0x16c693920>

    @patch("devsynth.adapters.cli.typer_adapter.init_cmd", autospec=True)
    @pytest.mark.medium
    def test_cli_init_succeeds(self, mock_init_cmd):
        """Test that cli init succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(app, ["init"])
        assert result.exit_code == 0
>       mock_init_cmd.assert_called_once_with(wizard=False, bridge=None)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:63:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (), kwargs = {'bridge': None, 'wizard': False}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'init_cmd' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
----------------------------- Captured stdout call -----------------------------
Project root directory [/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_cli_init_succeeds0/project]/private/var/fol
                                                                               d
ers/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_cli_
                                                                               i
nit_succeeds0/project                     Project root directory
[/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_cli_init_succeeds0/project]/private/var/fol
                                                                               d
ers/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/test_cli_
                                                                               i
nit_succeeds0/project
Primary language [python]python                               Primary language
[python]python
Project goals             Project goals
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘

























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘


























































Memory backend [memory]memory                             Memory backend
[memory]memory
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘





























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘






























































2025-10-28 10:31:52,579 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:52,583 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
_____________________ TestTyperCLI.test_cli_spec_succeeds ______________________

self = <MagicMock name='spec_cmd' spec='function' id='6123654320'>
args = ('reqs.md',), kwargs = {'bridge': None}
msg = "Expected 'spec_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'spec_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadcf50>
mock_spec_cmd = <function spec_cmd at 0x16cc70220>

    @patch("devsynth.adapters.cli.typer_adapter.spec_cmd", autospec=True)
    @pytest.mark.medium
    def test_cli_spec_succeeds(self, mock_spec_cmd):
        """Test that cli spec succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(app, ["spec", "--requirements-file",
"reqs.md"])
        assert result.exit_code == 0
>       mock_spec_cmd.assert_called_once_with("reqs.md", bridge=None)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:74:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = ('reqs.md',), kwargs = {'bridge': None}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'spec_cmd' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
----------------------------- Captured stdout call -----------------------------
                ┌─────────────────| DevSynth |─────────────────┐

                │                                              │

                │ Create empty 'reqs.md' file?                 │

                │                                              │

                │ ( ) Yes                                    ^ │

                │ (*) No                                     v │

                │           <    Ok    > <  Cancel  >          │

                │                                              │

                │                                              │

                └──────────────────────────────────────────────┘





























































                                   ┌─────────────────| DevSynth
|─────────────────┐

                │                                              │

                │ Create empty 'reqs.md' file?                 │

                │                                              │

                │ ( ) Yes                                    ^ │

                │ (*) No                                     v │

                │           <    Ok    > <  Cancel  >          │

                │                                              │

                │                                              │

                └──────────────────────────────────────────────┘






























































_____________________ TestTyperCLI.test_cli_test_succeeds ______________________

self = <MagicMock name='test_cmd' spec='function' id='5096765440'>
args = ('specs.md',), kwargs = {'bridge': None}
msg = "Expected 'test_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'test_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadd400>
mock_test_cmd = <function test_cmd at 0x16cae2480>

    @patch("devsynth.adapters.cli.typer_adapter.test_cmd", autospec=True)
    @pytest.mark.medium
    def test_cli_test_succeeds(self, mock_test_cmd):
        """Test that cli test succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(app, ["test", "--spec-file", "specs.md"])
        assert result.exit_code == 0
>       mock_test_cmd.assert_called_once_with("specs.md", bridge=None)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:85:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = ('specs.md',), kwargs = {'bridge': None}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'test_cmd' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
----------------------------- Captured stdout call -----------------------------
            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































2025-10-28 10:31:53,339 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:53,341 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'75974167-859e-45fc-87af-8ee3b2e0b4f6'}
2025-10-28 10:31:53,341 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'75974167-859e-45fc-87af-8ee3b2e0b4f6'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'75974167-859e-45fc-87af-8ee3b2e0b4f6'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'75974167-859e-45fc-87af-8ee3b2e0b4f6'}
_____________________ TestTyperCLI.test_cli_code_succeeds ______________________

self = <MagicMock name='code_cmd' spec='function' id='5097927840'>, args = ()
kwargs = {'bridge': None}
msg = "Expected 'code_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'code_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadd8b0>
mock_code_cmd = <function code_cmd at 0x16cae3740>

    @patch("devsynth.adapters.cli.typer_adapter.code_cmd", autospec=True)
    @pytest.mark.medium
    def test_cli_code_succeeds(self, mock_code_cmd):
        """Test that cli code succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(app, ["code"])
        assert result.exit_code == 0
>       mock_code_cmd.assert_called_once_with(bridge=None)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:96:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (), kwargs = {'bridge': None}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'code_cmd' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
----------------------------- Captured stdout call -----------------------------
             ┌────────────────────| DevSynth |────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘





























































                             ┌────────────────────| DevSynth
|────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘






























































            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































2025-10-28 10:31:53,924 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:53,925 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'5b6fa85f-b061-4c91-83fa-4da840d7e049'}
2025-10-28 10:31:53,925 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'5b6fa85f-b061-4c91-83fa-4da840d7e049'}
2025-10-28 10:31:53,928 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:53,928 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'88b3c47d-fbd8-4d3e-9fbc-779d3b0c823f'}
2025-10-28 10:31:53,928 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'88b3c47d-fbd8-4d3e-9fbc-779d3b0c823f'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'5b6fa85f-b061-4c91-83fa-4da840d7e049'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'5b6fa85f-b061-4c91-83fa-4da840d7e049'}
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'88b3c47d-fbd8-4d3e-9fbc-779d3b0c823f'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'88b3c47d-fbd8-4d3e-9fbc-779d3b0c823f'}
______________________ TestTyperCLI.test_cli_run_succeeds ______________________

self = <MagicMock name='run_pipeline_cmd' spec='function' id='5097919920'>
args = (), kwargs = {'bridge': None, 'report': None, 'target': 'unit-tests'}
msg = "Expected 'run_pipeline_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'run_pipeline_cmd' to be called once.
Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11faca840>
mock_run_pipeline_cmd = <function run_pipeline_cmd at 0x16cf92de0>

    @patch("devsynth.adapters.cli.typer_adapter.run_pipeline_cmd",
autospec=True)
    @pytest.mark.medium
    def test_cli_run_succeeds(self, mock_run_pipeline_cmd):
        """Test that cli run succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(app, ["run-pipeline", "--target",
"unit-tests"])
        assert result.exit_code == 0
>       mock_run_pipeline_cmd.assert_called_once_with(
            target="unit-tests", report=None, bridge=None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:107:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (), kwargs = {'bridge': None, 'report': None, 'target': 'unit-tests'}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'run_pipeline_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:54,065 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:54,066 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'7a7621e6-d700-4615-8b5e-c141f630098a'}
2025-10-28 10:31:54,066 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'7a7621e6-d700-4615-8b5e-c141f630098a'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'7a7621e6-d700-4615-8b5e-c141f630098a'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'7a7621e6-d700-4615-8b5e-c141f630098a'}
____________________ TestTyperCLI.test_cli_config_succeeds _____________________

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadc620>

    @pytest.mark.medium
    def test_cli_config_succeeds(self):
        """Test that cli config succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(
            app, ["config", "--key", "model", "--value", "gpt-4"]
        )
        assert result.exit_code == 0
>       assert "Configuration updated" in result.output
E       AssertionError: assert 'Configuration updated' in '╭─ Error
──────────────────────────────────────────────────────────────────────╮\n│
...
│\n╰────────────────────────────────────────────────────────────────────────────
──╯\n'
E        +  where '╭─ Error
──────────────────────────────────────────────────────────────────────╮\n│
...
│\n╰────────────────────────────────────────────────────────────────────────────
──╯\n' = <Result okay>.output

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:121: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:54,209 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:31:54,209 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'84d7f7f6-5e05-47ef-8fe0-ee5a63ff49e8'}
2025-10-28 10:31:54,209 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'84d7f7f6-5e05-47ef-8fe0-ee5a63ff49e8'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'84d7f7f6-5e05-47ef-8fe0-ee5a63ff49e8'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'84d7f7f6-5e05-47ef-8fe0-ee5a63ff49e8'}
_____________ TestTyperCLI.test_cli_inspect_config_update_succeeds _____________

self = <MagicMock name='inspect_config_cmd' spec='function' id='4317484416'>
args = ('./proj', True, False), kwargs = {}
msg = "Expected 'inspect_config_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'inspect_config_cmd' to be called once.
Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadec30>
mock_cmd = <function inspect_config_cmd at 0x16cfc9620>

    @patch("devsynth.adapters.cli.typer_adapter.inspect_config_cmd",
autospec=True)
    @pytest.mark.medium
    def test_cli_inspect_config_update_succeeds(self, mock_cmd):
        """Test that cli inspect config update succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(
            app, ["inspect-config", "--path", "./proj", "--update"]
        )
        assert result.exit_code == 0
>       mock_cmd.assert_called_once_with("./proj", True, False)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:156:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = ('./proj', True, False), kwargs = {}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'inspect_config_cmd' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
_____________ TestTyperCLI.test_cli_inspect_config_prune_succeeds ______________

self = <MagicMock name='inspect_config_cmd' spec='function' id='5082089472'>
args = ('./proj', False, True), kwargs = {}
msg = "Expected 'inspect_config_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'inspect_config_cmd' to be called once.
Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_cli.TestTyperCLI object at 0x11fadf0e0>
mock_cmd = <function inspect_config_cmd at 0x16c13cc20>

    @patch("devsynth.adapters.cli.typer_adapter.inspect_config_cmd",
autospec=True)
    @pytest.mark.medium
    def test_cli_inspect_config_prune_succeeds(self, mock_cmd):
        """Test that cli inspect config prune succeeds.

        ReqID: N/A"""
        app = build_app()
        result = self.runner.invoke(
            app, ["inspect-config", "--path", "./proj", "--prune"]
        )
        assert result.exit_code == 0
>       mock_cmd.assert_called_once_with("./proj", False, True)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_cli
.py:169:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = ('./proj', False, True), kwargs = {}

    def assert_called_once_with(*args, **kwargs):
>       return mock.assert_called_once_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Expected 'inspect_config_cmd' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:220: AssertionError
__________________ TestRDFLibStore.test_store_vector_succeeds __________________

self = <tests.unit.application.memory.test_rdflib_store.TestRDFLibStore object
at 0x11fb123f0>
store = <devsynth.application.memory.rdflib_store.RDFLibStore object at
0x12f64ed50>

    @pytest.mark.medium
    def test_store_vector_succeeds(self, store):
        """Test storing and retrieving a vector.

        ReqID: N/A"""
        vector = MemoryVector(
            id="",
            content="Test vector content",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"key": "value"},
            created_at=datetime.now(),
        )
        vector_id = store.store_vector(vector)
        assert vector_id
        assert vector.id == vector_id
        retrieved_vector = store.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector_id
        assert retrieved_vector.content == "Test vector content"
>       assert len(retrieved_vector.item.embedding) == 5
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryItem' object has no attribute 'embedding'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_rdflib_store.py:233: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:56,136 - devsynth.application.memory.rdflib_store - WARNING -
Failed to initialize tokenizer: Network access disabled during tests. Token
counting will be approximate.
2025-10-28 10:31:56,137 - devsynth.application.memory.rdflib_store - INFO - No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds1/memory.ttl
------------------------------ Captured log setup ------------------------------
WARNING  devsynth.application.memory.rdflib_store:logging_setup.py:615 Failed to
initialize tokenizer: Network access disabled during tests. Token counting will
be approximate.
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds1/memory.ttl
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:56,139 - devsynth.application.memory.rdflib_store - INFO -
Saved RDF graph to
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds1/memory.ttl
2025-10-28 10:31:56,139 - devsynth.application.memory.rdflib_store - INFO -
Stored vector with ID 41250b7c-4f48-4b9a-993c-3176f532e543 in RDFLib graph
2025-10-28 10:31:56,140 - devsynth.application.memory.rdflib_store - INFO -
Retrieved vector with ID 41250b7c-4f48-4b9a-993c-3176f532e543 from RDFLib graph
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Saved RDF
graph to
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_store_vector_succeeds1/memory.ttl
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Stored
vector with ID 41250b7c-4f48-4b9a-993c-3176f532e543 in RDFLib graph
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Retrieved
vector with ID 41250b7c-4f48-4b9a-993c-3176f532e543 from RDFLib graph
______________ TestRDFLibStore.test_get_collection_stats_succeeds ______________

self = <tests.unit.application.memory.test_rdflib_store.TestRDFLibStore object
at 0x11fb13320>
store = <devsynth.application.memory.rdflib_store.RDFLibStore object at
0x12fb925a0>

    @pytest.mark.medium
    def test_get_collection_stats_succeeds(self, store):
        """Test getting collection statistics.

        ReqID: N/A"""
        stats = store.get_collection_stats()
        assert stats["vector_count"] == 0
>       assert stats["num_triples"] == 0
               ^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'num_triples'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_rdflib_store.py:305: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:31:56,211 - devsynth.application.memory.rdflib_store - WARNING -
Failed to initialize tokenizer: Network access disabled during tests. Token
counting will be approximate.
2025-10-28 10:31:56,212 - devsynth.application.memory.rdflib_store - INFO - No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_get_collection_stats_succ2/memory.ttl
------------------------------ Captured log setup ------------------------------
WARNING  devsynth.application.memory.rdflib_store:logging_setup.py:615 Failed to
initialize tokenizer: Network access disabled during tests. Token counting will
be approximate.
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 No
existing RDF graph found at
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_get_collection_stats_succ2/memory.ttl
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:56,221 - devsynth.application.memory.rdflib_store - INFO -
Retrieved collection statistics: {'collection_name': 'memory.ttl',
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ2', 'metadata': {'graph_file':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ2/memory.ttl', 'triple_count': 0}}
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.rdflib_store:logging_setup.py:615 Retrieved
collection statistics: {'collection_name': 'memory.ttl', 'vector_count': 0,
'embedding_dimensions': 0, 'persist_directory':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ2', 'metadata': {'graph_file':
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_get_collection_stats_succ2/memory.ttl', 'triple_count': 0}}
________________ TestCLICommands.test_spec_cmd_success_succeeds ________________

self = <MagicMock name='execute_command' id='5085784560'>
args = ('spec', {'requirements_file': 'requirements.md'}), kwargs = {}
msg = "Expected 'execute_command' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_command' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb2b3e0>
mock_workflow_manager = <MagicMock name='execute_command' id='5085784560'>
mock_bridge = <MagicMock name='bridge' id='5085677376'>

    @pytest.mark.medium
    def test_spec_cmd_success_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test successful spec generation.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Specs generated successfully",
        }
        spec_cmd("requirements.md")
>       mock_workflow_manager.assert_called_once_with(
            "spec", {"requirements_file": "requirements.md"}
        )
E       AssertionError: Expected 'execute_command' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:143: AssertionError
----------------------------- Captured stdout call -----------------------------
File &#x27;requirements.md&#x27; does not exist
             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































________________ TestCLICommands.test_test_cmd_success_succeeds ________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb2b8f0>
mock_workflow_manager = <MagicMock name='execute_command' id='5090154240'>
mock_bridge = <MagicMock name='bridge' id='5099832672'>

    @pytest.mark.medium
    def test_test_cmd_success_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test successful test generation.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Tests generated successfully",
        }
        test_cmd("specs.md")
        mock_workflow_manager.assert_called_once_with("test", {"spec_file":
"specs.md"})
        success_message = "[green]Tests generated from specs.md.[/green]"
>       assert any(
            call.args[0] == success_message
            for call in mock_bridge.display_result.call_args_list
        )
E       assert False
E        +  where False = any(<generator object
TestCLICommands.test_test_cmd_success_succeeds.<locals>.<genexpr> at
0x13dbe4930>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:166: AssertionError
----------------------------- Captured stdout call -----------------------------
File &#x27;specs.md&#x27; does not exist
            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































File &#x27;requirements.md&#x27; does not exist
             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































  Finishing ━━━━━━━━╸                          1/4  25% Complete 0:00:00 0:00:00
         Test Generation Summary
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Test Type         ┃ Location          ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ Unit Tests        │ tests/unit        │
│ Integration Tests │ tests/integration │
│ Behavior Tests    │ tests/behavior    │
└───────────────────┴───────────────────┘
Tests successfully generated from specs.md.
Next Steps:
1. Review the generated tests
2. Generate code: devsynth code
3. Run tests: devsynth run-pipeline
________________ TestCLICommands.test_code_cmd_success_succeeds ________________

self = <MagicMock name='execute_command' id='5327899008'>, args = ('code', {})
kwargs = {}
msg = "Expected 'execute_command' to be called once. Called 2 times.\nCalls:
[call('test', {'spec_file': 'specs.md'}), call('code', {})]."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_command' to be called once. Called
2 times.
E           Calls: [call('test', {'spec_file': 'specs.md'}), call('code', {})].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb34350>
mock_workflow_manager = <MagicMock name='execute_command' id='5327899008'>
mock_bridge = <MagicMock name='bridge' id='5327899248'>

    @pytest.mark.medium
    def test_code_cmd_success_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test successful code generation.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Code generated successfully",
        }
        code_cmd()
>       mock_workflow_manager.assert_called_once_with("code", {})
E       AssertionError: Expected 'execute_command' to be called once. Called 2
times.
E       Calls: [call('test', {'spec_file': 'specs.md'}), call('code', {})].

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:181: AssertionError
----------------------------- Captured stdout call -----------------------------
No tests found in &#x27;tests&#x27; directory.
             ┌────────────────────| DevSynth |────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘





























































                             ┌────────────────────| DevSynth
|────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘






























































File &#x27;specs.md&#x27; does not exist
            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































File &#x27;requirements.md&#x27; does not exist
             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































  Finishing ━━━━━━━━╸                          1/4  25% Complete 0:00:00 0:00:00
         Test Generation Summary
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Test Type         ┃ Location          ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ Unit Tests        │ tests/unit        │
│ Integration Tests │ tests/integration │
│ Behavior Tests    │ tests/behavior    │
└───────────────────┴───────────────────┘
Tests successfully generated from specs.md.
Next Steps:
1. Review the generated tests
2. Generate code: devsynth code
3. Run tests: devsynth run-pipeline
Generating implementation code from tests...
Code generated successfully.
Code saved to directory: src
______ TestCLICommands.test_run_pipeline_cmd_success_with_target_succeeds ______

self = <MagicMock name='execute_command' id='5327613216'>
args = ('run-pipeline', {'report': None, 'target': 'unit-tests'}), kwargs = {}
msg = "Expected 'execute_command' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_command' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb34860>
mock_workflow_manager = <MagicMock name='execute_command' id='5327613216'>
mock_bridge = <MagicMock name='bridge' id='5095544624'>

    @pytest.mark.medium
    def test_run_pipeline_cmd_success_with_target_succeeds(
        self, mock_workflow_manager, mock_bridge
    ):
        """Test successful run with target.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Target executed successfully",
        }
        run_pipeline_cmd("unit-tests")
>       mock_workflow_manager.assert_called_once_with(
            "run-pipeline", {"target": "unit-tests", "report": None}
        )
E       AssertionError: Expected 'execute_command' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:200: AssertionError
----------------------------- Captured stdout call -----------------------------
Running target: unit-tests...
2025-10-28 10:31:57,469 - cli_commands - ERROR - Command error: the JSON object
must be str, bytes or bytearray, not OptionInfo
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 95, in run_pipeline_cmd
    parsed_report = _parse_report(report)
                    ^^^^^^^^^^^^^^^^^^^^^
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 24, in _parse_report
    data = json.loads(value)
           ^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/json/__init__.py", line 339, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo
2025-10-28 10:31:57,469 - devsynth.interface.cli - ERROR - Handling error: the
JSON object must be str, bytes or bytearray, not OptionInfo
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo  │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
------------------------------ Captured log call -------------------------------
ERROR    cli_commands:logging_setup.py:615 Command error: the JSON object must
be str, bytes or bytearray, not OptionInfo
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 95, in run_pipeline_cmd
    parsed_report = _parse_report(report)
                    ^^^^^^^^^^^^^^^^^^^^^
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 24, in _parse_report
    data = json.loads(value)
           ^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/json/__init__.py", line 339, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: the JSON
object must be str, bytes or bytearray, not OptionInfo
____ TestCLICommands.test_run_pipeline_cmd_success_without_target_succeeds _____

self = <MagicMock name='execute_command' id='5329228800'>
args = ('run-pipeline', {'report': None, 'target': None}), kwargs = {}
msg = "Expected 'execute_command' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_command' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb34d70>
mock_workflow_manager = <MagicMock name='execute_command' id='5329228800'>
mock_bridge = <MagicMock name='bridge' id='5329239552'>

    @pytest.mark.medium
    def test_run_pipeline_cmd_success_without_target_succeeds(
        self, mock_workflow_manager, mock_bridge
    ):
        """Test successful run without target.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Execution complete",
        }
        run_pipeline_cmd()
>       mock_workflow_manager.assert_called_once_with(
            "run-pipeline", {"target": None, "report": None}
        )
E       AssertionError: Expected 'execute_command' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:219: AssertionError
----------------------------- Captured stdout call -----------------------------
Running default pipeline...
2025-10-28 10:31:57,520 - cli_commands - ERROR - Command error: the JSON object
must be str, bytes or bytearray, not OptionInfo
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 95, in run_pipeline_cmd
    parsed_report = _parse_report(report)
                    ^^^^^^^^^^^^^^^^^^^^^
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 24, in _parse_report
    data = json.loads(value)
           ^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/json/__init__.py", line 339, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo
2025-10-28 10:31:57,520 - devsynth.interface.cli - ERROR - Handling error: the
JSON object must be str, bytes or bytearray, not OptionInfo
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo  │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
------------------------------ Captured log call -------------------------------
ERROR    cli_commands:logging_setup.py:615 Command error: the JSON object must
be str, bytes or bytearray, not OptionInfo
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 95, in run_pipeline_cmd
    parsed_report = _parse_report(report)
                    ^^^^^^^^^^^^^^^^^^^^^
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_pipeline_cmd.py", line 24, in _parse_report
    data = json.loads(value)
           ^^^^^^^^^^^^^^^^^
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/json/__init__.py", line 339, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not OptionInfo
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: the JSON
object must be str, bytes or bytearray, not OptionInfo
______________ TestCLICommands.test_config_cmd_set_value_succeeds ______________

self = <MagicMock name='bridge.display_result' id='5069621056'>
args = ('[green]Configuration updated: model = gpt-4[/green]',), kwargs = {}
msg = "Expected 'display_result' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'display_result' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb35280>
mock_workflow_manager = <MagicMock name='execute_command' id='5327506368'>
mock_bridge = <MagicMock name='bridge' id='5330804304'>

    @pytest.mark.medium
    def test_config_cmd_set_value_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test setting a configuration value.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "message": "Configuration updated",
        }
        config_cmd("model", "gpt-4")
        mock_workflow_manager.assert_called_once_with(
            "config", {"key": "model", "value": "gpt-4"}
        )
>       mock_bridge.display_result.assert_called_once_with(
            "[green]Configuration updated: model = gpt-4[/green]"
        )
E       AssertionError: Expected 'display_result' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:239: AssertionError
----------------------------- Captured stdout call -----------------------------
Configuration updated: model = gpt-4
______________ TestCLICommands.test_config_cmd_get_value_succeeds ______________

self = <MagicMock name='bridge.display_result' id='5098150304'>
args = ('[blue]model:[/blue] gpt-4',), kwargs = {}
msg = "Expected 'display_result' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'display_result' to be called once. Called
0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb35790>
mock_workflow_manager = <MagicMock name='execute_command' id='5331586944'>
mock_bridge = <MagicMock name='bridge' id='5328313072'>

    @pytest.mark.medium
    def test_config_cmd_get_value_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test getting a configuration value.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {"success": True, "value": "gpt-4"}
        config_cmd("model")
        mock_workflow_manager.assert_called_once_with(
            "config", {"key": "model", "value": None}
        )
>       mock_bridge.display_result.assert_called_once_with("[blue]model:[/blue]
gpt-4")
E       AssertionError: Expected 'display_result' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:253: AssertionError
----------------------------- Captured stdout call -----------------------------
model: gpt-4
______________ TestCLICommands.test_config_cmd_list_all_succeeds _______________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb2bf50>
mock_workflow_manager = <MagicMock name='execute_command' id='5329147792'>
mock_bridge = <MagicMock name='bridge' id='5329153552'>

    @pytest.mark.medium
    def test_config_cmd_list_all_succeeds(self, mock_workflow_manager,
mock_bridge):
        """Test listing all configuration values.

        ReqID: N/A"""
        mock_workflow_manager.return_value = {
            "success": True,
            "config": {"model": "gpt-4", "temperature": 0.7, "max_tokens":
2000},
        }
        config_cmd()
        mock_workflow_manager.assert_called_once_with(
            "config", {"key": None, "value": None}
        )
>       assert mock_bridge.display_result.call_count == 4
E       AssertionError: assert 0 == 4
E        +  where 0 = <MagicMock name='bridge.display_result'
id='5331486336'>.call_count
E        +    where <MagicMock name='bridge.display_result' id='5331486336'> =
<MagicMock name='bridge' id='5329153552'>.display_result

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:268: AssertionError
----------------------------- Captured stdout call -----------------------------
DevSynth Configuration:
model: gpt-4
temperature: 0.7
max_tokens: 2000
_______________ TestCLICommands.test_enable_feature_cmd_succeeds _______________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb35d00>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_enable_feature_cmd_succee0')
mock_bridge = <MagicMock name='bridge' id='5096971040'>

    @pytest.mark.medium
    def test_enable_feature_cmd_succeeds(self, tmp_path, mock_bridge):
        """enable_feature_cmd should toggle a flag in project.yaml.

        ReqID: N/A"""
        dev_dir = tmp_path / ".devsynth"
        dev_dir.mkdir()
        config_file = dev_dir / "project.yaml"
        config_file.write_text("features:\n  code_generation: false\n")
        cwd = os.getcwd()
        os.chdir(tmp_path)
        try:
            cli_commands.enable_feature_cmd("code_generation")
        finally:
            os.chdir(cwd)
        cfg = UnifiedConfigLoader.load(tmp_path).config
        assert cfg.features["code_generation"] is True
>       assert any(
            "Feature 'code_generation' enabled." == call.args[0]
            for call in mock_bridge.display_result.call_args_list
        )
E       assert False
E        +  where False = any(<generator object
TestCLICommands.test_enable_feature_cmd_succeeds.<locals>.<genexpr> at
0x12ff2e9b0>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:287: AssertionError
----------------------------- Captured stdout call -----------------------------
Feature 'code_generation' enabled.
______ TestCLICommands.test_enable_feature_cmd_load_error_displays_error _______

self = <MagicMock name='bridge.display_result' id='6123096448'>
args = ('[red]Error:[/red] Failed to load configuration',)
kwargs = {'highlight': False}
expected = call('[red]Error:[/red] Failed to load configuration',
highlight=False)
cause = None, actual = []
expected_string = "display_result('[red]Error:[/red] Failed to load
configuration', highlight=False)"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.

        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: display_result('[red]Error:[/red] Failed to load
configuration', highlight=False) call not found

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1020: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb373b0>
mock_bridge = <MagicMock name='bridge' id='6123098464'>

    @pytest.mark.medium
    def test_enable_feature_cmd_load_error_displays_error(self, mock_bridge):
        """enable_feature_cmd should display an error when loading the
configuration fails.

        ReqID: N/A"""
        with patch(
            "devsynth.config.unified_loader.UnifiedConfigLoader.load",
            side_effect=Exception("Failed to load configuration"),
        ):
            cli_commands.enable_feature_cmd("code_generation")

        # Verify that an error message was displayed
>       mock_bridge.display_result.assert_any_call(
            "[red]Error:[/red] Failed to load configuration", highlight=False
        )
E       AssertionError: display_result('[red]Error:[/red] Failed to load
configuration', highlight=False) call not found

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:381: AssertionError
----------------------------- Captured stdout call -----------------------------
Error: Failed to load configuration
______ TestCLICommands.test_enable_feature_cmd_save_error_displays_error _______

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb378c0>
mock_bridge = <MagicMock name='bridge' id='6119715376'>

    @pytest.mark.medium
    def test_enable_feature_cmd_save_error_displays_error(self, mock_bridge):
        """enable_feature_cmd should display an error when saving the
configuration fails.

        ReqID: N/A"""
        cfg = MagicMock()
        cfg.features = {}
        cfg.as_dict.return_value = {"features": {"code_generation": True}}

        with (
            patch(
                "devsynth.config.unified_loader.UnifiedConfigLoader.load",
                return_value=cfg,
            ),
>           patch(
                "devsynth.application.cli.cli_commands.save_config",
                side_effect=Exception("Failed to save configuration"),
            ),
            patch("pathlib.Path.exists", return_value=False),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:399:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16cc29430>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'save_config'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_ TestCLICommands.test_enable_feature_cmd_nonexistent_feature_creates_feature __

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb37dd0>
mock_bridge = <MagicMock name='bridge' id='6119641968'>

    @pytest.mark.medium
    def test_enable_feature_cmd_nonexistent_feature_creates_feature(self,
mock_bridge):
        """enable_feature_cmd should create a new feature if it doesn't exist.

        ReqID: N/A"""
        cfg = MagicMock()
        cfg.features = {}

        with (
            patch(
                "devsynth.config.unified_loader.UnifiedConfigLoader.load",
                return_value=cfg,
            ),
>           patch("devsynth.application.cli.cli_commands.save_config"),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch("pathlib.Path.exists", return_value=False),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:425:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16cc3c740>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'save_config'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
___ TestCLICommands.test_enable_feature_cmd_already_enabled_feature_succeeds ___

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb48320>
mock_bridge = <MagicMock name='bridge' id='5095106576'>

    @pytest.mark.medium
    def test_enable_feature_cmd_already_enabled_feature_succeeds(self,
mock_bridge):
        """enable_feature_cmd should succeed when enabling an already enabled
feature.

        ReqID: N/A"""
        cfg = MagicMock()
        cfg.features = {"code_generation": True}

        with (
            patch(
                "devsynth.config.unified_loader.UnifiedConfigLoader.load",
                return_value=cfg,
            ),
>           patch("devsynth.application.cli.cli_commands.save_config"),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch("pathlib.Path.exists", return_value=False),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:451:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12f671910>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'save_config'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__ TestCLICommands.test_init_creates_config_and_commands_use_loader_succeeds ___

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb48ce0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_init_creates_config_and_c0')
mock_bridge = <MagicMock name='bridge' id='6119740800'>

    @pytest.mark.medium
    def test_init_creates_config_and_commands_use_loader_succeeds(
        self, tmp_path, mock_bridge
    ):
        """Test that init creates config and commands use loader succeeds.

        ReqID: N/A"""
        cwd = os.getcwd()
        os.chdir(tmp_path)
        try:
            cli_commands.init_cmd(
                root=str(tmp_path),
                language="python",
                goals="goal",
                memory_backend="memory",
                auto_confirm=True,
            )
        finally:
            os.chdir(cwd)
        cfg_path = tmp_path / ".devsynth" / "project.yaml"
        assert cfg_path.exists()
>       real_load = cli_commands.load_config
                    ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.application.cli.cli_commands' has no
attribute 'load_config'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:509: AttributeError
----------------------------- Captured stdout call -----------------------------
⠋ Generating project files ━━━━━━━━━╸          1/2  50% Complete 0:00:00 -:--:--
2025-10-28 10:31:58,223 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
Initialization complete
------------------------------ Captured log call -------------------------------
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
__________ TestCLICommands.test_spec_cmd_missing_openai_key_succeeds ___________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb49c40>
mock_workflow_manager = <MagicMock name='execute_command' id='6119665264'>
mock_bridge = <MagicMock name='bridge' id='6123647008'>

    @pytest.mark.medium
    def test_spec_cmd_missing_openai_key_succeeds(
        self, mock_workflow_manager, mock_bridge
    ):
        """spec_cmd should warn when OpenAI API key is missing.

        ReqID: N/A"""

        class DummySettings:
            vector_store_enabled = False
            memory_store_type = "chromadb"
            provider_type = "openai"
            openai_api_key = None
            lm_studio_endpoint = None

        with (
>           patch(
                "devsynth.application.cli.cli_commands.get_settings",
                return_value=DummySettings,
            ),
            patch(
                "devsynth.application.cli.cli_commands._check_services",
                new=ORIG_CHECK_SERVICES,
            ),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:571:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16cff6f60>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'get_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______ TestCLICommands.test_spec_cmd_missing_chromadb_package_succeeds ________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb2be90>
mock_workflow_manager = <MagicMock name='execute_command' id='6119709376'>
mock_bridge = <MagicMock name='bridge' id='6119675792'>

    @pytest.mark.medium
    def test_spec_cmd_missing_chromadb_package_succeeds(
        self, mock_workflow_manager, mock_bridge
    ):
        """spec_cmd should warn when ChromaDB package is unavailable.

        ReqID: N/A"""

        class DummySettings:
            vector_store_enabled = True
            memory_store_type = "chromadb"
            provider_type = "openai"
            openai_api_key = "key"
            lm_studio_endpoint = None
            enable_chromadb = True

        with (
>           patch(
                "devsynth.application.cli.cli_commands.get_settings",
                return_value=DummySettings,
            ),
            patch(
                "devsynth.application.cli.cli_commands.importlib.util.find_spec"
,
                return_value=None,
            ),
            patch(
                "devsynth.application.cli.cli_commands._check_services",
                new=ORIG_CHECK_SERVICES,
            ),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:604:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16cc355b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'get_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________ TestCLICommands.test_spec_cmd_missing_kuzu_package_succeeds __________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fa8bd40>
mock_workflow_manager = <MagicMock name='execute_command' id='5328417232'>
mock_bridge = <MagicMock name='bridge' id='5069634544'>

    @pytest.mark.medium
    def test_spec_cmd_missing_kuzu_package_succeeds(
        self, mock_workflow_manager, mock_bridge
    ):
        """spec_cmd should warn when Kuzu package is unavailable.

        ReqID: N/A"""

        class DummySettings:
            vector_store_enabled = True
            memory_store_type = "kuzu"
            provider_type = "openai"
            openai_api_key = "key"
            lm_studio_endpoint = None

        with (
>           patch(
                "devsynth.application.cli.cli_commands.get_settings",
                return_value=DummySettings,
            ),
            patch(
                "devsynth.application.cli.cli_commands.importlib.util.find_spec"
,
                return_value=None,
            ),
            patch(
                "devsynth.application.cli.cli_commands._check_services",
                new=ORIG_CHECK_SERVICES,
            ),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:640:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13d991b50>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'get_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ TestCLICommands.test_config_key_autocomplete_succeeds _____________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb49dc0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_config_key_autocomplete_s0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16cc34470>

    @pytest.mark.medium
    def test_config_key_autocomplete_succeeds(self, tmp_path, monkeypatch):
        """Test that config key autocomplete succeeds.

        ReqID: N/A"""
        cfg_dir = tmp_path / ".devsynth"
        cfg_dir.mkdir()
        (cfg_dir / "project.yaml").write_text("language: python\nmodel:
gpt-4\n")
        monkeypatch.chdir(tmp_path)
>       result = cli_commands.config_key_autocomplete(None, "l")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.application.cli.cli_commands' has no
attribute 'config_key_autocomplete'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:669: AttributeError
______________ TestCLICommands.test_check_services_warns_succeeds ______________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb4a2a0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16cc37620>
capsys = <_pytest.capture.CaptureFixture object at 0x16cc29910>

    @pytest.mark.medium
    def test_check_services_warns_succeeds(self, monkeypatch, capsys):
        """Test that check services warns succeeds.

        ReqID: N/A"""

        class DummySettings:
            vector_store_enabled = False
            memory_store_type = "chromadb"
            provider_type = "openai"
            openai_api_key = None
            lm_studio_endpoint = None

>       monkeypatch.setattr(cli_commands, "get_settings", lambda: DummySettings)
E       AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> has no attribute 'get_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:685: AttributeError
___________ TestCLICommands.test_doctor_cmd_invokes_loader_succeeds ____________

self = <MagicMock name='load' id='6119657392'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'load' to have been called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb4a7b0>

    @pytest.mark.medium
    def test_doctor_cmd_invokes_loader_succeeds(self):
        """Test that doctor cmd invokes loader succeeds.

        ReqID: N/A"""
        with (
            patch(
                "devsynth.config.unified_loader.UnifiedConfigLoader.load"
            ) as mock_load,
            patch(
                "devsynth.application.cli.commands.doctor_cmd.bridge.print"
            ) as mock_print,
        ):
            cli_commands.doctor_cmd("config")
>           mock_load.assert_called_once()
E           AssertionError: Expected 'load' to have been called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:706: AssertionError
________________ TestCLICommands.test_check_cmd_alias_succeeds _________________

self = <MagicMock name='doctor_cmd' id='5095501904'>, args = ('config',)
kwargs = {}, msg = "Expected 'doctor_cmd' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'doctor_cmd' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb4ac90>

    @pytest.mark.medium
    def test_check_cmd_alias_succeeds(self):
        """Test that check cmd alias succeeds.

        ReqID: N/A"""
        with patch("devsynth.application.cli.cli_commands.doctor_cmd") as
mock_doctor:
            cli_commands.check_cmd("config")
>           mock_doctor.assert_called_once_with("config")
E           AssertionError: Expected 'doctor_cmd' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:716: AssertionError
----------------------------- Captured stdout call -----------------------------
No project configuration found. Run &#x27;devsynth init&#x27; to create it.
Missing expected directories: src, tests, docs
Missing environment variables: ANTHROPIC_API_KEY
Warning: configuration file not found: config/default.yml
Warning: configuration file not found: config/development.yml
Warning: configuration file not found: config/testing.yml
Warning: configuration file not found: config/staging.yml
Warning: configuration file not found: config/production.yml
Configuration issues detected. Run &#x27;devsynth init&#x27; to generate
defaults.
_______________ TestCLICommands.test_init_cmd_wizard_runs_wizard _______________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb4b620>

    @pytest.mark.medium
    def test_init_cmd_wizard_runs_wizard(self):
        """Wizard mode should invoke SetupWizard.run."""

>       with patch("devsynth.application.cli.cli_commands.SetupWizard") as wiz:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:752:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12fc6d2b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'SetupWizard'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ TestCLICommands.test_spec_cmd_invalid_file __________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb4bb30>
mock_workflow_manager = <MagicMock name='execute_command' id='5330809776'>
mock_bridge = <MagicMock name='bridge' id='5330808624'>

    @pytest.mark.medium
    def test_spec_cmd_invalid_file(self, mock_workflow_manager, mock_bridge):
        """Invalid requirements file should not trigger generation."""

        with (
>           patch(
                "devsynth.application.cli.cli_commands._validate_file_path",
                return_value="bad",
            ),
            patch.object(cli_commands.bridge, "confirm_choice",
return_value=False),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:762:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13da45580>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute '_validate_file_path'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ TestCLICommands.test_test_cmd_invalid_file __________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5c080>
mock_workflow_manager = <MagicMock name='execute_command' id='5328648000'>
mock_bridge = <MagicMock name='bridge' id='5328645072'>

    @pytest.mark.medium
    def test_test_cmd_invalid_file(self, mock_workflow_manager, mock_bridge):
        """Invalid spec file should not trigger generation."""

        with (
>           patch(
                "devsynth.application.cli.cli_commands._validate_file_path",
                return_value="bad",
            ),
            patch.object(cli_commands.bridge, "confirm_choice",
return_value=False),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:776:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x13da5bad0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute '_validate_file_path'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ TestCLICommands.test_code_cmd_no_tests ____________________

self = <MagicMock name='execute_command' id='5095105664'>

    def assert_not_called(self):
        """assert that the mock was never called.
        """
        if self.call_count != 0:
            msg = ("Expected '%s' to not have been called. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_command' to not have been called.
Called 2 times.
E           Calls: [call('test', {'spec_file': 'specs.md'}),
E            call().get('content'),
E            call('code', {}),
E            call().get('content'),
E            call().get('success'),
E            call().get().__bool__(),
E            call().get('output_dir', 'src'),
E            call().get().__str__()].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:910: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5c560>
mock_workflow_manager = <MagicMock name='execute_command' id='5095105664'>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_code_cmd_no_tests0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcda720>

    @pytest.mark.medium
    def test_code_cmd_no_tests(self, mock_workflow_manager, tmp_path,
monkeypatch):
        """code_cmd exits when no tests are present."""

        monkeypatch.chdir(tmp_path)
        with patch.object(cli_commands.bridge, "confirm_choice",
return_value=False):
            code_cmd()
>       mock_workflow_manager.assert_not_called()
E       AssertionError: Expected 'execute_command' to not have been called.
Called 2 times.
E       Calls: [call('test', {'spec_file': 'specs.md'}),
E        call().get('content'),
E        call('code', {}),
E        call().get('content'),
E        call().get('success'),
E        call().get().__bool__(),
E        call().get('output_dir', 'src'),
E        call().get().__str__()].
E
E       pytest introspection follows:
E
E       Args:
E       assert ('code', {}) == ()
E
E         Left contains 2 more items, first extra item: 'code'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:792: AssertionError
----------------------------- Captured stdout call -----------------------------
No tests found in &#x27;tests&#x27; directory.
             ┌────────────────────| DevSynth |────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘





























































                             ┌────────────────────| DevSynth
|────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘






























































File &#x27;specs.md&#x27; does not exist
            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































File &#x27;requirements.md&#x27; does not exist
             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































  Finishing ━━━━━━━━╸                          1/4  25% Complete 0:00:00 0:00:00
2025-10-28 10:31:59,447 - cli_commands - ERROR - Command error: {}
2025-10-28 10:31:59,447 - devsynth.interface.cli - ERROR - Handling error: {}
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  Error: {}                                                                   │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
Generating implementation code from tests...
Code generated successfully.
Code saved to directory: &lt;MagicMock name=&#x27;execute_command().get()&#x27;
id=&#x27;5095490000&#x27;&gt;
------------------------------ Captured log call -------------------------------
ERROR    cli_commands:logging_setup.py:615 Command error: {}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {}
_________________ TestCLICommands.test_config_cmd_invalid_key __________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5cf50>
mock_bridge = <MagicMock name='bridge' id='6129048976'>

    @pytest.mark.medium
    def test_config_cmd_invalid_key(self, mock_bridge):
        """config_cmd should report errors from workflow."""

        with (
            patch(
                "devsynth.core.workflows.execute_command",
                return_value={"success": False, "message": "bad"},
            ),
>           patch("devsynth.application.cli.cli_commands._handle_error") as
herr,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:811:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16d5b40b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute '_handle_error'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________ TestCLICommands.test_gather_cmd_error_propagates _______________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5d430>

    @pytest.mark.medium
    def test_gather_cmd_error_propagates(self):
        """Errors from gather_requirements propagate to caller."""

>       with patch(
            "devsynth.application.cli.cli_commands.gather_requirements",
            side_effect=FileNotFoundError("boom"),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:820:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16d5b4e00>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'gather_requirements'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
___________________ TestCLICommands.test_refactor_cmd_error ____________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5d910>
mock_bridge = <MagicMock name='bridge' id='6129268256'>

    @pytest.mark.medium
    def test_refactor_cmd_error(self, mock_bridge):
        """refactor_cmd should display error when workflow fails."""

>       with patch(
            "devsynth.application.cli.orchestration.refactor_workflow.refactor_w
orkflow_manager.execute_refactor_workflow",
            return_value={"success": False, "message": "fail"},
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:831:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pkgutil.py:528: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'orchestration'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""

        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute
'orchestration'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError
___________________ TestCLICommands.test_inspect_cmd_failure ___________________

self = <MagicMock name='bridge.display_result' id='5329524240'>
args = ('[red]Error:[/red] bad',), kwargs = {'highlight': False}
expected = call('[red]Error:[/red] bad', highlight=False), cause = None
actual = []
expected_string = "display_result('[red]Error:[/red] bad', highlight=False)"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.

        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: display_result('[red]Error:[/red] bad',
highlight=False) call not found

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1020: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb49eb0>
mock_bridge = <MagicMock name='bridge' id='6129714144'>

    @pytest.mark.medium
    def test_inspect_cmd_failure(self, mock_bridge):
        """inspect_cmd should show error message when workflow fails."""

        with patch(
            "devsynth.core.workflows.inspect_requirements",
            return_value={"success": False, "message": "bad"},
        ):
            inspect_cmd("req.md")
>       mock_bridge.display_result.assert_any_call(
            "[red]Error:[/red] bad", highlight=False
        )
E       AssertionError: display_result('[red]Error:[/red] bad', highlight=False)
call not found

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:847: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:31:59,923 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
Error: Command failed
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
______________ TestCLICommands.test_webapp_cmd_invalid_framework _______________

framework = 'bad', name = 'app'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_webapp_cmd_invalid_framew0'
force = False

    def webapp_cmd(
        framework: str = "flask",
        name: str = "webapp",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a web application with the specified framework.

        Example:
            `devsynth webapp --framework flask --name myapp --path ./apps`
        """
        bridge = _resolve_bridge(bridge)
        try:
            # Show a welcome message for the webapp command
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Web Application Generator[/bold
blue]\n\n",
                    f"This command will generate a basic web application using
the {framework} framework.",
                    title="Web Application Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x13daa11c0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5daf0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_webapp_cmd_invalid_framew0')

    @pytest.mark.medium
    def test_webapp_cmd_invalid_framework(self, tmp_path):
        """Unknown framework raises an error."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
        ):
>           webapp_cmd(framework="bad", name="app", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:871:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:290: in webapp_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x13daa1130>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during web application generation.
________________ TestCLICommands.test_serve_cmd_passes_options _________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5dfd0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d553c80>

    @pytest.mark.medium
    def test_serve_cmd_passes_options(self, monkeypatch):
        """serve_cmd forwards host and port to uvicorn."""

>       monkeypatch.setattr(cli_commands, "configure_logging", lambda: None)
E       AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> has no attribute 'configure_logging'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:881: AttributeError
________________ TestCLICommands.test_webapp_cmd_flask_succeeds ________________

framework = 'flask', name = 'myapp'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_webapp_cmd_flask_succeeds0'
force = False

    def webapp_cmd(
        framework: str = "flask",
        name: str = "webapp",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a web application with the specified framework.

        Example:
            `devsynth webapp --framework flask --name myapp --path ./apps`
        """
        bridge = _resolve_bridge(bridge)
        try:
            # Show a welcome message for the webapp command
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Web Application Generator[/bold
blue]\n\n",
                    f"This command will generate a basic web application using
the {framework} framework.",
                    title="Web Application Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5ad8e0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5e4b0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_webapp_cmd_flask_succeeds0')

    @pytest.mark.medium
    def test_webapp_cmd_flask_succeeds(self, tmp_path):
        """webapp_cmd successfully generates a Flask application."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           webapp_cmd(framework="flask", name="myapp", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:910:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:290: in webapp_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5a3650>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during web application generation.
_______________ TestCLICommands.test_webapp_cmd_fastapi_succeeds _______________

framework = 'fastapi', name = 'myapi'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_webapp_cmd_fastapi_succee0'
force = False

    def webapp_cmd(
        framework: str = "flask",
        name: str = "webapp",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a web application with the specified framework.

        Example:
            `devsynth webapp --framework flask --name myapp --path ./apps`
        """
        bridge = _resolve_bridge(bridge)
        try:
            # Show a welcome message for the webapp command
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Web Application Generator[/bold
blue]\n\n",
                    f"This command will generate a basic web application using
the {framework} framework.",
                    title="Web Application Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5af230>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5e990>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_webapp_cmd_fastapi_succee0')

    @pytest.mark.medium
    def test_webapp_cmd_fastapi_succeeds(self, tmp_path):
        """webapp_cmd successfully generates a FastAPI application."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           webapp_cmd(framework="fastapi", name="myapi", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:953:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:290: in webapp_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d515f40>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during web application generation.
_______ TestCLICommands.test_webapp_cmd_existing_dir_without_force_fails _______

framework = 'flask', name = 'myapp'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_webapp_cmd_existing_dir_w0'
force = False

    def webapp_cmd(
        framework: str = "flask",
        name: str = "webapp",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a web application with the specified framework.

        Example:
            `devsynth webapp --framework flask --name myapp --path ./apps`
        """
        bridge = _resolve_bridge(bridge)
        try:
            # Show a welcome message for the webapp command
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Web Application Generator[/bold
blue]\n\n",
                    f"This command will generate a basic web application using
the {framework} framework.",
                    title="Web Application Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c91b5f0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5ee70>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_webapp_cmd_existing_dir_w0')

    @pytest.mark.medium
    def test_webapp_cmd_existing_dir_without_force_fails(self, tmp_path):
        """webapp_cmd fails when directory exists and force is False."""

        with (
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.path.exists", return_value=True),
            patch("os.makedirs"),
            patch("builtins.open", mock_open()),
        ):
>           webapp_cmd(framework="flask", name="myapp", path=str(tmp_path),
force=False)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:971:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:290: in webapp_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d59f710>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during web application generation.
_______ TestCLICommands.test_webapp_cmd_existing_dir_with_force_succeeds _______

framework = 'flask', name = 'myapp'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_webapp_cmd_existing_dir_w1'
force = True

    def webapp_cmd(
        framework: str = "flask",
        name: str = "webapp",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a web application with the specified framework.

        Example:
            `devsynth webapp --framework flask --name myapp --path ./apps`
        """
        bridge = _resolve_bridge(bridge)
        try:
            # Show a welcome message for the webapp command
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Web Application Generator[/bold
blue]\n\n",
                    f"This command will generate a basic web application using
the {framework} framework.",
                    title="Web Application Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d590470>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5f350>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_webapp_cmd_existing_dir_w1')

    @pytest.mark.medium
    def test_webapp_cmd_existing_dir_with_force_succeeds(self, tmp_path):
        """webapp_cmd succeeds when directory exists and force is True."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.path.exists", return_value=True),
            patch("shutil.rmtree"),
            patch("os.makedirs"),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           webapp_cmd(framework="flask", name="myapp", path=str(tmp_path),
force=True)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1000:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/webapp_cmd.py:290: in webapp_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5ae1e0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during web application generation.
________________ TestCLICommands.test_dbschema_cmd_invalid_type ________________

db_type = 'foo', name = 'schema'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_invalid_type0'
force = False

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c7e8830>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5f830>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_invalid_type0')

    @pytest.mark.medium
    def test_dbschema_cmd_invalid_type(self, tmp_path):
        """Unknown db type raises an error."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
        ):
>           dbschema_cmd(db_type="foo", name="schema", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1027:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c7e9880>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
______________ TestCLICommands.test_dbschema_cmd_sqlite_succeeds _______________

db_type = 'sqlite', name = 'mydb'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_sqlite_succe0'
force = False

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c9336e0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5fd10>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_sqlite_succe0')

    @pytest.mark.medium
    def test_dbschema_cmd_sqlite_succeeds(self, tmp_path):
        """dbschema_cmd successfully generates a SQLite database schema."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           dbschema_cmd(db_type="sqlite", name="mydb", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1054:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c933b60>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
_______________ TestCLICommands.test_dbschema_cmd_mysql_succeeds _______________

db_type = 'mysql', name = 'mydb'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_mysql_succee0'
force = False

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c05f2f0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb64230>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_mysql_succee0')

    @pytest.mark.medium
    def test_dbschema_cmd_mysql_succeeds(self, tmp_path):
        """dbschema_cmd successfully generates a MySQL database schema."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           dbschema_cmd(db_type="mysql", name="mydb", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1091:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c7e9e50>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
______________ TestCLICommands.test_dbschema_cmd_mongodb_succeeds ______________

db_type = 'mongodb', name = 'mydb'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_mongodb_succ0'
force = False

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c068080>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb64710>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_mongodb_succ0')

    @pytest.mark.medium
    def test_dbschema_cmd_mongodb_succeeds(self, tmp_path):
        """dbschema_cmd successfully generates a MongoDB database schema."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.makedirs"),
            patch("os.path.exists", return_value=False),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           dbschema_cmd(db_type="mongodb", name="mydb", path=str(tmp_path))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1128:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16c06ac60>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
______ TestCLICommands.test_dbschema_cmd_existing_dir_without_force_fails ______

db_type = 'sqlite', name = 'mydb'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_existing_dir0'
force = False

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5c1580>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb64bf0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_existing_dir0')

    @pytest.mark.medium
    def test_dbschema_cmd_existing_dir_without_force_fails(self, tmp_path):
        """dbschema_cmd fails when directory exists and force is False."""

        with (
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.path.exists", return_value=True),
            patch("os.makedirs"),
            patch("builtins.open", mock_open()),
        ):
>           dbschema_cmd(db_type="sqlite", name="mydb", path=str(tmp_path),
force=False)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1155:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5c15e0>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
______ TestCLICommands.test_dbschema_cmd_existing_dir_with_force_succeeds ______

db_type = 'sqlite', name = 'mydb'
path =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_dbschema_cmd_existing_dir1'
force = True

    def dbschema_cmd(
        db_type: str = "sqlite",
        name: str = "database",
        path: str = ".",
        force: bool = False,
        *,
        bridge: Optional[UXBridge] = None,
    ) -> None:
        """Generate a database schema for the specified database type."""
        bridge = _resolve_bridge(bridge)
        try:
>           bridge.print(
                Panel(
                    f"[bold blue]DevSynth Database Schema Generator[/bold
blue]\n\n",
                    f"This command will generate a database schema for
{db_type}.",
                    title="Database Schema Generator",
                    border_style="blue",
                )
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5c2a20>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb650d0>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_dbschema_cmd_existing_dir1')

    @pytest.mark.medium
    def test_dbschema_cmd_existing_dir_with_force_succeeds(self, tmp_path):
        """dbschema_cmd succeeds when directory exists and force is True."""

        with (
            patch.object(
                cli_commands.bridge,
                "create_progress",
                return_value=MagicMock(
                    __enter__=lambda s: s,
                    __exit__=lambda s, *a: None,
                    update=lambda *a, **k: None,
                    complete=lambda *a, **k: None,
                ),
            ),
            patch.object(cli_commands.bridge, "print"),
            patch.object(cli_commands.bridge, "display_result") as disp,
            patch("os.path.exists", return_value=True),
            patch("shutil.rmtree"),
            patch("os.makedirs"),
            patch("builtins.open", mock_open()) as mock_file,
        ):
>           dbschema_cmd(db_type="sqlite", name="mydb", path=str(tmp_path),
force=True)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/dbschema_cmd.py:324: in dbschema_cmd
    bridge.print(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:705: in display_result
    if re.search(rich_markup_pattern, message):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

pattern =
'\\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|br...ack|on_bright_red|on_bright_green|on_br
ight_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_white)\\]'
string = <rich.panel.Panel object at 0x16d5c0050>, flags = 0

    def search(pattern, string, flags=0):
        """Scan through string looking for a match to the pattern, returning
        a Match object, or None if no match was found."""
>       return _compile(pattern, flags).search(string)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: expected string or bytes-like object, got 'Panel'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/re/__init__.py:177: TypeError
----------------------------- Captured stdout call -----------------------------
✗ Error: expected string or bytes-like object, got &#x27;Panel&#x27;
An unexpected error occurred during database schema generation.
_______________ TestCLICommands.test_webui_cmd_success_succeeds ________________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb655e0>
mock_bridge = <MagicMock name='bridge' id='6117090768'>

    @pytest.mark.medium
    def test_webui_cmd_success_succeeds(self, mock_bridge):
        """webui_cmd successfully launches the WebUI.

        ReqID: N/A"""
        # Mock the webui module and run function
        mock_run = MagicMock()
>       with patch("devsynth.application.cli.cli_commands.run", mock_run):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1198:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16c9b7080>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'run'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________ TestCLICommands.test_webui_cmd_import_error_displays_error __________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb65af0>
mock_bridge = <MagicMock name='bridge' id='6107330080'>

    @pytest.mark.medium
    def test_webui_cmd_import_error_displays_error(self, mock_bridge):
        """webui_cmd displays an error when the WebUI module is unavailable.

        ReqID: N/A"""
        # Mock an ImportError when importing the webui module
>       with patch(
            "devsynth.application.cli.cli_commands.run",
            side_effect=ImportError("No module named 'streamlit'"),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py:1216:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16c02d340>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'run'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________ TestCLICommands.test_webui_cmd_runtime_error_displays_error __________

self = <tests.unit.general.test_unit_cli_commands.TestCLICommands object at
0x11fb5dfa0>
mock_bridge = <MagicMock name='bridge' id='6122888112'>

    @pytest.mark.medium
    def test_webui_cmd_runtime_error_displays_error(self, mock_bridge):
        """webui_cmd displays an error when the WebUI fails to launch.

        ReqID: N/A"""
        # Mock a RuntimeError when running the WebUI
>       with patch(
            "devsynth.application.cli.cli_commands.run",
            side_effect=RuntimeError("Failed to launch WebUI"),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_uni
t_cli_commands.py🔢
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16cf3ea80>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.cli.cli_commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/cli_commands.py'> does not have the attribute 'run'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__ TestWSDETeamCoordinator.test_delegate_task_multi_agent_consensus_succeeds ___

self = <tests.unit.general.test_wsde_team_coordinator.TestWSDETeamCoordinator
object at 0x11fb67500>

    @pytest.mark.medium
    def test_delegate_task_multi_agent_consensus_succeeds(self):
        """Test delegating a task with multiple agents using consensus-based
decision making.

        ReqID: N/A"""
        self.coordinator.create_team("test_team")
        self.coordinator.add_agent(self.agent1)
        self.coordinator.add_agent(self.agent2)
        self.coordinator.add_agent(self.agent3)
        self.coordinator.add_agent(self.agent4)
        task = {
            "type": "code",
            "language": "python",
            "description": "Implement a feature",
        }
        self.agent1.process.return_value = {"result": "Design for the feature"}
        self.agent2.process.return_value = {"result": "Implementation of the
feature"}
        self.agent3.process.return_value = {"result": "Tests for the feature"}
        self.agent4.process.return_value = {"result": "Validation of the
feature"}
        team = self.coordinator.teams["test_team"]
        team.build_consensus = MagicMock(
            return_value={
                "consensus": "Consensus solution incorporating all
perspectives",
                "contributors": ["agent1", "agent2", "agent3", "agent4"],
                "method": "consensus_synthesis",
                "reasoning": "Combined the best elements from all solutions",
            }
        )
        result = self.coordinator.delegate_task(task)
        assert "result" in result
>       assert result["result"] == "Consensus solution incorporating all
perspectives"
E       AssertionError: assert None == 'Consensus solution incorporating all
perspectives'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_coordinator.py:121: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_team
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_team
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_team
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': 'agent3', 'designer': None, 'evaluator': None}
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_core - INFO - Added agent
agent4 to team test_team
2025-10-28 10:32:01,595 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': 'agent3', 'designer': 'agent4', 'evaluator': None}
2025-10-28 10:32:01,597 - devsynth.domain.models.wsde_roles - INFO - Selected
agent2 as primus based on expertise
2025-10-28 10:32:01,597 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,597 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_enhanced_dialectical -
INFO - Applying enhanced multi-solution dialectical reasoning to task:
84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 1 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 2 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 3 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 4 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
2025-10-28 10:32:01,598 - devsynth.domain.models.wsde_solution_analysis - INFO -
Generating comparative analysis for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor': None,
'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor':
'agent3', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent4 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor':
'agent3', 'designer': 'agent4', 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected agent2
as primus based on expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_enhanced_dialectical:logging_setup.py:615
Applying enhanced multi-solution dialectical reasoning to task:
84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 1 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 2 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 3 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 4 for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Generating comparative analysis for task: 84d7554f-3f43-42b8-b649-97a19f4d35c3
___ TestWSDETeamCoordinator.test_delegate_task_agent_failure_continues_fails ___

self = <tests.unit.general.test_wsde_team_coordinator.TestWSDETeamCoordinator
object at 0x11fb743e0>

    @pytest.mark.medium
    def test_delegate_task_agent_failure_continues_fails(self):
        """If one agent fails, others still contribute to the result.

        ReqID: N/A"""
        self.coordinator.create_team("test_team")
        self.coordinator.add_agent(self.agent1)
        self.coordinator.add_agent(self.agent2)
        self.coordinator.add_agent(self.agent3)
        task = {"type": "code", "language": "python"}
        self.agent1.process.side_effect = Exception("boom")
        self.agent2.process.return_value = {"result": "code"}
        self.agent3.process.return_value = {"result": "tests"}
        team = self.coordinator.teams["test_team"]
        team.build_consensus = MagicMock(
            return_value={
                "consensus": "done",
                "contributors": ["agent2", "agent3"],
                "method": "consensus_synthesis",
            }
        )
        result = self.coordinator.delegate_task(task)
>       assert result["result"] == "done"
E       AssertionError: assert None == 'done'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_coordinator.py:175: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:01,634 - devsynth.domain.models.wsde_core - INFO - Added agent
agent1 to team test_team
2025-10-28 10:32:01,634 - devsynth.domain.models.wsde_core - INFO - Added agent
agent2 to team test_team
2025-10-28 10:32:01,634 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:32:01,634 - devsynth.domain.models.wsde_core - INFO - Added agent
agent3 to team test_team
2025-10-28 10:32:01,634 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2',
'supervisor': 'agent3', 'designer': None, 'evaluator': None}
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_roles - INFO - Selected
agent2 as primus based on expertise
2025-10-28 10:32:01,635 - devsynth.adapters.agents.agent_adapter - WARNING -
Agent agent1 failed to process task: boom
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 01c40d04-c252-4e48-9176-5e8aa6743dc9
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 01c40d04-c252-4e48-9176-5e8aa6743dc9
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_enhanced_dialectical -
INFO - Applying enhanced multi-solution dialectical reasoning to task:
01c40d04-c252-4e48-9176-5e8aa6743dc9
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 1 for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 2 for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
2025-10-28 10:32:01,635 - devsynth.domain.models.wsde_solution_analysis - INFO -
Generating comparative analysis for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent2 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor': None,
'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
agent3 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor':
'agent3', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected agent2
as primus based on expertise
WARNING  devsynth.adapters.agents.agent_adapter:logging_setup.py:615 Agent
agent1 failed to process task: boom
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 01c40d04-c252-4e48-9176-5e8aa6743dc9
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 01c40d04-c252-4e48-9176-5e8aa6743dc9
INFO     devsynth.domain.models.wsde_enhanced_dialectical:logging_setup.py:615
Applying enhanced multi-solution dialectical reasoning to task:
01c40d04-c252-4e48-9176-5e8aa6743dc9
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 1 for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Analyzing solution 2 for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615
Generating comparative analysis for task: 01c40d04-c252-4e48-9176-5e8aa6743dc9
____________ test_health_endpoint_requires_authentication_succeeds _____________

client = <starlette.testclient.TestClient object at 0x13da00740>
clean_state = None

    @pytest.mark.medium
    def test_health_endpoint_requires_authentication_succeeds(client,
clean_state):
        """Test that the health endpoint requires authentication.

        ReqID: N/A"""
        response = client.get("/health")
        assert response.status_code == 401

        response = client.get("/health", headers={"Authorization": "Bearer
invalid_token"})
        assert response.status_code == 401

        success = client.get("/health", headers={"Authorization": "Bearer
test_token"})
        assert success.status_code == 200
>       payload = HealthResponse.model_validate(success.json())
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for
HealthResponse
E       uptime
E         Field required [type=missing, input_value={'status': 'ok'},
input_type=dict]
E           For further information visit
https://errors.pydantic.dev/2.12/v/missing

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
pi_endpoints.py:179: ValidationError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:01,840 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 401 Unauthorized"
2025-10-28 10:32:01,848 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 401 Unauthorized"
2025-10-28 10:32:01,857 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 200 OK"
____________ test_metrics_endpoint_requires_authentication_succeeds ____________

client = <starlette.testclient.TestClient object at 0x16cc280b0>
clean_state = None

    @pytest.mark.medium
    def test_metrics_endpoint_requires_authentication_succeeds(client,
clean_state):
        """Test that the metrics endpoint requires authentication.

        ReqID: N/A"""
        response = client.get("/metrics")
        assert response.status_code == 401

        response = client.get("/metrics", headers={"Authorization": "Bearer
invalid_token"})
        assert response.status_code == 401

        success = client.get("/metrics", headers={"Authorization": "Bearer
test_token"})
        assert success.status_code == 200
        metrics_lines = tuple(filter(None, success.text.splitlines()))
        metrics_payload = MetricsResponse(metrics=metrics_lines)
>       assert any(
            line.startswith("api_uptime_seconds") for line in
metrics_payload.metrics
        )
E       assert False
E        +  where False = any(<generator object
test_metrics_endpoint_requires_authentication_succeeds.<locals>.<genexpr> at
0x13d8e2cf0>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
pi_endpoints.py:200: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:01,893 - httpx - INFO - HTTP Request: GET
http://testserver/metrics "HTTP/1.1 401 Unauthorized"
2025-10-28 10:32:01,899 - httpx - INFO - HTTP Request: GET
http://testserver/metrics "HTTP/1.1 401 Unauthorized"
2025-10-28 10:32:01,914 - httpx - INFO - HTTP Request: GET
http://testserver/metrics "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics
"HTTP/1.1 200 OK"
_____________ test_cliprogressindicator_multiple_subtasks_succeeds _____________

clean_state = None

    @pytest.mark.medium
    def test_cliprogressindicator_multiple_subtasks_succeeds(clean_state):
        """Test handling of multiple subtasks in CLIProgressIndicator.

        ReqID: N/A"""
        console = MagicMock()
        progress_mock = MagicMock()
        with patch("devsynth.interface.cli.Progress",
return_value=progress_mock):
            indicator = CLIProgressIndicator(console, "Main task", 100)
            progress_mock.add_task.side_effect = ["subtask1", "subtask2",
"subtask3"]
            subtask1 = indicator.add_subtask("Subtask 1", 30)
            subtask2 = indicator.add_subtask("Subtask 2", 40)
            subtask3 = indicator.add_subtask("Subtask 3", 50)
            assert subtask1 == "subtask1"
            assert subtask2 == "subtask2"
            assert subtask3 == "subtask3"
            assert progress_mock.add_task.call_count == 4
            assert progress_mock.add_task.call_args_list[1] == call(
                "  ↳ Subtask 1", total=30
            )
            assert progress_mock.add_task.call_args_list[2] == call(
                "  ↳ Subtask 2", total=40
            )
            assert progress_mock.add_task.call_args_list[3] == call(
                "  ↳ Subtask 3", total=50
            )
            indicator.update_subtask(subtask2, 20)
            indicator.complete_subtask(subtask1)
            indicator.update_subtask(subtask3, 25)
            indicator.complete_subtask(subtask3)
            indicator.complete_subtask(subtask2)
>           assert progress_mock.update.call_args_list[0] == call(
                subtask2, advance=20, description=None
            )
E           AssertionError: assert call('subtask...'Starting...') ==
call('subtask...cription=None)
E
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_components.py:155: AssertionError
___________ test_cliuxbridge_display_result_heading_levels_succeeds ____________

self = <MagicMock name='print' id='5331557488'>, args = ('Level 1 Heading',)
kwargs = {'style': 'heading'}
expected = call('Level 1 Heading', style='heading')
actual = call('# Level 1 Heading', markup=False, highlight=False)
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16d145800>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: print('Level 1 Heading', style='heading')
E             Actual: print('# Level 1 Heading', markup=False, highlight=False)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_display_result_heading_levels_succeeds(clean_state):
        """Test handling of different heading levels in display_result.


        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("# Level 1 Heading")
>           out.assert_called_with("Level 1 Heading", style="heading")
E           AssertionError: expected call not found.
E           Expected: print('Level 1 Heading', style='heading')
E             Actual: print('# Level 1 Heading', markup=False, highlight=False)
E
E           pytest introspection follows:
E
E           Args:
E           assert ('# Level 1 Heading',) == ('Level 1 Heading',)
E
E             At index 0 diff: '# Level 1 Heading' != 'Level 1 Heading'
E             Use -v to get more diff
E           Kwargs:
E           assert {'highlight':...arkup': False} == {'style': 'heading'}
E
E             Left contains 2 more items:
E             {'highlight': False, 'markup': False}
E             Right contains 1 more item:
E             {'style': 'heading'}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_components.py:182: AssertionError
____________ test_cliuxbridge_display_result_smart_styling_succeeds ____________

self = <MagicMock name='print' id='5328867104'>
args = ('ERROR: Something went wrong',), kwargs = {'style': 'error'}
expected = call('ERROR: Something went wrong', style='error')
actual = call('ERROR: Something went wrong', markup=False, highlight=False)
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16d43c860>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: print('ERROR: Something went wrong', style='error')
E             Actual: print('ERROR: Something went wrong', markup=False,
highlight=False)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_display_result_smart_styling_succeeds(clean_state):
        """Test smart styling based on message content in display_result.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("ERROR: Something went wrong")
>           out.assert_called_with("ERROR: Something went wrong", style="error")
E           AssertionError: expected call not found.
E           Expected: print('ERROR: Something went wrong', style='error')
E             Actual: print('ERROR: Something went wrong', markup=False,
highlight=False)
E
E           pytest introspection follows:
E
E           Kwargs:
E           assert {'highlight':...arkup': False} == {'style': 'error'}
E
E             Left contains 2 more items:
E             {'highlight': False, 'markup': False}
E             Right contains 1 more item:
E             {'style': 'error'}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_components.py:210: AssertionError
_____________ test_cliuxbridge_display_result_rich_markup_succeeds _____________

self = <MagicMock name='print' id='5331564304'>
args = ('This is [bold red]important[/bold red] information',)
kwargs = {'highlight': False}
expected = call('This is [bold red]important[/bold red] information',
highlight=False)
actual = call('This is [bold red]important[/bold red] information',
markup=False, highlight=False)
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16d2b04a0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: print('This is [bold red]important[/bold red]
information', highlight=False)
E             Actual: print('This is [bold red]important[/bold red]
information', markup=False, highlight=False)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_display_result_rich_markup_succeeds(clean_state):
        """Test processing of Rich markup in display_result.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("This is [bold red]important[/bold red]
information")
>           out.assert_called_with(
                "This is [bold red]important[/bold red] information",
highlight=False
            )
E           AssertionError: expected call not found.
E           Expected: print('This is [bold red]important[/bold red]
information', highlight=False)
E             Actual: print('This is [bold red]important[/bold red]
information', markup=False, highlight=False)
E
E           pytest introspection follows:
E
E           Kwargs:
E           assert {'highlight':...arkup': False} == {'highlight': False}
E
E             Omitting 1 identical items, use -vv to show
E             Left contains 1 more item:
E             {'markup': False}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_components.py:240: AssertionError
______________ test_cliuxbridge_display_result_highlight_succeeds ______________

self = <MagicMock name='Panel' id='6117085968'>, args = ('Important message',)
kwargs = {'style': 'highlight'}
expected = "Panel('Important message', style='highlight')"
actual = 'not called.'
error_message = "expected call not found.\nExpected: Panel('Important message',
style='highlight')\n  Actual: not called."

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
>           raise AssertionError(error_message)
E           AssertionError: expected call not found.
E           Expected: Panel('Important message', style='highlight')
E             Actual: not called.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:940: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_display_result_highlight_succeeds(clean_state):
        """Test styling based on the highlight flag in display_result.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            with patch("rich.panel.Panel") as panel_mock:
                panel_mock.return_value = "PANEL:Important message"
                bridge.display_result("Important message", highlight=True)
>               panel_mock.assert_called_with("Important message",
style="highlight")
E               AssertionError: expected call not found.
E               Expected: Panel('Important message', style='highlight')
E                 Actual: not called.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_components.py:268: AssertionError
______________ test_cliuxbridge_display_result_highlight_succeeds ______________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_highlight_succeeds():
        """Test that cliuxbridge display result highlight succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("done", highlight=True)
>           assert isinstance(out.call_args[0][0], Panel)
E           AssertionError: assert False
E            +  where False = isinstance('done', Panel)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:63: AssertionError
________________ test_cliuxbridge_display_result_error_succeeds ________________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_error_succeeds():
        """Test that cliuxbridge display result error succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("ERROR: Something went wrong",
highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('ERROR: Something went wrong', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:75: AssertionError
_______________ test_cliuxbridge_display_result_warning_succeeds _______________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_warning_succeeds():
        """Test that cliuxbridge display result warning succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("WARNING: Be careful", highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('WARNING: Be careful', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:88: AssertionError
_______________ test_cliuxbridge_display_result_success_succeeds _______________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_success_succeeds():
        """Test that cliuxbridge display result success succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("Task completed successfully",
highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('Task completed successfully', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:101: AssertionError
_______________ test_cliuxbridge_display_result_heading_succeeds _______________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_heading_succeeds():
        """Test that cliuxbridge display result heading succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("# Heading", highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('# Heading', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:114: AssertionError
_____________ test_cliuxbridge_display_result_subheading_succeeds ______________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_subheading_succeeds():
        """Test that cliuxbridge display result subheading succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("## Subheading", highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('## Subheading', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:127: AssertionError
_______________ test_cliuxbridge_display_result_normal_succeeds ________________

    @pytest.mark.medium
    def test_cliuxbridge_display_result_normal_succeeds():
        """Test that cliuxbridge display result normal succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("Normal message", highlight=False)
>           assert isinstance(out.call_args[0][0], Text)
E           AssertionError: assert False
E            +  where False = isinstance('Normal message', Text)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
liuxbridge.py:151: AssertionError
____________________ test_cliuxbridge_escapes_html_succeeds ____________________

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_escapes_html_succeeds(clean_state):
        """Ensure raw HTML is escaped before printing to the console.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("<script>")
            out.assert_called_once()
            printed_text = out.call_args[0][0]
>           assert getattr(printed_text, "plain", str(printed_text)) ==
"&lt;script&gt;"
E           AssertionError: assert '<script>' == '&lt;script&gt;'
E
E             - &lt;script&gt;
E             + <script>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_o
utput_sanitization.py:41: AssertionError
_____________________ test_webui_sanitizes_output_succeeds _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16cc272f0>
clean_state = None

    @pytest.mark.medium
    def test_webui_sanitizes_output_succeeds(monkeypatch, clean_state):
        """Test that webui sanitizes output succeeds.

        ReqID: N/A"""
        st = ModuleType("streamlit")
        st.write = MagicMock()
        st.markdown = MagicMock()
        st.text_input = MagicMock(return_value="t")
        st.selectbox = MagicMock(return_value="c")
        st.checkbox = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "streamlit", st)
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_o
utput_sanitization.py:71:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ test_cliuxbridge_removes_self_closing_script _________________

clean_state = None

    @pytest.mark.medium
    def test_cliuxbridge_removes_self_closing_script(clean_state):
        """Self-closing script tags should be removed entirely."""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("<script src='evil.js'/>Hi")
            out.assert_called_once()
            printed_text = out.call_args[0][0]
>           assert getattr(printed_text, "plain", str(printed_text)) == "Hi"
E           assert "<script src='evil.js'/>Hi" == 'Hi'
E
E             - Hi
E             + <script src='evil.js'/>Hi

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_o
utput_sanitization.py:138: AssertionError
____________________ test_get_session_value_with_exception _____________________

self = <MagicMock name='logger.warning' id='5329801616'>
args = ("Error accessing session state key 'test_key': Test error",)
kwargs = {}, msg = "Expected 'warning' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_get_session_value_with_exception(clean_state):
        """Test get_session_value with an exception."""

        # Create a custom mock class that raises an exception when attributes
are accessed
        class ExceptionSessionState:
            def __getattr__(self, name):
                raise AttributeError("Test error")

            def get(self, key, default=None):
                raise TypeError("Test error")

        session_state = ExceptionSessionState()

        # Mock the logger
        with patch("devsynth.interface.state_access.logger") as mock_logger:
            # Call the function
            result = get_session_value(session_state, "test_key",
"default_value")

            # Check that the function returns the default value
            assert result == "default_value"

            # Check that the logger was called with the expected message
>           mock_logger.warning.assert_called_once_with(
                "Error accessing session state key 'test_key': Test error"
            )
E           AssertionError: Expected 'warning' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_s
tate_access.py:136: AssertionError
_______________ test_set_session_value_with_attribute_exception ________________

clean_state = None

    @pytest.mark.medium
    def test_set_session_value_with_attribute_exception(clean_state):
        """Test set_session_value with an exception during attribute access."""

        # Create a custom mock class that raises an exception when attributes
are set
        # but allows item access to succeed
        class AttributeExceptionSessionState:
            def __init__(self):
                self.items = {}

            def __setattr__(self, name, value):
                if name == "items":
                    # Allow setting the items dictionary
                    object.__setattr__(self, name, value)
                else:
                    # Raise exception for other attributes
                    raise AttributeError("Test error")

            def __setitem__(self, key, value):
                # Allow item access to succeed
                self.items[key] = value

        session_state = AttributeExceptionSessionState()

        # Mock the logger
        with patch("devsynth.interface.state_access.logger") as mock_logger:
            # Call the function
            result = set_session_value(session_state, "test_key", "test_value")

            # Check that the function returns True because item access succeeds
            # even though attribute access fails
>           assert result is True
E           assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_s
tate_access.py:215: AssertionError
__________________ test_set_session_value_with_dict_exception __________________

self = <MagicMock name='logger.warning' id='5096234912'>
args = ("Error setting via item session state key 'test_key': Test error",)
kwargs = {}, msg = "Expected 'warning' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to be called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_set_session_value_with_dict_exception(clean_state):
        """Test set_session_value with an exception during dictionary access."""

        # Create a custom mock class that allows attribute access but raises an
exception for item access
        class DictExceptionSessionState:
            def __init__(self):
                self.attributes = {}

            def __setattr__(self, name, value):
                if name == "attributes":
                    # Allow setting the attributes dictionary
                    object.__setattr__(self, name, value)
                else:
                    # Store other attributes in the dictionary
                    self.attributes[name] = value

            def __getattr__(self, name):
                if name in self.attributes:
                    return self.attributes[name]
                raise AttributeError(
                    f"'{self.__class__.__name__}' object has no attribute
'{name}'"
                )

            def __setitem__(self, key, value):
                raise TypeError("Test error")

        session_state = DictExceptionSessionState()

        # Mock the logger
        with patch("devsynth.interface.state_access.logger") as mock_logger:
            # Call the function
            result = set_session_value(session_state, "test_key", "test_value")

            # Check that the function returns True (attribute access succeeded)
            assert result is True

            # Check that the logger was called with the expected message
>           mock_logger.warning.assert_called_once_with(
                "Error setting via item session state key 'test_key': Test
error"
            )
E           AssertionError: Expected 'warning' to be called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_s
tate_access.py:261: AssertionError
_________________ test_set_session_value_with_both_exceptions __________________

self = <MagicMock name='logger.warning' id='6119658160'>
args = ("Error setting via item session state key 'test_key': Item error",)
kwargs = {}
expected = call("Error setting via item session state key 'test_key': Item
error")
cause = None
actual = [call("Error setting session state key 'test_key': Attribute error")]
expected_string = 'warning("Error setting via item session state key
\'test_key\': Item error")'

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.

        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: warning("Error setting via item session state key
'test_key': Item error") call not found

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1020: AssertionError

During handling of the above exception, another exception occurred:

clean_state = None

    @pytest.mark.medium
    def test_set_session_value_with_both_exceptions(clean_state):
        """Test set_session_value with exceptions during both attribute and
dictionary access."""

        # Create a custom mock class that raises exceptions for both attribute
and item access
        class BothExceptionsSessionState:
            def __setattr__(self, name, value):
                raise AttributeError("Attribute error")

            def __setitem__(self, key, value):
                raise TypeError("Item error")

        session_state = BothExceptionsSessionState()

        # Mock the logger
        with patch("devsynth.interface.state_access.logger") as mock_logger:
            # Call the function
            result = set_session_value(session_state, "test_key", "test_value")

            # Check that the function returns False
            assert result is False

            # Check that the logger was called with the expected messages
            mock_logger.warning.assert_any_call(
                "Error setting session state key 'test_key': Attribute error"
            )
>           mock_logger.warning.assert_any_call(
                "Error setting via item session state key 'test_key': Item
error"
            )
E           AssertionError: warning("Error setting via item session state key
'test_key': Item error") call not found
E
E           pytest introspection follows:
E
E           Args:
E           assert ('Error setti...ibute error',) == ("Error setti... Item
error",)
E
E             At index 0 diff: "Error setting session state key 'test_key':
Attribute error" != "Error setting via item session state key 'test_key': Item
error"
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_s
tate_access.py:292: AssertionError
________________________________ test_function _________________________________

clean_state = None
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16cc26150>

    @pytest.mark.medium
    def test_function(clean_state, monkeypatch):
        # Test with clean state
        """Test that bridge methods succeeds.

        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.prompt.Prompt.ask", return_value="ans"):
>           assert bridge.ask_question("q") == "ans"
E           AssertionError: assert '' == 'ans'
E
E             - ans

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
x_bridge.py:48: AssertionError
----------------------------- Captured stdout call -----------------------------
q q
______________________ test_prompt_and_result_consistency ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16c9dd790>
clean_state = None

    @pytest.mark.medium
    def test_prompt_and_result_consistency(monkeypatch, clean_state):
        """CLI and WebUI should return identical answers and formatted
output."""
        # Store original sanitize_output function
        original_sanitize = getattr(
            sys.modules.get(sanitize_output.__module__, {}), "sanitize_output",
None
        )

        try:
            # Set up sanitize_output mock
            monkeypatch.setattr(
                sanitize_output.__module__ + ".sanitize_output", lambda x: x
            )

            # Set up CLI and WebUI bridges
            cli_bridge, cli_out = _setup_cli(monkeypatch)
            web_bridge, _st = _setup_webui(monkeypatch)

            # Test question functionality
            cli_answer = cli_bridge.ask_question("Question?", default="foo")
            web_answer = web_bridge.ask_question("Question?", default="foo")
            assert cli_answer == web_answer == "foo"

            # Test result display functionality
            cli_bridge.display_result("Result")
            web_bridge.display_result("Result")

            printed_cli = cli_out.call_args[0][0]
>           assert printed_cli == web_bridge.messages[-1]
E           AssertionError: assert 'Result' == <text 'Result' [] None>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge.py:121: AssertionError
----------------------------- Captured stdout call -----------------------------
Question? [foo]foo                  Question? [foo]foo
___________ TestUXBridgeConfig.test_get_default_bridge_cli_succeeds ____________

self = <tests.unit.interface.test_uxbridge_config.TestUXBridgeConfig object at
0x11fbde7b0>
mock_apply = <MagicMock name='apply_uxbridge_settings' id='4400673760'>

    @patch("devsynth.interface.uxbridge_config.apply_uxbridge_settings")
    @pytest.mark.medium
    def test_get_default_bridge_cli_succeeds(self, mock_apply):
        """Test getting the default bridge when CLI is configured.

        ReqID: N/A"""
        mock_config = self._make_project_config_mock()
        mock_config.config.uxbridge_settings = {"default_interface": "cli"}
        mock_config.config.features = {
            "uxbridge_webui": False,
            "uxbridge_agent_api": False,
        }
        mock_apply.return_value = MockUXBridge()
>       bridge = get_default_bridge(mock_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_config.py:94:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = <MagicMock spec='ProjectUnifiedConfig' id='4400675968'>

    def get_default_bridge(config: Optional[ProjectUnifiedConfig] = None) ->
UXBridge:
        """Get the default UXBridge implementation based on configuration.

        Args:
            config: The project configuration, or None to load the default

        Returns:
            An instance of the default UXBridge implementation
        """
>       from devsynth.config import load_project_config
E       ImportError: cannot import name 'load_project_config' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/uxbr
idge_config.py:58: ImportError
__________ TestUXBridgeConfig.test_get_default_bridge_webui_succeeds ___________

self = <tests.unit.interface.test_uxbridge_config.TestUXBridgeConfig object at
0x11fbdec30>
mock_apply = <MagicMock name='apply_uxbridge_settings' id='6108634752'>

    @patch("devsynth.interface.uxbridge_config.apply_uxbridge_settings")
    @pytest.mark.medium
    def test_get_default_bridge_webui_succeeds(self, mock_apply):
        """Test getting the default bridge when WebUI is configured.

        ReqID: N/A"""
        mock_config = self._make_project_config_mock()
        mock_config.config.uxbridge_settings = {"default_interface": "webui"}
        mock_config.config.features = {
            "uxbridge_webui": True,
            "uxbridge_agent_api": False,
        }
        mock_apply.return_value = MockUXBridge()
>       bridge = get_default_bridge(mock_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_config.py:111:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = <MagicMock spec='ProjectUnifiedConfig' id='6108628992'>

    def get_default_bridge(config: Optional[ProjectUnifiedConfig] = None) ->
UXBridge:
        """Get the default UXBridge implementation based on configuration.

        Args:
            config: The project configuration, or None to load the default

        Returns:
            An instance of the default UXBridge implementation
        """
>       from devsynth.config import load_project_config
E       ImportError: cannot import name 'load_project_config' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/uxbr
idge_config.py:58: ImportError
___________ TestUXBridgeConfig.test_get_default_bridge_api_succeeds ____________

self = <tests.unit.interface.test_uxbridge_config.TestUXBridgeConfig object at
0x11fbdf0b0>
mock_apply = <MagicMock name='apply_uxbridge_settings' id='5096943456'>

    @patch("devsynth.interface.uxbridge_config.apply_uxbridge_settings")
    @pytest.mark.medium
    def test_get_default_bridge_api_succeeds(self, mock_apply):
        """Test getting the default bridge when API is configured.

        ReqID: N/A"""
        mock_config = self._make_project_config_mock()
        mock_config.config.uxbridge_settings = {"default_interface": "api"}
        mock_config.config.features = {
            "uxbridge_webui": False,
            "uxbridge_agent_api": True,
        }
        mock_apply.return_value = MockUXBridge()
>       bridge = get_default_bridge(mock_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_config.py:128:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = <MagicMock spec='ProjectUnifiedConfig' id='5330292320'>

    def get_default_bridge(config: Optional[ProjectUnifiedConfig] = None) ->
UXBridge:
        """Get the default UXBridge implementation based on configuration.

        Args:
            config: The project configuration, or None to load the default

        Returns:
            An instance of the default UXBridge implementation
        """
>       from devsynth.config import load_project_config
E       ImportError: cannot import name 'load_project_config' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/uxbr
idge_config.py:58: ImportError
______ TestUXBridgeConfig.test_get_default_bridge_webui_fallback_succeeds ______

self = <tests.unit.interface.test_uxbridge_config.TestUXBridgeConfig object at
0x11fbdf530>
mock_apply = <MagicMock name='apply_uxbridge_settings' id='5099271760'>

    @patch("devsynth.interface.uxbridge_config.apply_uxbridge_settings")
    @pytest.mark.medium
    def test_get_default_bridge_webui_fallback_succeeds(self, mock_apply):
        """Test fallback to CLI when WebUI is configured but not available.

        ReqID: N/A"""
        mock_config = self._make_project_config_mock()
        mock_config.config.uxbridge_settings = {"default_interface": "webui"}
        mock_config.config.features = {
            "uxbridge_webui": True,
            "uxbridge_agent_api": False,
        }
        mock_apply.return_value = MockUXBridge()
        stub_module = ModuleType("devsynth.interface.webui")
        with patch.dict(sys.modules, {"devsynth.interface.webui": stub_module}):
>           bridge = get_default_bridge(mock_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_config.py:147:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = <MagicMock spec='ProjectUnifiedConfig' id='5099262832'>

    def get_default_bridge(config: Optional[ProjectUnifiedConfig] = None) ->
UXBridge:
        """Get the default UXBridge implementation based on configuration.

        Args:
            config: The project configuration, or None to load the default

        Returns:
            An instance of the default UXBridge implementation
        """
>       from devsynth.config import load_project_config
E       ImportError: cannot import name 'load_project_config' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/uxbr
idge_config.py:58: ImportError
_______ TestUXBridgeConfig.test_get_default_bridge_api_fallback_succeeds _______

self = <tests.unit.interface.test_uxbridge_config.TestUXBridgeConfig object at
0x11fbdf9b0>
mock_apply = <MagicMock name='apply_uxbridge_settings' id='6108634320'>

    @patch("devsynth.interface.uxbridge_config.apply_uxbridge_settings")
    @pytest.mark.medium
    def test_get_default_bridge_api_fallback_succeeds(self, mock_apply):
        """Test fallback to CLI when API is configured but not available.

        ReqID: N/A"""
        mock_config = self._make_project_config_mock()
        mock_config.config.uxbridge_settings = {"default_interface": "api"}
        mock_config.config.features = {
            "uxbridge_webui": False,
            "uxbridge_agent_api": True,
        }
        mock_apply.return_value = MockUXBridge()
        stub_module = ModuleType("devsynth.interface.agentapi")
        with patch.dict(sys.modules, {"devsynth.interface.agentapi":
stub_module}):
>           bridge = get_default_bridge(mock_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_config.py:167:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = <MagicMock spec='ProjectUnifiedConfig' id='6108632640'>

    def get_default_bridge(config: Optional[ProjectUnifiedConfig] = None) ->
UXBridge:
        """Get the default UXBridge implementation based on configuration.

        Args:
            config: The project configuration, or None to load the default

        Returns:
            An instance of the default UXBridge implementation
        """
>       from devsynth.config import load_project_config
E       ImportError: cannot import name 'load_project_config' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/uxbr
idge_config.py:58: ImportError
________ test_ask_question_and_display_result_consistency[_cli_bridge] _________

self = <MagicMock id='5331562528'>, args = ('S:<bad>',)
kwargs = {'style': None}, expected = call('S:<bad>', style=None)
actual = call('<bad>', markup=False, highlight=False)
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16d1cc7c0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: mock('S:<bad>', style=None)
E             Actual: mock('<bad>', markup=False, highlight=False)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock id='5331562528'>, args = ('S:<bad>',)
kwargs = {'style': None}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected: mock('S:<bad>', style=None)
E         Actual: mock('<bad>', markup=False, highlight=False)
E
E       pytest introspection follows:
E
E       Args:
E       assert ('<bad>',) == ('S:<bad>',)
E
E         At index 0 diff: '<bad>' != 'S:<bad>'
E         Use -v to get more diff
E       Kwargs:
E       assert {'highlight':...arkup': False} == {'style': None}
E
E         Left contains 2 more items:
E         {'highlight': False, 'markup': False}
E         Right contains 1 more item:
E         {'style': None}
E         Use -v to get more diff

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

factory = <function _cli_bridge at 0x11fbcb4c0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13db5c1d0>
clean_state = None

    @pytest.mark.parametrize(
        "factory", [_cli_bridge, _web_bridge, _api_bridge, _dpg_bridge]
    )
    @pytest.mark.medium
    def test_ask_question_and_display_result_consistency(factory, monkeypatch,
clean_state):
        """All bridges should return the same answers and sanitized results."""
        bridge, tracker = factory(monkeypatch)

        # Test ask_question
        answer = bridge.ask_question("Q?", choices=["foo"], default="foo")
        assert answer == "foo"

        # Test display_result
        bridge.display_result("<bad>")

        # Verify results based on bridge type
        if isinstance(bridge, APIBridge):
            assert bridge.messages[-1] == "S:<bad>"
        elif isinstance(bridge, CLIUXBridge):
>           tracker.assert_called_once_with("S:<bad>", style=None)
E           AssertionError: expected call not found.
E           Expected: mock('S:<bad>', style=None)
E             Actual: mock('<bad>', markup=False, highlight=False)
E
E           pytest introspection follows:
E
E           Args:
E           assert ('<bad>',) == ('S:<bad>',)
E
E             At index 0 diff: '<bad>' != 'S:<bad>'
E             Use -v to get more diff
E           Kwargs:
E           assert {'highlight':...arkup': False} == {'style': None}
E
E             Left contains 2 more items:
E             {'highlight': False, 'markup': False}
E             Right contains 1 more item:
E             {'style': None}
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_question_result.py:173: AssertionError
____________________________ test_with_clean_state _____________________________

clean_state = None

    @pytest.mark.medium
    def test_with_clean_state(clean_state):
        """Test that cliuxbridge sanitizes display result succeeds.

        ReqID: N/A"""

        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("<script>")
            out.assert_called_once()
            (printed_obj,) = out.call_args.args
            kwargs = out.call_args.kwargs
            plain_text = getattr(printed_obj, "plain", printed_obj)
>           assert plain_text == "&lt;script&gt;"
E           AssertionError: assert '<script>' == '&lt;script&gt;'
E
E             - &lt;script&gt;
E             + <script>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_sanitization.py:42: AssertionError
_________________ test_webui_sanitizes_display_result_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fcd0560>

    @pytest.mark.medium
    def test_webui_sanitizes_display_result_succeeds(monkeypatch):
        """Test that webui sanitizes display result succeeds.

        ReqID: N/A"""
        st = _mock_streamlit(monkeypatch)
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_sanitization.py:67:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________________________ test_with_clean_state _____________________________

clean_state = None
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12ee7ae40>

    @pytest.mark.medium
    def test_with_clean_state(clean_state, monkeypatch):
        """Importing WebUI succeeds without CLI command modules.

        ReqID: FR-75"""
        cli_module = ModuleType("devsynth.application.cli")
        monkeypatch.setitem(sys.modules, "devsynth.application.cli", cli_module)

        import devsynth.interface.webui.commands as commands

>       importlib.reload(commands)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_cli_imports.py:34:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui.commands' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/commands.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui.commands not in
sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________________ test_init_cmd_error_handling _________________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d597dd0>
clean_state = None

    @pytest.mark.medium
    def test_init_cmd_error_handling(stub_streamlit, monkeypatch, clean_state):
        """Test error handling when init_cmd raises an exception.

        ReqID: N/A"""
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:67:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_____________ test_onboarding_page_setup_wizard_error_raises_error _____________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5977d0>

    @pytest.mark.medium
    def test_onboarding_page_setup_wizard_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when SetupWizard raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:103:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_requirements_page_spec_cmd_error_raises_error ______________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d594410>

    @pytest.mark.medium
    def test_requirements_page_spec_cmd_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when spec_cmd raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:149:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_requirements_page_inspect_cmd_error_raises_error _____________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d595ee0>

    @pytest.mark.medium
    def test_requirements_page_inspect_cmd_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when inspect_cmd raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:189:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_requirements_page_file_not_found_raises_error ______________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d553320>

    @pytest.mark.medium
    def test_requirements_page_file_not_found_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when file is not found.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:235:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_analysis_page_inspect_code_cmd_error_raises_error ____________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d524e60>

    @pytest.mark.medium
    def test_analysis_page_inspect_code_cmd_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when inspect_code_cmd raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:280:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ test_synthesis_page_test_cmd_error_raises_error ________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d594830>

    @pytest.mark.medium
    def test_synthesis_page_test_cmd_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when test_cmd raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:323:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ test_config_page_load_config_error_raises_error ________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d20c950>

    @pytest.mark.medium
    def test_config_page_load_config_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when load_project_config raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:363:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ test_config_page_save_config_error_raises_error ________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d525eb0>

    @pytest.mark.medium
    def test_config_page_save_config_error_raises_error(stub_streamlit,
monkeypatch):
        """Test error handling when save_config raises an exception.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_error_handling.py:392:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_gather_wizard_start_button_not_clicked __________________

stub_streamlit = <module 'streamlit'>
mock_gather_requirements = <MagicMock id='6131539392'>

    @pytest.mark.medium
    def test_gather_wizard_start_button_not_clicked(
        stub_streamlit, mock_gather_requirements
    ):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard.py:60:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_gather_wizard_finish_calls_gather_requirements ______________

stub_streamlit = <module 'streamlit'>
mock_gather_requirements = <MagicMock id='6131881920'>

    @pytest.mark.medium
    def test_gather_wizard_finish_calls_gather_requirements(
        stub_streamlit, mock_gather_requirements
    ):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard.py:78:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________________ test_gather_wizard_import_error ________________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d77fad0>

    @pytest.mark.medium
    def test_gather_wizard_import_error(stub_streamlit, monkeypatch):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard.py:109:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________________ test_gather_wizard_exception _________________________

stub_streamlit = <module 'streamlit'>
mock_gather_requirements = <MagicMock id='6131875920'>

    @pytest.mark.medium
    def test_gather_wizard_exception(stub_streamlit, mock_gather_requirements):
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard.py:143:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_gather_wizard_initialization_with_state _________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x16d756600>, <module 'streamlit'>)
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_initialization_with_state(gather_wizard_state,
clean_state):
        """Test that the gather wizard is properly initialized with
WizardState."""
        # Import the WebUI class after the streamlit module is patched
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:38:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________________ test_gather_wizard_navigation_with_state ___________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x16dc15a60>, <module 'streamlit'>)
mock_gather_requirements = <MagicMock name='gather_requirements'
id='6132902144'>
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_navigation_with_state(
        gather_wizard_state, mock_gather_requirements, clean_state
    ):
        """Test navigation through gather wizard steps with WizardState."""
        # Import the WebUI class after the streamlit module is patched
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_gather_wizard_data_persistence_with_state ________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x16d826330>, <module 'streamlit'>)
mock_gather_requirements = <MagicMock name='gather_requirements'
id='6125722112'>
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_data_persistence_with_state(
        gather_wizard_state, mock_gather_requirements, clean_state
    ):
        """Test that data persists between gather wizard steps with
WizardState."""
        # Import the WebUI class after the streamlit module is patched
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:137:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_gather_wizard_error_handling_with_state _________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x16d95daf0>, <module 'streamlit'>)
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_error_handling_with_state(gather_wizard_state,
clean_state):
        """Test error handling in the gather wizard with WizardState."""
        # Import the WebUI class after the streamlit module is patched
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:251:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________________ test_gather_wizard_validation_with_state ___________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x16d931eb0>, <module 'streamlit'>)
mock_gather_requirements = <MagicMock name='gather_requirements'
id='6134479376'>
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_validation_with_state(
        gather_wizard_state, mock_gather_requirements, clean_state
    ):
        """Test validation in the gather wizard with WizardState."""
        # Import the WebUI class after the streamlit module is patched
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:377:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_gather_wizard_start_resets_state _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d1f3920>

    @pytest.mark.medium
    def test_gather_wizard_start_resets_state(monkeypatch):
        """Starting the gather wizard clears any lingering state."""
        import types

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_gather_wizard_with_state.py:410:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_navigation_persists_wizard_state _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16dc0e8d0>
stub_streamlit = <module 'streamlit'>

    @pytest.mark.medium
    def test_navigation_persists_wizard_state(monkeypatch, stub_streamlit):
        """ReqID: FR-201 ensure wizard state persists across navigation."""
>       webui = _reload_webui()
                ^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_navigation_and_validation.py:28:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_navigation_and_validation.py:21: in _reload_webui
    importlib.reload(webui)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ test_analysis_page_invalid_path_shows_error __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16da5b2c0>
stub_streamlit = <module 'streamlit'>

    @pytest.mark.medium
    def test_analysis_page_invalid_path_shows_error(monkeypatch,
stub_streamlit):
        """ReqID: FR-202 surface an error when analysis path is invalid."""
>       webui = _reload_webui()
                ^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_navigation_and_validation.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_navigation_and_validation.py:21: in _reload_webui
    importlib.reload(webui)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________________________ test_onboarding_page_succeeds _________________________

mock_streamlit = <module 'streamlit'>
mock_init_cmd = <MagicMock id='6135665712'>, clean_state = None

    @pytest.mark.medium
    def test_onboarding_page_succeeds(mock_streamlit, mock_init_cmd,
clean_state):
        """Test the onboarding_page method.

        ReqID: N/A"""
        import importlib

        from devsynth.interface import webui

        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_onboarding.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

module = <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.

        The module must have been successfully imported before.

        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None

        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________________ test_onboarding_page_no_submit_succeeds ____________________

mock_streamlit = <module 'streamlit'>
mock_init_cmd = <MagicMock id='6136081360'>

    @pytest.mark.medium
    def test_onboarding_page_no_submit_succeeds(mock_streamlit, mock_init_cmd):
        """Test the onboarding_page method when the form is not submitted.

        ReqID: N/A"""
        import importlib

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_onboarding.py:68:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________________ test_ui_progress_init_succeeds ________________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_init_succeeds(mock_streamlit, clean_state):
        """Test the initialization of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:93:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________________ test_ui_progress_update_succeeds _______________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_update_succeeds(mock_streamlit, clean_state):
        """Test the update method of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:113:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_ui_progress_complete_succeeds ______________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_complete_succeeds(mock_streamlit, clean_state):
        """Test the complete method of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:136:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_ui_progress_complete_cascades_subtasks __________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_complete_cascades_subtasks(mock_streamlit,
clean_state):
        """Completing the parent task finalizes any outstanding subtasks."""

>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:150:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_ui_progress_add_subtask_succeeds _____________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_add_subtask_succeeds(mock_streamlit, clean_state):
        """Test the ``add_subtask`` method of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:173:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________________ test_ui_progress_update_subtask_succeeds ___________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_update_subtask_succeeds(mock_streamlit, clean_state):
        """Test the ``update_subtask`` method of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:195:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_ui_progress_complete_subtask_succeeds __________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    @pytest.mark.medium
    def test_ui_progress_complete_subtask_succeeds(mock_streamlit, clean_state):
        """Test the ``complete_subtask`` method of ``_UIProgress``.

        ReqID: N/A"""
>       module = _get_webui_module()
                 ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:221:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________________ test_requirements_page_succeeds ________________________

mock_streamlit = <module 'streamlit'>
mock_spec_cmd = <MagicMock id='6134534832'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16e06c410>

    @pytest.mark.medium
    def test_requirements_page_succeeds(mock_streamlit, mock_spec_cmd,
monkeypatch):
        """requirements_page renders and invokes spec_cmd on submission."""

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements.py:47:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_requirements_wizard_succeeds _______________________

mock_streamlit = <module 'streamlit'>

    @pytest.mark.medium
    def test_requirements_wizard_succeeds(mock_streamlit):
        """The requirements wizard advances steps and saves state."""

>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements.py:88:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    Web UI module for DevSynth.

    This module provides web interface components for DevSynth.
    """

>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_wizard_state_in_streamlit_context ____________________

gather_wizard_state = (<devsynth.interface.webui_state.WizardState object at
0x12f216450>, <module 'streamlit'>)

    @pytest.mark.medium
    def test_wizard_state_in_streamlit_context(gather_wizard_state):
        """Test using the wizard state in a simulated Streamlit context."""
        state, mock_st = gather_wizard_state

        # Track which buttons should be clicked
        clicked_buttons = set()

        # Mock Streamlit UI elements with controlled button behavior
        def mock_button(text, key=None, **kwargs):
            button_id = f"{text}_{key}"
            return button_id in clicked_buttons

        mock_st.button.side_effect = mock_button

        # Simulate a Streamlit app with wizard state
        def run_wizard_step():
            current_step = state.get_current_step()

            if current_step == 1:
                mock_st.header("Step 1: Resource Type")
                resource_type = mock_st.selectbox(
                    "Select resource type",
                    ["documentation", "code", "data"],
                    key="resource_type_select",
                )
                state.set("resource_type", resource_type)

            elif current_step == 2:
                mock_st.header("Step 2: Resource Location")
                resource_location = mock_st.text_input(
                    "Enter resource location", key="resource_location_input"
                )
                state.set("resource_location", resource_location)

            elif current_step == 3:
                mock_st.header("Step 3: Resource Metadata")
                author = mock_st.text_input("Author", key="author_input")
                version = mock_st.text_input("Version", key="version_input")
                tags = mock_st.text_input("Tags (comma-separated)",
key="tags_input")

                metadata = {
                    "author": author,
                    "version": version,
                    "tags": tags.split(",") if tags else [],
                }
                state.set("resource_metadata", metadata)

            # Navigation buttons
            col1, col2 = mock_st.columns(2)

            with col1:
                if current_step > 1 and mock_st.button(
                    "Previous", key=f"prev_step_{current_step}"
                ):
                    state.previous_step()

            with col2:
                if current_step < state.get_total_steps():
                    if mock_st.button("Next", key=f"next_step_{current_step}"):
                        state.next_step()
                else:
                    if mock_st.button("Finish", key="finish_button"):
                        state.set_completed(True)

        # Step 1: Set resource type
        mock_st.selectbox.return_value = "documentation"
>       run_wizard_step()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_wizard_state.py:320:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def run_wizard_step():
        current_step = state.get_current_step()

        if current_step == 1:
            mock_st.header("Step 1: Resource Type")
            resource_type = mock_st.selectbox(
                "Select resource type",
                ["documentation", "code", "data"],
                key="resource_type_select",
            )
            state.set("resource_type", resource_type)

        elif current_step == 2:
            mock_st.header("Step 2: Resource Location")
            resource_location = mock_st.text_input(
                "Enter resource location", key="resource_location_input"
            )
            state.set("resource_location", resource_location)

        elif current_step == 3:
            mock_st.header("Step 3: Resource Metadata")
            author = mock_st.text_input("Author", key="author_input")
            version = mock_st.text_input("Version", key="version_input")
            tags = mock_st.text_input("Tags (comma-separated)",
key="tags_input")

            metadata = {
                "author": author,
                "version": version,
                "tags": tags.split(",") if tags else [],
            }
            state.set("resource_metadata", metadata)

        # Navigation buttons
>       col1, col2 = mock_st.columns(2)
        ^^^^^^^^^^
E       ValueError: too many values to unpack (expected 2)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_wizard_state.py:302: ValueError
__________________________ test_get_wizard_state_new ___________________________

mock_session_state = {}, clean_state = None

    @pytest.mark.medium
    def test_get_wizard_state_new(mock_session_state, clean_state):
        """Test get_wizard_state when no wizard state exists."""
        # Create a manager with a clean session state
        manager = WizardStateManager(
            mock_session_state,
            "test_wizard",
            3,
            {"step1_data": "", "step2_data": "", "step3_data": ""},
        )

        # Mock the logger
        with patch("devsynth.interface.wizard_state_manager.logger") as
mock_logger:
            # Get the wizard state
>           wizard_state = manager.get_wizard_state()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:108:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16da44680>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
________________________ test_get_wizard_state_existing ________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12ee32c30>, {'test_wizard_current_step': 2, '...izard_step1_data': 'Step 1
Value', 'test_wizard_step2_data': 'Step 2 Value', 'test_wizard_step3_data':
'Step 3 Value'})
clean_state = None

    @pytest.mark.medium
    def test_get_wizard_state_existing(wizard_state_manager, clean_state):
        """Test get_wizard_state when a wizard state already exists."""
        manager, mock_session = wizard_state_manager

        # Set up the session state to simulate an existing wizard state
        mock_session["test_wizard_current_step"] = 2
        mock_session["test_wizard_total_steps"] = 3
        mock_session["test_wizard_completed"] = False
        mock_session["test_wizard_step1_data"] = "Step 1 Value"
        mock_session["test_wizard_step2_data"] = "Step 2 Value"
        mock_session["test_wizard_step3_data"] = "Step 3 Value"

        # Mock the logger
        with patch("devsynth.interface.wizard_state_manager.logger") as
mock_logger:
            # Get the wizard state
>           wizard_state = manager.get_wizard_state()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:63: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16d77c110>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_______________________ test_validate_wizard_state_valid _______________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16c119e20>, {})
clean_state = None

    @pytest.mark.medium
    def test_validate_wizard_state_valid(wizard_state_manager, clean_state):
        """Test validate_wizard_state with a valid state."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:185:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16da44620>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
____________________ test_validate_wizard_state_missing_key ____________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12ee319d0>, {})
clean_state = None

    @pytest.mark.medium
    def test_validate_wizard_state_missing_key(wizard_state_manager,
clean_state):
        """Test validate_wizard_state with a missing key."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:202:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16da44650>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
___________________ test_validate_wizard_state_invalid_step ____________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12ed09b80>, {})
clean_state = None

    @pytest.mark.medium
    def test_validate_wizard_state_invalid_step(wizard_state_manager,
clean_state):
        """Test validate_wizard_state with an invalid step."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:226:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16e06f980>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_________________ test_validate_wizard_state_mismatched_steps __________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12ee32e70>, {})
clean_state = None

    @pytest.mark.medium
    def test_validate_wizard_state_mismatched_steps(wizard_state_manager,
clean_state):
        """Test validate_wizard_state with mismatched total steps."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:248:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x12f8765a0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
___________________________ test_reset_wizard_state ____________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x13d9a8950>, {})
clean_state = None

    @pytest.mark.medium
    def test_reset_wizard_state(wizard_state_manager, clean_state):
        """Test reset_wizard_state method."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state and set some data
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:270:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x12e52dd60>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
________________________ test_reset_wizard_state_error _________________________

self = <MagicMock name='logger.error' id='6108058336'>
args = ('Error resetting wizard state for test_wizard: Test error',)
kwargs = {}
expected = call('Error resetting wizard state for test_wizard: Test error')
actual = call('Error resetting wizard state for test_wizard:
DeltaGeneratorSingleton instance already exists!')
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16d2b32e0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: error('Error resetting wizard state for test_wizard: Test
error')
E             Actual: error('Error resetting wizard state for test_wizard:
DeltaGeneratorSingleton instance already exists!')

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='logger.error' id='6108058336'>
args = ('Error resetting wizard state for test_wizard: Test error',)
kwargs = {}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected: error('Error resetting wizard state for test_wizard: Test
error')
E         Actual: error('Error resetting wizard state for test_wizard:
DeltaGeneratorSingleton instance already exists!')
E
E       pytest introspection follows:
E
E       Args:
E       assert ('Error reset...ady exists!',) == ('Error reset... Test error',)
E
E         At index 0 diff: 'Error resetting wizard state for test_wizard:
DeltaGeneratorSingleton instance already exists!' != 'Error resetting wizard
state for test_wizard: Test error'
E         Use -v to get more diff

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16d0f76e0>, {})
clean_state = None

    @pytest.mark.medium
    def test_reset_wizard_state_error(wizard_state_manager, clean_state):
        """Test reset_wizard_state method with an error."""
        manager, mock_session = wizard_state_manager

        # Mock the WizardState.reset method to raise an exception
        with patch(
            "devsynth.interface.webui_state.WizardState.reset",
            side_effect=Exception("Test error"),
        ):
            # Mock the logger
            with patch("devsynth.interface.wizard_state_manager.logger") as
mock_logger:
                # Reset the state
                result = manager.reset_wizard_state()

                # Check that the function returns False
                assert result is False

                # Check that the logger was called with the expected message
>               mock_logger.error.assert_called_once_with(
                    f"Error resetting wizard state for test_wizard: Test error"
                )
E               AssertionError: expected call not found.
E               Expected: error('Error resetting wizard state for test_wizard:
Test error')
E                 Actual: error('Error resetting wizard state for test_wizard:
DeltaGeneratorSingleton instance already exists!')
E
E               pytest introspection follows:
E
E               Args:
E               assert ('Error reset...ady exists!',) == ('Error reset... Test
error',)
E
E                 At index 0 diff: 'Error resetting wizard state for
test_wizard: DeltaGeneratorSingleton instance already exists!' != 'Error
resetting wizard state for test_wizard: Test error'
E                 Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:316: AssertionError
____________________________ test_get_current_step _____________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x13da85d60>, {})
clean_state = None

    @pytest.mark.medium
    def test_get_current_step(wizard_state_manager, clean_state):
        """Test get_current_step method."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state and set the current step
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:327:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16ceb7710>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_______________________________ test_go_to_step ________________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12e54ea80>, {})
clean_state = None

    @pytest.mark.medium
    def test_go_to_step(wizard_state_manager, clean_state):
        """Test go_to_step method."""
        manager, mock_session = wizard_state_manager

        # Go to step 2
        with patch(
            "devsynth.interface.webui_state.WizardState.go_to_step",
return_value=True
        ) as mock_go_to_step:
>           result = manager.go_to_step(2)
                     ^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:343:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:171: in go_to_step
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16c11a180>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
________________________________ test_next_step ________________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12ee30f80>, {})
clean_state = None

    @pytest.mark.medium
    def test_next_step(wizard_state_manager, clean_state):
        """Test next_step method."""
        manager, mock_session = wizard_state_manager

        # Move to the next step
        with patch(
            "devsynth.interface.webui_state.WizardState.next_step",
return_value=True
        ) as mock_next_step:
>           result = manager.next_step()
                     ^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:361:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:180: in next_step
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x12f876750>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
______________________________ test_previous_step ______________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16d0f7cb0>, {})
clean_state = None

    @pytest.mark.medium
    def test_previous_step(wizard_state_manager, clean_state):
        """Test previous_step method."""
        manager, mock_session = wizard_state_manager

        # Move to the previous step
        with patch(
            "devsynth.interface.webui_state.WizardState.previous_step",
return_value=True
        ) as mock_previous_step:
>           result = manager.previous_step()
                     ^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:379:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:189: in previous_step
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x13da85970>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
______________________________ test_set_completed ______________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16ca20aa0>, {})
clean_state = None

    @pytest.mark.medium
    def test_set_completed(wizard_state_manager, clean_state):
        """Test set_completed method."""
        manager, mock_session = wizard_state_manager

        # Set the wizard as completed
        with patch(
            "devsynth.interface.webui_state.WizardState.set_completed",
return_value=True
        ) as mock_set_completed:
>           result = manager.set_completed(True)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:397:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:201: in set_completed
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16ceb54c0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
______________________________ test_is_completed _______________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12e54cb90>, {})
clean_state = None

    @pytest.mark.medium
    def test_is_completed(wizard_state_manager, clean_state):
        """Test is_completed method."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state and set it as completed
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:412:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16d0f7c50>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
________________________________ test_get_value ________________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12f9bc740>, {})
clean_state = None

    @pytest.mark.medium
    def test_get_value(wizard_state_manager, clean_state):
        """Test get_value method."""
        manager, mock_session = wizard_state_manager

        # Get a wizard state and set a value
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:425:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16c11a180>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
________________________________ test_set_value ________________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16d0f6480>, {})
clean_state = None

    @pytest.mark.medium
    def test_set_value(wizard_state_manager, clean_state):
        """Test set_value method."""
        manager, mock_session = wizard_state_manager

        # Set a value
        with patch(
            "devsynth.interface.webui_state.WizardState.set", return_value=True
        ) as mock_set:
>           result = manager.set_value("step1_data", "Step 1 Value")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:442:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:246: in set_value
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16c119fd0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
___________________ test_simulate_wizard_manager_navigation ____________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16ca80890>, {})
clean_state = None

    @pytest.mark.medium
    def test_simulate_wizard_manager_navigation(wizard_state_manager,
clean_state):
        """Test the simulate_wizard_manager_navigation helper function."""
        manager, mock_session = wizard_state_manager

        # Define a navigation sequence
        navigation_steps = ["next", "next", "previous", "next", "goto_1",
"next"]

        # Simulate the navigation
>       final_step = simulate_wizard_manager_navigation(manager,
navigation_steps)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:460:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/state_access
_fixture.py:118: in simulate_wizard_manager_navigation
    manager.next_step()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:180: in next_step
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x12f876bd0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_________________________ test_set_wizard_manager_data _________________________

wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x12e54d040>, {})
clean_state = None

    @pytest.mark.medium
    def test_set_wizard_manager_data(wizard_state_manager, clean_state):
        """Test the set_wizard_manager_data helper function."""
        manager, mock_session = wizard_state_manager

        # Define data for multiple steps
        step_data = {
            "step1_data": "Step 1 Value",
            "step2_data": "Step 2 Value",
            "step3_data": "Step 3 Value",
        }

        # Set the data
>       set_wizard_manager_data(manager, step_data)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:480:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/state_access
_fixture.py:137: in set_wizard_manager_data
    manager.set_value(key, value)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:246: in set_value
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16caf2600>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_______________________ test_gather_wizard_state_manager _______________________

gather_wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16ca10fe0>, {})
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_state_manager(gather_wizard_state_manager,
clean_state):
        """Test the gather_wizard_state_manager fixture."""
        manager, mock_session = gather_wizard_state_manager

        # Check that the manager has the correct properties
        assert manager.wizard_name == "gather_wizard"
        assert manager.steps == 3
        assert isinstance(manager.initial_state, dict)
        assert manager.initial_state == {
            "resource_type": "",
            "resource_location": "",
            "resource_metadata": {},
        }

        # Get the wizard state
>       wizard_state = manager.get_wizard_state()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:504:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x16d0f44a0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_________________________ test_gather_wizard_workflow __________________________

gather_wizard_state_manager =
(<devsynth.interface.wizard_state_manager.WizardStateManager object at
0x16cae6390>, {})
clean_state = None

    @pytest.mark.medium
    def test_gather_wizard_workflow(gather_wizard_state_manager, clean_state):
        """Test a complete gather wizard workflow with state persistence."""
        manager, mock_session = gather_wizard_state_manager

        # Step 1: Set resource type
>       assert manager.get_current_step() == 1
               ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
izard_state_manager.py:524:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:159: in get_current_step
    wizard_state = self.get_wizard_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:59: in get_wizard_state
    wizard_state = WizardState(self.wizard_name, self.steps, self.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:252: in __init__
    super().__init__(wizard_name, full_initial_state)
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:135: in __init__
    self._initialize_state()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:140: in _initialize_state
    if not self.has(key):
           ^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:187: in has
    value = get_session_value(full_key, sentinel)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:57: in get_session_value
    s = _get_st()
        ^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:43: in _get_st
    return st if st is not None else _require_streamlit()
                                     ^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_state.py:28: in _require_streamlit
    return importlib.import_module("streamlit")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x12f02c4a0>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
_________ TestLMStudioProviderTextGeneration.test_generate_basic_text __________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16ca8dc70>
prompt = 'Hello, how are you?', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
>               self._lmstudio.llm(self.model).complete,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                prompt,
                config=params,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:500:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: in __call__
    return getattr(real, self._attr)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1753: in llm
    return get_default_client().llm.model(model_key, ttl=ttl, config=config)
           ^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1726: in get_default_client
    _default_client._ensure_api_host_is_valid()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1599: in _ensure_api_host_is_valid
    elif self.is_valid_api_host(specified_api_host):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:81: in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1581: in is_valid_api_host
    probe_response = cls._query_probe_url(probe_url)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sync_api.py:1573: in _query_probe_url
    return httpx.get(url, timeout=1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_api.py:195: in get
    return request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_api.py:109: in request
    return client.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x16ca8c830>, method = 'GET'
url = 'http://127.0.0.1:1234/lmstudio-greeting', args = ()
kwargs = {'auth': None, 'content': None, 'data': None, 'files': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderTextGeneration object at
0x11fc92180>
mock_lmstudio_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from LM Studio.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'test-model', ...}

    @pytest.mark.medium
    def test_generate_basic_text(self, mock_lmstudio_response):
        """Test basic text generation."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock the sync API response
            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                mock_lmstudio_response
            )

            provider = LMStudioProvider(config)
>           response = provider.generate("Hello, how are you?")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:186:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16ca8dc70>
prompt = 'Hello, how are you?', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
                self._lmstudio.llm(self.model).complete,
                prompt,
                config=params,
            )
            content = getattr(result, "content", None)
            if isinstance(content, str) and content:
                return content
            raise LMStudioModelError("Invalid response from LM Studio")
        except LMStudioModelError:
            raise
        except LMStudioTokenLimitError:
            raise  # Re-raise token limit errors as-is
        except LMStudioConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:  # noqa: BLE001
            error_msg = f"LM Studio API error: {str(e)}. Check that LM Studio is
running and accessible."
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:517: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,071 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,073 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,073 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,076 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,076 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,077 - AsyncWebsocketThread - INFO - {"client":
"<lmstudio.sync_api.Client object at 0x12f02f3b0>", "event": "Websocket handling
thread started", "thread_id": "Thread-72"}
2025-10-28 10:32:08,084 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
INFO     AsyncWebsocketThread:_ws_thread.py:138 {"client":
"<lmstudio.sync_api.Client object at 0x12f02f3b0>", "event": "Websocket handling
thread started", "thread_id": "Thread-72"}
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error: Network access disabled during tests (httpx). Check that LM
Studio is running and accessible.
___ TestLMStudioProviderTextGeneration.test_generate_with_custom_parameters ____

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e20d0d0>
prompt = 'Tell me a story', parameters = {'max_tokens': 500, 'temperature': 0.9}

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
>               self._lmstudio.llm(self.model).complete,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                prompt,
                config=params,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:500:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider._AttrForwarder object at
0x118cc61e0>
args = ('qwen/qwen3-4b-2507',), kwargs = {}
real = <module 'lmstudio' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/__init__.py'>

    def __call__(self, *args, **kwargs):  # pragma: no cover - thin forwarder
        real = self._proxy._ensure()
>       return getattr(real, self._attr)(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       lmstudio.LMStudioWebsocketError:
E           LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E           Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: LMStudioWebsocketError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderTextGeneration object at
0x11fc925a0>
mock_lmstudio_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from LM Studio.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'test-model', ...}

    @pytest.mark.medium
    def test_generate_with_custom_parameters(self, mock_lmstudio_response):
        """Test text generation with custom parameters."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                mock_lmstudio_response
            )

            provider = LMStudioProvider(config)
>           response = provider.generate(
                "Tell me a story", parameters={"temperature": 0.9, "max_tokens":
500}
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:207:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x12e20d0d0>
prompt = 'Tell me a story', parameters = {'max_tokens': 500, 'temperature': 0.9}

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
                self._lmstudio.llm(self.model).complete,
                prompt,
                config=params,
            )
            content = getattr(result, "content", None)
            if isinstance(content, str) and content:
                return content
            raise LMStudioModelError("Invalid response from LM Studio")
        except LMStudioModelError:
            raise
        except LMStudioTokenLimitError:
            raise  # Re-raise token limit errors as-is
        except LMStudioConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:  # noqa: BLE001
            error_msg = f"LM Studio API error: {str(e)}. Check that LM Studio is
running and accessible."
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
LM Studio API error:
E               LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E               Is LM Studio running?. Check that LM Studio is running and
accessible.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:517: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,206 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,210 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,213 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,213 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,216 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,216 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,234 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?. Check that LM Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?. Check that LM Studio is running and accessible.
________ TestLMStudioProviderTextGeneration.test_generate_with_context _________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16ca10260>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using LM
Studio.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
>               self._lmstudio.llm(self.model).respond,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                {"messages": messages},
                config=params,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:558:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider._AttrForwarder object at
0x118cc61e0>
args = ('qwen/qwen3-4b-2507',), kwargs = {}
real = <module 'lmstudio' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/__init__.py'>

    def __call__(self, *args, **kwargs):  # pragma: no cover - thin forwarder
        real = self._proxy._ensure()
>       return getattr(real, self._attr)(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       lmstudio.LMStudioWebsocketError:
E           LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E           Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: LMStudioWebsocketError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderTextGeneration object at
0x11fc92a50>
mock_lmstudio_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from LM Studio.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'test-model', ...}

    @pytest.mark.medium
    def test_generate_with_context(self, mock_lmstudio_response):
        """Test text generation with conversation context."""
        config = {"base_url": "http://localhost:1234/v1"}

        context = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What's the weather like?"},
        ]

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                mock_lmstudio_response
            )

            provider = LMStudioProvider(config)
>           response = provider.generate_with_context("How about tomorrow?",
context)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:239:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16ca10260>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using LM
Studio.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            result = self._execute_with_resilience(
                self._lmstudio.llm(self.model).respond,
                {"messages": messages},
                config=params,
            )
            if hasattr(result, "content"):
                return result.content
            raise LMStudioModelError("Invalid response from LM Studio")
        except LMStudioModelError:
            raise
        except Exception as e:  # noqa: BLE001
            error_msg = f"Failed to connect to LM Studio: {str(e)}"
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
Failed to connect to LM Studio:
E               LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E               Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:570: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,282 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,286 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,287 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,288 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,290 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,291 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,308 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
__ TestLMStudioProviderTextGeneration.test_generate_with_auto_model_selection __

self = <MagicMock name='_require_lmstudio().sync_api.models.list'
id='6118385680'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'list' to have been called once. Called 0
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderTextGeneration object at
0x11fc92f00>
mock_lmstudio_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from LM Studio.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'test-model', ...}

    @pytest.mark.medium
    def test_generate_with_auto_model_selection(self, mock_lmstudio_response):
        """Test text generation with automatic model selection."""
        config = {"base_url": "http://localhost:1234/v1", "auto_select_model":
True}

        # Mock available models
        available_models = [
            {"id": "model1", "object": "model"},
            {"id": "model2", "object": "model"},
        ]

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock model listing and generation
            mock_lmstudio.sync_api.models.list.return_value = available_models
            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                mock_lmstudio_response
            )

            provider = LMStudioProvider(config)

            # First call should list models
>           mock_lmstudio.sync_api.models.list.assert_called_once()
E           AssertionError: Expected 'list' to have been called once. Called 0
times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:277: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,357 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,361 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,363 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,364 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,366 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,366 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___ TestLMStudioProviderTextGeneration.test_generate_with_invalid_parameters ___

self = <test_lmstudio_provider.TestLMStudioProviderTextGeneration object at
0x11fc933e0>

    @pytest.mark.medium
    def test_generate_with_invalid_parameters(self):
        """Test text generation with invalid parameters."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            provider = LMStudioProvider(config)

            # Test invalid temperature
            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello", {"temperature": 3.0})

>           assert "temperature must be between" in str(exc_info.value)
E           AssertionError: assert 'temperature must be between' in 'LM Studio
API error: \n    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unh... errors in a TaskGroup (1 sub-exception)).\n
Is LM Studio running?. Check that LM Studio is running and accessible.'
E            +  where 'LM Studio API error: \n    LM Studio is not reachable at
ws://127.0.0.1:1234/llm (due to builtins.ExceptionGroup: unh... errors in a
TaskGroup (1 sub-exception)).\n    Is LM Studio running?. Check that LM Studio
is running and accessible.' = str(LMStudioConnectionError('LM Studio API error:
\n    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
buil...errors in a TaskGroup (1 sub-exception)).\n    Is LM Studio running?.
Check that LM Studio is running and accessible.'))
E            +    where LMStudioConnectionError('LM Studio API error: \n    LM
Studio is not reachable at ws://127.0.0.1:1234/llm (due to buil...errors in a
TaskGroup (1 sub-exception)).\n    Is LM Studio running?. Check that LM Studio
is running and accessible.') = <ExceptionInfo LMStudioConnectionError('LM Studio
API error: \n    LM Studio is not reachable at ws://127.0.0.1:1234/l... a
TaskGroup (1 sub-exception)).\n    Is LM Studio running?. Check that LM Studio
is running and accessible.') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:299: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,442 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,446 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,449 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,449 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,451 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,452 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,468 - devsynth.application.llm.lmstudio_provider - ERROR -
LM Studio API error:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?. Check that LM Studio is running and accessible.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio API error:
    LM Studio is not reachable at ws://127.0.0.1:1234/llm (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?. Check that LM Studio is running and accessible.
_______ TestLMStudioProviderStreaming.test_generate_stream_with_context ________

self = <test_lmstudio_provider.TestLMStudioProviderStreaming object at
0x11fc916d0>

    @pytest.mark.medium
    def test_generate_stream_with_context(self):
        """Test streaming with conversation context."""
        config = {"base_url": "http://localhost:1234/v1"}

        context = [
            {"role": "system", "content": "You are helpful."},
        ]

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_response = MagicMock()
            mock_response.iter_lines.return_value = [
                '{"id": "chatcmpl-abc123", "object": "chat.completion.chunk",
"choices": [{"delta": {"content": "Hi"}}]}',
            ]

            mock_lmstudio.sync_api.chat.completions.create.return_value =
mock_response

            provider = LMStudioProvider(config)
>           stream_gen = provider.generate_with_context_stream("Hello", context)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'generate_with_context_stream'. Did you mean: 'generate_with_context'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:361: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,505 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,509 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,512 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,512 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,515 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,515 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
________ TestLMStudioProviderEmbeddings.test_get_embedding_single_text _________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16c0dc110>
text = 'The quick brown fox'

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using LM Studio.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
        """
        try:
            result = self._execute_with_resilience(
>               self._lmstudio.embedding_model(self.model).embed,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                text,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:587:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider._AttrForwarder object at
0x118cc6240>
args = ('qwen/qwen3-4b-2507',), kwargs = {}
real = <module 'lmstudio' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/__init__.py'>

    def __call__(self, *args, **kwargs):  # pragma: no cover - thin forwarder
        real = self._proxy._ensure()
>       return getattr(real, self._attr)(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       lmstudio.LMStudioWebsocketError:
E           LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E           Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: LMStudioWebsocketError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderEmbeddings object at
0x11fca40e0>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-model', 'object':
'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_single_text(self, mock_embedding_response):
        """Test embedding generation for single text."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.embeddings.create.return_value = (
                mock_embedding_response
            )

            provider = LMStudioProvider(config)
>           embedding = provider.get_embedding("The quick brown fox")
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:406:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16c0dc110>
text = 'The quick brown fox'

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using LM Studio.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
        """
        try:
            result = self._execute_with_resilience(
                self._lmstudio.embedding_model(self.model).embed,
                text,
            )
            if isinstance(result, list) and result:
                return result if isinstance(result[0], float) else result[0]
            raise LMStudioModelError("Invalid embedding response")
        except Exception as e:  # noqa: BLE001
            error_msg = f"Failed to connect to LM Studio: {str(e)}"
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
Failed to connect to LM Studio:
E               LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due
to builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E               Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:596: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,538 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,543 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,546 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,546 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,549 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,549 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,564 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
_______ TestLMStudioProviderEmbeddings.test_get_embedding_multiple_texts _______

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16c8bfa70>
text = ['Text one', 'Text two']

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using LM Studio.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
        """
        try:
            result = self._execute_with_resilience(
>               self._lmstudio.embedding_model(self.model).embed,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                text,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:587:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider._AttrForwarder object at
0x118cc6240>
args = ('qwen/qwen3-4b-2507',), kwargs = {}
real = <module 'lmstudio' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/__init__.py'>

    def __call__(self, *args, **kwargs):  # pragma: no cover - thin forwarder
        real = self._proxy._ensure()
>       return getattr(real, self._attr)(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       lmstudio.LMStudioWebsocketError:
E           LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E           Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: LMStudioWebsocketError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderEmbeddings object at
0x11fca4590>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-model', 'object':
'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_multiple_texts(self, mock_embedding_response):
        """Test embedding generation for multiple texts."""
        config = {"base_url": "http://localhost:1234/v1"}

        # Mock response for multiple texts
        multi_response = {
            "object": "list",
            "data": [
                {"object": "embedding", "embedding": [0.1, 0.2, 0.3], "index":
0},
                {"object": "embedding", "embedding": [0.4, 0.5, 0.6], "index":
1},
            ],
            "model": "text-embedding-model",
            "usage": {"prompt_tokens": 16, "total_tokens": 16},
        }

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            mock_lmstudio.sync_api.embeddings.create.return_value =
multi_response

            provider = LMStudioProvider(config)
>           embeddings = provider.get_embedding(["Text one", "Text two"])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:440:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at
0x16c8bfa70>
text = ['Text one', 'Text two']

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using LM Studio.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
        """
        try:
            result = self._execute_with_resilience(
                self._lmstudio.embedding_model(self.model).embed,
                text,
            )
            if isinstance(result, list) and result:
                return result if isinstance(result[0], float) else result[0]
            raise LMStudioModelError("Invalid embedding response")
        except Exception as e:  # noqa: BLE001
            error_msg = f"Failed to connect to LM Studio: {str(e)}"
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError:
Failed to connect to LM Studio:
E               LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due
to builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
E               Is LM Studio running?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:596: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,613 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,617 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,620 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,620 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,622 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,622 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-28 10:32:08,637 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio:
    LM Studio is not reachable at ws://127.0.0.1:1234/embedding (due to
builtins.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)).
    Is LM Studio running?
___ TestLMStudioProviderErrorHandling.test_server_connection_error_handling ____

self = <test_lmstudio_provider.TestLMStudioProviderErrorHandling object at
0x11e9f9520>

    @pytest.mark.medium
    def test_server_connection_error_handling(self):
        """Test handling of server connection errors."""
        config = {"base_url": "http://localhost:1234/v1"}

        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio

            # Mock connection error during model listing
            from requests.exceptions import ConnectionError

            mock_lmstudio.sync_api.models.list.side_effect = ConnectionError(
                "Connection refused"
            )

            provider = LMStudioProvider(config)

            # Should handle connection errors gracefully
>           assert provider.server_unavailable is True
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute
'server_unavailable'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:725: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,677 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: Default client is already
created, cannot set its API host.
2025-10-28 10:32:08,683 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,686 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,688 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio:
Network access disabled during tests, falling back to auto-selection
2025-10-28 10:32:08,691 - devsynth.application.llm.lmstudio_provider - ERROR -
Failed to connect to LM Studio: Network access disabled during tests
2025-10-28 10:32:08,691 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM
Studio default client configuration failed: Default client is already created,
cannot set its API host.
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could
not connect to LM Studio: Failed to connect to LM Studio: Network access
disabled during tests. Using fallback: qwen/qwen3-4b-2507
__________ TestOpenAIProviderTextGeneration.test_generate_basic_text ___________

self = <test_openai_provider.TestOpenAIProviderTextGeneration object at
0x11fca46e0>
mock_openai_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from OpenAI.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'gpt-3.5-turbo', ...}

    @pytest.mark.medium
    def test_generate_basic_text(self, mock_openai_response):
        """Test basic text generation."""
        config = {"api_key": "test-key"}

        with (
            patch(
                "devsynth.application.llm.openai_provider.OpenAI"
            ) as mock_sync_client_class,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_client_class,
        ):

            # Mock the sync client (used for generate())
            mock_sync_client = MagicMock()
            mock_sync_client_class.return_value = mock_sync_client

            # Mock the response
            mock_response = MagicMock()
            mock_response.choices = [MagicMock()]
            mock_response.choices[0].message = MagicMock()
            mock_response.choices[0].message.content = (
                "This is a test response from OpenAI."
            )
            mock_response.usage = MagicMock()
            mock_response.usage.prompt_tokens = 10
            mock_response.usage.completion_tokens = 12
            mock_response.usage.total_tokens = 22

            mock_sync_client.chat.completions.create.return_value =
mock_response

            provider = OpenAIProvider(config)
            response = provider.generate("Hello, how are you?")

        assert response == "This is a test response from OpenAI."

        # Verify the API was called correctly
        mock_sync_client.chat.completions.create.assert_called_once()
        call_args = mock_sync_client.chat.completions.create.call_args[1]
>       assert call_args["model"] == "gpt-3.5-turbo"  # Default model
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 'stub-model' == 'gpt-3.5-turbo'
E
E         - gpt-3.5-turbo
E         + stub-model

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:167: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,767 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,789 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
_________ TestOpenAIProviderTextGeneration.test_generate_with_context __________

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12f72ef90>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using OpenAI.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
>           message = getattr(response.choices[0], "message", None)
                              ^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'choices'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:407: AttributeError

During handling of the above exception, another exception occurred:

self = <test_openai_provider.TestOpenAIProviderTextGeneration object at
0x11fca4fe0>
mock_openai_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from OpenAI.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'gpt-3.5-turbo', ...}

    @pytest.mark.medium
    def test_generate_with_context(self, mock_openai_response):
        """Test text generation with conversation context."""
        config = {"api_key": "test-key"}

        context = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What's the weather like?"},
        ]

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=mock_openai_response,
                status=200,
            )

            provider = OpenAIProvider(config)
>           response = provider.generate_with_context("How about tomorrow?",
context)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:229:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12f72ef90>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using OpenAI.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
            message = getattr(response.choices[0], "message", None)
            content = getattr(message, "content", None)
            if content is None:
                raise OpenAIModelError("Invalid response from OpenAI")
            return content
        except OpenAIModelError:
            raise
        except OpenAITokenLimitError:
            raise  # Re-raise token limit errors as-is
        except OpenAIConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:
            error_msg = f"OpenAI API error: {str(e)}. Check your API key and
model configuration."
            logger.error(error_msg)
>           raise OpenAIConnectionError(error_msg)
E           devsynth.application.llm.openai_provider.OpenAIConnectionError:
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:421: OpenAIConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,866 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:08,890 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:32:08,890 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
____ TestOpenAIProviderTextGeneration.test_generate_with_invalid_parameters ____

self = <test_openai_provider.TestOpenAIProviderTextGeneration object at
0x11fca54c0>

    @pytest.mark.medium
    def test_generate_with_invalid_parameters(self):
        """Test text generation with invalid parameters."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)

        # Test invalid temperature
        with pytest.raises(DevSynthError) as exc_info:
            provider.generate("Hello", {"temperature": 3.0})

>       assert "temperature must be between" in str(exc_info.value)
E       AssertionError: assert 'temperature must be between' in 'OpenAI
temperature must be a number between 0.0 and 2.0, got 3.0'
E        +  where 'OpenAI temperature must be a number between 0.0 and 2.0, got
3.0' = str(OpenAIConfigurationError('OpenAI temperature must be a number between
0.0 and 2.0, got 3.0'))
E        +    where OpenAIConfigurationError('OpenAI temperature must be a
number between 0.0 and 2.0, got 3.0') = <ExceptionInfo
OpenAIConfigurationError('OpenAI temperature must be a number between 0.0 and
2.0, got 3.0') tblen=3>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:251: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,931 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:08,953 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
______ TestOpenAIProviderTextGeneration.test_generate_api_error_handling _______

self = <test_openai_provider.TestOpenAIProviderTextGeneration object at
0x11fca59a0>

    @pytest.mark.medium
    def test_generate_api_error_handling(self):
        """Test API error handling."""
        config = {"api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:264:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c8bf3b0>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://api.openai.com/v1/chat/completions')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:08,967 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:08,986 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:32:08,987 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
______ TestOpenAIProviderTextGeneration.test_generate_rate_limit_handling ______

self = <test_openai_provider.TestOpenAIProviderTextGeneration object at
0x11fca5e50>

    @pytest.mark.medium
    def test_generate_rate_limit_handling(self):
        """Test rate limit error handling."""
        config = {"api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:284:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c297c80>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://api.openai.com/v1/chat/completions')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,038 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:09,060 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
2025-10-28 10:32:09,061 - devsynth.application.llm.openai_provider - ERROR -
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI
API error: 'NoneType' object has no attribute 'choices'. Check your API key and
model configuration.
____ TestOpenAIProviderStreaming.test_generate_stream_without_async_client _____

self = <test_openai_provider.TestOpenAIProviderStreaming object at 0x11fca6870>

    @pytest.mark.medium
    def test_generate_stream_without_async_client(self):
        """Test streaming when async client is not available."""
        config = {"api_key": "test-key"}

        with patch("devsynth.application.llm.openai_provider.AsyncOpenAI",
None):
            provider = OpenAIProvider(config)

>           with pytest.raises(DevSynthError) as exc_info:
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           Failed: DID NOT RAISE <class 'devsynth.exceptions.DevSynthError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:346: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,125 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:09,149 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
_________ TestOpenAIProviderEmbeddings.test_get_embedding_single_text __________

self = <test_openai_provider.TestOpenAIProviderEmbeddings object at 0x11fca6e10>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-ada-002',
'object': 'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_single_text(self, mock_embedding_response):
        """Test embedding generation for single text."""
        config = {"api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:376:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c083fb0>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://api.openai.com/v1/embeddings')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,168 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:09,192 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
________ TestOpenAIProviderEmbeddings.test_get_embedding_multiple_texts ________

self = <test_openai_provider.TestOpenAIProviderEmbeddings object at 0x11fca7230>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-ada-002',
'object': 'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_multiple_texts(self, mock_embedding_response):
        """Test embedding generation for multiple texts."""
        config = {"api_key": "test-key"}

        # Mock response for multiple texts
        multi_response = {
            "object": "list",
            "data": [
                {"object": "embedding", "embedding": [0.1, 0.2, 0.3], "index":
0},
                {"object": "embedding", "embedding": [0.4, 0.5, 0.6], "index":
1},
            ],
            "model": "text-embedding-ada-002",
            "usage": {"prompt_tokens": 16, "total_tokens": 16},
        }

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:414:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c6ad970>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://api.openai.com/v1/embeddings')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,265 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:09,283 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
__________ TestOpenAIProviderEmbeddings.test_get_embedding_api_error ___________

self = <test_openai_provider.TestOpenAIProviderEmbeddings object at 0x11fca5f40>

    @pytest.mark.medium
    def test_get_embedding_api_error(self):
        """Test embedding API error handling."""
        config = {"api_key": "test-key"}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/embeddings",
                json={"error": {"message": "Invalid model"}},
                status=400,
            )

            provider = OpenAIProvider(config)

>           with pytest.raises(DevSynthError) as exc_info:
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           Failed: DID NOT RAISE <class 'devsynth.exceptions.DevSynthError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:444: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,342 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:09,363 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://api.openai.com/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
_________ TestOpenAIProviderResilience.test_retry_on_transient_errors __________

self = <test_openai_provider.TestOpenAIProviderResilience object at 0x11ea44350>

    @pytest.mark.medium
    def test_retry_on_transient_errors(self):
        """Test retry behavior on transient errors."""
        config = {"api_key": "test-key"}

        call_count = 0

        def mock_api_call():
            nonlocal call_count
            call_count += 1
            if call_count < 3:  # Fail first 2 attempts
                raise Exception("Transient error")
            return {"choices": [{"message": {"content": "Success after
retry"}}]}

        with patch.object(OpenAIProvider, "generate") as mock_generate:
            mock_generate.side_effect = mock_api_call

            provider = OpenAIProvider(config)

            # This would test retry logic
            # For unit tests, we verify the structure exists
            assert hasattr(provider, "_should_retry")
>           assert hasattr(provider, "_get_retry_config")
E           AssertionError: assert False
E            +  where False =
hasattr(<devsynth.application.llm.openai_provider.OpenAIProvider object at
0x12f8b1040>, '_get_retry_config')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:607: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,430 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:09,447 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
________ TestOpenRouterProviderTextGeneration.test_generate_basic_text _________

args = (), kwargs = {}, num_retries = 3, delay = 15.925109619000095
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:209: in _api_call
    response = self.sync_client.post("/chat/completions", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x16caf22d0>, method = 'POST'
url = '/chat/completions', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16caf3d40>
prompt = 'Hello, how are you?', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:219:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x12ecfaf10, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fca7590>
mock_openrouter_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from OpenRouter.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'google/gemini-flash-1.5', ...}

    @pytest.mark.medium
    def test_generate_basic_text(self, mock_openrouter_response):
        """Test basic text generation."""
        config = {"openrouter_api_key": "test-key"}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=mock_openrouter_response,
                status=200,
                headers={"Content-Type": "application/json"},
            )

            provider = OpenRouterProvider(config)
>           response = provider.generate("Hello, how are you?")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:141:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16caf3d40>
prompt = 'Hello, how are you?', parameters = None

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:223: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:09,479 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:09,500 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:09,501 - fallback - WARNING - Retry attempt 1/3 after 2.58s
delay
2025-10-28 10:32:09,501 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.58s)
2025-10-28 10:32:12,092 - fallback - WARNING - Retry attempt 2/3 after 6.02s
delay
2025-10-28 10:32:12,092 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 6.02s)
2025-10-28 10:32:18,115 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:32:18,116 - fallback - WARNING - Retry attempt 3/3 after 15.93s
delay
2025-10-28 10:32:18,116 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 15.93s)
2025-10-28 10:32:19,479 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.58s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.58s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 6.02s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 6.02s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 15.93s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 15.93s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
__ TestOpenRouterProviderTextGeneration.test_generate_with_custom_parameters ___

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16c081190>
prompt = 'Tell me a story', parameters = {'max_tokens': 500, 'temperature': 0.9}

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:219:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7: in wrapper
    result = wrapped_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.fallback.CircuitBreaker object at 0x16c081f10>
func = <function OpenRouterProvider.generate.<locals>._api_call at 0x16cfe2660>
args = (), kwargs = {}, state = 'OPEN', last_failure_time = 1761672742.967963

    def call(self, func: Callable[P, T], *args: P.args, **kwargs: P.kwargs) ->
T:
        """
        Call a function with circuit breaker protection.

        Parameters
        ----------
        func : Callable
            The function to call
        *args: Any
            Positional arguments for the function
        **kwargs: Any
            Keyword arguments for the function

        Returns
        -------
        Any
            The result of the function call

        Raises
        ------
        DevSynthError
            If the circuit is open
        """
        # Check if the circuit is open
        with self.lock:
            state = self.state
            last_failure_time = self.last_failure_time
        if state == self.OPEN:
            if time.time() - last_failure_time >= self.recovery_timeout:
                with self.lock:
                    self.state = self.HALF_OPEN
                    self.test_calls_remaining = self.test_calls
                inc_circuit_breaker_state(func.__name__, self.HALF_OPEN)
                self.logger.info(
                    f"Circuit breaker for {func.__name__} transitioned from OPEN
to HALF_OPEN",
                    function=func.__name__,
                    state=self.state,
                )
                self._safe_hook(self.on_half_open, func.__name__)
            else:
                inc_circuit_breaker_state(func.__name__, self.OPEN)
                self.logger.warning(
                    f"Circuit breaker for {func.__name__} is OPEN, failing
fast",
                    function=func.__name__,
                    state=state,
                    recovery_time_remaining=self.recovery_timeout
                    - (time.time() - last_failure_time),
                )
>           raise DevSynthError(
                f"Circuit breaker for {func.__name__} is open",
                error_code="CIRCUIT_OPEN",
                details={
                    "function": func.__name__,
                    "recovery_time_remaining": self.recovery_timeout
                    - (time.time() - last_failure_time),
                },
            )
E           devsynth.exceptions.DevSynthError: Circuit breaker for _api_call is
open

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:92
6: DevSynthError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fca79b0>
mock_openrouter_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from OpenRouter.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'google/gemini-flash-1.5', ...}

    @pytest.mark.medium
    def test_generate_with_custom_parameters(self, mock_openrouter_response):
        """Test text generation with custom parameters."""
        config = {"openrouter_api_key": "test-key"}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=mock_openrouter_response,
                status=200,
            )

            provider = OpenRouterProvider(config)
>           response = provider.generate(
                "Tell me a story", parameters={"temperature": 0.9, "max_tokens":
500}
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:166:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16c081190>
prompt = 'Tell me a story', parameters = {'max_tokens': 500, 'temperature': 0.9}

    def generate(self, prompt: str, parameters: Dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenRouter.

        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Circuit breaker for _api_call is open

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:223: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:19,574 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:19,587 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:19,588 - fallback - WARNING - Retry attempt 1/3 after 1.14s
delay
2025-10-28 10:32:19,588 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.14s)
2025-10-28 10:32:20,739 - fallback - WARNING - Retry attempt 2/3 after 2.22s
delay
2025-10-28 10:32:20,740 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 2.22s)
2025-10-28 10:32:22,968 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:32:22,969 - fallback - WARNING - Retry attempt 3/3 after 5.38s
delay
2025-10-28 10:32:22,969 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 5.38s)
2025-10-28 10:32:28,357 - circuit_breaker - WARNING - Circuit breaker for
_api_call is OPEN, failing fast
2025-10-28 10:32:28,358 - fallback - WARNING - Circuit open - aborting retries
for _wrapped
2025-10-28 10:32:28,358 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Circuit breaker for _api_call is open
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.14s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.14s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 2.22s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 2.22s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 5.38s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 5.38s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call is
OPEN, failing fast
WARNING  fallback:logging_setup.py:615 Circuit open - aborting retries for
_wrapped
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Circuit breaker for _api_call is open
_______ TestOpenRouterProviderTextGeneration.test_generate_with_context ________

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12f874da0>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using
OpenRouter.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:283:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7: in wrapper
    result = wrapped_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.fallback.CircuitBreaker object at 0x16d554290>
func = <function OpenRouterProvider.generate_with_context.<locals>._api_call at
0x16cf87ce0>
args = (), kwargs = {}, state = 'OPEN', last_failure_time = 1761672753.3910432

    def call(self, func: Callable[P, T], *args: P.args, **kwargs: P.kwargs) ->
T:
        """
        Call a function with circuit breaker protection.

        Parameters
        ----------
        func : Callable
            The function to call
        *args: Any
            Positional arguments for the function
        **kwargs: Any
            Keyword arguments for the function

        Returns
        -------
        Any
            The result of the function call

        Raises
        ------
        DevSynthError
            If the circuit is open
        """
        # Check if the circuit is open
        with self.lock:
            state = self.state
            last_failure_time = self.last_failure_time
        if state == self.OPEN:
            if time.time() - last_failure_time >= self.recovery_timeout:
                with self.lock:
                    self.state = self.HALF_OPEN
                    self.test_calls_remaining = self.test_calls
                inc_circuit_breaker_state(func.__name__, self.HALF_OPEN)
                self.logger.info(
                    f"Circuit breaker for {func.__name__} transitioned from OPEN
to HALF_OPEN",
                    function=func.__name__,
                    state=self.state,
                )
                self._safe_hook(self.on_half_open, func.__name__)
            else:
                inc_circuit_breaker_state(func.__name__, self.OPEN)
                self.logger.warning(
                    f"Circuit breaker for {func.__name__} is OPEN, failing
fast",
                    function=func.__name__,
                    state=state,
                    recovery_time_remaining=self.recovery_timeout
                    - (time.time() - last_failure_time),
                )
>           raise DevSynthError(
                f"Circuit breaker for {func.__name__} is open",
                error_code="CIRCUIT_OPEN",
                details={
                    "function": func.__name__,
                    "recovery_time_remaining": self.recovery_timeout
                    - (time.time() - last_failure_time),
                },
            )
E           devsynth.exceptions.DevSynthError: Circuit breaker for _api_call is
open

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:92
6: DevSynthError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fcb43b0>
mock_openrouter_response = {'choices': [{'finish_reason': 'stop', 'index': 0,
'message': {'content': 'This is a test response from OpenRouter.', 'role':
'assistant'}}], 'created': 1677652288, 'id': 'chatcmpl-abc123', 'model':
'google/gemini-flash-1.5', ...}

    @pytest.mark.medium
    def test_generate_with_context(self, mock_openrouter_response):
        """Test text generation with conversation context."""
        config = {"openrouter_api_key": "test-key"}

        context = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What's the weather like?"},
        ]

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=mock_openrouter_response,
                status=200,
            )

            provider = OpenRouterProvider(config)
>           response = provider.generate_with_context("How about tomorrow?",
context)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:197:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12f874da0>
prompt = 'How about tomorrow?'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': "What's the weather like?", 'role': 'user'}]
parameters = None

    def generate_with_context(
        self,
        prompt: str,
        context: List[Dict[str, str]],
        parameters: Dict[str, Any] = None,
    ) -> str:
        """Generate text from a prompt with conversation context using
OpenRouter.

        Args:
            prompt: The prompt to generate text from
            context: List of conversation messages in the format [{"role":
"...", "content": "..."}]
            parameters: Additional parameters for the generation

        Returns:
            The generated text

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the conversation exceeds the token limit
        """
        # Create a copy of the context and add the new prompt
        messages = context.copy()
        messages.append({"role": "user", "content": prompt})

        # Check token count and prune if necessary
        token_count = self.token_tracker.count_conversation_tokens(messages)
        if token_count > self.max_tokens:
            messages = self.token_tracker.prune_conversation(messages,
self.max_tokens)

        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": self.model,
                "messages": messages,
                **params,
            }

            response = self.sync_client.post("/chat/completions", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"]
            else:
                raise OpenRouterModelError("Invalid response from OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter API error: Circuit breaker for _api_call is open

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:287: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:28,430 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:28,450 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:28,451 - fallback - WARNING - Retry attempt 1/3 after 1.57s
delay
2025-10-28 10:32:28,451 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.57s)
2025-10-28 10:32:30,019 - fallback - WARNING - Retry attempt 2/3 after 3.36s
delay
2025-10-28 10:32:30,019 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 3.36s)
2025-10-28 10:32:33,391 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:32:33,392 - fallback - WARNING - Retry attempt 3/3 after 4.79s
delay
2025-10-28 10:32:33,392 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 4.79s)
2025-10-28 10:32:38,183 - circuit_breaker - WARNING - Circuit breaker for
_api_call is OPEN, failing fast
2025-10-28 10:32:38,184 - fallback - WARNING - Circuit open - aborting retries
for _wrapped
2025-10-28 10:32:38,185 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Circuit breaker for _api_call is open
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.57s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.57s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 3.36s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 3.36s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 4.79s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 4.79s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call is
OPEN, failing fast
WARNING  fallback:logging_setup.py:615 Circuit open - aborting retries for
_wrapped
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Circuit breaker for _api_call is open
__ TestOpenRouterProviderTextGeneration.test_generate_with_invalid_parameters __

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fcb4890>

    @pytest.mark.medium
    def test_generate_with_invalid_parameters(self):
        """Test text generation with invalid parameters."""
        config = {"openrouter_api_key": "test-key"}
        provider = OpenRouterProvider(config)

        # Test invalid temperature
        with pytest.raises(OpenRouterConnectionError) as exc_info:
            provider.generate("Hello", {"temperature": 3.0})

>       assert "temperature must be between 0 and 2" in str(exc_info.value)
E       AssertionError: assert 'temperature must be between 0 and 2' in
'OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)'
E        +  where 'OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)' = str(OpenRouterConnectionError('OpenRouter API
error: Test timed out after 10 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)'))
E        +    where OpenRouterConnectionError('OpenRouter API error: Test timed
out after 10 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)') = <ExceptionInfo
OpenRouterConnectionError('OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)') tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:219: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:38,257 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:32:38,271 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:38,272 - fallback - WARNING - Retry attempt 1/3 after 1.32s
delay
2025-10-28 10:32:38,272 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.32s)
2025-10-28 10:32:39,602 - fallback - WARNING - Retry attempt 2/3 after 3.37s
delay
2025-10-28 10:32:39,602 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 3.37s)
2025-10-28 10:32:42,971 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:32:42,972 - fallback - WARNING - Retry attempt 3/3 after 8.22s
delay
2025-10-28 10:32:42,973 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 8.22s)
2025-10-28 10:32:48,258 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.32s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.32s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 3.37s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 3.37s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 8.22s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 8.22s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
____ TestOpenRouterProviderTextGeneration.test_generate_api_error_handling _____

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fcb4d40>

    @pytest.mark.medium
    def test_generate_api_error_handling(self):
        """Test API error handling."""
        config = {"openrouter_api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:232:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c0c7e60>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://openrouter.ai/api/v1/chat/completions')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:48,294 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:48,311 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:48,312 - fallback - WARNING - Retry attempt 1/3 after 2.27s
delay
2025-10-28 10:32:48,312 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.27s)
2025-10-28 10:32:50,590 - fallback - WARNING - Retry attempt 2/3 after 4.29s
delay
2025-10-28 10:32:50,590 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 4.29s)
2025-10-28 10:32:54,894 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:32:54,896 - fallback - WARNING - Retry attempt 3/3 after 6.69s
delay
2025-10-28 10:32:54,896 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 6.69s)
2025-10-28 10:32:58,290 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.27s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.27s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 4.29s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 4.29s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 6.69s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 6.69s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
____ TestOpenRouterProviderTextGeneration.test_generate_rate_limit_handling ____

self = <test_openrouter_provider.TestOpenRouterProviderTextGeneration object at
0x11fcb51f0>

    @pytest.mark.medium
    def test_generate_rate_limit_handling(self):
        """Test rate limit error handling."""
        config = {"openrouter_api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:252:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c6ad520>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://openrouter.ai/api/v1/chat/completions')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:32:58,345 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
2025-10-28 10:32:58,365 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:32:58,366 - fallback - WARNING - Retry attempt 1/3 after 1.16s
delay
2025-10-28 10:32:58,366 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.16s)
2025-10-28 10:32:59,542 - fallback - WARNING - Retry attempt 2/3 after 2.28s
delay
2025-10-28 10:32:59,542 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 2.28s)
2025-10-28 10:33:01,831 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:33:01,833 - fallback - WARNING - Retry attempt 3/3 after 5.87s
delay
2025-10-28 10:33:01,833 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 5.87s)
2025-10-28 10:33:07,715 - circuit_breaker - WARNING - Circuit breaker for
_api_call is OPEN, failing fast
2025-10-28 10:33:07,716 - fallback - WARNING - Circuit open - aborting retries
for _wrapped
2025-10-28 10:33:07,716 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter API error: Circuit breaker for _api_call is open
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/chat/completions Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.16s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.16s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 2.28s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 2.28s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 5.87s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 5.87s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call is
OPEN, failing fast
WARNING  fallback:logging_setup.py:615 Circuit open - aborting retries for
_wrapped
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter API error: Circuit breaker for _api_call is open
______ TestOpenRouterProviderStreaming.test_generate_stream_without_httpx ______

self = <test_openrouter_provider.TestOpenRouterProviderStreaming object at
0x11fcb5760>

    @pytest.mark.medium
    def test_generate_stream_without_httpx(self):
        """Test streaming when httpx is not available."""
        config = {"openrouter_api_key": "test-key"}

        with patch("devsynth.application.llm.openrouter_provider.httpx", None):
            provider = OpenRouterProvider(config)

>           with pytest.raises(OpenRouterConnectionError) as exc_info:
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           Failed: DID NOT RAISE <class
'devsynth.application.llm.openrouter_provider.OpenRouterConnectionError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:314: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:07,783 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:33:07,799 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
_______ TestOpenRouterProviderEmbeddings.test_get_embedding_single_text ________

args = (), kwargs = {}, num_retries = 3, delay = 8.476630790123924
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:478: in _api_call
    response = self.sync_client.post("/embeddings", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x12ee2cc20>, method = 'POST'
url = '/embeddings', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12ffb6a80>
text = 'The quick brown fox'

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using OpenRouter.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
        """

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": "text-embedding-ada-002",  # Use OpenAI-compatible
embedding model
                "input": text,
            }

            response = self.sync_client.post("/embeddings", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "data" in response_data and len(response_data["data"]) > 0:
                return response_data["data"][0]["embedding"]
            else:
                raise OpenRouterModelError("Invalid embedding response from
OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:488:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x16f976c30, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderEmbeddings object at
0x11fcb5d30>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-ada-002',
'object': 'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_single_text(self, mock_embedding_response):
        """Test embedding generation for single text."""
        config = {"openrouter_api_key": "test-key"}

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/embeddings",
                json=mock_embedding_response,
                status=200,
            )

            provider = OpenRouterProvider(config)
>           embedding = provider.get_embedding("The quick brown fox")
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:353:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x12ffb6a80>
text = 'The quick brown fox'

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using OpenRouter.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
        """

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": "text-embedding-ada-002",  # Use OpenAI-compatible
embedding model
                "input": text,
            }

            response = self.sync_client.post("/embeddings", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "data" in response_data and len(response_data["data"]) > 0:
                return response_data["data"][0]["embedding"]
            else:
                raise OpenRouterModelError("Invalid embedding response from
OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter embedding API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:492: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:07,814 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:33:07,834 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:33:07,834 - fallback - WARNING - Retry attempt 1/3 after 1.48s
delay
2025-10-28 10:33:07,835 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 1.48s)
2025-10-28 10:33:09,312 - fallback - WARNING - Retry attempt 2/3 after 3.14s
delay
2025-10-28 10:33:09,313 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 3.14s)
2025-10-28 10:33:12,461 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:33:12,462 - fallback - WARNING - Retry attempt 3/3 after 8.48s
delay
2025-10-28 10:33:12,462 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 8.48s)
2025-10-28 10:33:17,816 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 1.48s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 1.48s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 3.14s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 3.14s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 8.48s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 8.48s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
______ TestOpenRouterProviderEmbeddings.test_get_embedding_multiple_texts ______

args = (), kwargs = {}, num_retries = 3, delay = 8.822681393488468
anonymous_conditions = [], named_conditions = [], cb_named = [], cb_anon = []
anonymous_predicates = []

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        # Initialize variables
        num_retries = 0
        delay = initial_delay
        anonymous_conditions: list[RetryConditionFunc] = []
        named_conditions: list[tuple[str, RetryConditionFunc]] = []
        if retry_conditions is not None:
            if isinstance(retry_conditions, Mapping):
                for name, cond in retry_conditions.items():
                    named_conditions.append((name, _condition_from_spec(cond)))
            else:
                if isinstance(retry_conditions, (str, bytes)):
                    raise TypeError(
                        "retry_conditions must be a sequence of callables,
strings, or exception types"
                    )
                for cond in retry_conditions:
                    anonymous_conditions.append(_condition_from_spec(cond))
        cb_named: list[tuple[str, ConditionCallbackFunc]] = []
        cb_anon: list[ConditionCallbackFunc] = []
        if condition_callbacks is not None:
            if isinstance(condition_callbacks, Mapping):
                for name, cb in condition_callbacks.items():
                    cb_named.append((name, cb))
            else:
                if isinstance(condition_callbacks, (str, bytes)):
                    raise TypeError(
                        "condition_callbacks must be a sequence of callables"
                    )
                cb_anon = list(condition_callbacks)

        anonymous_predicates: list[Callable[[T], bool]] = []
        named_predicates: list[tuple[str, Callable[[T], bool]]] = []
        if retry_predicates is not None:
            if isinstance(retry_predicates, Mapping):
                for name, pred in retry_predicates.items():
                    named_predicates.append((name, _predicate_from_spec(pred)))
            else:
                if isinstance(retry_predicates, (str, bytes)):
                    raise TypeError(
                        "retry_predicates must be a sequence of callables or
ints"
                    )
                anonymous_predicates = [
                    _predicate_from_spec(pred) for pred in retry_predicates
                ]

        # Loop until max retries reached
        while True:
            try:
>               result = wrapped_func(*args, **kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:29
7:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:166: in _wrapped
    return future.result(timeout=self.timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:93
8: in call
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:478: in _api_call
    response = self.sync_client.post("/embeddings", json=payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x16c254080>, method = 'POST'
url = '/embeddings', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError

During handling of the above exception, another exception occurred:

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16c2556a0>
text = ['Text one', 'Text two']

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using OpenRouter.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
        """

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": "text-embedding-ada-002",  # Use OpenAI-compatible
embedding model
                "input": text,
            }

            response = self.sync_client.post("/embeddings", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "data" in response_data and len(response_data["data"]) > 0:
                return response_data["data"][0]["embedding"]
            else:
                raise OpenRouterModelError("Invalid embedding response from
OpenRouter")

        try:
>           return self._execute_with_resilience(_api_call)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:488:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience
    return _wrapped()
           ^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py:57
2: in wrapper
    time.sleep(delay)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signum = 14
frame = <frame at 0x13ee3c4b0, file
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.py',
line 572, code wrapper>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_openrouter_provider.TestOpenRouterProviderEmbeddings object at
0x11fcb6150>
mock_embedding_response = {'data': [{'embedding': [0.1, 0.2, 0.3, 0.4, 0.5],
'index': 0, 'object': 'embedding'}], 'model': 'text-embedding-ada-002',
'object': 'list', 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}

    @pytest.mark.medium
    def test_get_embedding_multiple_texts(self, mock_embedding_response):
        """Test embedding generation for multiple texts."""
        config = {"openrouter_api_key": "test-key"}

        # Mock response for multiple texts
        multi_response = {
            "object": "list",
            "data": [
                {"object": "embedding", "embedding": [0.1, 0.2, 0.3], "index":
0},
                {"object": "embedding", "embedding": [0.4, 0.5, 0.6], "index":
1},
            ],
            "model": "text-embedding-ada-002",
            "usage": {"prompt_tokens": 16, "total_tokens": 16},
        }

        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/embeddings",
                json=multi_response,
                status=200,
            )

            provider = OpenRouterProvider(config)
>           embeddings = provider.get_embedding(["Text one", "Text two"])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:389:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16c2556a0>
text = ['Text one', 'Text two']

    def get_embedding(self, text: str) -> List[float]:
        """Get an embedding vector for the given text using OpenRouter.

        Args:
            text: The text to get an embedding for

        Returns:
            The embedding vector as a list of floats

        Raises:
            OpenRouterConnectionError: If there's an issue connecting to
OpenRouter
            OpenRouterModelError: If there's an issue with the model or response
        """

        def _api_call():
            if self.sync_client is None:
                raise OpenRouterConnectionError("HTTP client not available")

            payload = {
                "model": "text-embedding-ada-002",  # Use OpenAI-compatible
embedding model
                "input": text,
            }

            response = self.sync_client.post("/embeddings", json=payload)
            response.raise_for_status()
            response_data = response.json()

            if "data" in response_data and len(response_data["data"]) > 0:
                return response_data["data"][0]["embedding"]
            else:
                raise OpenRouterModelError("Invalid embedding response from
OpenRouter")

        try:
            return self._execute_with_resilience(_api_call)
        except Exception as e:
            error_msg = f"OpenRouter embedding API error: {str(e)}"
            logger.error(error_msg)
>           raise OpenRouterConnectionError(error_msg)
E
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError:
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:492: OpenRouterConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:18,307 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:33:18,323 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:33:18,323 - fallback - WARNING - Retry attempt 1/3 after 2.71s
delay
2025-10-28 10:33:18,324 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.71s)
2025-10-28 10:33:21,043 - fallback - WARNING - Retry attempt 2/3 after 3.94s
delay
2025-10-28 10:33:21,044 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 3.94s)
2025-10-28 10:33:24,993 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:33:24,994 - fallback - WARNING - Retry attempt 3/3 after 8.82s
delay
2025-10-28 10:33:24,995 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 8.82s)
2025-10-28 10:33:28,304 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.71s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.71s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 3.94s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 3.94s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 8.82s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 8.82s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
________ TestOpenRouterProviderEmbeddings.test_get_embedding_api_error _________

self = <test_openrouter_provider.TestOpenRouterProviderEmbeddings object at
0x11fcb6600>

    @pytest.mark.medium
    def test_get_embedding_api_error(self):
        """Test embedding API error handling."""
        config = {"openrouter_api_key": "test-key"}

>       with responses.RequestsMock() as rsps:
             ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:400:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:996: in __exit__
    self.stop(allow_assert=success)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <responses.RequestsMock object at 0x16c0ac710>, allow_assert = True

    def stop(self, allow_assert: bool = True) -> None:
        if self._patcher:
            # prevent stopping unstarted patchers
            self._patcher.stop()

            # once patcher is stopped, clean it. This is required to create a
new
            # fresh patcher on self.start()
            self._patcher = None

        if not self.assert_all_requests_are_fired:
            return

        if not allow_assert:
            return

        not_called = [m for m in self.registered() if m.call_count == 0]
        if not_called:
>           raise AssertionError(
                "Not all requests have been executed {!r}".format(
                    [(match.method, match.url) for match in not_called]
                )
            )
E           AssertionError: Not all requests have been executed [('POST',
'https://openrouter.ai/api/v1/embeddings')]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/responses/__init__.py:1232: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:28,424 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused
by Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
2025-10-28 10:33:28,437 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
2025-10-28 10:33:28,438 - fallback - WARNING - Retry attempt 1/3 after 2.85s
delay
2025-10-28 10:33:28,438 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 1, delay 2.85s)
2025-10-28 10:33:31,295 - fallback - WARNING - Retry attempt 2/3 after 4.75s
delay
2025-10-28 10:33:31,295 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 2, delay 4.75s)
2025-10-28 10:33:36,056 - circuit_breaker - WARNING - Circuit breaker for
_api_call transitioned to OPEN due to failure
2025-10-28 10:33:36,057 - fallback - WARNING - Retry attempt 3/3 after 10.60s
delay
2025-10-28 10:33:36,058 - devsynth.application.llm.openrouter_provider - WARNING
- Retrying OpenRouterProvider due to Network access disabled during tests
(httpx) (attempt 3, delay 10.60s)
2025-10-28 10:33:38,426 - devsynth.application.llm.openrouter_provider - ERROR -
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Connection refused by
Responses - the call doesn't match any registered mock.

Request:
- GET https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

Available matches:
- POST https://openrouter.ai/api/v1/embeddings Method does not match
. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
WARNING  fallback:logging_setup.py:615 Retry attempt 1/3 after 2.85s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 1, delay 2.85s)
WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 4.75s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 2, delay 4.75s)
WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for _api_call
transitioned to OPEN due to failure
WARNING  fallback:logging_setup.py:615 Retry attempt 3/3 after 10.60s delay
WARNING  devsynth.application.llm.openrouter_provider:logging_setup.py:615
Retrying OpenRouterProvider due to Network access disabled during tests (httpx)
(attempt 3, delay 10.60s)
ERROR    devsynth.application.llm.openrouter_provider:logging_setup.py:615
OpenRouter embedding API error: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
_______ TestOpenRouterProviderResilience.test_retry_on_transient_errors ________

self = <test_openrouter_provider.TestOpenRouterProviderResilience object at
0x11ea70440>

    @pytest.mark.medium
    def test_retry_on_transient_errors(self):
        """Test retry behavior on transient errors."""
        config = {"openrouter_api_key": "test-key"}

        call_count = 0

        def mock_api_call():
            nonlocal call_count
            call_count += 1
            if call_count < 3:  # Fail first 2 attempts
                raise Exception("Transient error")
            return {"choices": [{"message": {"content": "Success after
retry"}}]}

        with patch.object(OpenRouterProvider, "generate") as mock_generate:
            mock_generate.side_effect = mock_api_call

            provider = OpenRouterProvider(config)

            # This would test retry logic
            # For unit tests, we verify the structure exists
            assert hasattr(provider, "_should_retry")
>           assert hasattr(provider, "_get_retry_config")
E           AssertionError: assert False
E            +  where False =
hasattr(<devsynth.application.llm.openrouter_provider.OpenRouterProvider object
at 0x16ca10dd0>, '_get_retry_config')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:573: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:38,530 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:33:38,546 - devsynth.application.llm.openrouter_provider - INFO -
Initialized OpenRouter provider with model: google/gemini-flash-1.5
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openrouter_provider:logging_setup.py:615
Initialized OpenRouter provider with model: google/gemini-flash-1.5
______________ test_evaluate_change_persists_reasoning_to_memory _______________

    @pytest.mark.medium
    def test_evaluate_change_persists_reasoning_to_memory():
        memory = DummyMemoryManager()
        service = DialecticalReasonerService(
            requirement_repository=RequirementRepositoryInterface(),
            reasoning_repository=DialecticalReasoningRepositoryInterface(),
            impact_repository=ImpactAssessmentRepositoryInterface(),
            chat_repository=ChatRepositoryInterface(),
            notification_service=DummyNotification(),
            llm_service=DummyLLM(),
            memory_manager=memory,
        )

        # Simplify generation steps
        service._generate_thesis = lambda change: "thesis"
        service._generate_antithesis = lambda change: "antithesis"
        service._generate_arguments = lambda change, thesis, antithesis: []
        service._generate_synthesis = lambda change, arguments: "synthesis"
        service._generate_conclusion_and_recommendation = lambda change, syn: (
            "conclusion",
            "recommendation",
        )

        change = RequirementChange(requirement_id=uuid4(), created_by="user")
>       service.evaluate_change(change)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoner.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:225: in evaluate_change
    consensus_reached = self._evaluate_consensus(reasoning)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.requirements.dialectical_reasoner.DialecticalReasonerServi
ce object at 0x16ca8eb70>
reasoning =
DialecticalReasoning(id=UUID('a631e2b5-b9f3-4493-8aea-56628b2c2dff'),
change_id=UUID('0c4a57bd-4a00-4a8f-a3ff-5c179fc9...e(2025, 10, 28, 10, 33, 38,
572158), updated_at=datetime.datetime(2025, 10, 28, 10, 33, 38, 572159),
created_by='user')

    def _evaluate_consensus(self, reasoning: DialecticalReasoning) -> bool:
        """Use the LLM to determine if consensus was achieved."""
        prompt = (
            "Determine if the following reasoning reaches consensus. "
            "Respond with 'yes' or 'no'.\n"
            f"Thesis: {reasoning.thesis}\n"
            f"Antithesis: {reasoning.antithesis}\n"
            f"Synthesis: {reasoning.synthesis}\n"
            f"Conclusion: {reasoning.conclusion}\n"
            f"Recommendation: {reasoning.recommendation}\n"
        )
        try:
            response = self.llm_service.query(prompt)
        except Exception as exc:  # pragma: no cover - defensive
            raise ConsensusError(f"Consensus check failed: {exc}") from exc

        if not isinstance(response, str):
            raise ConsensusError("Consensus check returned non-textual
response")

        normalized = response.strip().lower()
        if normalized.startswith("yes"):
            return True
        if normalized.startswith("no"):
            return False

        logger.error(
            "Unexpected consensus response",  # pragma: no cover - log path
            extra={"response": response, "event":
"consensus_unexpected_response"},
        )
>       raise ConsensusError(f"Invalid consensus response: {response}")
E       devsynth.application.requirements.dialectical_reasoner.ConsensusError:
Invalid consensus response: text

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:533: ConsensusError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:38,572 - devsynth.application.requirements.dialectical_reasoner
- INFO - Evaluating change
2025-10-28 10:33:38,572 - devsynth.application.requirements.dialectical_reasoner
- ERROR - Unexpected consensus response
2025-10-28 10:33:38,572 - devsynth.application.requirements.dialectical_reasoner
- ERROR - Consensus evaluation failed
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615
Evaluating change
ERROR
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615
Unexpected consensus response
ERROR
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615
Consensus evaluation failed
_________ test_phase_progress_flushes_pending_memory_before_and_after __________

    @pytest.mark.medium
    def test_phase_progress_flushes_pending_memory_before_and_after() -> None:
        """Coordinator flushes memory queue to avoid hangs."""

        mm = MemoryManager(adapters={"default": MagicMock(flush=MagicMock())})
        team = WSDE(name="RoleTeam")
        agent = MagicMock()
        agent.id = "agent-1"
        agent.name = "Agent One"
        team.add_agent(agent)

        def assign_roles_for_phase(self, phase, _context=None):
            self.roles = {"explorer": agent}

        team.assign_roles_for_phase = assign_roles_for_phase.__get__(team, WSDE)
        coordinator = WSDEEDRRCoordinator(team, memory_manager=mm)

        mm.queue_update(
            "default",
            MemoryItem(id="m1", content={}, memory_type=MemoryType.TEAM_STATE),
        )
        assert mm.sync_manager._queue  # ensure queue populated

        with (
            patch(
                "devsynth.methodology.wsde_edrr_coordinator.flush_memory_queue",
                wraps=flush_memory_queue,
            ) as pre_flush,
            patch(
                "devsynth.domain.wsde.workflow.flush_memory_queue",
                wraps=flush_memory_queue,
            ) as post_flush,
        ):
>           assignments = coordinator.progress_to_phase(Phase.EXPAND)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_wsde_edrr_coordinator.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/ws
de_edrr_coordinator.py:59: in progress_to_phase
    flush_memory_queue(self.memory_manager)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1216: in _execute_mock_call
    return self._mock_wraps(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x16c6f17c0>

    def flush_memory_queue(memory_manager: Any) -> List[MemoryQueueEntry]:
        """Flush queued memory updates and return flushed items
deterministically.

        The sync manager maintains an internal queue protected by
``_queue_lock``.
        This helper snapshots the queue under that lock, performs a flush, waits
for
        any asynchronous synchronization to finish, and *does not* clear the
queue
        again afterwards. Clearing is handled by the sync manager itself during
the
        flush to avoid races with new updates being queued concurrently.

        Args:
            memory_manager: The memory manager instance coordinating updates.

        Returns:
            List of ``(store, MemoryItem)`` tuples that were flushed.
        """

        if not memory_manager or not hasattr(memory_manager, "sync_manager"):
            return []

        sync_manager = memory_manager.sync_manager
        lock = getattr(sync_manager, "_queue_lock", None)
        if lock:
            with lock:
                queue_snapshot: List[tuple[str, MemoryItem]] = list(
                    getattr(sync_manager, "_queue", [])
                )
        else:
            queue_snapshot = list(getattr(sync_manager, "_queue", []))

        normalized_queue: List[MemoryQueueEntry] = []
>       for store, item in queue_snapshot:
            ^^^^^^^^^^^
E       ValueError: too many values to unpack (expected 2)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/collaboration_memory_utils.py:86: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:38,652 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:33:38,652 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:33:38,652 - devsynth.domain.models.wsde_core - INFO - Added agent
Agent One to team RoleTeam
2025-10-28 10:33:38,652 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Agent
One to team RoleTeam
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
_______________ test_voting_summary_in_edrr_phases[Phase.EXPAND] _______________

coordinator =
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x16cae44d0>
phase = <Phase.EXPAND: 'expand'>

    @pytest.mark.parametrize(
        "phase",
        [Phase.EXPAND, Phase.DIFFERENTIATE, Phase.REFINE, Phase.RETROSPECT],
    )
    @pytest.mark.medium
    def test_voting_summary_in_edrr_phases(coordinator, phase):
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": ["option_a", "option_b"],
            "phase": phase.value,
            "team_task": True,
        }
        result = coordinator.delegate_task(task)
        assert result["phase"] == phase.value
>       assert "Voting was completed" in result["summary"]
E       AssertionError: assert 'Voting was completed' in 'Vote distribution:
option_a: 2 votes, option_b: 0 votes\nVote weights: '

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion/test_voting_summary_edrr.py:41: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:39,612 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:39,612 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
___________ test_voting_summary_in_edrr_phases[Phase.DIFFERENTIATE] ____________

coordinator =
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x16c587110>
phase = <Phase.DIFFERENTIATE: 'differentiate'>

    @pytest.mark.parametrize(
        "phase",
        [Phase.EXPAND, Phase.DIFFERENTIATE, Phase.REFINE, Phase.RETROSPECT],
    )
    @pytest.mark.medium
    def test_voting_summary_in_edrr_phases(coordinator, phase):
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": ["option_a", "option_b"],
            "phase": phase.value,
            "team_task": True,
        }
        result = coordinator.delegate_task(task)
        assert result["phase"] == phase.value
>       assert "Voting was completed" in result["summary"]
E       AssertionError: assert 'Voting was completed' in 'Vote distribution:
option_a: 2 votes, option_b: 0 votes\nVote weights: '

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion/test_voting_summary_edrr.py:41: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:39,623 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:39,623 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
_______________ test_voting_summary_in_edrr_phases[Phase.REFINE] _______________

coordinator =
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x12f6b8ad0>
phase = <Phase.REFINE: 'refine'>

    @pytest.mark.parametrize(
        "phase",
        [Phase.EXPAND, Phase.DIFFERENTIATE, Phase.REFINE, Phase.RETROSPECT],
    )
    @pytest.mark.medium
    def test_voting_summary_in_edrr_phases(coordinator, phase):
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": ["option_a", "option_b"],
            "phase": phase.value,
            "team_task": True,
        }
        result = coordinator.delegate_task(task)
        assert result["phase"] == phase.value
>       assert "Voting was completed" in result["summary"]
E       AssertionError: assert 'Voting was completed' in 'Vote distribution:
option_a: 2 votes, option_b: 0 votes\nVote weights: '

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion/test_voting_summary_edrr.py:41: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:39,641 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:39,642 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
_____________ test_voting_summary_in_edrr_phases[Phase.RETROSPECT] _____________

coordinator =
<devsynth.application.collaboration.coordinator.AgentCoordinatorImpl object at
0x16ca82a80>
phase = <Phase.RETROSPECT: 'retrospect'>

    @pytest.mark.parametrize(
        "phase",
        [Phase.EXPAND, Phase.DIFFERENTIATE, Phase.REFINE, Phase.RETROSPECT],
    )
    @pytest.mark.medium
    def test_voting_summary_in_edrr_phases(coordinator, phase):
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": ["option_a", "option_b"],
            "phase": phase.value,
            "team_task": True,
        }
        result = coordinator.delegate_task(task)
        assert result["phase"] == phase.value
>       assert "Voting was completed" in result["summary"]
E       AssertionError: assert 'Voting was completed' in 'Vote distribution:
option_a: 2 votes, option_b: 0 votes\nVote weights: '

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion/test_voting_summary_edrr.py:41: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:39,657 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:39,658 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
___________________________ test_init_route_succeeds ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fca6180>

    @pytest.mark.medium
    def test_init_route_succeeds(monkeypatch):
        """Test that init route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post("/init", json={"path": "proj"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["init"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['init']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:113: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:39,824 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 200 OK"
__________________________ test_gather_route_succeeds __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5622a0>

    @pytest.mark.medium
    def test_gather_route_succeeds(monkeypatch):
        """Test that gather route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post(
            "/gather", json={"goals": "g1", "constraints": "c1", "priority":
"high"}
        )
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["g1,c1,high"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['g1,c1,high']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:128: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:39,864 - httpx - INFO - HTTP Request: POST
http://testserver/gather "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/gather
"HTTP/1.1 200 OK"
_____________________ test_synthesize_and_status_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e5629c0>

    @pytest.mark.medium
    def test_synthesize_and_status_succeeds(monkeypatch):
        """Test that synthesize and status.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post("/synthesize", json={"target": "unit"})
>       assert resp.json() == {"messages": ["run:unit"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['run:unit']}
E
E         Differing items:
E         {'messages': ['run:SynthesisTarget.UNIT']} != {'messages':
['run:unit']}
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:140: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:39,915 - httpx - INFO - HTTP Request: POST
http://testserver/synthesize "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/synthesize
"HTTP/1.1 200 OK"
___________________________ test_spec_route_succeeds ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f112c00>

    @pytest.mark.medium
    def test_spec_route_succeeds(monkeypatch):
        """Test that spec route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post("/spec", json={"requirements_file":
"custom_reqs.md"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["spec:custom_reqs.md"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
...tom_reqs.md']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:155: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:39,964 - httpx - INFO - HTTP Request: POST
http://testserver/spec "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/spec
"HTTP/1.1 200 OK"
___________________________ test_test_route_succeeds ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12edbf560>

    @pytest.mark.medium
    def test_test_route_succeeds(monkeypatch):
        """Test that test route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post(
            "/test", json={"spec_file": "custom_specs.md", "output_dir":
"tests"}
        )
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["test:custom_specs.md"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
...om_specs.md']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:172: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,007 - httpx - INFO - HTTP Request: POST
http://testserver/test "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/test
"HTTP/1.1 200 OK"
___________________________ test_code_route_succeeds ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f87dca0>

    @pytest.mark.medium
    def test_code_route_succeeds(monkeypatch):
        """Test that code route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post("/code", json={"output_dir": "src"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["code"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['code']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:185: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,059 - httpx - INFO - HTTP Request: POST
http://testserver/code "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/code
"HTTP/1.1 200 OK"
__________________________ test_doctor_route_succeeds __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f87d9a0>

    @pytest.mark.medium
    def test_doctor_route_succeeds(monkeypatch):
        """Test that doctor route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post("/doctor", json={"path": "project", "fix": True})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["doctor:project:True"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
...roject:True']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:198: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,103 - httpx - INFO - HTTP Request: POST
http://testserver/doctor "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/doctor
"HTTP/1.1 200 OK"
________________________ test_edrr_cycle_route_succeeds ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13daa0080>

    @pytest.mark.medium
    def test_edrr_cycle_route_succeeds(monkeypatch):
        """Test that edrr cycle route.

        ReqID: N/A"""
        setup = _setup(monkeypatch)
        client = _get_testclient()(setup["agentapi"].app)
        resp = client.post(
            "/edrr-cycle", json={"prompt": "Improve code", "max_iterations": 5}
        )
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["edrr:Improve code"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
...mprove code']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api.py:213: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,149 - httpx - INFO - HTTP Request: POST
http://testserver/edrr-cycle "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/edrr-cycle
"HTTP/1.1 200 OK"
__________________ test_api_requires_authentication_succeeds ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12fb58c20>

    @pytest.mark.medium
    def test_api_requires_authentication_succeeds(monkeypatch):
        """Test that API endpoints require authentication when access control is
enabled.

        ReqID: N/A"""
        setup = _setup_with_auth(monkeypatch)
        client = setup["client"]
        resp = client.post("/init", json={"path": "proj"})
        assert resp.status_code == 401
        assert resp.json() == {"detail": "Unauthorized"}
        resp = client.post(
            "/init",
            json={"path": "proj"},
            headers={"Authorization": "Bearer invalid_token"},
        )
        assert resp.status_code == 401
        assert resp.json() == {"detail": "Unauthorized"}
        resp = client.post(
            "/init", json={"path": "proj"}, headers={"Authorization": "Bearer
test_token"}
        )
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["init"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['init']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api_security.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,369 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 401 Unauthorized"
2025-10-28 10:33:40,375 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 401 Unauthorized"
2025-10-28 10:33:40,382 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 200 OK"
__________________ test_api_authentication_disabled_succeeds ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16c975b80>

    @pytest.mark.medium
    def test_api_authentication_disabled_succeeds(monkeypatch):
        """Test that API endpoints don't require authentication when access
control is disabled.

        ReqID: N/A"""
        setup = _setup_with_auth(monkeypatch, access_token="")
        client = setup["client"]
        resp = client.post("/init", json={"path": "proj"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["init"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['init']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api_security.py:84: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,418 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 200 OK"
______________________ test_api_health_endpoint_succeeds _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16c9771d0>

    @pytest.mark.medium
    def test_api_health_endpoint_succeeds(monkeypatch):
        """Test the health endpoint.

        ReqID: N/A"""
        setup = _setup_with_auth(monkeypatch)
        client = setup["client"]
        resp = client.get("/health")
        assert resp.status_code == 401
        resp = client.get("/health", headers={"Authorization": "Bearer
test_token"})
        assert resp.status_code == 200
>       assert resp.json() == {"status": "ok"}
E       AssertionError: assert {'additional_...2092514038086} == {'status':
'ok'}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 2 more items:
E         {'additional_info': None, 'uptime': 0.02842092514038086}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_api_security.py:135: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,562 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 401 Unauthorized"
2025-10-28 10:33:40,569 - httpx - INFO - HTTP Request: GET
http://testserver/health "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 401 Unauthorized"
INFO     httpx:_client.py:1025 HTTP Request: GET http://testserver/health
"HTTP/1.1 200 OK"
____________ TestAgentCollaborationSystem.test_create_team_succeeds ____________

self =
<tests.integration.general.test_agent_collaboration_integration.TestAgentCollabo
rationSystem object at 0x11fd199d0>

    def test_create_team_succeeds(self):
        """Test creating a team of agents.

        ReqID: N/A"""
        collaboration_system = AgentCollaborationSystem()
        agent1 = MockAgent("agent1", ["planning", "design"])
        agent2 = MockAgent("agent2", ["coding", "review"])
        agent3 = MockAgent("agent3", ["testing", "qa"])
        collaboration_system.register_agent(agent1)
        collaboration_system.register_agent(agent2)
        collaboration_system.register_agent(agent3)
>       team = collaboration_system.create_team("team1", ["agent1", "agent2",
"agent3"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_collaboration_integration.py:92:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/agent_collaboration.py:422: in create_team
    team.assign_roles()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:663: in assign_roles
    return _manager(self).assign(role_mapping)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:384: in assign
    {role.value: _agent_name(agent) for role, agent in self.team.roles.items()},
                 ^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

agent =
<tests.integration.general.test_agent_collaboration_integration.MockAgent object
at 0x12f6b3950>

    def _agent_name(agent: SupportsTeamAgent | None) -> str | None:
>       return None if agent is None else agent.name
                                          ^^^^^^^^^^
E       AttributeError: 'MockAgent' object has no attribute 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:352: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,654 - devsynth.application.collaboration.agent_collaboration
- INFO - Registered agent agent1 with capabilities: ['planning', 'design']
2025-10-28 10:33:40,654 - devsynth.application.collaboration.agent_collaboration
- INFO - Registered agent agent2 with capabilities: ['coding', 'review']
2025-10-28 10:33:40,654 - devsynth.application.collaboration.agent_collaboration
- INFO - Registered agent agent3 with capabilities: ['testing', 'qa']
2025-10-28 10:33:40,654 - devsynth.domain.models.wsde_core - INFO - Added agent
agent to team AgentCollaborationTeam
2025-10-28 10:33:40,654 - devsynth.domain.models.wsde_core - INFO - Added agent
agent to team AgentCollaborationTeam
2025-10-28 10:33:40,654 - devsynth.domain.models.wsde_core - INFO - Added agent
agent to team AgentCollaborationTeam
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.collaboration.agent_collaboration:logging_setup.py:615
Registered agent agent1 with capabilities: ['planning', 'design']
INFO
devsynth.application.collaboration.agent_collaboration:logging_setup.py:615
Registered agent agent2 with capabilities: ['coding', 'review']
INFO
devsynth.application.collaboration.agent_collaboration:logging_setup.py:615
Registered agent agent3 with capabilities: ['testing', 'qa']
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent
to team AgentCollaborationTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent
to team AgentCollaborationTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent
to team AgentCollaborationTeam
_______ TestAgentCollaborationSystem.test_peer_review_workflow_succeeds ________

self =
<tests.integration.general.test_agent_collaboration_integration.TestAgentCollabo
rationSystem object at 0x11fd33080>

    def test_peer_review_workflow_succeeds(self):
        """Run a full peer review using workflow helpers.

        ReqID: N/A"""
        author = MockAgent("author")
        reviewer1 = MockAgent("r1")
        reviewer2 = MockAgent("r2")
        result = run_peer_review(
            work_product={"text": "demo"},
            author=author,
            reviewers=[reviewer1, reviewer2],
        )
>       assert result["status"] == "approved"
E       AssertionError: assert 'rejected' == 'approved'
E
E         - approved
E         + rejected

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agent_collaboration_integration.py:256: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:40,742 - devsynth.application.collaboration.peer_review - INFO
- Requesting revision from author: Unknown
2025-10-28 10:33:40,742 - devsynth.application.collaboration.peer_review -
WARNING - Could not get actual revision from author, using simulated revision
2025-10-28 10:33:40,742 - devsynth.application.collaboration.peer_review - INFO
- Requesting revision from author: Unknown
2025-10-28 10:33:40,743 - devsynth.application.collaboration.peer_review -
WARNING - Could not get actual revision from author, using simulated revision
2025-10-28 10:33:40,743 - devsynth.application.collaboration.peer_review - INFO
- Requesting revision from author: Unknown
2025-10-28 10:33:40,743 - devsynth.application.collaboration.peer_review -
WARNING - Could not get actual revision from author, using simulated revision
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.collaboration.peer_review:logging_setup.py:615
Requesting revision from author: Unknown
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615
Could not get actual revision from author, using simulated revision
INFO     devsynth.application.collaboration.peer_review:logging_setup.py:615
Requesting revision from author: Unknown
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615
Could not get actual revision from author, using simulated revision
INFO     devsynth.application.collaboration.peer_review:logging_setup.py:615
Requesting revision from author: Unknown
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615
Could not get actual revision from author, using simulated revision
______________________________ test_init_succeeds ______________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f8cef90>

    @pytest.mark.medium
    def test_init_succeeds(monkeypatch):
        """Test that init succeeds.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        bridge = agentapi.APIBridge()
        api = agentapi.AgentAPI(bridge)
        msgs = api.init(path="proj")
>       assert msgs == ["init"]
E       AssertionError: assert WorkflowRespo... progress=())) == ['init']
E
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi.py:45: AssertionError
_____________________________ test_gather_succeeds _____________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12e8d6de0>

    @pytest.mark.medium
    def test_gather_succeeds(monkeypatch):
        """Test that gather succeeds.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        bridge = agentapi.APIBridge()
        api = agentapi.AgentAPI(bridge)
>       msgs = api.gather(goals="g1", constraints="c1", priority="high")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi.py:57:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.interface.agentapi.AgentAPI object at 0x12efe5ac0>

    def gather(
        self,
        *,
        goals: str,
        constraints: str,
        priority: PriorityLevel = PriorityLevel.MEDIUM,
    ) -> WorkflowResponse:
        from devsynth.application.cli import gather_cmd

        if isinstance(self.bridge, APIBridge):
>           self.bridge._answers.extend([goals, constraints, priority.value])
                                                             ^^^^^^^^^^^^^^
E           AttributeError: 'str' object has no attribute 'value'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/agen
tapi.py:528: AttributeError
_____________________ test_synthesize_and_status_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f607800>

    @pytest.mark.medium
    def test_synthesize_and_status_succeeds(monkeypatch):
        """Test that synthesize and status.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        bridge = agentapi.APIBridge()
        api = agentapi.AgentAPI(bridge)
>       msgs = api.synthesize(target="unit")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi.py:70:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.interface.agentapi.AgentAPI object at 0x12e0ae930>

    def synthesize(self, *, target: SynthesisTarget | None = None) ->
WorkflowResponse:
        from devsynth.application.cli import run_pipeline_cmd

        self._clear_messages()
>       run_pipeline_cmd(target=target.value if target else None,
bridge=self.bridge)
                                ^^^^^^^^^^^^
E       AttributeError: 'str' object has no attribute 'value'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/agen
tapi.py:538: AttributeError
___________________________ test_init_route_succeeds ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f6064e0>

    @pytest.mark.medium
    def test_init_route_succeeds(monkeypatch):
        """Test that init route succeeds.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        client = _get_testclient()(agentapi.app)
        resp = client.post("/init", json={"path": "proj"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["init"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['init']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi_routes.py:61: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:41,004 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 200 OK"
__________________________ test_gather_route_succeeds __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f71e900>

    @pytest.mark.medium
    def test_gather_route_succeeds(monkeypatch):
        """Test that gather route succeeds.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        client = _get_testclient()(agentapi.app)
        resp = client.post(
            "/gather", json={"goals": "g1", "constraints": "c1", "priority":
"high"}
        )
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["g1,c1,high"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['g1,c1,high']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi_routes.py:76: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:41,045 - httpx - INFO - HTTP Request: POST
http://testserver/gather "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/gather
"HTTP/1.1 200 OK"
_____________________ test_synthesize_and_status_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f71f6b0>

    @pytest.mark.medium
    def test_synthesize_and_status_succeeds(monkeypatch):
        """Test that synthesize and status succeeds.

        ReqID: N/A"""
        cli_stub, agentapi = _setup(monkeypatch)
        client = _get_testclient()(agentapi.app)
        resp = client.post("/synthesize", json={"target": "unit"})
>       assert resp.json() == {"messages": ["run:unit"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['run:unit']}
E
E         Differing items:
E         {'messages': ['run:SynthesisTarget.UNIT']} != {'messages':
['run:unit']}
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_agentapi_routes.py:88: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:41,115 - httpx - INFO - HTTP Request: POST
http://testserver/synthesize "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/synthesize
"HTTP/1.1 200 OK"
_______ TestCLIWebUIAgentAPIPipeline.test_test_command_pipeline_succeeds _______

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284bcdd0>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpq39ysobo'
api_client = <starlette.testclient.TestClient object at 0x12fbb2330>
cli_bridge = <devsynth.interface.cli.CLIUXBridge object at 0x12fbb0f50>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9eb350>

    def test_test_command_pipeline_succeeds(
        self, temp_project_dir, api_client, cli_bridge, monkeypatch
    ):
        """Test the test command pipeline from CLI through AgentAPI.

        ReqID: N/A"""
        # Test the CLI part of the pipeline directly
        spec_file = os.path.join(temp_project_dir, "specs.md")
        with open(spec_file, "w") as f:
            f.write("# Specifications\n\n1. Spec 1\n2. Spec 2\n")
        with patch("devsynth.application.cli.cli_commands.test_cmd") as
mock_test_cmd:
            mock_test_cmd.return_value = {
                "status": "success",
                "message": "Tests generated",
            }
            # Call the CLI command directly
            from devsynth.application.cli import test_cmd

            result = test_cmd(spec_file=spec_file, bridge=cli_bridge)
>           assert result == mock_test_cmd.return_value
E           AssertionError: assert None == {'message': 'Tests generated',
'status': 'success'}
E            +  where {'message': 'Tests generated', 'status': 'success'} =
<MagicMock name='test_cmd' id='5095763664'>.return_value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:176: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:41,179 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
  Finishing ━━━━━━━━╸                          1/4  25% Complete 0:00:00 0:00:00
2025-10-28 10:33:41,183 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'51435149-a8ff-450b-94c6-030511bce56e'}
2025-10-28 10:33:41,183 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'51435149-a8ff-450b-94c6-030511bce56e'}
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  Error: {'success': False, 'message': 'Command failed', 'workflow_id':       │
│  '51435149-a8ff-450b-94c6-030511bce56e'}                                     │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'51435149-a8ff-450b-94c6-030511bce56e'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'51435149-a8ff-450b-94c6-030511bce56e'}
_______ TestCLIWebUIAgentAPIPipeline.test_code_command_pipeline_succeeds _______

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284bd310>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpkr0442nj'
api_client = <starlette.testclient.TestClient object at 0x12ff962a0>
cli_bridge = <devsynth.interface.cli.CLIUXBridge object at 0x12ff97260>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f9eb050>

    def test_code_command_pipeline_succeeds(
        self, temp_project_dir, api_client, cli_bridge, monkeypatch
    ):
        """Test the code command pipeline from CLI through AgentAPI.

        ReqID: N/A"""
        # Test the CLI part of the pipeline directly
        with patch("devsynth.application.cli.cli_commands.code_cmd") as
mock_code_cmd:
            mock_code_cmd.return_value = {
                "status": "success",
                "message": "Code generated",
            }
            # Call the CLI command directly
            from devsynth.application.cli import code_cmd

            result = code_cmd(bridge=cli_bridge)
>           assert result == mock_code_cmd.return_value
E           AssertionError: assert None == {'message': 'Code generated',
'status': 'success'}
E            +  where {'message': 'Code generated', 'status': 'success'} =
<MagicMock name='code_cmd' id='5099834640'>.return_value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:197: AssertionError
----------------------------- Captured stdout call -----------------------------
No tests found in &#x27;tests&#x27; directory.
             ┌────────────────────| DevSynth |────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘





























































                             ┌────────────────────| DevSynth
|────────────────────┐

             │                                                    │

             │ Run 'devsynth test' to generate tests?             │

             │                                                    │

             │ (*) Yes                                          ^ │

             │ ( ) No                                           v │

             │              <    Ok    > <  Cancel  >             │

             │                                                    │

             │                                                    │

             └────────────────────────────────────────────────────┘






























































File &#x27;specs.md&#x27; does not exist
            ┌─────────────────────| DevSynth |─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘





























































                           ┌─────────────────────| DevSynth
|─────────────────────┐

            │                                                      │

            │ Run 'devsynth spec' to generate specs.md?            │

            │                                                      │

            │ (*) Yes                                            ^ │

            │ ( ) No                                             v │

            │               <    Ok    > <  Cancel  >              │

            │                                                      │

            │                                                      │

            └──────────────────────────────────────────────────────┘






























































File &#x27;requirements.md&#x27; does not exist
             ┌───────────────────| DevSynth |────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘





























































                             ┌───────────────────| DevSynth
|────────────────────┐

             │                                                   │

             │ Create empty 'requirements.md' file?              │

             │                                                   │

             │ ( ) Yes                                         ^ │

             │ (*) No                                          v │

             │             <    Ok    > <  Cancel  >             │

             │                                                   │

             │                                                   │

             └───────────────────────────────────────────────────┘






























































2025-10-28 10:33:41,688 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
  Finishing ━━━━━━━━╸                          1/4  25% Complete 0:00:00 0:00:00
2025-10-28 10:33:41,691 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'58533745-61af-401d-950a-764ff1839fa0'}
2025-10-28 10:33:41,692 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'58533745-61af-401d-950a-764ff1839fa0'}
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  Error: {'success': False, 'message': 'Command failed', 'workflow_id':       │
│  '58533745-61af-401d-950a-764ff1839fa0'}                                     │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
Generating implementation code from tests...
2025-10-28 10:33:41,695 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
2025-10-28 10:33:41,696 - cli_commands - ERROR - Command error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'a4afe91c-f863-4ecc-8354-c70d457f41cd'}
2025-10-28 10:33:41,696 - devsynth.interface.cli - ERROR - Handling error:
{'success': False, 'message': 'Command failed', 'workflow_id':
'a4afe91c-f863-4ecc-8354-c70d457f41cd'}
╭─ Error ──────────────────────────────────────────────────────────────────────╮
│                                                                              │
│  Error: {'success': False, 'message': 'Command failed', 'workflow_id':       │
│  'a4afe91c-f863-4ecc-8354-c70d457f41cd'}                                     │
│                                                                              │
│  Suggestions:                                                                │
│  1. For more information, run the doctor command to diagnose issues.         │
│     Documentation:                                                           │
│  https://devsynth.readthedocs.io/en/latest/user_guides/troubleshooting.html  │
│     Example: devsynth doctor                                                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'58533745-61af-401d-950a-764ff1839fa0'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'58533745-61af-401d-950a-764ff1839fa0'}
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
ERROR    cli_commands:logging_setup.py:615 Command error: {'success': False,
'message': 'Command failed', 'workflow_id':
'a4afe91c-f863-4ecc-8354-c70d457f41cd'}
ERROR    devsynth.interface.cli:logging_setup.py:615 Handling error: {'success':
False, 'message': 'Command failed', 'workflow_id':
'a4afe91c-f863-4ecc-8354-c70d457f41cd'}
__ TestCLIWebUIAgentAPIPipeline.test_error_handling_in_pipeline_raises_error ___

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1280bb260>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpzoar94td'
cli_bridge = <devsynth.interface.cli.CLIUXBridge object at 0x16cfaa810>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f656420>

    def test_error_handling_in_pipeline_raises_error(
        self, temp_project_dir, cli_bridge, monkeypatch
    ):
        """Test error handling in the CLI/WebUI/AgentAPI pipeline.

        ReqID: N/A"""
        # Test error handling in the CLI part of the pipeline directly
        with patch("devsynth.application.cli.cli_commands.init_cmd") as
mock_init_cmd:
            mock_init_cmd.side_effect = ValueError("Test error")
            # Call the CLI command directly and expect an exception
            from devsynth.application.cli import init_cmd

            try:
                init_cmd(bridge=cli_bridge)
>               assert False, "Expected ValueError was not raised"
E               AssertionError: Expected ValueError was not raised
E               assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:241: AssertionError
----------------------------- Captured stdout call -----------------------------
Project root directory [/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_error_handling_in_pipelin0/project]/private
                                                                               /
var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/t
                                                                               e
st_error_handling_in_pipelin0/project
Project root directory [/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/
                                                                               T
/pytest-of-caitlyn/pytest-1440/test_error_handling_in_pipelin0/project]/private
                                                                               /
var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1440/t
                                                                               e
st_error_handling_in_pipelin0/project
Primary language [python]python                               Primary language
[python]python
Project goals             Project goals
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘

























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Memory backend                            │

                  │                                           │

                  │ (*) memory                              ^ │

                  │ ( ) file                                  │

                  │ ( ) kuzu                                  │

                  │ ( ) chromadb                            v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘


























































Memory backend [memory]memory                             Memory backend
[memory]memory
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘





























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Enable offline mode                       │

                  │                                           │

                  │ ( ) Yes                                 ^ │

                  │ (*) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘






























































2025-10-28 10:33:42,084 - devsynth.adapters.orchestration.langgraph_adapter -
INFO - Error executing workflow: object() takes no arguments
⠋ Generating project files ━━━━━━━━━╸          1/2  50% Complete 0:00:00 -:--:--
2025-10-28 10:33:42,085 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
Initialization complete
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615
Error executing workflow: object() takes no arguments
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
______ TestCLIWebUIAgentAPIPipeline.test_webui_command_pipeline_succeeds _______

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284bdfd0>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpo7s3z90i'
api_client = <starlette.testclient.TestClient object at 0x12f1a70b0>
cli_bridge = <devsynth.interface.cli.CLIUXBridge object at 0x12f1a75f0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f635700>

    def test_webui_command_pipeline_succeeds(
        self, temp_project_dir, api_client, cli_bridge, monkeypatch
    ):
        """Test the WebUI command pipeline from CLI through AgentAPI.

        ReqID: N/A"""
        # Test the CLI part of the pipeline directly
        with patch("devsynth.application.cli.cli_commands.webui_cmd") as
mock_webui_cmd:
            mock_webui_cmd.return_value = None  # webui_cmd doesn't return a
value

            # Mock the run function to avoid actually launching the WebUI
>           with patch("devsynth.interface.webui.run") as mock_run:
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:257:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16c04ba40>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> does not have the attribute 'run'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
___ TestCLIWebUIAgentAPIPipeline.test_webui_command_error_handling_succeeds ____

self =
<tests.integration.general.test_cli_webui_agentapi_pipeline.TestCLIWebUIAgentAPI
Pipeline object at 0x1284be4e0>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp6djq8tg7'
cli_bridge = <devsynth.interface.cli.CLIUXBridge object at 0x16cac9a90>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f634170>

    def test_webui_command_error_handling_succeeds(
        self, temp_project_dir, cli_bridge, monkeypatch
    ):
        """Test error handling in the WebUI command pipeline.

        ReqID: N/A"""
        # Test error handling in the CLI part of the pipeline directly
>       with patch(
            "devsynth.interface.webui.run",
            side_effect=ImportError('No module named "streamlit"'),
        ) as mock_run:

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_cli_webui_agentapi_pipeline.py:276:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16caca240>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.interface.webui' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> does not have the attribute 'run'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_____________ test_dynamic_role_reassignment_integration_succeeds ______________

    @pytest.mark.medium
    def test_dynamic_role_reassignment_integration_succeeds():
        team = CollaborativeWSDETeam("team")
        doc = SimpleAgent("doc", ["documentation"])
        dev = SimpleAgent("dev", ["python"])
        team.add_agents([doc, dev])
        task = {"type": "documentation", "description": "Write API docs"}
        team.process_task(task)
>       assert team.get_primus() == doc
E       assert
<tests.integration.general.test_collaborative_decision_making.SimpleAgent object
at 0x16d5ac440> ==
<tests.integration.general.test_collaborative_decision_making.SimpleAgent object
at 0x16c9de5a0>
E        +  where
<tests.integration.general.test_collaborative_decision_making.SimpleAgent object
at 0x16d5ac440> = get_primus()
E        +    where get_primus =
<devsynth.application.collaboration.collaborative_wsde_team.CollaborativeWSDETea
m object at 0x16d5ac290>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_collaborative_decision_making.py:24: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,245 - devsynth.domain.models.wsde_core - INFO - Added agent
doc to team team
2025-10-28 10:33:42,245 - devsynth.domain.models.wsde_core - INFO - Added agent
dev to team team
2025-10-28 10:33:42,245 - devsynth.domain.models.wsde_core - INFO - Processing
task c92c2de0-eb67-4840-9859-54a880cac0be: Untitled task
2025-10-28 10:33:42,245 - devsynth.domain.models.wsde_roles - INFO - Selected
doc as primus based on expertise
2025-10-28 10:33:42,245 - devsynth.domain.models.wsde_roles - INFO - Selected
dev as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc
to team team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent dev
to team team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Processing task
c92c2de0-eb67-4840-9859-54a880cac0be: Untitled task
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected dev as
primus based on expertise
___________________ test_consensus_vote_tie_breaker_succeeds ___________________

    @pytest.mark.medium
    def test_consensus_vote_tie_breaker_succeeds():
        team = CollaborativeWSDETeam("team")
        a1 = SimpleAgent("a1", ["a"])
        a2 = SimpleAgent("a2", ["b"])
        team.add_agents([a1, a2])
        task = {
            "id": "vote1",
            "type": "decision_task",
            "description": "Choose option",
            "options": ["a", "b"],
            "voting_method": "majority",
        }
        team.select_primus_by_expertise({})
        result = team.vote_on_critical_decision(task)
>       assert result["status"] == "completed"
E       AssertionError: assert 'tied' == 'completed'
E
E         - completed
E         + tied

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_collaborative_decision_making.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,257 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team team
2025-10-28 10:33:42,257 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team team
2025-10-28 10:33:42,257 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
________________________ test_majority_voting_succeeds _________________________

    @pytest.mark.medium
    def test_majority_voting_succeeds():
        """Test that majority voting succeeds.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TestCollaborativeVotingTeam")
        team.add_agents(
            [VotingAgent("a1", "o1"), VotingAgent("a2", "o2"), VotingAgent("a3",
"o2")]
        )
        decision = {
            "type": "critical_decision",
            "is_critical": True,
            "options": ["o1", "o2"],
        }
        result = team.collaborative_decision(decision)
>       assert result["result"] in {"o1", "o2"}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: unhashable type: 'dict'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_collaborative_voting.py:81: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,278 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestCollaborativeVotingTeam
2025-10-28 10:33:42,278 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestCollaborativeVotingTeam
2025-10-28 10:33:42,278 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team TestCollaborativeVotingTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestCollaborativeVotingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestCollaborativeVotingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestCollaborativeVotingTeam
________________________ test_weighted_voting_succeeds _________________________

filters = {'metadata.task_id': 'ebb4d239-e3bf-41c9-9597-12b249da7de7', 'type':
'opinion'}

    def ensure_message_filter(
        filters: Optional[MessageFilterInput],
    ) -> Optional[MessageFilter]:
        if filters is None:
            return None
        if isinstance(filters, MessageFilter):
            return filters
        if isinstance(filters, MessageFilterLike):
            return filters.to_message_filter()
        if isinstance(filters, MappingABC):
            try:
                base: Dict[str, Any] = dict(filters)
                mt_value: Any = base.get("message_type")
                if isinstance(mt_value, Enum):
                    base["message_type"] = str(mt_value.value)
                elif mt_value is not None and not isinstance(mt_value, str):
                    base["message_type"] = str(mt_value)
>               return MessageFilter.from_dict(base)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:693:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'devsynth.application.collaboration.dto.MessageFilter'>
data = {'metadata.task_id': 'ebb4d239-e3bf-41c9-9597-12b249da7de7', 'type':
'opinion'}

    @classmethod
    def from_dict(cls: Type[T], data: Mapping[str, Any]) -> T:
        if not isinstance(data, MappingABC):
            raise TypeError(
                f"Expected mapping for {cls.__name__}, received {type(data)!r}"
            )

        prepared: Dict[str, Any] = {}
        for key, value in data.items():
            if key == "dto_type":
                continue
            prepared[str(key)] = value

        dataclass_type = cast(Type[Any], cls)
        field_map = {f.name: f for f in fields(dataclass_type)}
        kwargs: Dict[str, Any] = {}
        extras: Dict[str, Any] = {}

        for name, value in prepared.items():
            field_info = field_map.get(name)
            if field_info is None:
                extras[name] = _deserialize_arbitrary(value)
                continue
            kwargs[name] = _coerce_value(field_info.type, value)

        extra_field = cls.extra_field_name
        if extras:
            if extra_field and extra_field in field_map:
                current = kwargs.get(extra_field)
                if current is None:
                    current_map: Dict[str, Any] = {}
                elif isinstance(current, MappingABC):
                    current_map = dict(current)
                else:
                    current_map = {}
                current_map.update(extras)
                kwargs[extra_field] = _normalize_mapping(current_map)
            else:
>               raise ValueError(
                    f"Unexpected extra fields for {cls.__name__}:
{sorted(extras.keys())}"
                )
E               ValueError: Unexpected extra fields for MessageFilter:
['metadata.task_id', 'type']

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:256: ValueError

The above exception was the direct cause of the following exception:

    @pytest.mark.medium
    def test_weighted_voting_succeeds():
        """Test that weighted voting succeeds.

        ReqID: N/A"""
        team = CollaborativeWSDETeam(name="TestCollaborativeVotingTeam")
        team.add_agents(
            [
                VotingAgent("e1", "o1", expertise=["ml"], level="expert"),
                VotingAgent("e2", "o2", expertise=["ml"], level="novice"),
            ]
        )
        decision = {
            "type": "critical_decision",
            "is_critical": True,
            "domain": "ml",
            "options": ["o1", "o2"],
        }
>       result = team.collaborative_decision(decision)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_collaborative_voting.py:102:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/collaborative_wsde_team.py:106: in collaborative_decision
    consensus_outcome = self.build_consensus(task)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:60: in build_consensus
    return self._build_consensus_inner(task, phase)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:78: in _build_consensus_inner
    agent_opinions = self._collect_agent_opinion_records(task,
keywords=keywords)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:153: in _collect_agent_opinion_records
    record = self._build_agent_opinion_record(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:171: in _build_agent_opinion_record
    messages = self.get_messages(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_utils.py:166: in get_messages
    return protocol.get_messages(agent, filters)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/message_protocol.py:305: in get_messages
    filter_obj = ensure_message_filter(filters) if filters is not None else None
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

filters = {'metadata.task_id': 'ebb4d239-e3bf-41c9-9597-12b249da7de7', 'type':
'opinion'}

    def ensure_message_filter(
        filters: Optional[MessageFilterInput],
    ) -> Optional[MessageFilter]:
        if filters is None:
            return None
        if isinstance(filters, MessageFilter):
            return filters
        if isinstance(filters, MessageFilterLike):
            return filters.to_message_filter()
        if isinstance(filters, MappingABC):
            try:
                base: Dict[str, Any] = dict(filters)
                mt_value: Any = base.get("message_type")
                if isinstance(mt_value, Enum):
                    base["message_type"] = str(mt_value.value)
                elif mt_value is not None and not isinstance(mt_value, str):
                    base["message_type"] = str(mt_value)
                return MessageFilter.from_dict(base)
            except Exception as exc:  # pragma: no cover - defensive
>               raise TypeError("Invalid mapping for MessageFilter") from exc
E               TypeError: Invalid mapping for MessageFilter

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:695: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,289 - devsynth.domain.models.wsde_core - INFO - Added agent
e1 to team TestCollaborativeVotingTeam
2025-10-28 10:33:42,289 - devsynth.domain.models.wsde_core - INFO - Added agent
e2 to team TestCollaborativeVotingTeam
2025-10-28 10:33:42,289 - devsynth.domain.models.wsde_core - INFO - Building
consensus for task ebb4d239-e3bf-41c9-9597-12b249da7de7: Untitled
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent e1 to
team TestCollaborativeVotingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent e2 to
team TestCollaborativeVotingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Building
consensus for task ebb4d239-e3bf-41c9-9597-12b249da7de7: Untitled
______________________ test_voting_result_syncs_to_memory ______________________

filters = {'metadata.task_id': 'vote1', 'type': 'opinion'}

    def ensure_message_filter(
        filters: Optional[MessageFilterInput],
    ) -> Optional[MessageFilter]:
        if filters is None:
            return None
        if isinstance(filters, MessageFilter):
            return filters
        if isinstance(filters, MessageFilterLike):
            return filters.to_message_filter()
        if isinstance(filters, MappingABC):
            try:
                base: Dict[str, Any] = dict(filters)
                mt_value: Any = base.get("message_type")
                if isinstance(mt_value, Enum):
                    base["message_type"] = str(mt_value.value)
                elif mt_value is not None and not isinstance(mt_value, str):
                    base["message_type"] = str(mt_value)
>               return MessageFilter.from_dict(base)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:693:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'devsynth.application.collaboration.dto.MessageFilter'>
data = {'metadata.task_id': 'vote1', 'type': 'opinion'}

    @classmethod
    def from_dict(cls: Type[T], data: Mapping[str, Any]) -> T:
        if not isinstance(data, MappingABC):
            raise TypeError(
                f"Expected mapping for {cls.__name__}, received {type(data)!r}"
            )

        prepared: Dict[str, Any] = {}
        for key, value in data.items():
            if key == "dto_type":
                continue
            prepared[str(key)] = value

        dataclass_type = cast(Type[Any], cls)
        field_map = {f.name: f for f in fields(dataclass_type)}
        kwargs: Dict[str, Any] = {}
        extras: Dict[str, Any] = {}

        for name, value in prepared.items():
            field_info = field_map.get(name)
            if field_info is None:
                extras[name] = _deserialize_arbitrary(value)
                continue
            kwargs[name] = _coerce_value(field_info.type, value)

        extra_field = cls.extra_field_name
        if extras:
            if extra_field and extra_field in field_map:
                current = kwargs.get(extra_field)
                if current is None:
                    current_map: Dict[str, Any] = {}
                elif isinstance(current, MappingABC):
                    current_map = dict(current)
                else:
                    current_map = {}
                current_map.update(extras)
                kwargs[extra_field] = _normalize_mapping(current_map)
            else:
>               raise ValueError(
                    f"Unexpected extra fields for {cls.__name__}:
{sorted(extras.keys())}"
                )
E               ValueError: Unexpected extra fields for MessageFilter:
['metadata.task_id', 'type']

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:256: ValueError

The above exception was the direct cause of the following exception:

    @pytest.mark.medium
    def test_voting_result_syncs_to_memory():
        """Voting results should be stored across memory stores."""
        adapters = {
            "tinydb": TinyDBMemoryAdapter(),
            "secondary": TinyDBMemoryAdapter(),
        }
        manager = MemoryManager(adapters=adapters)
        team = CollaborativeWSDETeam(name="TestTeam", memory_manager=manager)
        team.add_agents([VotingAgent("a1", "o1"), VotingAgent("a2", "o2")])
        decision = {
            "id": "vote1",
            "type": "critical_decision",
            "is_critical": True,
            "options": ["o1", "o2"],
        }
>       team.collaborative_decision(decision)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_collaborative_voting.py:122:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/collaborative_wsde_team.py:106: in collaborative_decision
    consensus_outcome = self.build_consensus(task)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:60: in build_consensus
    return self._build_consensus_inner(task, phase)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:78: in _build_consensus_inner
    agent_opinions = self._collect_agent_opinion_records(task,
keywords=keywords)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:153: in _collect_agent_opinion_records
    record = self._build_agent_opinion_record(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:171: in _build_agent_opinion_record
    messages = self.get_messages(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_utils.py:166: in get_messages
    return protocol.get_messages(agent, filters)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/message_protocol.py:305: in get_messages
    filter_obj = ensure_message_filter(filters) if filters is not None else None
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

filters = {'metadata.task_id': 'vote1', 'type': 'opinion'}

    def ensure_message_filter(
        filters: Optional[MessageFilterInput],
    ) -> Optional[MessageFilter]:
        if filters is None:
            return None
        if isinstance(filters, MessageFilter):
            return filters
        if isinstance(filters, MessageFilterLike):
            return filters.to_message_filter()
        if isinstance(filters, MappingABC):
            try:
                base: Dict[str, Any] = dict(filters)
                mt_value: Any = base.get("message_type")
                if isinstance(mt_value, Enum):
                    base["message_type"] = str(mt_value.value)
                elif mt_value is not None and not isinstance(mt_value, str):
                    base["message_type"] = str(mt_value)
                return MessageFilter.from_dict(base)
            except Exception as exc:  # pragma: no cover - defensive
>               raise TypeError("Invalid mapping for MessageFilter") from exc
E               TypeError: Invalid mapping for MessageFilter

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:695: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,387 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:42,387 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:42,387 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb, secondary
2025-10-28 10:33:42,387 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:33:42,387 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team TestTeam
2025-10-28 10:33:42,387 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team TestTeam
2025-10-28 10:33:42,388 - devsynth.domain.models.wsde_core - INFO - Building
consensus for task vote1: Untitled
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb, secondary
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Building
consensus for task vote1: Untitled
__ TestComplexWorkflow.test_complex_workflow_with_inconsistent_state_succeeds __

self = <tests.integration.general.test_complex_workflow.TestComplexWorkflow
object at 0x1284d0080>
temp_project_dir =
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp3ljoh4c1'
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d5901a0>

    @pytest.mark.medium
    def test_complex_workflow_with_inconsistent_state_succeeds(
        self, temp_project_dir, monkeypatch
    ):
        """Test a complex workflow with an inconsistent project state.

        This test simulates a complex workflow with an inconsistent project
state:
        1. Initialize a new project
        2. Create requirements
        3. Create specifications that only partially cover the requirements
        4. Create code that only partially implements the specifications
        5. Analyze the project state (should detect inconsistencies)
        6. Use the adaptive workflow manager to determine the optimal workflow
        7. Execute the workflow
        8. Verify that the workflow execution improves the project state

        ReqID: N/A"""
        original_dir = os.getcwd()
        os.chdir(temp_project_dir)
        try:
            init_cmd(
                root=temp_project_dir,
                language="python",
                goals="",
                memory_backend="memory",
                auto_confirm=True,
            )
            assert os.path.exists(os.path.join(temp_project_dir, ".devsynth"))
            requirements_dir = os.path.join(temp_project_dir, "docs")
            os.makedirs(requirements_dir, exist_ok=True)
            requirements_content = """
            # Data Processing API Requirements

            ## Data Ingestion
            1. The system shall accept CSV file uploads
            2. The system shall accept JSON file uploads
            3. The system shall validate file formats before processing
            4. The system shall handle files up to 100MB in size

            ## Data Processing
            1. The system shall parse CSV files into structured data
            2. The system shall parse JSON files into structured data
            3. The system shall transform data according to user-defined rules
            4. The system shall aggregate data based on specified criteria

            ## Data Export
            1. The system shall export processed data as CSV
            2. The system shall export processed data as JSON
            3. The system shall provide a download link for exported data
            4. The system shall allow scheduling of regular exports
            """
            with open(os.path.join(requirements_dir, "requirements.md"), "w") as
f:
                f.write(requirements_content)
            specs_content = """
            # Data Processing API Specifications

            ## Data Ingestion API

            ### CSV Upload Endpoint
            - **Endpoint**: POST /api/upload/csv
            - **Description**: Uploads a CSV file for processing
            - **Request**:
              - Content-Type: multipart/form-data
              - Body: CSV file
            - **Response**:
              - 200 OK: File uploaded successfully
              - 400 Bad Request: Invalid file format
              - 413 Payload Too Large: File exceeds size limit

            ### JSON Upload Endpoint
            - **Endpoint**: POST /api/upload/json
            - **Description**: Uploads a JSON file for processing
            - **Request**:
              - Content-Type: multipart/form-data
              - Body: JSON file
            - **Response**:
              - 200 OK: File uploaded successfully
              - 400 Bad Request: Invalid file format
              - 413 Payload Too Large: File exceeds size limit

            ## Data Processing API

            ### Process CSV Endpoint
            - **Endpoint**: POST /api/process/csv
            - **Description**: Processes a previously uploaded CSV file
            - **Request**:
              - Content-Type: application/json
              - Body: Processing parameters
            - **Response**:
              - 200 OK: File processed successfully
              - 404 Not Found: File not found
              - 500 Internal Server Error: Processing error
            """
            with open(os.path.join(temp_project_dir, "specs.md"), "w") as f:
                f.write(specs_content)
            src_dir = os.path.join(temp_project_dir, "src")
            os.makedirs(src_dir, exist_ok=True)
            data_ingestion_code = """
            class DataIngestion:
                def __init__(self, max_file_size=104857600):  # 100MB in bytes
                    self.max_file_size = max_file_size

                def upload_csv(self, file):
                    # Check file size
                    if len(file.read()) > self.max_file_size:
                        return {"status": "error", "message": "File exceeds size
limit"}

                    # Reset file pointer
                    file.seek(0)

                    # Validate CSV format
                    try:
                        # Implementation for CSV validation
                        pass
                    except Exception as e:
                        return {"status": "error", "message": f"Invalid CSV
format: {str(e)}"}

                    # Store the file
                    # Implementation for file storage

                    return {"status": "success", "message": "CSV file uploaded
successfully"}

                def upload_json(self, file):
                    # Check file size
                    if len(file.read()) > self.max_file_size:
                        return {"status": "error", "message": "File exceeds size
limit"}

                    # Reset file pointer
                    file.seek(0)

                    # Validate JSON format
                    try:
                        # Implementation for JSON validation
                        pass
                    except Exception as e:
                        return {"status": "error", "message": f"Invalid JSON
format: {str(e)}"}

                    # Store the file
                    # Implementation for file storage

                    return {"status": "success", "message": "JSON file uploaded
successfully"}
            """
            with open(os.path.join(src_dir, "data_ingestion.py"), "w") as f:
                f.write(data_ingestion_code)
            tests_dir = os.path.join(temp_project_dir, "tests")
            os.makedirs(tests_dir, exist_ok=True)
            test_data_ingestion_code = """
            import unittest
            from src.data_ingestion import DataIngestion

            class TestDataIngestion(unittest.TestCase):
                def setUp(self):
                    self.data_ingestion = DataIngestion()

                def test_upload_csv_file_too_large(self):
                    # Mock a file that's too large
                    class MockFile:
                        def read(self):
                            return b'x' * (104857600 + 1)  # 100MB + 1 byte

                        def seek(self, position):
                            pass

                    result = self.data_ingestion.upload_csv(MockFile())
                    self.assertEqual(result["status"], "error")
                    self.assertEqual(result["message"], "File exceeds size
limit")

            if __name__ == '__main__':
                unittest.main()
            """
            with open(os.path.join(tests_dir, "test_data_ingestion.py"), "w") as
f:
                f.write(test_data_ingestion_code)
            analyzer = ProjectStateAnalyzer(temp_project_dir)
            initial_report = analyzer.analyze()
>           assert initial_report["requirements_count"] > 0
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'requirements_count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_complex_workflow.py:217: KeyError
----------------------------- Captured stdout call -----------------------------
⠋ Generating project files ━━━━━━━━━╸          1/2  50% Complete 0:00:00 -:--:--
2025-10-28 10:33:42,494 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
Initialization complete
2025-10-28 10:33:42,497 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp3ljoh4c1
2025-10-28 10:33:42,497 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 5
files
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Could not
confidently infer architecture
2025-10-28 10:33:42,498 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:42,500 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting specifications from specs.md: 'NoneType' object has no attribute
'strip'
2025-10-28 10:33:42,500 -
devsynth.application.code_analysis.project_state_analyzer - INFO -
Requirements-specification alignment score: 0.00
2025-10-28 10:33:42,500 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:42,500 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting specifications from specs.md: 'NoneType' object has no attribute
'strip'
2025-10-28 10:33:42,501 -
devsynth.application.code_analysis.project_state_analyzer - INFO -
Specification-code implementation score: 0.00
2025-10-28 10:33:42,501 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:42,501 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp3ljoh4c1
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 5 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Could not confidently infer architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting specifications from specs.md: 'NoneType' object has no
attribute 'strip'
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Requirements-specification alignment score: 0.00
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting specifications from specs.md: 'NoneType' object has no
attribute 'strip'
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Specification-code implementation score: 0.00
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
_
TestComprehensiveWorkflow.test_project_state_analyzer_with_external_codebase_suc
ceeds _

self =
<tests.integration.general.test_comprehensive_workflow.TestComprehensiveWorkflow
object at 0x1284d0d10>
sample_project = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpip_kq88g'

    @pytest.mark.medium
    def test_project_state_analyzer_with_external_codebase_succeeds(
        self, sample_project
    ):
        """Test that ProjectStateAnalyzer can analyze an external codebase.

        ReqID: N/A"""
        analyzer = ProjectStateAnalyzer(sample_project)
        report = analyzer.analyze()
>       assert "project_path" in report
E       AssertionError: assert 'project_path' in {'architecture': {'components':
[{'name': 'user', 'path': 'src/sample_app/models/user.py', 'type': 'Model'},
{'name': ...': 'Controller'}], 'confidence': 0.9, 'type': 'MVC'}, 'code_count':
3, 'file_count': 5, 'health_score': 0.0, ...}, ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_comprehensive_workflow.py:172: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpip_kq88g
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 5
files
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferred
architecture: MVC (confidence: 0.90)
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:42,537 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpip_kq88g
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 5 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferred architecture: MVC (confidence: 0.90)
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
_________ TestComprehensiveWorkflow.test_end_to_end_workflow_succeeds __________

self =
<tests.integration.general.test_comprehensive_workflow.TestComprehensiveWorkflow
object at 0x1284d11c0>
sample_project = '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5'

    @pytest.mark.medium
    def test_end_to_end_workflow_succeeds(self, sample_project):
        """Test an end-to-end workflow from requirements to code.

        ReqID: N/A"""
        project_analyzer = ProjectStateAnalyzer(sample_project)
        project_state = project_analyzer.analyze()
        code_analyzer = SelfAnalyzer(sample_project)
        code_analysis = code_analyzer.analyze()
        workflow_manager = RefactorWorkflowManager()
>       workflow = workflow_manager.determine_optimal_workflow(project_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_comprehensive_workflow.py:222:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.orchestration.refactor_workflow.RefactorWorkflowManager
object at 0x13db6c770>
project_state = {'architecture': {'components': [{'name': 'user', 'path':
'src/sample_app/models/user.py', 'type': 'Model'}, {'name': ...':
'Controller'}], 'confidence': 0.9, 'type': 'MVC'}, 'code_count': 3,
'file_count': 5, 'health_score': 0.0, ...}, ...}

    def determine_optimal_workflow(self, project_state: Dict[str, Any]) -> str:
        """
        Determine the optimal workflow based on the project state.

        Args:
            project_state: The current state of the project

        Returns:
            The name of the optimal workflow
        """
        logger.info("Determining optimal workflow")

        # Check if the project has requirements
>       has_requirements = project_state["requirements_count"] > 0
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'requirements_count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/or
chestration/refactor_workflow.py:75: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,575 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 5
files
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferred
architecture: MVC (confidence: 0.90)
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:42,576 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
2025-10-28 10:33:42,576 - devsynth.application.code_analysis.self_analyzer -
INFO - SelfAnalyzer initialized with project root:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
2025-10-28 10:33:42,576 - devsynth.application.code_analysis.self_analyzer -
INFO - Starting analysis of project at
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Generating insights from code analysis
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing architecture
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Detecting architecture type
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Detected architecture type: MVC (confidence: 0.75)
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Identifying layers
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing layer dependencies
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Checking for MVC architecture violations
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing code quality
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing test coverage
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5/tests
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Identifying improvement opportunities
2025-10-28 10:33:42,578 - devsynth.application.code_analysis.self_analyzer -
INFO - Self-analysis completed
2025-10-28 10:33:42,578 - devsynth.application.orchestration.refactor_workflow -
INFO - Determining optimal workflow
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 5 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferred architecture: MVC (confidence: 0.90)
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
SelfAnalyzer initialized with project root:
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Starting analysis of project at
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Generating insights from code analysis
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing architecture
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Detecting architecture type
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Detected architecture type: MVC (confidence: 0.75)
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Identifying layers
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing layer dependencies
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Checking for MVC architecture violations
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing code quality
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing test coverage
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphqv57nv5/tests
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Identifying improvement opportunities
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Self-analysis completed
INFO
devsynth.application.orchestration.refactor_workflow:logging_setup.py:615
Determining optimal workflow
___________________ test_load_and_save_yaml_config_succeeds ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_load_and_save_yaml_config0')

    @pytest.mark.medium
    def test_load_and_save_yaml_config_succeeds(tmp_path):
        """Test that load and save yaml config succeeds.

        ReqID: N/A"""
        cfg = ConfigModel(project_root=str(tmp_path), features={"test_flag":
True})
        path = save_config(cfg, path=str(tmp_path))
        assert path.name == "project.yaml"
        loaded = load_config(tmp_path)
>       assert loaded.features == {"test_flag": True}
E       AssertionError: assert {} == {'test_flag': True}
E
E         Right contains 1 more item:
E         {'test_flag': True}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_config_loader.py:17: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,615 - devsynth.config.loader - WARNING - Unknown feature
flag: test_flag
------------------------------ Captured log call -------------------------------
WARNING  devsynth.config.loader:logging_setup.py:615 Unknown feature flag:
test_flag
_________________ test_load_and_save_pyproject_config_succeeds _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_load_and_save_pyproject_c0')

    @pytest.mark.medium
    def test_load_and_save_pyproject_config_succeeds(tmp_path):
        """Test that load and save pyproject config succeeds.

        ReqID: N/A"""
        cfg = ConfigModel(project_root=str(tmp_path), features={"test_flag":
True})
        path = save_config(cfg, use_pyproject=True, path=str(tmp_path))
        assert path.name == "pyproject.toml"
        loaded = load_config(tmp_path)
>       assert loaded.features == {"test_flag": True}
E       AssertionError: assert {} == {'test_flag': True}
E
E         Right contains 1 more item:
E         {'test_flag': True}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_config_loader.py:30: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,639 - devsynth.config.loader - WARNING - Unknown feature
flag: test_flag
------------------------------ Captured log call -------------------------------
WARNING  devsynth.config.loader:logging_setup.py:615 Unknown feature flag:
test_flag
__________________ test_delegate_task_team_consensus_succeeds __________________

self = <MagicMock name='build_consensus' id='6121841504'>
args = ({'team_task': True},), kwargs = {}, expected = call({'team_task': True})
actual = call({'team_task': True, 'id': '05628e0d-8bc8-4f3f-8db7-8564313fe45a',
'solutions': [{'agent': 'planner', 'role': 'Pri...dator-sol', 'confidence':
1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content': 'critic-sol',
'confidence': 1.0}]})
_error_message = <function
NonCallableMock.assert_called_with.<locals>._error_message at 0x16dc66980>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual:
%s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: build_consensus({'team_task': True})
E             Actual: build_consensus({'team_task': True, 'id':
'05628e0d-8bc8-4f3f-8db7-8564313fe45a', 'solutions': [{'agent': 'planner',
'role': 'Primus', 'content': 'planner-sol', 'confidence': 1.0}, {'agent':
'coder', 'role': 'Primus', 'content': 'coder-sol', 'confidence': 1.0}, {'agent':
'tester', 'role': 'Supervisor', 'content': 'tester-sol', 'confidence': 1.0},
{'agent': 'validator', 'role': 'Designer', 'content': 'validator-sol',
'confidence': 1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content':
'critic-sol', 'confidence': 1.0}]})

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='build_consensus' id='6121841504'>
args = ({'team_task': True},), kwargs = {}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: expected call not found.
E       Expected: build_consensus({'team_task': True})
E         Actual: build_consensus({'team_task': True, 'id':
'05628e0d-8bc8-4f3f-8db7-8564313fe45a', 'solutions': [{'agent': 'planner',
'role': 'Primus', 'content': 'planner-sol', 'confidence': 1.0}, {'agent':
'coder', 'role': 'Primus', 'content': 'coder-sol', 'confidence': 1.0}, {'agent':
'tester', 'role': 'Supervisor', 'content': 'tester-sol', 'confidence': 1.0},
{'agent': 'validator', 'role': 'Designer', 'content': 'validator-sol',
'confidence': 1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content':
'critic-sol', 'confidence': 1.0}]})
E
E       pytest introspection follows:
E
E       Args:
E       assert ({'id': '0562...task': True},) == ({'team_task': True},)
E
E         At index 0 diff: {'team_task': True, 'id':
'05628e0d-8bc8-4f3f-8db7-8564313fe45a', 'solutions': [{'agent': 'planner',
'role': 'Primus', 'content': 'planner-sol', 'confidence': 1.0}, {'agent':
'coder', 'role': 'Primus', 'content': 'coder-sol', 'confidence': 1.0}, {'agent':
'tester', 'role': 'Supervisor', 'content': 'tester-sol', 'confidence': 1.0},
{'agent': 'validator', 'role': 'Designer', 'content': 'validator-sol',
'confidence': 1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content':
'critic-sol', 'confidence': 1.0}]} != {'team_task': True}
E         Use -v to get more diff

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: AssertionError

During handling of the above exception, another exception occurred:

mocker = <pytest_mock.plugin.MockerFixture object at 0x16c04af30>

    @pytest.mark.medium
    def test_delegate_task_team_consensus_succeeds(mocker):
        """Test that delegate task team consensus succeeds.

        ReqID: N/A"""
        coordinator = AgentCoordinatorImpl({"features": {"wsde_collaboration":
True}})
        planner = SimpleAgent("planner", "planner", ["planning"])
        coder = SimpleAgent("coder", "code", ["python"])
        tester = SimpleAgent("tester", "test", ["pytest"])
        validator = SimpleAgent("validator", "validation", ["qa"])
        critic = SimpleAgent("critic", "critic", ["analysis"])
        for agent in [planner, coder, tester, validator, critic]:
            coordinator.add_agent(agent)
        team = coordinator.team
        consensus = {
            "consensus": "final",
            "contributors": [a.name for a in [planner, coder, tester, validator,
critic]],
            "method": "consensus_synthesis",
        }
        mock_build = mocker.patch.object(team, "build_consensus",
return_value=consensus)
        mock_dialectical = mocker.patch.object(
            team, "apply_enhanced_dialectical_reasoning",
return_value={"evaluation": "ok"}
        )
        result = coordinator.delegate_task({"team_task": True})
>       mock_build.assert_called_once_with({"team_task": True})
E       AssertionError: expected call not found.
E       Expected: build_consensus({'team_task': True})
E         Actual: build_consensus({'team_task': True, 'id':
'05628e0d-8bc8-4f3f-8db7-8564313fe45a', 'solutions': [{'agent': 'planner',
'role': 'Primus', 'content': 'planner-sol', 'confidence': 1.0}, {'agent':
'coder', 'role': 'Primus', 'content': 'coder-sol', 'confidence': 1.0}, {'agent':
'tester', 'role': 'Supervisor', 'content': 'tester-sol', 'confidence': 1.0},
{'agent': 'validator', 'role': 'Designer', 'content': 'validator-sol',
'confidence': 1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content':
'critic-sol', 'confidence': 1.0}]})
E
E       pytest introspection follows:
E
E       Args:
E       assert ({'id': '0562...task': True},) == ({'team_task': True},)
E
E         At index 0 diff: {'team_task': True, 'id':
'05628e0d-8bc8-4f3f-8db7-8564313fe45a', 'solutions': [{'agent': 'planner',
'role': 'Primus', 'content': 'planner-sol', 'confidence': 1.0}, {'agent':
'coder', 'role': 'Primus', 'content': 'coder-sol', 'confidence': 1.0}, {'agent':
'tester', 'role': 'Supervisor', 'content': 'tester-sol', 'confidence': 1.0},
{'agent': 'validator', 'role': 'Designer', 'content': 'validator-sol',
'confidence': 1.0}, {'agent': 'critic', 'role': 'Evaluator', 'content':
'critic-sol', 'confidence': 1.0}]} != {'team_task': True}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_delegate_task.py:48: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,777 - devsynth.domain.models.wsde_core - INFO - Added agent
planner to team AgentCoordinatorTeam
2025-10-28 10:33:42,777 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team AgentCoordinatorTeam
2025-10-28 10:33:42,777 - devsynth.domain.models.wsde_core - INFO - Added agent
tester to team AgentCoordinatorTeam
2025-10-28 10:33:42,777 - devsynth.domain.models.wsde_core - INFO - Added agent
validator to team AgentCoordinatorTeam
2025-10-28 10:33:42,777 - devsynth.domain.models.wsde_core - INFO - Added agent
critic to team AgentCoordinatorTeam
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'planner', 'worker':
'coder', 'supervisor': 'tester', 'designer': 'validator', 'evaluator': 'critic'}
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Selected coder as Primus based on task expertise
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Agent planner (Primus) proposed a solution
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Agent coder (Primus) proposed a solution
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Agent tester (Supervisor) proposed a solution
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Agent validator (Designer) proposed a solution
2025-10-28 10:33:42,778 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Agent critic (Evaluator) proposed a solution
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Applied dialectical reasoning to solutions
2025-10-28 10:33:42,778 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 5 solutions (method=consensus_synthesis)
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
planner to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
tester to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
validator to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
critic to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'planner', 'worker': 'coder',
'supervisor': 'tester', 'designer': 'validator', 'evaluator': 'critic'}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected coder as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent planner (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent coder (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent tester (Supervisor) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent validator (Designer) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05628e0d-8bc8-4f3f-8db7-8564313fe45a
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent critic (Evaluator) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Applied dialectical reasoning to solutions
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 5 solutions (method=consensus_synthesis)
__________________ test_delegate_task_team_consensus_succeeds __________________

    @pytest.mark.medium
    def test_delegate_task_team_consensus_succeeds():
        """Test that delegate task team consensus succeeds.

        ReqID: N/A"""
        coordinator = AgentCoordinatorImpl({"features": {"wsde_collaboration":
True}})
        planner = SimpleAgent("planner", "planner", ["planning"])
        coder = SimpleAgent("coder", "code", ["python"])
        writer = SimpleAgent("writer", "docs", ["documentation"])
        for a in [planner, coder, writer]:
            coordinator.add_agent(a)

        def fake_consensus(_task):
            primus_name = coordinator.team.get_primus().name
            return {
                "consensus": "final",
                "contributors": [primus_name, writer.name],
                "method": "consensus_synthesis",
            }

        with patch.object(coordinator.team, "build_consensus",
side_effect=fake_consensus):
            result = coordinator.delegate_task(
                {"team_task": True, "type": "coding", "language": "python"}
            )
        primus = coordinator.team.get_primus()
        assert primus == coder
        assert coordinator.team.primus_index ==
coordinator.team.agents.index(coder)
>       assert primus.name in result["team_result"]["consensus"]["contributors"]
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'contributors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_delegate_task_consensus.py:47: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,880 - devsynth.domain.models.wsde_core - INFO - Added agent
planner to team AgentCoordinatorTeam
2025-10-28 10:33:42,880 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team AgentCoordinatorTeam
2025-10-28 10:33:42,880 - devsynth.domain.models.wsde_core - INFO - Added agent
writer to team AgentCoordinatorTeam
2025-10-28 10:33:42,881 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'planner', 'worker':
'coder', 'supervisor': 'writer', 'designer': None, 'evaluator': None}
2025-10-28 10:33:42,881 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
2025-10-28 10:33:42,881 - devsynth.application.collaboration.coordinator - INFO
- Selected coder as Primus based on task expertise
2025-10-28 10:33:42,881 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 78e12901-638a-42f9-9446-b35f114ffb8f
2025-10-28 10:33:42,881 - devsynth.application.collaboration.coordinator - INFO
- Agent planner (Primus) proposed a solution
2025-10-28 10:33:42,881 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 78e12901-638a-42f9-9446-b35f114ffb8f
2025-10-28 10:33:42,881 - devsynth.application.collaboration.coordinator - INFO
- Agent coder (Primus) proposed a solution
2025-10-28 10:33:42,881 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 78e12901-638a-42f9-9446-b35f114ffb8f
2025-10-28 10:33:42,881 - devsynth.application.collaboration.coordinator - INFO
- Agent writer (Supervisor) proposed a solution
2025-10-28 10:33:42,881 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 3 solutions (method=consensus_synthesis)
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
planner to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
writer to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'planner', 'worker': 'coder',
'supervisor': 'writer', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected coder as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 78e12901-638a-42f9-9446-b35f114ffb8f
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent planner (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 78e12901-638a-42f9-9446-b35f114ffb8f
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent coder (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 78e12901-638a-42f9-9446-b35f114ffb8f
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent writer (Supervisor) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 3 solutions (method=consensus_synthesis)
______ test_critical_decision_tied_vote_falls_back_to_consensus_succeeds _______

self = <MagicMock name='build_consensus' id='6129227856'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'build_consensus' to have been called once.
Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError

During handling of the above exception, another exception occurred:

    @pytest.mark.medium
    def test_critical_decision_tied_vote_falls_back_to_consensus_succeeds():
        """Test that critical decision tied vote falls back to consensus
succeeds.

        ReqID: N/A"""
        coordinator = AgentCoordinatorImpl({"features": {"wsde_collaboration":
True}})
        a1 = VotingAgent("a1", "o1")
        a2 = VotingAgent("a2", "o2")
        a3 = VotingAgent("a3", "o1")
        a4 = VotingAgent("a4", "o2")
        for a in [a1, a2, a3, a4]:
            coordinator.add_agent(a)
        consensus_result = {
            "consensus": "combined",
            "contributors": [a.name for a in [a1, a2, a3, a4]],
            "method": "consensus_synthesis",
        }
        task = {
            "team_task": True,
            "type": "critical_decision",
            "is_critical": True,
            "options": [{"id": "o1"}, {"id": "o2"}],
        }
        with (
            patch.object(
                coordinator.team, "build_consensus",
return_value=consensus_result
            ) as consensus_spy,
            patch.object(
                coordinator.team,
                "vote_on_critical_decision",
                wraps=coordinator.team.vote_on_critical_decision,
            ) as vote_spy,
        ):
            result = coordinator.delegate_task(task)
        vote_spy.assert_called_once_with(task)
>       consensus_spy.assert_called_once()
E       AssertionError: Expected 'build_consensus' to have been called once.
Called 0 times.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_delegate_task_consensus.py:138: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,913 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:42,913 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
2025-10-28 10:33:42,913 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team AgentCoordinatorTeam
2025-10-28 10:33:42,913 - devsynth.domain.models.wsde_core - INFO - Added agent
a4 to team AgentCoordinatorTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a4 to
team AgentCoordinatorTeam
__________ test_primus_rotation_resets_after_all_have_served_succeeds __________

    @pytest.mark.medium
    def test_primus_rotation_resets_after_all_have_served_succeeds():
        """Test that primus rotation resets after all have served succeeds.

        ReqID: N/A"""
        coordinator = AgentCoordinatorImpl({"features": {"wsde_collaboration":
True}})
        a1 = SimpleAgent("a1", "code", ["python"])
        a2 = SimpleAgent("a2", "docs", ["docs"])
        a3 = SimpleAgent("a3", "test", ["testing"])
        coordinator.add_agent(a1)
        coordinator.add_agent(a2)
        coordinator.add_agent(a3)
        consensus = {
            "consensus": "x",
            "contributors": ["a1", "a2", "a3"],
            "method": "consensus_synthesis",
        }
        with patch.object(coordinator.team, "build_consensus",
return_value=consensus):
            coordinator.delegate_task({"team_task": True, "type": "coding"})
            first = coordinator.team.get_primus().name
            coordinator.delegate_task({"team_task": True, "type":
"documentation"})
            second = coordinator.team.get_primus().name
            coordinator.delegate_task({"team_task": True, "type": "testing"})
            third = coordinator.team.get_primus().name
            assert {first, second, third} == {"a1", "a2", "a3"}
>           assert all(agent.has_been_primus for agent in
coordinator.team.agents)
E           assert False
E            +  where False = all(<generator object
test_primus_rotation_resets_after_all_have_served_succeeds.<locals>.<genexpr> at
0x16d768700>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_delegate_task_primus_selection.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:42,986 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team AgentCoordinatorTeam
2025-10-28 10:33:42,986 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team AgentCoordinatorTeam
2025-10-28 10:33:42,986 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team AgentCoordinatorTeam
2025-10-28 10:33:42,987 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2',
'supervisor': 'a3', 'designer': None, 'evaluator': None}
2025-10-28 10:33:42,987 - devsynth.domain.models.wsde_roles - INFO - Selected a2
as primus based on expertise
2025-10-28 10:33:42,987 - devsynth.application.collaboration.coordinator - INFO
- Selected a2 as Primus based on task expertise
2025-10-28 10:33:42,988 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05d242ee-af3d-4728-a994-becb412858ba
2025-10-28 10:33:42,988 - devsynth.application.collaboration.coordinator - INFO
- Agent a1 (Primus) proposed a solution
2025-10-28 10:33:42,988 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05d242ee-af3d-4728-a994-becb412858ba
2025-10-28 10:33:42,988 - devsynth.application.collaboration.coordinator - INFO
- Agent a2 (Primus) proposed a solution
2025-10-28 10:33:42,988 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 05d242ee-af3d-4728-a994-becb412858ba
2025-10-28 10:33:42,988 - devsynth.application.collaboration.coordinator - INFO
- Agent a3 (Supervisor) proposed a solution
2025-10-28 10:33:42,988 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 3 solutions (method=consensus_synthesis)
2025-10-28 10:33:42,989 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2',
'supervisor': 'a3', 'designer': None, 'evaluator': None}
2025-10-28 10:33:42,989 - devsynth.domain.models.wsde_roles - INFO - Selected a3
as primus based on expertise
2025-10-28 10:33:42,989 - devsynth.application.collaboration.coordinator - INFO
- Selected a3 as Primus based on task expertise
2025-10-28 10:33:42,989 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 8444e557-56b8-4300-aa2c-dbc164627267
2025-10-28 10:33:42,989 - devsynth.application.collaboration.coordinator - INFO
- Agent a1 (Primus) proposed a solution
2025-10-28 10:33:42,989 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 8444e557-56b8-4300-aa2c-dbc164627267
2025-10-28 10:33:42,989 - devsynth.application.collaboration.coordinator - INFO
- Agent a2 (Worker) proposed a solution
2025-10-28 10:33:42,989 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 8444e557-56b8-4300-aa2c-dbc164627267
2025-10-28 10:33:42,989 - devsynth.application.collaboration.coordinator - INFO
- Agent a3 (Primus) proposed a solution
2025-10-28 10:33:42,990 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 3 solutions (method=consensus_synthesis)
2025-10-28 10:33:42,990 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2',
'supervisor': 'a3', 'designer': None, 'evaluator': None}
2025-10-28 10:33:42,990 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
2025-10-28 10:33:42,990 - devsynth.application.collaboration.coordinator - INFO
- Selected a1 as Primus based on task expertise
2025-10-28 10:33:42,990 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 53507c14-e03a-4157-b0b6-d212989deb61
2025-10-28 10:33:42,990 - devsynth.application.collaboration.coordinator - INFO
- Agent a1 (Primus) proposed a solution
2025-10-28 10:33:42,990 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 53507c14-e03a-4157-b0b6-d212989deb61
2025-10-28 10:33:42,990 - devsynth.application.collaboration.coordinator - INFO
- Agent a2 (Worker) proposed a solution
2025-10-28 10:33:42,991 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 53507c14-e03a-4157-b0b6-d212989deb61
2025-10-28 10:33:42,991 - devsynth.application.collaboration.coordinator - INFO
- Agent a3 (Supervisor) proposed a solution
2025-10-28 10:33:42,991 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 3 solutions (method=consensus_synthesis)
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2', 'supervisor':
'a3', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a2 as
primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected a2 as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05d242ee-af3d-4728-a994-becb412858ba
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a1 (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05d242ee-af3d-4728-a994-becb412858ba
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a2 (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 05d242ee-af3d-4728-a994-becb412858ba
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a3 (Supervisor) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 3 solutions (method=consensus_synthesis)
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2', 'supervisor':
'a3', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a3 as
primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected a3 as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 8444e557-56b8-4300-aa2c-dbc164627267
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a1 (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 8444e557-56b8-4300-aa2c-dbc164627267
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a2 (Worker) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 8444e557-56b8-4300-aa2c-dbc164627267
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a3 (Primus) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 3 solutions (method=consensus_synthesis)
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'a1', 'worker': 'a2', 'supervisor':
'a3', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected a1 as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 53507c14-e03a-4157-b0b6-d212989deb61
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a1 (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 53507c14-e03a-4157-b0b6-d212989deb61
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a2 (Worker) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 53507c14-e03a-4157-b0b6-d212989deb61
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent a3 (Supervisor) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 3 solutions (method=consensus_synthesis)
__________________ test_delegate_task_full_workflow_succeeds ___________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x16d524290>

    @pytest.mark.medium
    def test_delegate_task_full_workflow_succeeds(mocker):
        """Test that delegate task full workflow succeeds.

        ReqID: N/A"""
        coordinator = AgentCoordinatorImpl({"features": {"wsde_collaboration":
True}})
        planner = SimpleAgent("planner", "planner", ["planning"])
        coder = SimpleAgent("coder", "code", ["python"])
        validator = SimpleAgent("validator", "validation", ["testing"])
        critic = SimpleAgent("critic", "critic", ["analysis"])
        for agent in [planner, coder, validator, critic]:
            coordinator.add_agent(agent)
        team = coordinator.team
        consensus = {
            "consensus": "final",
            "contributors": [planner.name, coder.name, validator.name,
critic.name],
            "method": "consensus_synthesis",
        }
        mock_build = mocker.patch.object(team, "build_consensus",
return_value=consensus)
        mock_dialectical = mocker.patch.object(
            team, "apply_enhanced_dialectical_reasoning",
return_value={"evaluation": "ok"}
        )
        task = {"team_task": True, "type": "coding", "language": "python"}
        result = coordinator.delegate_task(task)
        mock_build.assert_called_once_with(task)
        mock_dialectical.assert_called_once_with(task, critic)
>       assert result["team_result"]["consensus"] == consensus
E       AssertionError: assert {'achieved': ...cts': [], ...} ==
{'consensus':...us_synthesis'}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 15 more items:
E         {'achieved': None,
E          'agent_opinions': [],
E          'confidence': None,
E          'conflicts': [],...
E
E         ...Full output truncated (16 lines hidden), use '-vv' to show

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_delegate_task_workflow.py:48: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:43,005 - devsynth.domain.models.wsde_core - INFO - Added agent
planner to team AgentCoordinatorTeam
2025-10-28 10:33:43,005 - devsynth.domain.models.wsde_core - INFO - Added agent
coder to team AgentCoordinatorTeam
2025-10-28 10:33:43,005 - devsynth.domain.models.wsde_core - INFO - Added agent
validator to team AgentCoordinatorTeam
2025-10-28 10:33:43,006 - devsynth.domain.models.wsde_core - INFO - Added agent
critic to team AgentCoordinatorTeam
2025-10-28 10:33:43,006 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team AgentCoordinatorTeam: {'primus': 'planner', 'worker':
'coder', 'supervisor': 'validator', 'designer': 'critic', 'evaluator': None}
2025-10-28 10:33:43,007 - devsynth.domain.models.wsde_roles - INFO - Selected
coder as primus based on expertise
2025-10-28 10:33:43,007 - devsynth.application.collaboration.coordinator - INFO
- Selected coder as Primus based on task expertise
2025-10-28 10:33:43,007 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
2025-10-28 10:33:43,007 - devsynth.application.collaboration.coordinator - INFO
- Agent planner (Primus) proposed a solution
2025-10-28 10:33:43,008 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
2025-10-28 10:33:43,008 - devsynth.application.collaboration.coordinator - INFO
- Agent coder (Primus) proposed a solution
2025-10-28 10:33:43,008 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
2025-10-28 10:33:43,008 - devsynth.application.collaboration.coordinator - INFO
- Agent validator (Supervisor) proposed a solution
2025-10-28 10:33:43,008 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
2025-10-28 10:33:43,008 - devsynth.application.collaboration.coordinator - INFO
- Agent critic (Designer) proposed a solution
2025-10-28 10:33:43,008 - devsynth.application.collaboration.coordinator - INFO
- Applied dialectical reasoning to solutions
2025-10-28 10:33:43,008 - devsynth.application.collaboration.coordinator - INFO
- Built consensus among 4 solutions (method=consensus_synthesis)
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
planner to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
validator to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
critic to team AgentCoordinatorTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team AgentCoordinatorTeam: {'primus': 'planner', 'worker': 'coder',
'supervisor': 'validator', 'designer': 'critic', 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder
as primus based on expertise
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Selected coder as Primus based on task expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent planner (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent coder (Primus) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent validator (Supervisor) proposed a solution
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 5b6770a0-2c19-4df8-91e6-21cb9f196920
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Agent critic (Designer) proposed a solution
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Applied dialectical reasoning to solutions
INFO     devsynth.application.collaboration.coordinator:logging_setup.py:615
Built consensus among 4 solutions (method=consensus_synthesis)
_____________ TestEndToEndWorkflow.test_complete_workflow_succeeds _____________

self = <tests.integration.general.test_end_to_end_workflow.TestEndToEndWorkflow
object at 0x128500620>
temp_project_dir =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_complete_workflow_succeed0'
tmpdir =
local('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitly
n/pytest-1440/test_complete_workflow_succeed0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x16d54b710>

    @pytest.mark.medium
    def test_complete_workflow_succeeds(self, temp_project_dir, tmpdir,
monkeypatch):
        """Test a complete workflow from requirements to code.

        This test simulates a complete development workflow:
        1. Initialize a new project
        2. Create requirements
        3. Analyze the project state (should detect missing specifications and
code)
        4. Generate specifications from requirements
        5. Analyze the project state again (should detect missing code)
        6. Generate tests from specifications
        7. Generate code from tests
        8. Analyze the final project state (should have good alignment)

        ReqID: N/A"""
        original_dir = os.getcwd()
        os.chdir(temp_project_dir)
        try:
            init_cmd(
                root=temp_project_dir,
                language="python",
                goals="",
                memory_backend="memory",
                auto_confirm=True,
            )
            assert os.path.exists(os.path.join(temp_project_dir, ".devsynth"))
            # Create docs directory in tmpdir
            requirements_dir = tmpdir.join("docs")
            requirements_dir.ensure(dir=True)
            requirements_content = """
            # Task Manager API Requirements

            ## User Management
            1. The system shall allow users to register with a username and
password
            2. The system shall allow users to log in with their credentials
            3. The system shall allow users to log out
            4. The system shall allow users to update their profile information

            ## Task Management
            1. The system shall allow users to create new tasks with a title and
description
            2. The system shall allow users to view their tasks
            3. The system shall allow users to update their tasks
            4. The system shall allow users to delete their tasks
            5. The system shall allow users to mark tasks as completed

            ## API Requirements
            1. The API shall use REST principles
            2. The API shall return responses in JSON format
            3. The API shall use proper HTTP status codes
            4. The API shall require authentication for protected endpoints
            """
            # Use tmpdir directly for file operations
            requirements_file = requirements_dir.join("requirements.md")
            requirements_file.write(requirements_content)
            analyzer = ProjectStateAnalyzer(temp_project_dir)
            initial_report = analyzer.analyze()
>           assert initial_report["requirements_count"] > 0
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'requirements_count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_end_to_end_workflow.py:96: KeyError
----------------------------- Captured stdout call -----------------------------
⠋ Generating project files ━━━━━━━━━╸          1/2  50% Complete 0:00:00 -:--:--
2025-10-28 10:33:43,038 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
Initialization complete
2025-10-28 10:33:43,039 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_complete_workflow_succeed0
2025-10-28 10:33:43,039 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 3
files
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages:
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Could not
confidently infer architecture
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:43,040 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_complete_workflow_succeed0
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 3 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages:
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Could not confidently infer architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
_______ TestEndToEndWorkflow.test_inconsistent_project_workflow_succeeds _______

self = <tests.integration.general.test_end_to_end_workflow.TestEndToEndWorkflow
object at 0x128500aa0>
temp_project_dir =
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1440/test_inconsistent_project_work0'
tmpdir =
local('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitly
n/pytest-1440/test_inconsistent_project_work0')

    @pytest.mark.medium
    def test_inconsistent_project_workflow_succeeds(self, temp_project_dir,
tmpdir):
        """Test workflow with an inconsistent project state.

        This test simulates a workflow with an inconsistent project state:
        1. Initialize a new project
        2. Create requirements
        3. Create code without specifications or tests
        4. Analyze the project state (should detect inconsistencies)
        5. Generate specifications from requirements
        6. Generate tests from specifications
        7. Analyze the final project state (should have improved alignment)

        ReqID: N/A"""
        original_dir = os.getcwd()
        os.chdir(temp_project_dir)
        try:
            init_cmd(
                root=temp_project_dir,
                language="python",
                goals="",
                memory_backend="memory",
                auto_confirm=True,
            )
            assert os.path.exists(os.path.join(temp_project_dir, ".devsynth"))
            # Create docs directory in tmpdir
            requirements_dir = tmpdir.join("docs")
            requirements_dir.ensure(dir=True)
            requirements_content = """
            # User Authentication Requirements

            ## Registration
            1. The system shall allow users to register with email and password
            2. The system shall validate email format
            3. The system shall enforce password complexity rules

            ## Authentication
            1. The system shall allow users to log in with email and password
            2. The system shall provide token-based authentication
            3. The system shall allow users to log out

            ## Password Management
            1. The system shall allow users to reset their password
            2. The system shall allow users to change their password
            """
            # Use tmpdir directly for file operations
            requirements_file = requirements_dir.join("requirements.md")
            requirements_file.write(requirements_content)

            # Create src directory in tmpdir
            src_dir = tmpdir.join("src")
            src_dir.ensure(dir=True)
            user_code = """
            class User:
                def __init__(self, email, password):
                    self.email = email
                    self.password = password
                    self.is_authenticated = False

                def register(self):
                    # Implementation for user registration
                    pass

                def login(self):
                    # Implementation for user login
                    self.is_authenticated = True
                    return True

                def logout(self):
                    # Implementation for user logout
                    self.is_authenticated = False
                    return True

                def reset_password(self, new_password):
                    # Implementation for password reset
                    self.password = new_password
                    return True
            """
            # Use tmpdir directly for file operations
            user_file = src_dir.join("user.py")
            user_file.write(user_code)
            analyzer = ProjectStateAnalyzer(temp_project_dir)
            initial_report = analyzer.analyze()
>           assert initial_report["requirements_count"] > 0
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'requirements_count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_end_to_end_workflow.py:233: KeyError
----------------------------- Captured stdout call -----------------------------
⠋ Generating project files ━━━━━━━━━╸          1/2  50% Complete 0:00:00 -:--:--
2025-10-28 10:33:43,065 - devsynth.interface.cli - INFO - Displaying success:
[green]Initialization complete[/green]
Initialization complete
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_inconsistent_project_work0
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 4
files
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Could not
confidently infer architecture
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:43,066 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:43,067 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO     devsynth.interface.cli:logging_setup.py:615 Displaying success:
[green]Initialization complete[/green]
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_inconsistent_project_work0
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 4 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Could not confidently infer architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
_
TestErrorHandlingAtIntegrationPoints.test_error_handling_in_edrr_wsde_integratio
n_raises_error _

self =
<tests.integration.general.test_error_handling_at_integration_points.TestErrorHa
ndlingAtIntegrationPoints object at 0x128501d00>
coordinator =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x12f71ccb0>

    @pytest.mark.medium
    def test_error_handling_in_edrr_wsde_integration_raises_error(self,
coordinator):
        """Test error handling in the EDRR-WSDE integration.

        ReqID: N/A"""
        task = {
            "description": "Create a Python function to calculate factorial",
            "language": "python",
            "domain": "code_generation",
        }
        coordinator.start_cycle(task)
        coordinator.wsde_team.generate_diverse_ideas.side_effect = ValueError(
            "Test error in generate_diverse_ideas"
        )
        try:
            expand_results = coordinator.execute_current_phase()
>           assert False, "Expected EDRRCoordinatorError was not raised"
E           AssertionError: Expected EDRRCoordinatorError was not raised
E           assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_error_handling_at_integration_points.py:140: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:43,088 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:43,088 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:33:43,088 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:33:43,088 - devsynth.domain.models.wsde_core - INFO - Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,088 - devsynth.domain.models.wsde_core - INFO - Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,090 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:33:43,090 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:33:43,090 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:33:43,090 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:33:43,090 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:33:43,090 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:43,091 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 0696cd44-3daa-4e21-a4de-c6ead10e3633 in TinyDB Memory
Adapter
2025-10-28 10:33:43,091 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,091 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,091 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,093 - devsynth.domain.models.wsde_roles - INFO - Assigning
roles for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,093 - devsynth.domain.models.wsde_roles - INFO - Selected
Explorer as primus based on expertise
2025-10-28 10:33:43,094 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 6a6ca6fa-9fec-4547-b1cd-ded8978b3b10 in TinyDB Memory
Adapter
2025-10-28 10:33:43,094 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,094 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,094 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID dfdd912a-bfe8-4f9d-b433-8eecd234ef46 in TinyDB Memory
Adapter
2025-10-28 10:33:43,094 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,094 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,095 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Create a Python function to
calculate factorial
2025-10-28 10:33:43,095 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Create a Python function to calculate factorial
2025-10-28 10:33:43,095 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,095 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.persistence -
ERROR - Failed to store memory item with EDRR phase: Object of type MagicMock is
not JSON serializable
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase expand: 'dict' object has no attribute 'name'
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from 'dict' object has no attribute 'name' in phase
Phase.EXPAND
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.persistence -
ERROR - Failed to store memory item with EDRR phase: Object of type MagicMock is
not JSON serializable
2025-10-28 10:33:43,097 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
2025-10-28 10:33:43,097 - devsynth.application.edrr.edrr_phase_transitions -
WARNING - CoreValues is not iterable, using default values
2025-10-28 10:33:43,098 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 1ae96985-bb03-4b51-ba3a-6bffea95a995 in TinyDB Memory
Adapter
2025-10-28 10:33:43,098 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,098 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,098 - devsynth.application.edrr.edrr_coordinator_enhanced -
INFO - Phase EXPAND metrics: {'quality': 0.48006, 'completeness':
0.10120000000000001, 'consistency': 0.5, 'conflicts': 0, 'idea_diversity': 0.5,
'duration': 0.006186}
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 0696cd44-3daa-4e21-a4de-c6ead10e3633 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles
for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Explorer as primus based on expertise
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 6a6ca6fa-9fec-4547-b1cd-ded8978b3b10 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID dfdd912a-bfe8-4f9d-b433-8eecd234ef46 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Create a Python function to calculate
factorial
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Create a Python function to calculate factorial
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.persistence:logging_setup.py:615
Failed to store memory item with EDRR phase: Object of type MagicMock is not
JSON serializable
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase expand: 'dict' object has no attribute 'name'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from 'dict' object has no attribute 'name' in phase
Phase.EXPAND
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.persistence:logging_setup.py:615
Failed to store memory item with EDRR phase: Object of type MagicMock is not
JSON serializable
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
WARNING  devsynth.application.edrr.edrr_phase_transitions:logging_setup.py:615
CoreValues is not iterable, using default values
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 1ae96985-bb03-4b51-ba3a-6bffea95a995 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.edrr.edrr_coordinator_enhanced:logging_setup.py:615 Phase
EXPAND metrics: {'quality': 0.48006, 'completeness': 0.10120000000000001,
'consistency': 0.5, 'conflicts': 0, 'idea_diversity': 0.5, 'duration': 0.006186}
_
TestErrorHandlingAtIntegrationPoints.test_error_handling_in_memory_integration_r
aises_error _

self =
<tests.integration.general.test_error_handling_at_integration_points.TestErrorHa
ndlingAtIntegrationPoints object at 0x128502210>
memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x16cf6a2a0>

    @pytest.mark.medium
    def test_error_handling_in_memory_integration_raises_error(self,
memory_manager):
        """Test error handling in the memory integration.

        ReqID: N/A"""
        memory_manager.adapters["default"].store = MagicMock(
            side_effect=Exception("Test error in store")
        )
        try:
            memory_manager.store(
                MemoryItem(
                    id="",
                    content="Test content",
                    memory_type=MemoryType.KNOWLEDGE,
                    metadata={"source": "test"},
                )
            )
            assert False, "Expected Exception was not raised"
        except Exception as e:
            assert "Test error in store" in str(e)
        memory_manager.adapters["default"].store = MagicMock()
        memory_id = memory_manager.store(
            MemoryItem(
                id="",
                content="Test content",
                memory_type=MemoryType.KNOWLEDGE,
                metadata={"source": "test"},
            )
        )
        memory_manager.adapters["default"].retrieve = MagicMock(
            side_effect=Exception("Test error in retrieve")
        )
        try:
            memory_manager.retrieve(memory_id)
>           assert False, "Expected Exception was not raised"
E           AssertionError: Expected Exception was not raised
E           assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_error_handling_at_integration_points.py:193: AssertionError

During handling of the above exception, another exception occurred:

self =
<tests.integration.general.test_error_handling_at_integration_points.TestErrorHa
ndlingAtIntegrationPoints object at 0x128502210>
memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x16cf6a2a0>

    @pytest.mark.medium
    def test_error_handling_in_memory_integration_raises_error(self,
memory_manager):
        """Test error handling in the memory integration.

        ReqID: N/A"""
        memory_manager.adapters["default"].store = MagicMock(
            side_effect=Exception("Test error in store")
        )
        try:
            memory_manager.store(
                MemoryItem(
                    id="",
                    content="Test content",
                    memory_type=MemoryType.KNOWLEDGE,
                    metadata={"source": "test"},
                )
            )
            assert False, "Expected Exception was not raised"
        except Exception as e:
            assert "Test error in store" in str(e)
        memory_manager.adapters["default"].store = MagicMock()
        memory_id = memory_manager.store(
            MemoryItem(
                id="",
                content="Test content",
                memory_type=MemoryType.KNOWLEDGE,
                metadata={"source": "test"},
            )
        )
        memory_manager.adapters["default"].retrieve = MagicMock(
            side_effect=Exception("Test error in retrieve")
        )
        try:
            memory_manager.retrieve(memory_id)
            assert False, "Expected Exception was not raised"
        except Exception as e:
>           assert "Test error in retrieve" in str(e)
E           AssertionError: assert 'Test error in retrieve' in 'Expected
Exception was not raised\nassert False'
E            +  where 'Expected Exception was not raised\nassert False' =
str(AssertionError('Expected Exception was not raised\nassert False'))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_error_handling_at_integration_points.py:195: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:43,114 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:43,114 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:33:43,114 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:43,115 - devsynth.application.memory.circuit_breaker - INFO -
Creating new circuit breaker: memory_store_default
2025-10-28 10:33:43,116 - devsynth.application.memory.circuit_breaker - ERROR -
Circuit breaker 'memory_store_default' recorded failure: Test error in store
2025-10-28 10:33:43,116 - devsynth.application.memory.memory_manager - ERROR -
Failed to store memory item in default: Test error in store
2025-10-28 10:33:43,116 - devsynth.application.memory.error_logger - ERROR -
Memory operation 'store_item' failed on adapter 'default': Exception: Test error
in store
2025-10-28 10:33:43,116 - devsynth.application.memory.error_logger - ERROR -
Failed to persist error log: [Errno 2] No such file or directory:
'/Users/caitlyn/.devsynth/logs/memory/memory_error_20251028_103343_116558.json'
2025-10-28 10:33:43,116 - devsynth.application.memory.memory_manager - ERROR -
Failed to store memory item in any adapter: {'default': 'Test error in store'}
2025-10-28 10:33:43,117 - devsynth.application.memory.circuit_breaker - INFO -
Creating new circuit breaker: memory_retrieve_default
2025-10-28 10:33:43,118 - devsynth.application.memory.circuit_breaker - ERROR -
Circuit breaker 'memory_retrieve_default' recorded failure: Test error in
retrieve
2025-10-28 10:33:43,119 - devsynth.application.memory.memory_manager - ERROR -
Failed to retrieve memory item from default: Test error in retrieve
2025-10-28 10:33:43,120 - devsynth.application.memory.error_logger - ERROR -
Memory operation 'retrieve' failed on adapter 'default': Exception: Test error
in retrieve
2025-10-28 10:33:43,120 - devsynth.application.memory.error_logger - ERROR -
Failed to persist error log: [Errno 2] No such file or directory:
'/Users/caitlyn/.devsynth/logs/memory/memory_error_20251028_103343_120091.json'
2025-10-28 10:33:43,120 - devsynth.application.memory.memory_manager - WARNING -
Failed to retrieve item <MagicMock name='mock()' id='5093886096'> from any
adapter: {'default': 'Test error in retrieve'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.circuit_breaker:logging_setup.py:615
Creating new circuit breaker: memory_store_default
ERROR    devsynth.application.memory.circuit_breaker:logging_setup.py:615
Circuit breaker 'memory_store_default' recorded failure: Test error in store
ERROR    devsynth.application.memory.memory_manager:logging_setup.py:615 Failed
to store memory item in default: Test error in store
ERROR    devsynth.application.memory.error_logger:logging_setup.py:615 Memory
operation 'store_item' failed on adapter 'default': Exception: Test error in
store
ERROR    devsynth.application.memory.error_logger:logging_setup.py:615 Failed to
persist error log: [Errno 2] No such file or directory:
'/Users/caitlyn/.devsynth/logs/memory/memory_error_20251028_103343_116558.json'
ERROR    devsynth.application.memory.memory_manager:logging_setup.py:615 Failed
to store memory item in any adapter: {'default': 'Test error in store'}
INFO     devsynth.application.memory.circuit_breaker:logging_setup.py:615
Creating new circuit breaker: memory_retrieve_default
ERROR    devsynth.application.memory.circuit_breaker:logging_setup.py:615
Circuit breaker 'memory_retrieve_default' recorded failure: Test error in
retrieve
ERROR    devsynth.application.memory.memory_manager:logging_setup.py:615 Failed
to retrieve memory item from default: Test error in retrieve
ERROR    devsynth.application.memory.error_logger:logging_setup.py:615 Memory
operation 'retrieve' failed on adapter 'default': Exception: Test error in
retrieve
ERROR    devsynth.application.memory.error_logger:logging_setup.py:615 Failed to
persist error log: [Errno 2] No such file or directory:
'/Users/caitlyn/.devsynth/logs/memory/memory_error_20251028_103343_120091.json'
WARNING  devsynth.application.memory.memory_manager:logging_setup.py:615 Failed
to retrieve item <MagicMock name='mock()' id='5093886096'> from any adapter:
{'default': 'Test error in retrieve'}
_
TestErrorHandlingAtIntegrationPoints.test_error_handling_in_code_analysis_integr
ation_raises_error _

self =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x16ce3c740>
error = Exception('Test error in analyze_code')
phase = <Phase.EXPAND: 'expand'>

    def _attempt_recovery(self, error: Exception, phase: Phase) -> dict[str,
Any]:
        """Attempt a simple recovery strategy after errors."""
        logger.warning("Attempting recovery from %s in phase %s", error, phase)

        info = self._execute_recovery_hooks(error, phase)
        if info.get("recovered"):
            return info

        try:
>           results = self._execute_phase(phase)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:3326:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:3315: in _execute_phase
    return executor(context)
           ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:1260: in _execute_expand_phase
    analysis = self.code_analyzer.analyze_code(self.task["code"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1198: in _execute_mock_call
    raise effect
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:2080: in execute_current_phase
    results = executor(context)
              ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:1260: in _execute_expand_phase
    analysis = self.code_analyzer.analyze_code(self.task["code"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <MagicMock name='mock.analyze_code' id='5097077648'>
args = ('def factorial(n):\n    return 1 if n <= 1 else n * factorial(n-1)',)
kwargs = {}, effect = Exception('Test error in analyze_code')

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this
method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect
E               Exception: Test error in analyze_code

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1198: Exception

During handling of the above exception, another exception occurred:

self =
<tests.integration.general.test_error_handling_at_integration_points.TestErrorHa
ndlingAtIntegrationPoints object at 0x128502c30>
coordinator =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x16ce3c740>

    @pytest.mark.medium
    def test_error_handling_in_code_analysis_integration_raises_error(
        self, coordinator
    ):
        """Test error handling in the code analysis integration.

        ReqID: N/A"""
        coordinator.code_analyzer.analyze_code = MagicMock(
            side_effect=Exception("Test error in analyze_code")
        )
        task = {
            "description": "Analyze this code",
            "language": "python",
            "code": """def factorial(n):
    return 1 if n <= 1 else n * factorial(n-1)""",
        }
        coordinator.start_cycle(task)
>       expand_results = coordinator.execute_current_phase()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_error_handling_at_integration_points.py:244:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/edrr_coordinator_enhanced.py:537: in execute_current_phase
    results = super().execute_current_phase(context)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x16ce3c740>
context = {}

    def execute_current_phase(
        self, context: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        Execute the current phase of the EDRR cycle.

        Args:
            context: Optional context for the phase execution

        Returns:
            The results of the phase execution

        Raises:
            EDRRCoordinatorError: If no phase is currently active
        """
        if not self.current_phase:
            raise EDRRCoordinatorError("No active phase to execute")

        context = context or {}

        # Execute the appropriate phase
        phase_executors = {
            Phase.EXPAND: self._execute_expand_phase,
            Phase.DIFFERENTIATE: self._execute_differentiate_phase,
            Phase.REFINE: self._execute_refine_phase,
            Phase.RETROSPECT: self._execute_retrospect_phase,
        }

        executor = phase_executors.get(self.current_phase)
        if not executor:
            raise EDRRCoordinatorError(
                f"No executor available for phase {self.current_phase}"
            )

        try:
            try:
                self.memory_manager.flush_updates()
            except Exception as flush_error:  # pragma: no cover - defensive
                logger.debug(
                    f"Failed to flush memory updates before phase execution:
{flush_error}"
                )

            start_time = datetime.now()
            results = executor(context)
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            perf = self.performance_metrics.setdefault(self.current_phase.name,
{})
            perf["duration"] = duration
            perf["memory_usage"] = len(str(results))
            perf.setdefault(
                "component_calls",
                {
                    "wsde_team": 1,
                    "code_analyzer": 1,
                    "prompt_manager": 1,
                    "documentation_manager": 1,
                },
            )
            if "duration_adjustment_factor" in perf:
                perf["original_duration"] = duration
                perf["adjusted_duration"] = (
                    duration * perf["duration_adjustment_factor"]
                )

            if self._enable_enhanced_logging:
                self._execution_history.append(
                    {
                        "timestamp": end_time.isoformat(),
                        "phase": self.current_phase.value,
                        "action": "end",
                        "details": {"duration": duration},
                    }
                )

            self.results[self.current_phase.name] = results

            # Optional peer review for the phase results
            pr_result = self._execute_peer_review(self.current_phase, results)
            if pr_result is not None:
                results["peer_review"] = pr_result

            # Run additional micro-cycles until quality thresholds are met
            results = self._run_micro_cycles(self.current_phase, results)
            self.results[self.current_phase.name] = results

            # Evaluate phase quality and completion status
            assessment = self._assess_phase_quality(self.current_phase)
            results["quality_score"] = assessment["quality"]
            results["phase_complete"] = assessment["can_progress"]
            results["quality_metrics"] = assessment["metrics"]
            self.results[self.current_phase.name] = results

            if hasattr(self, "_preserved_context"):
                results.setdefault("context", {})["previous_phases"] =
copy.deepcopy(
                    self._preserved_context
                )

            # Refresh aggregated results
            self._aggregate_results()

            # Complete tracking the phase if using a manifest
            if self.manifest is not None and self.manifest_parser:
                self.manifest_parser.complete_phase(
                    self.current_phase, self.results.get(self.current_phase)
                )

            # Persist context for future phases
            self._persist_context_snapshot(self.current_phase)

            try:
                self.memory_manager.flush_updates()
            except Exception as flush_error:  # pragma: no cover - defensive
                logger.debug(
                    f"Failed to flush memory updates after phase execution:
{flush_error}"
                )

            logger.info(
                f"Completed {self.current_phase.value} phase for task:
{self.task.get('description', 'Unknown')}"
            )

            # Automatically transition to next phase if enabled
            self._maybe_auto_progress()

            return results.get("aggregated_results", results)
        except Exception as e:
            logger.error(f"Error executing phase {self.current_phase.value}:
{str(e)}")
            recovery = self._attempt_recovery(e, self.current_phase)
            if recovery.get("recovered"):
                phase_key = self.current_phase.name
                self.results.setdefault(phase_key, {}).setdefault(
                    "recovery_info", recovery
                )
                return self.results[phase_key]
>           raise EDRRCoordinatorError(
                f"Failed to execute phase {self.current_phase.value}: {e}"
            )
E           devsynth.application.edrr.coordinator.core.EDRRCoordinatorError:
Failed to execute phase expand: Test error in analyze_code

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ed
rr/coordinator/core.py:2171: EDRRCoordinatorError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:43,177 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:43,177 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:33:43,177 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:33:43,177 - devsynth.domain.models.wsde_core - INFO - Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,177 - devsynth.domain.models.wsde_core - INFO - Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,179 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:33:43,179 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:33:43,179 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:33:43,179 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:33:43,179 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:33:43,179 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:43,180 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 44b006e3-2662-4e6a-92a0-1b5b3dce3b3c in TinyDB Memory
Adapter
2025-10-28 10:33:43,180 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,180 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,180 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,182 - devsynth.domain.models.wsde_roles - INFO - Assigning
roles for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,183 - devsynth.domain.models.wsde_roles - INFO - Selected
Explorer as primus based on expertise
2025-10-28 10:33:43,183 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 8c78a70b-ab2c-45b3-b55b-e67daa6a43a6 in TinyDB Memory
Adapter
2025-10-28 10:33:43,183 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,184 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,184 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 20a9d476-f93a-4c20-99f4-b4460fee82e1 in TinyDB Memory
Adapter
2025-10-28 10:33:43,184 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,184 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,184 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Analyze this code
2025-10-28 10:33:43,184 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Analyze this code
2025-10-28 10:33:43,184 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,184 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,185 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase expand: Test error in analyze_code
2025-10-28 10:33:43,185 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from Test error in analyze_code in phase Phase.EXPAND
2025-10-28 10:33:43,185 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,185 - devsynth.application.edrr.coordinator.core - ERROR -
Recovery failed: Test error in analyze_code
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 44b006e3-2662-4e6a-92a0-1b5b3dce3b3c in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles
for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Explorer as primus based on expertise
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 8c78a70b-ab2c-45b3-b55b-e67daa6a43a6 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 20a9d476-f93a-4c20-99f4-b4460fee82e1 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Analyze this code
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Analyze this code
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase expand: Test error in analyze_code
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from Test error in analyze_code in phase Phase.EXPAND
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615
Recovery failed: Test error in analyze_code
_
TestErrorHandlingAtIntegrationPoints.test_error_recovery_in_edrr_cycle_raises_er
ror _

self =
<tests.integration.general.test_error_handling_at_integration_points.TestErrorHa
ndlingAtIntegrationPoints object at 0x1285031d0>
coordinator =
<devsynth.application.edrr.edrr_coordinator_enhanced.EnhancedEDRRCoordinator
object at 0x12fb73c20>

    @pytest.mark.medium
    def test_error_recovery_in_edrr_cycle_raises_error(self, coordinator):
        """Test error recovery in the EDRR cycle.

        ReqID: N/A"""
        task = {
            "description": "Create a Python function to calculate factorial",
            "language": "python",
            "domain": "code_generation",
        }
        coordinator.start_cycle(task)
        coordinator.wsde_team.generate_diverse_ideas.side_effect = [
            ValueError("Test error in generate_diverse_ideas"),
            [
                {"id": "idea1", "content": "First idea"},
                {"id": "idea2", "content": "Second idea"},
            ],
        ]
        try:
            expand_results = coordinator.execute_current_phase()
>           assert False, "Expected EDRRCoordinatorError was not raised"
E           AssertionError: Expected EDRRCoordinatorError was not raised
E           assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_error_handling_at_integration_points.py:273: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:43,497 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:33:43,497 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:33:43,497 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:33:43,497 - devsynth.domain.models.wsde_core - INFO - Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,497 - devsynth.domain.models.wsde_core - INFO - Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,500 - devsynth.application.edrr.templates - INFO -
Registered expand_phase template
2025-10-28 10:33:43,500 - devsynth.application.edrr.templates - INFO -
Registered differentiate_phase template
2025-10-28 10:33:43,500 - devsynth.application.edrr.templates - INFO -
Registered refine_phase template
2025-10-28 10:33:43,500 - devsynth.application.edrr.templates - INFO -
Registered retrospect_phase template
2025-10-28 10:33:43,500 - devsynth.application.edrr.manifest_parser - INFO -
Manifest Parser initialized with enhanced traceability
2025-10-28 10:33:43,500 - devsynth.application.edrr.coordinator.core - INFO -
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Explorer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
Analyzer to team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:43,501 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID e6b9da57-0f71-46b6-aea4-fefd76e1d838 in TinyDB Memory
Adapter
2025-10-28 10:33:43,501 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,501 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,501 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,502 - devsynth.domain.models.wsde_roles - INFO - Assigning
roles for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
2025-10-28 10:33:43,502 - devsynth.domain.models.wsde_roles - INFO - Selected
Explorer as primus based on expertise
2025-10-28 10:33:43,503 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID ff622cd7-c439-4166-bea2-4f4895f916fe in TinyDB Memory
Adapter
2025-10-28 10:33:43,503 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,503 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,503 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 842e80bb-e388-481d-ba27-1d2aa2a0817e in TinyDB Memory
Adapter
2025-10-28 10:33:43,503 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,503 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,503 - devsynth.application.edrr.coordinator.phase_management
- INFO - Transitioned to expand phase for task: Create a Python function to
calculate factorial
2025-10-28 10:33:43,503 - devsynth.application.edrr.coordinator.core - INFO -
Started EDRR cycle for task: Create a Python function to calculate factorial
2025-10-28 10:33:43,503 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,503 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.persistence -
ERROR - Failed to store memory item with EDRR phase: Object of type MagicMock is
not JSON serializable
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.core - ERROR -
Error executing phase expand: 'dict' object has no attribute 'name'
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.core - WARNING -
Attempting recovery from 'dict' object has no attribute 'name' in phase
Phase.EXPAND
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.core - INFO -
Executing Expand phase (recursion depth: 0)
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.persistence -
ERROR - Failed to store memory item with EDRR phase: Object of type MagicMock is
not JSON serializable
2025-10-28 10:33:43,504 - devsynth.application.edrr.coordinator.core - INFO -
Expand phase completed with 2 ideas generated (recursion depth: 0)
2025-10-28 10:33:43,504 - devsynth.application.edrr.edrr_phase_transitions -
WARNING - CoreValues is not iterable, using default values
2025-10-28 10:33:43,505 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID 12088050-1a76-46e2-97b0-4e26490688ce in TinyDB Memory
Adapter
2025-10-28 10:33:43,505 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,505 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:33:43,505 - devsynth.application.edrr.edrr_coordinator_enhanced -
INFO - Phase EXPAND metrics: {'quality': 0.48006, 'completeness':
0.10120000000000001, 'consistency': 0.5, 'conflicts': 0, 'idea_diversity': 0.5,
'duration': 0.003918}
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID e6b9da57-0f71-46b6-aea4-fefd76e1d838 in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles
for phase EXPAND in team TestErrorHandlingAtIntegrationPointsTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Explorer as primus based on expertise
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID ff622cd7-c439-4166-bea2-4f4895f916fe in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 842e80bb-e388-481d-ba27-1d2aa2a0817e in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.edrr.coordinator.phase_management:logging_setup.py:615
Transitioned to expand phase for task: Create a Python function to calculate
factorial
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Started
EDRR cycle for task: Create a Python function to calculate factorial
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.persistence:logging_setup.py:615
Failed to store memory item with EDRR phase: Object of type MagicMock is not
JSON serializable
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.core:logging_setup.py:615 Error
executing phase expand: 'dict' object has no attribute 'name'
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615
Attempting recovery from 'dict' object has no attribute 'name' in phase
Phase.EXPAND
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615
Executing Expand phase (recursion depth: 0)
ERROR    devsynth.application.edrr.coordinator.persistence:logging_setup.py:615
Failed to store memory item with EDRR phase: Object of type MagicMock is not
JSON serializable
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 Expand
phase completed with 2 ideas generated (recursion depth: 0)
WARNING  devsynth.application.edrr.edrr_phase_transitions:logging_setup.py:615
CoreValues is not iterable, using default values
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID 12088050-1a76-46e2-97b0-4e26490688ce in TinyDB Memory
Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO
devsynth.application.edrr.edrr_coordinator_enhanced:logging_setup.py:615 Phase
EXPAND metrics: {'quality': 0.48006, 'completeness': 0.10120000000000001,
'consistency': 0.5, 'conflicts': 0, 'idea_diversity': 0.5, 'duration': 0.003918}
________________________ test_real_lmstudio_integration ________________________

    def test_real_lmstudio_integration():
        """Test DevSynth with real LM Studio connection."""

        print("🔍 Testing DevSynth with Real LM Studio Integration")
        print("=" * 60)

        # First check if LM Studio is available
>       available, first_model = check_lmstudio_availability()
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_lmstudio_integration_real.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_lmstudio_integration_real.py:32: in check_lmstudio_availability
    response = requests.get("http://127.0.0.1:1234/v1/models", timeout=30)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:589: in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:703: in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/adapters.py:644: in send
    resp = conn.urlopen(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:787: in urlopen
    response = self._make_request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:493: in _make_request
    conn.request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:445: in request
    self.endheaders()
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1333: in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1093: in _send_output
    self.send(msg)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1037: in send
    self.connect()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:276: in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:198: in _new_conn
    sock = connection.create_connection(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/util/connection.py:73: in create_connection
    sock.connect(sa)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<socket.socket fd=41, family=2, type=1, proto=6, laddr=('0.0.0.0', 0)>,
('127.0.0.1', 1234))
kwargs = {}

    def guard_connect(*args: Any, **kwargs: Any) -> None:
>       raise RuntimeError("Network access disabled during tests")
E       RuntimeError: Network access disabled during tests

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:32: RuntimeError
----------------------------- Captured stdout call -----------------------------
🔍 Testing DevSynth with Real LM Studio Integration
============================================================
_____________________________ test_lmstudio_models _____________________________

    def test_lmstudio_models():
        """Test different LM Studio models if available."""

        print("\n🧪 Testing Different LM Studio Models")
        print("=" * 50)

        # Check what models are available
>       available, first_model = check_lmstudio_availability()
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_lmstudio_integration_real.py:239:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_lmstudio_integration_real.py:32: in check_lmstudio_availability
    response = requests.get("http://127.0.0.1:1234/v1/models", timeout=30)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:589: in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/sessions.py:703: in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/requests/adapters.py:644: in send
    resp = conn.urlopen(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:787: in urlopen
    response = self._make_request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connectionpool.py:493: in _make_request
    conn.request(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:445: in request
    self.endheaders()
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1333: in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1093: in _send_output
    self.send(msg)
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/http/client.py:1037: in send
    self.connect()
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:276: in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/connection.py:198: in _new_conn
    sock = connection.create_connection(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/urllib3/util/connection.py:73: in create_connection
    sock.connect(sa)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<socket.socket fd=46, family=2, type=1, proto=6, laddr=('0.0.0.0', 0)>,
('127.0.0.1', 1234))
kwargs = {}

    def guard_connect(*args: Any, **kwargs: Any) -> None:
>       raise RuntimeError("Network access disabled during tests")
E       RuntimeError: Network access disabled during tests

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:32: RuntimeError
----------------------------- Captured stdout call -----------------------------

🧪 Testing Different LM Studio Models
==================================================
__________ TestOpenAIProvider.test_init_with_default_config_succeeds ___________

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a1580>

    @pytest.mark.medium
    def test_init_with_default_config_succeeds(self):
        """Test initialization with default configuration.

        ReqID: N/A"""
        with (
            patch("devsynth.application.llm.openai_provider.OpenAI") as
mock_openai,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_openai,
        ):
            provider = OpenAIProvider({"openai_api_key": "test_key"})
>           assert provider.model == "gpt-3.5-turbo"
E           AssertionError: assert 'stub-model' == 'gpt-3.5-turbo'
E
E             - gpt-3.5-turbo
E             + stub-model

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:46,205 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:33:46,221 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
__________ TestOpenAIProvider.test_init_with_specified_model_succeeds __________

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a19d0>

    @pytest.mark.medium
    def test_init_with_specified_model_succeeds(self):
        """Test initialization with a specified model.

        ReqID: N/A"""
        with (
            patch("devsynth.application.llm.openai_provider.OpenAI") as
mock_openai,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_openai,
        ):
            provider = OpenAIProvider(
                {"openai_api_key": "test_key", "openai_model": "gpt-4"}
            )
>           assert provider.model == "gpt-4"
E           AssertionError: assert 'stub-model' == 'gpt-4'
E
E             - gpt-4
E             + stub-model

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:52: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:46,236 - devsynth.application.utils.token_tracker - WARNING -
Failed to load tiktoken encoding for model 'gpt-3.5-turbo': Network access
disabled during tests. Falling back to approximate token counting
2025-10-28 10:33:46,256 - devsynth.application.llm.openai_provider - INFO -
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to
load tiktoken encoding for model 'gpt-3.5-turbo': Network access disabled during
tests. Falling back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615
Initialized OpenAI provider with model: stub-model
______ TestOpenAIProvider.test_init_without_api_key_uses_stub_in_offline _______

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a0710>

    @pytest.mark.medium
    def test_init_without_api_key_uses_stub_in_offline(self):
        """Initialization without an API key should use no-op stub
(offline-safe).

        ReqID: N/A"""
        with (
            patch.dict(os.environ, {"DEVSYNTH_OFFLINE": "true"}, clear=False),
>           patch(
                "devsynth.application.llm.openai_provider.get_llm_settings"
            ) as mock_settings,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x12f177c80>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.openai_provider'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/openai_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ TestOpenAIProvider.test_generate_integration_succeeds _____________

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a2db0>

    @pytest.mark.medium
    def test_generate_integration_succeeds(self):
        """Integration test for generating text from OpenAI.

        ReqID: N/A"""
        with (
            patch("devsynth.application.llm.openai_provider.OpenAI") as
mock_openai,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_openai,
>           patch(
                "devsynth.application.llm.openai_provider.get_llm_settings"
            ) as mock_settings,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:167:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16da47e90>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.openai_provider'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/openai_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______ TestOpenAIProvider.test_generate_with_context_integration_succeeds ______

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a3290>

    @pytest.mark.medium
    def test_generate_with_context_integration_succeeds(self):
        """Integration test for generating text with context from OpenAI.

        ReqID: N/A"""
        with (
            patch("devsynth.application.llm.openai_provider.OpenAI") as
mock_openai,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_openai,
>           patch(
                "devsynth.application.llm.openai_provider.get_llm_settings"
            ) as mock_settings,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:209:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16d8ca1b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.openai_provider'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/openai_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________ TestOpenAIProvider.test_get_embedding_integration_succeeds __________

self = <tests.integration.general.test_openai_provider.TestOpenAIProvider object
at 0x1285a3770>

    @pytest.mark.medium
    def test_get_embedding_integration_succeeds(self):
        """Integration test for getting embeddings from OpenAI.

        ReqID: N/A"""
        with (
            patch("devsynth.application.llm.openai_provider.OpenAI") as
mock_openai,
            patch(
                "devsynth.application.llm.openai_provider.AsyncOpenAI"
            ) as mock_async_openai,
>           patch(
                "devsynth.application.llm.openai_provider.get_llm_settings"
            ) as mock_settings,
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_openai_provider.py:264:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x16de56f30>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.openai_provider'
from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/openai_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______ TestProjectStateAnalyzer.test_analyze_devsynth_project_succeeds ________

self =
<tests.integration.general.test_project_state_analyzer.TestProjectStateAnalyzer
object at 0x1285a8320>

    @pytest.mark.medium
    def test_analyze_devsynth_project_succeeds(self):
        """Test analyzing the DevSynth project itself.

        ReqID: N/A"""
        project_root = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "..")
        )
        analyzer = ProjectStateAnalyzer(project_root)
        report = analyzer.analyze()
>       assert "project_path" in report
E       AssertionError: assert 'project_path' in {'architecture': {'components':
[{'name': '__init__', 'path': 'unit/domain/__init__.py', 'type': 'Domain
Entity'}, {'n..., ...], 'confidence': 0.9, 'type': 'Hexagonal'}, 'code_count':
109, 'file_count': 1886, 'health_score': 0.0, ...}, ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_project_state_analyzer.py:34: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,639 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
2025-10-28 10:33:56,639 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:56,706 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 1886
files
2025-10-28 10:33:56,706 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:56,707 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:56,707 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:56,711 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferred
architecture: Hexagonal (confidence: 0.90)
2025-10-28 10:33:56,712 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:56,721 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from behavior/steps/test_requirement_analysis_steps.py:
'NoneType' object has no attribute 'strip'
2025-10-28 10:33:56,721 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from behavior/steps/test_requirements_wizard_steps.py:
'NoneType' object has no attribute 'strip'
2025-10-28 10:33:56,721 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from
behavior/steps/test_webui_requirements_wizard_with_wizardstate_steps.py:
'NoneType' object has no attribute 'strip'
2025-10-28 10:33:56,723 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from
behavior/steps/test_requirements_gathering_steps.py: 'NoneType' object has no
attribute 'strip'
2025-10-28 10:33:56,727 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from unit/application/requirements/test_interactions.py:
'NoneType' object has no attribute 'strip'
2025-10-28 10:33:56,727 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting requirements from
unit/application/requirements/test_dialectical_reasoner_parsing_payloads.py:
'NoneType' object has no attribute 'strip'
2025-10-28 10:33:56,745 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting specifications from
unit/specifications/test_mvuu_config_schema_validation.py: 'NoneType' object has
no attribute 'strip'
2025-10-28 10:33:56,747 -
devsynth.application.code_analysis.project_state_analyzer - INFO -
Requirements-specification alignment score: 0.00
2025-10-28 10:33:56,747 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:56,749 -
devsynth.application.code_analysis.project_state_analyzer - ERROR - Error
extracting specifications from
unit/specifications/test_mvuu_config_schema_validation.py: 'NoneType' object has
no attribute 'strip'
2025-10-28 10:33:56,750 -
devsynth.application.code_analysis.project_state_analyzer - INFO -
Specification-code implementation score: 0.00
2025-10-28 10:33:56,750 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:56,750 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 1886 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferred architecture: Hexagonal (confidence: 0.90)
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
behavior/steps/test_requirement_analysis_steps.py: 'NoneType' object has no
attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
behavior/steps/test_requirements_wizard_steps.py: 'NoneType' object has no
attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
behavior/steps/test_webui_requirements_wizard_with_wizardstate_steps.py:
'NoneType' object has no attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
behavior/steps/test_requirements_gathering_steps.py: 'NoneType' object has no
attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
unit/application/requirements/test_interactions.py: 'NoneType' object has no
attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting requirements from
unit/application/requirements/test_dialectical_reasoner_parsing_payloads.py:
'NoneType' object has no attribute 'strip'
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting specifications from
unit/specifications/test_mvuu_config_schema_validation.py: 'NoneType' object has
no attribute 'strip'
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Requirements-specification alignment score: 0.00
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
ERROR
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Error extracting specifications from
unit/specifications/test_mvuu_config_schema_validation.py: 'NoneType' object has
no attribute 'strip'
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Specification-code implementation score: 0.00
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
______ TestProjectStateAnalyzer.test_analyze_with_missing_files_succeeds _______

self =
<tests.integration.general.test_project_state_analyzer.TestProjectStateAnalyzer
object at 0x1285a8770>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_analyze_with_missing_file0')

    @pytest.mark.medium
    def test_analyze_with_missing_files_succeeds(self, tmp_path):
        """Test analyzing a project with missing files.

        ReqID: N/A"""
        project_dir = tmp_path / "empty_project"
        project_dir.mkdir()
        analyzer = ProjectStateAnalyzer(str(project_dir))
        report = analyzer.analyze()
>       assert "project_path" in report
E       AssertionError: assert 'project_path' in {'architecture': {'components':
[], 'confidence': 0.0, 'type': 'Unknown'}, 'components': [], 'files': {},
'health_repo...mponents': [], 'confidence': 0.0, 'type': 'Unknown'},
'code_count': 0, 'file_count': 0, 'health_score': 0.0, ...}, ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_project_state_analyzer.py:79: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_analyze_with_missing_file0/empty_project
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 0
files
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages:
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Could not
confidently infer architecture
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:56,763 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_analyze_with_missing_file0/empty_project
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 0 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages:
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Could not confidently infer architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
__ TestProjectStateAnalyzer.test_analyze_with_requirements_and_code_succeeds ___

self =
<tests.integration.general.test_project_state_analyzer.TestProjectStateAnalyzer
object at 0x1285a8c20>
tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_analyze_with_requirements0')

    @pytest.mark.medium
    def test_analyze_with_requirements_and_code_succeeds(self, tmp_path):
        """Test analyzing a project with requirements and code but no
specifications.

        ReqID: N/A"""
        project_dir = tmp_path / "partial_project"
        project_dir.mkdir()
        requirements_dir = project_dir / "docs"
        requirements_dir.mkdir()
        requirements_file = requirements_dir / "requirements.md"
        requirements_file.write_text(
            """
        # Project Requirements

        ## User Authentication
        1. The system shall provide user registration functionality
        2. The system shall support login with username and password
        3. The system shall implement password reset via email

        ## Data Management
        1. The system shall store user data securely
        2. The system shall provide CRUD operations for user profiles
        """
        )
        src_dir = project_dir / "src"
        src_dir.mkdir()
        code_file = src_dir / "user.py"
        code_file.write_text(
            """
        class User:
            def __init__(self, username, password):
                self.username = username
                self.password = password

            def register(self):
                # Implementation for user registration
                pass

            def login(self):
                # Implementation for user login
                pass

            def reset_password(self):
                # Implementation for password reset
                pass
        """
        )
        analyzer = ProjectStateAnalyzer(str(project_dir))
        report = analyzer.analyze()
>       assert "project_path" in report
E       AssertionError: assert 'project_path' in {'architecture': {'components':
[], 'confidence': 0.0, 'type': 'Unknown'}, 'components': [], 'files':
{'docs/requireme...mponents': [], 'confidence': 0.0, 'type': 'Unknown'},
'code_count': 1, 'file_count': 2, 'health_score': 0.0, ...}, ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_project_state_analyzer.py:175: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,775 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting
project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_analyze_with_requirements0/partial_project
2025-10-28 10:33:56,775 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexing
project files
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Indexed 2
files
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detecting
programming languages
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Detected
languages: Python
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Inferring
project architecture
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Could not
confidently infer architecture
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
requirements-specification alignment
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
requirements-specification alignment: missing files
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Analyzing
specification-code alignment
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Skipping
specification-code alignment: no specification files
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Generating
project health report
2025-10-28 10:33:56,776 -
devsynth.application.code_analysis.project_state_analyzer - INFO - Project
health score: 0.00
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Starting project analysis for
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_analyze_with_requirements0/partial_project
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexing project files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Indexed 2 files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detecting programming languages
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Detected languages: Python
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Inferring project architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Could not confidently infer architecture
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing requirements-specification alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping requirements-specification alignment: missing files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Analyzing specification-code alignment
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Skipping specification-code alignment: no specification files
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Generating project health report
INFO
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615
Project health score: 0.00
_
TestProviderConfigurations.test_openai_provider_with_different_models_has_expect
ed _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285cc620>
mock_openai_response = {'choices': [{'message': {'content': 'This is a mock
response from OpenAI'}}]}

    @responses.activate
    @pytest.mark.medium
    def test_openai_provider_with_different_models_has_expected(
        self, mock_openai_response
    ):
        """Test OpenAI provider with different models.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "https://api.openai.com/v1/chat/completions",
            json=mock_openai_response,
            status=200,
        )
        with patch.dict(
            os.environ, {"OPENAI_API_KEY": "test_key", "OPENAI_MODEL": "gpt-4"}
        ):
            provider = ProviderFactory.create_provider(ProviderType.OPENAI)
>           assert provider.model == "gpt-4"
                   ^^^^^^^^^^^^^^
E           AttributeError: 'StubProvider' object has no attribute 'model'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:72: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,901 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_
TestProviderConfigurations.test_openai_provider_with_different_parameters_has_ex
pected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285cca70>
mock_openai_response = {'choices': [{'message': {'content': 'This is a mock
response from OpenAI'}}]}

    @responses.activate
    @pytest.mark.medium
    def test_openai_provider_with_different_parameters_has_expected(
        self, mock_openai_response
    ):
        """Test OpenAI provider with different parameters.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "https://api.openai.com/v1/chat/completions",
            json=mock_openai_response,
            status=200,
        )
        with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
            provider = ProviderFactory.create_provider(ProviderType.OPENAI)
            result = provider.complete("Test prompt")
>           assert result == "This is a mock response from OpenAI"
E           AssertionError: assert '[stub:stub-llm] Test prompt' == 'This is a
mo...e from OpenAI'
E
E             - This is a mock response from OpenAI
E             + [stub:stub-llm] Test prompt

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:107: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,914 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_
TestProviderConfigurations.test_lm_studio_provider_with_different_endpoints_has_
expected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285aa150>
mock_lm_studio_response = {'choices': [{'text': 'This is a mock response from LM
Studio'}]}

    @responses.activate
    @pytest.mark.medium
    def test_lm_studio_provider_with_different_endpoints_has_expected(
        self, mock_lm_studio_response
    ):
        """Test LM Studio provider with different endpoints.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "http://127.0.0.1:1234/v1/chat/completions",
            json=mock_lm_studio_response,
            status=200,
        )
        with patch.dict(os.environ, {"LM_STUDIO_ENDPOINT":
"http://127.0.0.1:1234"}):
            provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
            result = provider.complete("Test prompt")
>           assert result == "This is a mock response from LM Studio"
E           AssertionError: assert '[stub:stub-llm] Test prompt' == 'This is a
mo...rom LM Studio'
E
E             - This is a mock response from LM Studio
E             + [stub:stub-llm] Test prompt

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:140: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,934 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_
TestProviderConfigurations.test_lm_studio_provider_with_different_parameters_has
_expected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285cd2e0>
mock_lm_studio_response = {'choices': [{'text': 'This is a mock response from LM
Studio'}]}

    @responses.activate
    @pytest.mark.medium
    def test_lm_studio_provider_with_different_parameters_has_expected(
        self, mock_lm_studio_response
    ):
        """Test LM Studio provider with different parameters.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "http://127.0.0.1:1234/v1/chat/completions",
            json=mock_lm_studio_response,
            status=200,
        )
        with patch.dict(os.environ, {"LM_STUDIO_ENDPOINT":
"http://127.0.0.1:1234"}):
            provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
            result = provider.complete("Test prompt")
>           assert result == "This is a mock response from LM Studio"
E           AssertionError: assert '[stub:stub-llm] Test prompt' == 'This is a
mo...rom LM Studio'
E
E             - This is a mock response from LM Studio
E             + [stub:stub-llm] Test prompt

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:172: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,954 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_
TestProviderConfigurations.test_fallback_provider_with_different_configurations_
has_expected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285cd7f0>
mock_openai_response = {'choices': [{'message': {'content': 'This is a mock
response from OpenAI'}}]}
mock_lm_studio_response = {'choices': [{'text': 'This is a mock response from LM
Studio'}]}

    @responses.activate
    @pytest.mark.medium
    def test_fallback_provider_with_different_configurations_has_expected(
        self, mock_openai_response, mock_lm_studio_response
    ):
        """Test fallback provider with different configurations.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "https://api.openai.com/v1/chat/completions",
            json=mock_openai_response,
            status=200,
        )
        responses.add(
            responses.POST,
            "http://127.0.0.1:1234/v1/chat/completions",
            json=mock_lm_studio_response,
            status=200,
        )
        with patch.dict(
            os.environ,
            {
                "OPENAI_API_KEY": "test_key",
                "LM_STUDIO_ENDPOINT": "http://127.0.0.1:1234",
            },
        ):
            openai_provider =
ProviderFactory.create_provider(ProviderType.OPENAI)
            lm_studio_provider =
ProviderFactory.create_provider(ProviderType.LMSTUDIO)
            fallback_provider = FallbackProvider(
                providers=[openai_provider, lm_studio_provider]
            )
            result = fallback_provider.complete("Test prompt")
>           assert result == "This is a mock response from OpenAI"
E           AssertionError: assert '[stub:stub-llm] Test prompt' == 'This is a
mo...e from OpenAI'
E
E             - This is a mock response from OpenAI
E             + [stub:stub-llm] Test prompt

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:221: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,971 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
2025-10-28 10:33:56,971 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
2025-10-28 10:33:56,971 - devsynth.adapters.provider_system - INFO - Initialized
fallback provider order: StubProvider, StubProvider
2025-10-28 10:33:56,971 - devsynth.adapters.provider_system - INFO - Trying
completion with provider: StubProvider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Initialized
fallback provider order: StubProvider, StubProvider
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Trying
completion with provider: StubProvider
_
TestProviderConfigurations.test_provider_system_with_different_default_providers
_has_expected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285cdd00>
mock_openai_response = {'choices': [{'message': {'content': 'This is a mock
response from OpenAI'}}]}
mock_lm_studio_response = {'choices': [{'text': 'This is a mock response from LM
Studio'}]}

    @responses.activate
    @pytest.mark.medium
    def test_provider_system_with_different_default_providers_has_expected(
        self, mock_openai_response, mock_lm_studio_response
    ):
        """Test provider system with different default providers.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "https://api.openai.com/v1/chat/completions",
            json=mock_openai_response,
            status=200,
        )
        with patch.dict(
            os.environ, {"DEVSYNTH_PROVIDER": "openai", "OPENAI_API_KEY":
"test_key"}
        ):
            provider = get_provider()
>           assert isinstance(provider, OpenAIProvider)
E           assert False
E            +  where False =
isinstance(<devsynth.adapters.provider_system.StubProvider object at
0x16d7c9d00>, OpenAIProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:268: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:56,988 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_
TestProviderConfigurations.test_provider_system_with_context_aware_completion_ha
s_expected _

self =
<tests.integration.general.test_provider_system_configurations.TestProviderConfi
gurations object at 0x1285ce210>
mock_openai_response = {'choices': [{'message': {'content': 'This is a mock
response from OpenAI'}}]}

    @responses.activate
    @pytest.mark.medium
    def test_provider_system_with_context_aware_completion_has_expected(
        self, mock_openai_response
    ):
        """Test provider system with context-aware completion.

        ReqID: N/A"""
        responses.add(
            responses.POST,
            "https://api.openai.com/v1/chat/completions",
            json=mock_openai_response,
            status=200,
        )
        with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
            provider = ProviderFactory.create_provider(ProviderType.OPENAI)
>           result = provider.complete_with_context(
                "Test prompt",
                [
                    {"role": "system", "content": "You are a helpful
assistant."},
                    {"role": "user", "content": "What is the capital of
France?"},
                    {"role": "assistant", "content": "The capital of France is
Paris."},
                ],
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_provider_system_configurations.py:306:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.adapters.provider_system.StubProvider object at 0x16e07a000>
prompt = 'Test prompt'
context = [{'content': 'You are a helpful assistant.', 'role': 'system'},
{'content': 'What is the capital of France?', 'role': 'user'}, {'content': 'The
capital of France is Paris.', 'role': 'assistant'}]

    def complete_with_context(
        self,
        prompt: str,
        context: list[dict[str, str]],
        *,
        parameters: dict[str, Any] | None = None,
    ) -> str:
        """Generate a completion given a chat ``context``."""
>       raise NotImplementedError("Subclasses must implement
complete_with_context()")
E       NotImplementedError: Subclasses must implement complete_with_context()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/provi
der_system.py:543: NotImplementedError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,010 - devsynth.adapters.provider_system - INFO - Falling
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_________________ TestQueryRouterIntegration.test_direct_query _________________

self =
<tests.integration.general.test_query_router_integration.TestQueryRouterIntegrat
ion object at 0x1285cecc0>
manager = <devsynth.application.memory.memory_manager.MemoryManager object at
0x16d526240>

    @pytest.mark.medium
    def test_direct_query(self, manager):
        self._populate(manager)
        results = manager.route_query("apple", store="vector",
strategy="direct")
>       assert len(results) == 1
E       AssertionError: assert 2 == 1
E        +  where 2 = len({'records': [MemoryRecord(item=MemoryItem(id='v1',
content='apple vector', memory_type=<MemoryType.CODE: 'code'>,
meta...'memory_type': 'code', 'embedding': [20.0, 28.666666666666668, 17.75,
17.25, 18.083333333333332]})], 'store': 'vector'})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_query_router_integration.py:72: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:57,056 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:33:57,056 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: vector, tinydb, document
2025-10-28 10:33:57,056 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: vector, tinydb, document
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,057 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID v1 in Vector Memory Adapter
2025-10-28 10:33:57,057 - devsynth.application.memory.memory_manager - INFO -
Searching memory with query: apple, type: None, filter: None, limit: 10
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID v1 in Vector Memory Adapter
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615
Searching memory with query: apple, type: None, filter: None, limit: 10
______________ TestQueryRouterIntegration.test_cross_store_query _______________

self =
<tests.integration.general.test_query_router_integration.TestQueryRouterIntegrat
ion object at 0x1285cf110>
manager = <devsynth.application.memory.memory_manager.MemoryManager object at
0x16d8255e0>

    @pytest.mark.medium
    def test_cross_store_query(self, manager):
        self._populate(manager)
        results = manager.route_query("apple", strategy="cross")
>       assert set(results.keys()) >= {"vector", "tinydb", "document"}
E       AssertionError: assert {'by_store', 'query'} >= {'document', ...db',
'vector'}
E
E         Extra items in the right set:
E         'tinydb'
E         'vector'
E         'document'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_query_router_integration.py:80: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:57,069 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:33:57,069 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: vector, tinydb, document
2025-10-28 10:33:57,069 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: vector, tinydb, document
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,070 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID v1 in Vector Memory Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID v1 in Vector Memory Adapter
___________ TestQueryRouterIntegration.test_cross_store_query_subset ___________

self =
<tests.integration.general.test_query_router_integration.TestQueryRouterIntegrat
ion object at 0x1285cf5f0>
manager = <devsynth.application.memory.memory_manager.MemoryManager object at
0x16e0002c0>

    @pytest.mark.medium
    def test_cross_store_query_subset(self, manager):
        self._populate(manager)
        results = manager.route_query(
            "apple", strategy="cross", stores=["vector", "tinydb"]
        )
>       assert set(results.keys()) == {"vector", "tinydb"}
E       AssertionError: assert {'by_store', 'query'} == {'tinydb', 'vector'}
E
E         Extra items in the left set:
E         'query'
E         'by_store'
E         Extra items in the right set:
E         'tinydb'
E         'vector'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_query_router_integration.py:91: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:57,092 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:33:57,093 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: vector, tinydb, document
2025-10-28 10:33:57,093 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: vector, tinydb, document
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,093 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID v1 in Vector Memory Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID v1 in Vector Memory Adapter
_______________ TestQueryRouterIntegration.test_federated_query ________________

self =
<tests.integration.general.test_query_router_integration.TestQueryRouterIntegrat
ion object at 0x1285e40b0>
manager = <devsynth.application.memory.memory_manager.MemoryManager object at
0x16e0039b0>

    @pytest.mark.medium
    def test_federated_query(self, manager):
        self._populate(manager)
        results = manager.route_query("apple", strategy="federated")
>       assert len(results) == 3
E       AssertionError: assert 5 == 3
E        +  where 5 = len([MemoryRecord(item=MemoryItem(id='', content='apple',
memory_type=<MemoryType.CONTEXT: 'context'>, metadata={}, create...={},
created_at=datetime.datetime(2025, 10, 28, 10, 33, 57, 106746)),
similarity=None, source='by_store', metadata={})])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_query_router_integration.py:97: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:33:57,106 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Vector
Memory Adapter initialized
2025-10-28 10:33:57,106 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: vector, tinydb, document
2025-10-28 10:33:57,106 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Vector Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: vector, tinydb, document
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,106 -
devsynth.application.memory.adapters.vector_memory_adapter - INFO - Stored
memory vector with ID v1 in Vector Memory Adapter
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.vector_memory_adapter:logging_setup.py:615
Stored memory vector with ID v1 in Vector Memory Adapter
___________ TestSelfAnalyzer.test_analyze_devsynth_codebase_succeeds ___________

self = <tests.integration.general.test_self_analyzer.TestSelfAnalyzer object at
0x1285e7dd0>

    @codebase_available
    @pytest.mark.medium
    def test_analyze_devsynth_codebase_succeeds(self):
        """Test that the SelfAnalyzer can analyze the DevSynth codebase.

        ReqID: N/A"""
        project_root = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "..")
        )
        analyzer = SelfAnalyzer(project_root)
        result = analyzer.analyze()
        assert "code_analysis" in result
        assert "insights" in result
        insights = result["insights"]
        assert "metrics_summary" in insights
        assert "architecture" in insights
        assert "code_quality" in insights
        assert "test_coverage" in insights
        assert "improvement_opportunities" in insights
        architecture = insights["architecture"]
        assert "layers" in architecture
        layers = architecture["layers"]
>       assert "domain" in layers
E       AssertionError: assert 'domain' in {'acceptance': ['interface'],
'backend_subset': ['test_chromadb_store_import.py', '__init__.py',
'test_optional_backen..._tests_cli.py', 'test_cross_interface_consistency.py',
...], 'cli': ['test_devsynth_cli_smoke.py', '__init__.py'], ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_self_analyzer.py:45: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:33:57,369 - devsynth.application.code_analysis.self_analyzer -
INFO - SelfAnalyzer initialized with project root:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
2025-10-28 10:33:57,369 - devsynth.application.code_analysis.self_analyzer -
INFO - Starting analysis of project at
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
2025-10-28 10:34:00,065 - devsynth.application.code_analysis.self_analyzer -
INFO - Generating insights from code analysis
2025-10-28 10:34:00,065 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing architecture
2025-10-28 10:34:00,065 - devsynth.application.code_analysis.self_analyzer -
INFO - Detecting architecture type
2025-10-28 10:34:00,087 - devsynth.application.code_analysis.self_analyzer -
INFO - Detected architecture type: Hexagonal (confidence: 1.00)
2025-10-28 10:34:00,088 - devsynth.application.code_analysis.self_analyzer -
INFO - Identifying layers
2025-10-28 10:34:00,108 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing layer dependencies
2025-10-28 10:34:00,159 - devsynth.application.code_analysis.self_analyzer -
INFO - Checking for Hexagonal architecture violations
2025-10-28 10:34:00,159 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing code quality
2025-10-28 10:34:00,166 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing test coverage
2025-10-28 10:34:00,173 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
2025-10-28 10:34:04,371 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior
2025-10-28 10:34:05,121 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/requirements
_wizard
2025-10-28 10:34:05,132 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps
2025-10-28 10:34:06,224 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit
2025-10-28 10:34:07,366 - devsynth.application.code_analysis.analyzer - ERROR -
Error analyzing code: Test timed out after 10 seconds
(DEVSYNTH_TEST_TIMEOUT_SECONDS)
2025-10-28 10:34:08,131 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior
2025-10-28 10:34:08,147 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/devsynth
2025-10-28 10:34:08,151 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface
2025-10-28 10:34:08,514 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui
2025-10-28 10:34:08,521 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm
2025-10-28 10:34:08,556 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/specifications
2025-10-28 10:34:08,557 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core
2025-10-28 10:34:08,585 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/mvu
2025-10-28 10:34:08,588 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory
2025-10-28 10:34:08,595 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fallback
2025-10-28 10:34:08,617 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/config
2025-10-28 10:34:08,630 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security
2025-10-28 10:34:08,656 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general
2025-10-28 10:34:08,873 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/providers
2025-10-28 10:34:08,923 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology
2025-10-28 10:34:08,982 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/edrr
2025-10-28 10:34:09,038 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/agents
2025-10-28 10:34:09,055 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/utils
2025-10-28 10:34:09,064 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/requirements
2025-10-28 10:34:09,067 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/docs
2025-10-28 10:34:09,069 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli
2025-10-28 10:34:09,091 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fakes
2025-10-28 10:34:09,092 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/policies
2025-10-28 10:34:09,094 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/integrations
2025-10-28 10:34:09,100 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing
2025-10-28 10:34:09,274 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters
2025-10-28 10:34:09,801 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/llm
2025-10-28 10:34:09,806 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/memory
2025-10-28 10:34:09,819 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/provide
rs
2025-10-28 10:34:09,825 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli
2025-10-28 10:34:09,828 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/issues
2025-10-28 10:34:09,829 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/retrieval
2025-10-28 10:34:09,829 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment
2025-10-28 10:34:09,838 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts
2025-10-28 10:34:09,859 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration
2025-10-28 10:34:09,864 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/api
2025-10-28 10:34:09,866 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application
2025-10-28 10:34:10,541 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration
2025-10-28 10:34:10,596 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/docu
mentation
2025-10-28 10:34:10,608 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/code
_analysis
2025-10-28 10:34:10,643 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/inge
stion
2025-10-28 10:34:10,645 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm
2025-10-28 10:34:10,667 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry
2025-10-28 10:34:10,869 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/conf
ig
2025-10-28 10:34:10,872 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/agen
ts
2025-10-28 10:34:10,900 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/util
s
2025-10-28 10:34:10,901 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements
2025-10-28 10:34:10,911 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
2025-10-28 10:34:11,053 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands
2025-10-28 10:34:11,138 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/prom
ises
2025-10-28 10:34:11,148 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing
2025-10-28 10:34:11,168 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/orch
estration
2025-10-28 10:34:11,169 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/prom
pts
2025-10-28 10:34:11,172 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
2025-10-28 10:34:11,296 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator
2025-10-28 10:34:11,318 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/wsde
2025-10-28 10:34:11,320 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/spri
nt
2025-10-28 10:34:11,321 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/infrastructure
2025-10-28 10:34:11,322 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain
2025-10-28 10:34:11,401 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models
2025-10-28 10:34:11,456 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/interface
s
2025-10-28 10:34:11,457 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging
2025-10-28 10:34:11,521 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/backend_subset
2025-10-28 10:34:11,524 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/property
2025-10-28 10:34:11,555 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration
2025-10-28 10:34:11,889 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/generated
2025-10-28 10:34:11,891 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion
2025-10-28 10:34:11,896 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
2025-10-28 10:34:11,919 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm
2025-10-28 10:34:11,967 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/memory
2025-10-28 10:34:11,992 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/config
2025-10-28 10:34:11,993 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general
2025-10-28 10:34:12,187 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/sprint_ed
rr
2025-10-28 10:34:12,188 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/agents/te
st_generation
2025-10-28 10:34:12,189 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/utils
2025-10-28 10:34:12,190 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/adapters
2025-10-28 10:34:12,190 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/generated
_tests
2025-10-28 10:34:12,191 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/deploymen
t
2025-10-28 10:34:12,193 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/live
2025-10-28 10:34:12,194 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/mvu
2025-10-28 10:34:12,196 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/edrr
2025-10-28 10:34:12,198 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/api
2025-10-28 10:34:12,199 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/templates
2025-10-28 10:34:12,201 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/webui
2025-10-28 10:34:12,202 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/wsde
2025-10-28 10:34:12,203 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/cli
2025-10-28 10:34:12,207 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/acceptance/interface
2025-10-28 10:34:12,211 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/examples
2025-10-28 10:34:12,214 - devsynth.application.code_analysis.self_analyzer -
INFO - Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/performance
2025-10-28 10:34:12,221 - devsynth.application.code_analysis.self_analyzer -
INFO - Identifying improvement opportunities
2025-10-28 10:34:12,222 - devsynth.application.code_analysis.self_analyzer -
INFO - Self-analysis completed
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
SelfAnalyzer initialized with project root:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Starting analysis of project at
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Generating insights from code analysis
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing architecture
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Detecting architecture type
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Detected architecture type: Hexagonal (confidence: 1.00)
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Identifying layers
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing layer dependencies
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Checking for Hexagonal architecture violations
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing code quality
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing test coverage
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/requirements
_wizard
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit
ERROR    devsynth.application.code_analysis.analyzer:logging_setup.py:615 Error
analyzing code: Test timed out after 10 seconds (DEVSYNTH_TEST_TIMEOUT_SECONDS)
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/devsynth
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/specifications
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/mvu
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fallback
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/config
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/providers
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/edrr
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/agents
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/utils
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/requirements
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/docs
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/fakes
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/policies
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/integrations
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/llm
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/memory
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/provide
rs
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/cli
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/issues
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/retrieval
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/api
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/docu
mentation
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/code
_analysis
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/inge
stion
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/conf
ig
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/agen
ts
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/util
s
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/prom
ises
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/orch
estration
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/prom
pts
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/wsde
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/spri
nt
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/infrastructure
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/interface
s
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/backend_subset
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/property
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/generated
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/collabora
tion
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/memory
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/config
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/sprint_ed
rr
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/agents/te
st_generation
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/utils
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/adapters
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/generated
_tests
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/deploymen
t
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/live
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/mvu
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/edrr
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/api
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/templates
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/webui
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/wsde
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/cli
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/acceptance/interface
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/examples
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Analyzing tests in
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/performance
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Identifying improvement opportunities
INFO     devsynth.application.code_analysis.self_analyzer:logging_setup.py:615
Self-analysis completed
_
TestWSDEEDRRComponentInteractions.test_retrospective_flushes_pending_memory_with
out_record _

self =
<tests.integration.general.test_wsde_edrr_component_interactions.TestWSDEEDRRCom
ponentInteractions object at 0x12861a090>
memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x336913530>

    @pytest.mark.medium
    def test_retrospective_flushes_pending_memory_without_record(self,
memory_manager):
        """Ensure retrospective flushes memory even without record method.

        ReqID: N/A"""

        team = WSDE(name="FlushTeam")
        team.memory_manager = memory_manager
        coordinator_agent = WSDETeamCoordinatorAgent(team)

        memory_manager.queue_update(
            "default",
            MemoryItem(
                id="queued-item",
                content={},
                memory_type=MemoryType.TEAM_STATE,
            ),
        )
        assert memory_manager.sync_manager._queue  # ensure queue populated

        coordinator_agent.run_retrospective(
            [{"positives": [], "improvements": [], "action_items": []}],
            sprint=1,
        )

>       assert memory_manager.sync_manager._queue == []
E       AssertionError: assert [{'record': M...on_id': None}] == []
E
E         Left contains one more item: {'record':
MemoryRecord(item=MemoryItem(id='queued-item', content={},
memory_type=<MemoryType.TEAM_STATE: 'team_state'... 28, 10, 34, 45, 258726)),
similarity=None, source='default', metadata={}), 'store': 'default',
'transaction_id': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_wsde_edrr_component_interactions.py:376: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:34:45,257 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:34:45,257 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: default
2025-10-28 10:34:45,257 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: default
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:45,258 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
--------------------------- Captured stdout teardown ---------------------------
2025-10-28 10:34:45,272 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored
memory item with ID queued-item in TinyDB Memory Adapter
2025-10-28 10:34:45,272 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,272 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
---------------------------- Captured log teardown -----------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
Stored memory item with ID queued-item in TinyDB Memory Adapter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
______________________ test_peer_review_stores_in_memory _______________________

wsde_team =
<devsynth.application.collaboration.collaborative_wsde_team.CollaborativeWSDETea
m object at 0x3369d4dd0>
memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x3369d4e60>
memory_adapter =
<tests.integration.general.test_wsde_peer_review_memory_integration.MockMemoryAd
apter object at 0x3369d4f50>

    @pytest.mark.medium
    def test_peer_review_stores_in_memory(wsde_team, memory_manager,
memory_adapter):
        """Test that the peer review stores data in memory."""
        # Get author and reviewers
        author = wsde_team.agents[0]
        reviewers = wsde_team.agents[1:]

        # Create a work product
        work_product = {"text": "Test work product"}

        # Request peer review
        review = wsde_team.request_peer_review(work_product, author, reviewers)

        # Verify that the review is stored in memory during assign_reviews
        assert len(memory_adapter.store_calls) > 0

        # Find the stored review
        stored_review = None
        for item in memory_adapter.items.values():
            if item.memory_type == MemoryType.PEER_REVIEW:
                stored_review = item
                break

>       assert stored_review is not None
E       assert None is not None

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_wsde_peer_review_memory_integration.py:136: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:34:45,326 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb
2025-10-28 10:34:45,326 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:34:45,326 - devsynth.domain.models.wsde_core - INFO - Added agent
author to team test_team
2025-10-28 10:34:45,326 - devsynth.domain.models.wsde_core - INFO - Added agent
reviewer0 to team test_team
2025-10-28 10:34:45,326 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:34:45,326 - devsynth.domain.models.wsde_core - INFO - Added agent
reviewer1 to team test_team
2025-10-28 10:34:45,326 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': 'reviewer1', 'designer': None, 'evaluator': None}
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
author to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
reviewer0 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
reviewer1 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
'reviewer1', 'designer': None, 'evaluator': None}
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:45,327 - devsynth.domain.models.wsde_roles - INFO - Selected
reviewer0 as primus based on expertise
2025-10-28 10:34:45,327 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': 'reviewer0', 'designer': 'reviewer1', 'evaluator': None}
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,327 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
reviewer0 as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
'reviewer0', 'designer': 'reviewer1', 'evaluator': None}
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
_______________________ test_cross_store_synchronization _______________________

wsde_team =
<devsynth.application.collaboration.collaborative_wsde_team.CollaborativeWSDETea
m object at 0x3369e3410>
memory_manager = <devsynth.application.memory.memory_manager.MemoryManager
object at 0x3369e34a0>

    @pytest.mark.medium
    def test_cross_store_synchronization(wsde_team, memory_manager):
        """Test that peer review results persist across multiple memory
stores."""
        second_adapter = MockMemoryAdapter()
        memory_manager.adapters["graph"] = second_adapter

        author = wsde_team.agents[0]
        reviewers = wsde_team.agents[1:]
        work_product = {"text": "Test work product"}

        review = wsde_team.request_peer_review(work_product, author, reviewers)
        review.collect_reviews()
        review.finalize(approved=True)

        def has_review(adapter):
            return any(
                item.memory_type == MemoryType.PEER_REVIEW
                for item in adapter.items.values()
            )

>       assert has_review(memory_manager.adapters["tinydb"])
E       assert False
E        +  where False = <function
test_cross_store_synchronization.<locals>.has_review at
0x16d8beac0>(<tests.integration.general.test_wsde_peer_review_memory_integration
.MockMemoryAdapter object at 0x3369e3590>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/general/t
est_wsde_peer_review_memory_integration.py:210: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-28 10:34:45,375 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb
2025-10-28 10:34:45,375 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
2025-10-28 10:34:45,376 - devsynth.domain.models.wsde_core - INFO - Added agent
author to team test_team
2025-10-28 10:34:45,376 - devsynth.domain.models.wsde_core - INFO - Added agent
reviewer0 to team test_team
2025-10-28 10:34:45,376 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-28 10:34:45,377 - devsynth.domain.models.wsde_core - INFO - Added agent
reviewer1 to team test_team
2025-10-28 10:34:45,377 - devsynth.domain.models.wsde_roles - INFO - Role
assignments for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': 'reviewer1', 'designer': None, 'evaluator': None}
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
author to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
reviewer0 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
reviewer1 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
'reviewer1', 'designer': None, 'evaluator': None}
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:45,378 - devsynth.domain.models.wsde_roles - INFO - Selected
reviewer0 as primus based on expertise
2025-10-28 10:34:45,378 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team test_team: {'primus': 'author', 'worker': 'reviewer0',
'supervisor': 'reviewer0', 'designer': 'reviewer1', 'evaluator': None}
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,379 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,380 - devsynth.domain.models.wsde_roles - INFO - Rotated
roles for team test_team: {'primus': 'reviewer1', 'worker': 'author',
'supervisor': 'reviewer0', 'designer': 'reviewer0', 'evaluator': None}
2025-10-28 10:34:45,380 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6
2025-10-28 10:34:45,380 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,380 - devsynth.domain.models.wsde_utils - INFO - Added
solution for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6
2025-10-28 10:34:45,381 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,381 - devsynth.domain.models.wsde_core - INFO - Building
consensus for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6: Untitled
2025-10-28 10:34:45,381 - devsynth.application.collaboration.peer_review -
WARNING - Error building consensus: Invalid mapping for MessageFilter
2025-10-28 10:34:45,381 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,381 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,382 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,383 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,383 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,383 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,383 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,383 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,384 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,384 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,384 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,384 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
2025-10-28 10:34:45,384 - devsynth.application.memory.tiered_cache - INFO -
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
reviewer0 as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team test_team: {'primus': 'author', 'worker': 'reviewer0', 'supervisor':
'reviewer0', 'designer': 'reviewer1', 'evaluator': None}
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles
for team test_team: {'primus': 'reviewer1', 'worker': 'author', 'supervisor':
'reviewer0', 'designer': 'reviewer0', 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Building
consensus for task 7d2d0d40-2265-44e5-b069-ef1894af8ef6: Untitled
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615
Error building consensus: Invalid mapping for MessageFilter
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache
cleared
________________ test_scaffold_integration_tests_creates_files _________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_scaffold_integration_test0')

    def test_scaffold_integration_tests_creates_files(tmp_path: Path) -> None:
        agent = TestAgent()
        result = agent.scaffold_integration_tests(["sample"],
output_dir=tmp_path)

        expected = tmp_path / "test_sample.py"
        assert expected.exists(), "scaffold file should be created"
        content = expected.read_text()
>       assert "pytest.mark.skip" in content
E       assert 'pytest.mark.skip' in '"""Scaffolded integration test for
sample.\n\nReplace this file with real tests.\n"""\n\ndef test_sample() ->
None:\n    """Integration test placeholder for sample."""\n    assert 2 + 2 ==
4\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/generated
_tests/test_scaffold_output.py:19: AssertionError
___________________ test_bridge_output_consistency_succeeds ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336678050>

    @pytest.mark.medium
    def test_bridge_output_consistency_succeeds(monkeypatch):
        """Test that bridge output consistency succeeds.

        ReqID: N/A"""
        raw_input = "<script>alert(1)</script><b>demo</b>"
        expected = _sanitize(raw_input)
        out = MagicMock()
        monkeypatch.setattr("devsynth.interface.cli.Prompt.ask", lambda *a, **k:
raw_input)
        monkeypatch.setattr("devsynth.interface.cli.validate_safe_input", lambda
x: x)
        monkeypatch.setattr(
            "rich.console.Console.print",
            lambda self, msg, *, highlight=False, style=None:
out(html.unescape(str(msg))),
        )
        cli_bridge = CLIUXBridge()
>       _init_cmd(bridge=cli_bridge)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_bridge_consistency.py:40:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_bridge_consistency.py:22: in _init_cmd
    bridge.display_result(value)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.interface.cli.CLIUXBridge object at 0x3369e6510>, message = ''

    def display_result(
        self,
        message: str,
        *,
        highlight: bool = False,
        message_type: str | None = None,
    ) -> None:
        """Format and display a message to the user.

        Args:
            message: The message to display
            highlight: Whether to highlight the message
            message_type: Optional type of message (info, success, warning,
error, etc.)
        """
        # Handle errors with enhanced error messages
        if message_type == "error":
            logger.error(f"Displaying error: {message}")
            # Use the error handler for error messages
            self.handle_error(message)
            return
        elif message_type == "warning":
            logger.warning(f"Displaying warning: {message}")
        elif message_type == "success":
            logger.info(f"Displaying success: {message}")
        else:
            logger.debug(f"Displaying message: {message}")

        # For test output and other content that may contain brackets (like file
paths),
        # disable Rich markup parsing by default to avoid parsing errors
        # Only enable markup parsing for explicitly formatted content
        import re
        # Rich markup consists of simple color/style names without slashes or
complex characters
        rich_markup_pattern =
r'\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|bright_red|bright_green|bright_yellow|bright
_blue|bright_magenta|bright_cyan|bright_white|on_black|on_red|on_green|on_yellow
|on_blue|on_magenta|on_cyan|on_white|on_bright_black|on_bright_red|on_bright_gre
en|on_bright_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_wh
ite)\]'

        # Only consider it Rich markup if it matches the pattern AND the
brackets don't contain slashes or spaces
        # (file paths and complex strings should not be treated as markup)
        has_rich_markup = False
        if re.search(rich_markup_pattern, message):
            # Double-check that this isn't a false positive (e.g., file path
containing a style name)
            # Valid Rich markup should be simple tags without slashes between
brackets
            bracket_contents = re.findall(r'\[([^\]]+)\]', message)
            for content in bracket_contents:
                if '/' not in content and ' ' not in content and len(content) <
50:  # reasonable limits
                    has_rich_markup = True
                    break

        if has_rich_markup:
            # Content has explicit Rich markup - sanitize and preserve it
            self.console.print(sanitize_output(message), highlight=highlight)
            return
        else:
            # No Rich markup detected - print as plain text to avoid parsing
brackets as markup
>           self.console.print(message, markup=False, highlight=False)
E           TypeError:
test_bridge_output_consistency_succeeds.<locals>.<lambda>() got an unexpected
keyword argument 'markup'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:720: TypeError
----------------------------- Captured stdout call -----------------------------
val?    val?
_______________ test_cli_and_api_bridges_multi_agent_consistent ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x3369e6000>

    @pytest.mark.medium
    def test_cli_and_api_bridges_multi_agent_consistent(monkeypatch):
        """Test that CLI and API bridges behave the same for multi-agent
collaboration."""

        cli_bridge = _setup_cli_bridge(monkeypatch)
>       cli_msgs = collaborative_vote_workflow(cli_bridge)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_multi_agent_collaboration.py:115:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_multi_agent_collaboration.py:78: in collaborative_vote_workflow
    bridge.display_result(f"Winner {result['result']['winner']}")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.interface.cli.CLIUXBridge object at 0x3369e2bd0>
message = 'Winner y'

    def display_result(
        self,
        message: str,
        *,
        highlight: bool = False,
        message_type: str | None = None,
    ) -> None:
        """Format and display a message to the user.

        Args:
            message: The message to display
            highlight: Whether to highlight the message
            message_type: Optional type of message (info, success, warning,
error, etc.)
        """
        # Handle errors with enhanced error messages
        if message_type == "error":
            logger.error(f"Displaying error: {message}")
            # Use the error handler for error messages
            self.handle_error(message)
            return
        elif message_type == "warning":
            logger.warning(f"Displaying warning: {message}")
        elif message_type == "success":
            logger.info(f"Displaying success: {message}")
        else:
            logger.debug(f"Displaying message: {message}")

        # For test output and other content that may contain brackets (like file
paths),
        # disable Rich markup parsing by default to avoid parsing errors
        # Only enable markup parsing for explicitly formatted content
        import re
        # Rich markup consists of simple color/style names without slashes or
complex characters
        rich_markup_pattern =
r'\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|bright_red|bright_green|bright_yellow|bright
_blue|bright_magenta|bright_cyan|bright_white|on_black|on_red|on_green|on_yellow
|on_blue|on_magenta|on_cyan|on_white|on_bright_black|on_bright_red|on_bright_gre
en|on_bright_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_wh
ite)\]'

        # Only consider it Rich markup if it matches the pattern AND the
brackets don't contain slashes or spaces
        # (file paths and complex strings should not be treated as markup)
        has_rich_markup = False
        if re.search(rich_markup_pattern, message):
            # Double-check that this isn't a false positive (e.g., file path
containing a style name)
            # Valid Rich markup should be simple tags without slashes between
brackets
            bracket_contents = re.findall(r'\[([^\]]+)\]', message)
            for content in bracket_contents:
                if '/' not in content and ' ' not in content and len(content) <
50:  # reasonable limits
                    has_rich_markup = True
                    break

        if has_rich_markup:
            # Content has explicit Rich markup - sanitize and preserve it
            self.console.print(sanitize_output(message), highlight=highlight)
            return
        else:
            # No Rich markup detected - print as plain text to avoid parsing
brackets as markup
>           self.console.print(message, markup=False, highlight=False)
E           TypeError: _setup_cli_bridge.<locals>.<lambda>() got an unexpected
keyword argument 'markup'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:720: TypeError
----------------------------- Captured stdout call -----------------------------
Agent name? [a1]a1                  Agent name? [a1]a1
Agent name? [a2]a2                  Agent name? [a2]a2
Agent name? [a3]a3                  Agent name? [a3]a3
Vote? [x]x          Vote? [x]x
Vote? [x]x          Vote? [x]x
Vote? [x]x          Vote? [x]x
2025-10-28 10:34:45,530 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team Team
2025-10-28 10:34:45,530 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team Team
2025-10-28 10:34:45,530 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team Team
2025-10-28 10:34:45,530 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team Team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team Team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team Team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
__________________ test_vote_with_role_reassignment_succeeds ___________________

    @pytest.mark.medium
    def test_vote_with_role_reassignment_succeeds():
        team = CollaborativeWSDETeam(name="VoteTestTeam")
        agents = [
            SimpleAgent("a1", ["security"]),
            SimpleAgent("a2", ["testing"]),
            SimpleAgent("a3", ["security"]),
        ]
        team.add_agents(agents)

        task = {
            "id": "decision1",
            "type": "critical_decision",
            "is_critical": True,
            "domain": "security",
            "options": ["A", "B"],
        }

        result = team.vote_with_role_reassignment(task)
        assert result["status"] == "completed"
>       assert result["result"] in task["options"]
E       AssertionError: assert {'method': 'majority_vote', 'winner': 'B'} in
['A', 'B']

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_multi_agent_collaboration.py:154: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:45,565 - devsynth.domain.models.wsde_core - INFO - Added agent
a1 to team VoteTestTeam
2025-10-28 10:34:45,566 - devsynth.domain.models.wsde_core - INFO - Added agent
a2 to team VoteTestTeam
2025-10-28 10:34:45,566 - devsynth.domain.models.wsde_core - INFO - Added agent
a3 to team VoteTestTeam
2025-10-28 10:34:45,566 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
2025-10-28 10:34:45,566 - devsynth.domain.models.wsde_roles - INFO - Selected a2
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team VoteTestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team VoteTestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team VoteTestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a2 as
primus based on expertise
_________________ test_dashboard_renders_from_generated_report _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x3369d8980>

    def test_dashboard_renders_from_generated_report(monkeypatch):
        sample = {
            "DSY-0001": {
                "issue": "TEST-1",
                "files": ["src/example.py"],
                "features": ["Example feature"],
            }
        }

        def fake_run(cmd, check=True, **kwargs):
            if cmd[:3] == ["devsynth", "mvu", "report"]:
                out_idx = cmd.index("--output") + 1
                Path(cmd[out_idx]).write_text(json.dumps(sample),
encoding="utf-8")
            return subprocess.CompletedProcess(cmd, 0)

        monkeypatch.setattr(subprocess, "run", fake_run)

        script_path = (
            Path(__file__).resolve().parents[3]
            / "src"
            / "devsynth"
            / "interface"
            / "mvuu_dashboard.py"
        )
        at = AppTest.from_file(script_path)
>       at.run()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_mvuu_dashboard_report.py:39:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/testing/v1/app_test.py:396: in run
    return self._tree.run(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/testing/v1/element_tree.py:1910: in run
    return self._runner._run(widget_states, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/testing/v1/app_test.py:327: in _run
    import streamlit as st
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/__init__.py:85: in <module>
    _dg_singleton = _DeltaGeneratorSingleton(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <streamlit.delta_generator_singletons.DeltaGeneratorSingleton object at
0x3366b2c90>
delta_generator_cls = <class 'streamlit.delta_generator.DeltaGenerator'>
status_container_cls = <class
'streamlit.elements.lib.mutable_status_container.StatusContainer'>
dialog_container_cls = <class 'streamlit.elements.lib.dialog.Dialog'>

    def __init__(
        self,
        delta_generator_cls: type[DeltaGenerator],
        status_container_cls: type[StatusContainer],
        dialog_container_cls: type[Dialog],
    ) -> None:
        """Registers and initializes all delta-generator classes.

        Parameters
        ----------
        delta_generator_cls : type[DeltaGenerator]
            The main DeltaGenerator class.
        status_container_cls : type[StatusContainer]
            The delta-generator class that is used as return value for
`st.status`.
        dialog_container_cls : type[Dialog]
            The delta-generator class used is used as return value for
`st.dialog`.

        Raises
        ------
        RuntimeError
            If the DeltaGeneratorSingleton instance already exists.
        """
        if DeltaGeneratorSingleton._instance is not None:
>           raise RuntimeError("DeltaGeneratorSingleton instance already
exists!")
E           RuntimeError: DeltaGeneratorSingleton instance already exists!

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/streamlit/delta_generator_singletons.py:73: RuntimeError
----------------------------- Captured stderr call -----------------------------
2025-10-28 10:34:45.597 WARNING
streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread':
missing ScriptRunContext! This warning can be ignored when running in bare mode.
_________________ test_cli_and_api_bridges_consistent_succeeds _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x3369d8740>

    @pytest.mark.medium
    def test_cli_and_api_bridges_consistent_succeeds(monkeypatch):
        """Test that cli and api bridges consistent succeeds.

        ReqID: N/A"""
        cli_bridge = _setup_cli_bridge(monkeypatch)
>       cli_msgs = small_workflow(cli_bridge)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_small_workflow_bridge.py:82:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_small_workflow_bridge.py:48: in small_workflow
    bridge.print(f"Done {name}")
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: in print
    self.display_result(message, highlight=highlight, message_type=message_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <devsynth.interface.cli.CLIUXBridge object at 0x3369e28d0>
message = 'Done demo'

    def display_result(
        self,
        message: str,
        *,
        highlight: bool = False,
        message_type: str | None = None,
    ) -> None:
        """Format and display a message to the user.

        Args:
            message: The message to display
            highlight: Whether to highlight the message
            message_type: Optional type of message (info, success, warning,
error, etc.)
        """
        # Handle errors with enhanced error messages
        if message_type == "error":
            logger.error(f"Displaying error: {message}")
            # Use the error handler for error messages
            self.handle_error(message)
            return
        elif message_type == "warning":
            logger.warning(f"Displaying warning: {message}")
        elif message_type == "success":
            logger.info(f"Displaying success: {message}")
        else:
            logger.debug(f"Displaying message: {message}")

        # For test output and other content that may contain brackets (like file
paths),
        # disable Rich markup parsing by default to avoid parsing errors
        # Only enable markup parsing for explicitly formatted content
        import re
        # Rich markup consists of simple color/style names without slashes or
complex characters
        rich_markup_pattern =
r'\[/?(?:bold|dim|italic|underline|strike|reverse|blink|black|red|green|yellow|b
lue|magenta|cyan|white|bright_black|bright_red|bright_green|bright_yellow|bright
_blue|bright_magenta|bright_cyan|bright_white|on_black|on_red|on_green|on_yellow
|on_blue|on_magenta|on_cyan|on_white|on_bright_black|on_bright_red|on_bright_gre
en|on_bright_yellow|on_bright_blue|on_bright_magenta|on_bright_cyan|on_bright_wh
ite)\]'

        # Only consider it Rich markup if it matches the pattern AND the
brackets don't contain slashes or spaces
        # (file paths and complex strings should not be treated as markup)
        has_rich_markup = False
        if re.search(rich_markup_pattern, message):
            # Double-check that this isn't a false positive (e.g., file path
containing a style name)
            # Valid Rich markup should be simple tags without slashes between
brackets
            bracket_contents = re.findall(r'\[([^\]]+)\]', message)
            for content in bracket_contents:
                if '/' not in content and ' ' not in content and len(content) <
50:  # reasonable limits
                    has_rich_markup = True
                    break

        if has_rich_markup:
            # Content has explicit Rich markup - sanitize and preserve it
            self.console.print(sanitize_output(message), highlight=highlight)
            return
        else:
            # No Rich markup detected - print as plain text to avoid parsing
brackets as markup
>           self.console.print(message, markup=False, highlight=False)
E           TypeError: _setup_cli_bridge.<locals>.<lambda>() got an unexpected
keyword argument 'markup'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/cli.
py:720: TypeError
----------------------------- Captured stdout call -----------------------------
Name? [demo]demo                Name? [demo]demo
                  ┌───────────────| DevSynth |────────────────┐

                  │                                           │

                  │ Proceed?                                  │

                  │                                           │

                  │ (*) Yes                                 ^ │

                  │ ( ) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘





























































                                       ┌───────────────| DevSynth
|────────────────┐

                  │                                           │

                  │ Proceed?                                  │

                  │                                           │

                  │ (*) Yes                                 ^ │

                  │ ( ) No                                  v │

                  │         <    Ok    > <  Cancel  >         │

                  │                                           │

                  │                                           │

                  └───────────────────────────────────────────┘






























































_______________________ test_cli_fallback_to_cli_module ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x3369da240>

    @pytest.mark.medium
    def test_cli_fallback_to_cli_module(monkeypatch):
        dummy = object()
        cli_mod = SimpleNamespace(sample_cmd=dummy)
        monkeypatch.setattr(webui, "_cli_mod", cli_mod, raising=False)
        monkeypatch.delattr(webui, "sample_cmd", raising=False)
>       assert webui._cli("sample_cmd") is dummy
               ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute
'_cli'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/interface
/test_webui_cli_lookup.py:14: AttributeError
____________________ test_openai_chat_completion_live_smoke ____________________

    @pytest.mark.medium
    @requires_openai
    def test_openai_chat_completion_live_smoke():
        """Minimal live prompt against OpenAI with a very short timeout.

        - Deterministic, short prompt
        - Explicit model via OPENAI_MODEL env var (caller responsibility)
        - Short timeout to fail fast if misconfigured
        - Assert on basic shape only
        """
        import httpx

        api_key = os.environ.get("OPENAI_API_KEY")
        model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
        assert api_key, "OPENAI_API_KEY must be set for live OpenAI smoke test"

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        payload: Dict[str, Any] = {
            "model": model,
            "messages": [
                {"role": "system", "content": "You are a concise assistant."},
                {"role": "user", "content": "Say the word 'ok'."},
            ],
            "max_tokens": 5,
            "temperature": 0,
        }

        with httpx.Client(timeout=5.0) as client:
>           resp = client.post(
                "https://api.openai.com/v1/chat/completions", json=payload,
headers=headers
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/live/test
_openai_live_smoke.py:50:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x336ae1370>, method = 'POST'
url = 'https://api.openai.com/v1/chat/completions', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError
______________________ test_openai_embeddings_live_smoke _______________________

    @pytest.mark.medium
    @requires_openai
    def test_openai_embeddings_live_smoke():
        """Minimal live embeddings request with explicit model and short
timeout.
        Asserts on the basic shape only.
        """
        import httpx

        api_key = os.environ.get("OPENAI_API_KEY")
        emb_model = os.environ.get(
            "OPENAI_EMBEDDINGS_MODEL",
            os.environ.get("OPENAI_MODEL", "text-embedding-3-small"),
        )
        assert api_key, "OPENAI_API_KEY must be set for live OpenAI embeddings
smoke test"

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        payload = {"model": emb_model, "input": ["tiny"]}

        with httpx.Client(timeout=5.0) as client:
>           resp = client.post(
                "https://api.openai.com/v1/embeddings", json=payload,
headers=headers
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/live/test
_openai_live_smoke.py:86:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/httpx/_client.py:1144: in post
    return self.request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.Client object at 0x3369db080>, method = 'POST'
url = 'https://api.openai.com/v1/embeddings', args = ()
kwargs = {'auth': <httpx._client.UseClientDefault object at 0x1054e39b0>,
'content': None, 'cookies': None, 'data': None, ...}
host = None, host_str = ''

    def guard_httpx_request(self, method: str, url, *args: Any, **kwargs: Any):
# type: ignore[no-redef]
        # Allow in-memory TestClient requests against the ASGI test server
        try:
            host = getattr(url, "host", None) or getattr(url, "netloc", None)
            if isinstance(host, bytes):
                host = host.decode("utf-8", "ignore")
            host_str = str(host or "")
            if host_str.split(":")[0] == "testserver":
                return _orig_client_request(self, method, url, *args, **kwargs)
        except Exception:
            host_str = str(url)
            if host_str.startswith("http://testserver") or host_str.startswith(
                "https://testserver"
            ):
                return _orig_client_request(self, method, url, *args, **kwargs)
>       raise RuntimeError("Network access disabled during tests (httpx)")
E       RuntimeError: Network access disabled during tests (httpx)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/networking.p
y:101: RuntimeError
_
TestProviderInterfaceConsistency.test_provider_has_required_methods[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1286eb410>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_has_required_methods(self, provider_name):
        """Test that all providers have required core methods."""
        # Skip providers that require external resources unless available
        if provider_name in ["openrouter", "openai"] and not
_is_provider_available(
            provider_name
        ):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:58:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openrouter'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderInterfaceConsistency.test_provider_has_required_methods[openai] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1286eb5f0>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_has_required_methods(self, provider_name):
        """Test that all providers have required core methods."""
        # Skip providers that require external resources unless available
        if provider_name in ["openrouter", "openai"] and not
_is_provider_available(
            provider_name
        ):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:58:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openai'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderInterfaceConsistency.test_provider_has_required_methods[offline] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1286eb770>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_has_required_methods(self, provider_name):
        """Test that all providers have required core methods."""
        # Skip providers that require external resources unless available
        if provider_name in ["openrouter", "openai"] and not
_is_provider_available(
            provider_name
        ):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:58:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_
TestProviderInterfaceConsistency.test_provider_method_signatures_consistent[open
router] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1286ebcb0>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_method_signatures_consistent(self, provider_name):
        """Test that provider methods have consistent signatures."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderInterfaceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:95: AttributeError
_
TestProviderInterfaceConsistency.test_provider_method_signatures_consistent[open
ai] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1286ebe90>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_method_signatures_consistent(self, provider_name):
        """Test that provider methods have consistent signatures."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderInterfaceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:95: AttributeError
_
TestProviderInterfaceConsistency.test_provider_method_signatures_consistent[offl
ine] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x128700050>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_method_signatures_consistent(self, provider_name):
        """Test that provider methods have consistent signatures."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:102:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_
TestProviderInterfaceConsistency.test_provider_error_handling_consistency[openro
uter] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x128700590>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_error_handling_consistency(self, provider_name):
        """Test that providers handle errors consistently."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderInterfaceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:141: AttributeError
_
TestProviderInterfaceConsistency.test_provider_error_handling_consistency[openai
] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x128700770>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_error_handling_consistency(self, provider_name):
        """Test that providers handle errors consistently."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderInterfaceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:141: AttributeError
_
TestProviderInterfaceConsistency.test_provider_error_handling_consistency[offlin
e] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderInterfaceConsistenc
y object at 0x1287008f0>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_provider_error_handling_consistency(self, provider_name):
        """Test that providers handle errors consistently."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:148:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderErrorConsistency.test_authentication_error_consistency[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x128700ec0>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_authentication_error_consistency(self, provider_name):
        """Test that authentication errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderErrorConsistency' object has no attribute
'_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:176: AttributeError
__ TestProviderErrorConsistency.test_authentication_error_consistency[openai] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x1287010a0>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_authentication_error_consistency(self, provider_name):
        """Test that authentication errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderErrorConsistency' object has no attribute
'_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:176: AttributeError
_ TestProviderErrorConsistency.test_authentication_error_consistency[offline] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x128701220>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_authentication_error_consistency(self, provider_name):
        """Test that authentication errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
            # Use invalid API key to trigger authentication error
            config = {"api_key": "invalid-key-for-testing"}

            if provider_name == "openrouter":
                config["openrouter_api_key"] = "invalid-key-for-testing"
            elif provider_name == "openai":
                config["api_key"] = "invalid-key-for-testing"
            elif provider_name == "lmstudio":
                config["base_url"] = "http://localhost:1234/v1"

>           provider = get_llm_provider({"provider": provider_name, **config})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:193:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'api_key': 'invalid-key-for-testing', 'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderErrorConsistency.test_configuration_error_consistency[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x128701760>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_configuration_error_consistency(self, provider_name):
        """Test that configuration errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderErrorConsistency' object has no attribute
'_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:219: AttributeError
__ TestProviderErrorConsistency.test_configuration_error_consistency[openai] ___

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x128701940>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_configuration_error_consistency(self, provider_name):
        """Test that configuration errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderErrorConsistency' object has no attribute
'_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:219: AttributeError
__ TestProviderErrorConsistency.test_configuration_error_consistency[offline] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderErrorConsistency
object at 0x128701ac0>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_configuration_error_consistency(self, provider_name):
        """Test that configuration errors are consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:226:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_
TestProviderConfigurationConsistency.test_standardized_config_acceptance[openrou
ter] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x128701d90>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_standardized_config_acceptance(self, provider_name):
        """Test that all providers accept standardized configuration."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderConfigurationConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:254: AttributeError
_
TestProviderConfigurationConsistency.test_standardized_config_acceptance[openai]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x128701f70>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_standardized_config_acceptance(self, provider_name):
        """Test that all providers accept standardized configuration."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderConfigurationConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:254: AttributeError
_
TestProviderConfigurationConsistency.test_standardized_config_acceptance[offline
] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x1287020f0>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_standardized_config_acceptance(self, provider_name):
        """Test that all providers accept standardized configuration."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
            # Standard configuration that should work for all providers
            config = {
                "provider": provider_name,
                "temperature": 0.7,
                "max_tokens": 1000,
                "timeout": 30,
            }

            # Add provider-specific keys if needed
            if provider_name == "openrouter":
                config["openrouter_api_key"] = "test-key"
            elif provider_name == "openai":
                config["api_key"] = "test-key"
            elif provider_name == "lmstudio":
                config["base_url"] = "http://localhost:1234/v1"

>           provider = get_llm_provider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:277:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'max_tokens': 1000, 'provider': 'offline', 'temperature': 0.7,
'timeout': 30}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_
TestProviderConfigurationConsistency.test_environment_variable_consistency[openr
outer] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x128702630>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_environment_variable_consistency(self, provider_name):
        """Test environment variable handling consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderConfigurationConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:298: AttributeError
_
TestProviderConfigurationConsistency.test_environment_variable_consistency[opena
i] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x128702810>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_environment_variable_consistency(self, provider_name):
        """Test environment variable handling consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderConfigurationConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:298: AttributeError
_
TestProviderConfigurationConsistency.test_environment_variable_consistency[offli
ne] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderConfigurationConsis
tency object at 0x128702990>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_environment_variable_consistency(self, provider_name):
        """Test environment variable handling consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
            # Set environment variables for each provider
            env_vars = {}
            if provider_name == "openrouter":
                env_vars["OPENROUTER_API_KEY"] = "test-key"
            elif provider_name == "openai":
                env_vars["OPENAI_API_KEY"] = "test-key"
            elif provider_name == "lmstudio":
                env_vars["DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE"] = "true"

            with pytest.MonkeyPatch().context() as m:
                for key, value in env_vars.items():
                    m.setenv(key, value)

>               provider = get_llm_provider({"provider": provider_name})
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:318:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderPerformanceConsistency.test_response_time_consistency[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x128702f00>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_response_time_consistency(self, provider_name):
        """Test response time characteristics across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderPerformanceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:342: AttributeError
__ TestProviderPerformanceConsistency.test_response_time_consistency[openai] ___

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x1287030e0>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_response_time_consistency(self, provider_name):
        """Test response time characteristics across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderPerformanceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:342: AttributeError
__ TestProviderPerformanceConsistency.test_response_time_consistency[offline] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x128703260>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_response_time_consistency(self, provider_name):
        """Test response time characteristics across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
            import time

>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:351:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderPerformanceConsistency.test_token_usage_consistency[openrouter] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x1287037a0>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_token_usage_consistency(self, provider_name):
        """Test token usage tracking consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderPerformanceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:387: AttributeError
___ TestProviderPerformanceConsistency.test_token_usage_consistency[openai] ____

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x128703980>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_token_usage_consistency(self, provider_name):
        """Test token usage tracking consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
>       ] and not self._is_provider_available(provider_name):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestProviderPerformanceConsistency' object has no
attribute '_is_provider_available'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:387: AttributeError
___ TestProviderPerformanceConsistency.test_token_usage_consistency[offline] ___

self =
<tests.integration.llm.test_provider_consistency.TestProviderPerformanceConsiste
ncy object at 0x128703b00>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_token_usage_consistency(self, provider_name):
        """Test token usage tracking consistency."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:394:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_
TestProviderBehaviorConsistency.test_generation_behavior_consistency[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x128703fb0>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_generation_behavior_consistency(self, provider_name):
        """Test that generation behavior is consistent across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:439:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openrouter'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderBehaviorConsistency.test_generation_behavior_consistency[openai] _

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x12871c260>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_generation_behavior_consistency(self, provider_name):
        """Test that generation behavior is consistent across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:439:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openai'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderBehaviorConsistency.test_generation_behavior_consistency[offline]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x12871c3e0>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_generation_behavior_consistency(self, provider_name):
        """Test that generation behavior is consistent across providers."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:439:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
_ TestProviderBehaviorConsistency.test_context_behavior_consistency[openrouter]
_

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x12871c920>
provider_name = 'openrouter'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_context_behavior_consistency(self, provider_name):
        """Test that context handling behavior is consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:480:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openrouter'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
__ TestProviderBehaviorConsistency.test_context_behavior_consistency[openai] ___

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x12871cb00>
provider_name = 'openai'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_context_behavior_consistency(self, provider_name):
        """Test that context handling behavior is consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:480:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'openai'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
__ TestProviderBehaviorConsistency.test_context_behavior_consistency[offline] __

self =
<tests.integration.llm.test_provider_consistency.TestProviderBehaviorConsistency
object at 0x128703320>
provider_name = 'offline'

    @pytest.mark.parametrize(
        "provider_name", ["openrouter", "openai", "lmstudio", "offline"]
    )
    def test_context_behavior_consistency(self, provider_name):
        """Test that context handling behavior is consistent."""
        # Skip providers that require external resources unless available
        if provider_name in [
            "openrouter",
            "openai",
        ] and not self._is_provider_available(provider_name):
            pytest.skip(f"{provider_name} provider not available")

        if provider_name == "lmstudio" and not _is_lmstudio_available():
            pytest.skip("LM Studio server not available")

        try:
>           provider = get_llm_provider({"provider": provider_name})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_consistency.py:480:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

config = {'provider': 'offline'}

    def get_llm_provider(config: Dict[str, Any] | None = None) -> LLMProvider:
        """Return an LLM provider based on configuration.

        Safe-by-default policy:
        - If offline_mode is true, select 'offline'.
        - If DEVSYNTH_OFFLINE=true (env), force 'offline'.
        - If a provider is explicitly configured, use it but validate required
credentials.
        - If 'lmstudio' is requested but not marked available via
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE, fall back to 'offline'.
        - Otherwise, default to 'offline'.
        """

        cfg = config or load_config().as_dict()
        offline = cfg.get("offline_mode", False)

        # Honor global offline kill-switch
        offline_env = os.getenv("DEVSYNTH_OFFLINE", "").lower() in {"1", "true",
"yes"}
        if offline_env:
            offline = True

        # Tests default: require explicit opt-in to use real providers
        allow_providers = os.getenv("DEVSYNTH_TEST_ALLOW_PROVIDERS",
"false").lower() in {
            "1",
            "true",
            "yes",
        }
        if not allow_providers:
            offline = True

>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:390: ImportError
__________ test_offline_mode_returns_offline_provider_for_app_wrapper __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336ae3050>

    @pytest.mark.medium
    def test_offline_mode_returns_offline_provider_for_app_wrapper(monkeypatch):
        """application.llm.get_llm_provider selects OfflineProvider when
offline_mode is True."""
        monkeypatch.setattr(app_llm, "load_config", lambda: _mock_config(True,
"local"))
>       monkeypatch.setattr(app_llm, "get_llm_settings", lambda:
_llm_settings("openai"))
E       AttributeError: <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_selection_integration.py:56: AttributeError
_______ test_offline_mode_returns_offline_provider_for_providers_module ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336ae08f0>

    @pytest.mark.medium
    def
test_offline_mode_returns_offline_provider_for_providers_module(monkeypatch):
        """providers.get_llm_provider selects 'offline' when offline_mode is
True."""
        monkeypatch.setattr(prov, "load_config", lambda: _mock_config(True,
"local"))
>       monkeypatch.setattr(prov, "get_llm_settings", lambda:
_llm_settings("openai"))
E       AttributeError: <module 'devsynth.application.llm.providers' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_selection_integration.py:66: AttributeError
_______ test_openai_without_api_key_raises_validation_error_in_providers _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336ae29c0>

    @pytest.mark.medium
    def
test_openai_without_api_key_raises_validation_error_in_providers(monkeypatch):
        """Requesting OpenAI without credentials should fail fast in
providers.get_llm_provider.

        This path validates credentials before constructing the provider class.
        """
        # Ensure no OPENAI_API_KEY in environment for this test
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)

        monkeypatch.setattr(prov, "load_config", lambda: _mock_config(False))
        # get_llm_settings returns no explicit api_key
>       monkeypatch.setattr(prov, "get_llm_settings", lambda: {"provider":
"openai"})
E       AttributeError: <module 'devsynth.application.llm.providers' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_selection_integration.py:83: AttributeError
______ test_openai_without_api_key_raises_connection_error_in_app_wrapper ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336ae02f0>

    @pytest.mark.medium
    def
test_openai_without_api_key_raises_connection_error_in_app_wrapper(monkeypatch):
        """application.llm.get_llm_provider constructs the OpenAIProvider which
raises OpenAIConnectionError
        when no API key is configured. This still avoids any network calls in
tests.
        """
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)

        monkeypatch.setattr(app_llm, "load_config", lambda: _mock_config(False))
        # application.llm.get_llm_provider pulls settings via get_llm_settings()
>       monkeypatch.setattr(app_llm, "get_llm_settings", lambda: {"provider":
"openai"})
E       AttributeError: <module 'devsynth.application.llm' from
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/llm/test_
provider_selection_integration.py:98: AttributeError
____________________________ test_report_generation ____________________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_report_generation0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336a4f050>

    @pytest.mark.medium
    def test_report_generation(tmp_path, monkeypatch):
        repo = tmp_path / "repo"
        repo.mkdir()
        subprocess.run(["git", "init", "-b", "main"], cwd=repo, check=True)
        monkeypatch.chdir(repo)

        file1 = repo / "one.txt"
        file1.write_text("1", encoding="utf-8")
>       _write_commit(file1, _mvuu_message("DSY-0001", "one.txt"))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/mvu/test_
report_generation.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/mvu/test_
report_generation.py:48: in _write_commit
    subprocess.run(["git", "commit", "-m", message], check=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

input = None, capture_output = False, timeout = None, check = True
popenargs = (['git', 'commit', '-m', 'feat: add\n\n```json\n{\n
"utility_statement": "Add one.txt",\n  "affected_files": [\n    "..."\n  ],\n
"tests": [\n    "pytest"\n  ],\n  "TraceID": "DSY-0001",\n  "mvuu": true,\n
"issue": "DSY-0001"\n}\n```'],)
kwargs = {}
process = <Popen: returncode: 128 args: ['git', 'commit', '-m', 'feat:
add\n\n```json\...>
stdout = None, stderr = None, retcode = 128

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False,
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.

        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture
them,
        or pass capture_output=True to capture both.

        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return
code
        in the returncode attribute, and output & stderr attributes if those
streams
        were captured.

        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.

        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.

        By default, all communication is in bytes, and therefore any "input"
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or
universal_newlines.

        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be
used.')
            kwargs['stdin'] = PIPE

        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE

        with Popen(*popenargs, **kwargs) as process:
            try:
                stdout, stderr = process.communicate(input, timeout=timeout)
            except TimeoutExpired as exc:
                process.kill()
                if _mswindows:
                    # Windows accumulates the output in a single blocking
                    # read() call run on child threads, with the timeout
                    # being done in a join() on those threads.  communicate()
                    # _after_ kill() is required to collect that and add it
                    # to the exception.
                    exc.stdout, exc.stderr = process.communicate()
                else:
                    # POSIX _communicate already populated the output so
                    # far into the TimeoutExpired exception.
                    process.wait()
                raise
            except:  # Including KeyboardInterrupt, communicate handled that.
                process.kill()
                # We don't call process.wait() as .__exit__ does that for us.
                raise
            retcode = process.poll()
            if check and retcode:
>               raise CalledProcessError(retcode, process.args,
                                         output=stdout, stderr=stderr)
E               subprocess.CalledProcessError: Command '['git', 'commit', '-m',
'feat: add\n\n```json\n{\n  "utility_statement": "Add one.txt",\n
"affected_files": [\n    "one.txt"\n  ],\n  "tests": [\n    "pytest"\n  ],\n
"TraceID": "DSY-0001",\n  "mvuu": true,\n  "issue": "DSY-0001"\n}\n```']'
returned non-zero exit status 128.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:571: CalledProcessError
----------------------------- Captured stdout call -----------------------------
Initialized empty Git repository in
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1440/test_report_generation0/repo/.git/
----------------------------- Captured stderr call -----------------------------
Author identity unknown

*** Please tell me who you are.

Run

  git config --global user.email "you@example.com"
  git config --global user.name "Your Name"

to set your account's default identity.
Omit --global to set the identity only in this repository.

fatal: unable to auto-detect email address (got
'caitlyn@caitlyns-MacBook-Pro.(none)')
____________________ test_message_persisted_with_edrr_phase ____________________

tmp_path =
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-1440/test_message_persisted_with_ed0')

    @pytest.mark.medium
    def test_message_persisted_with_edrr_phase(tmp_path):
        """Messages should be stored with their EDRR phase metadata."""
        mem_file = tmp_path / "mem.json"
        adapter = TinyDBMemoryAdapter(str(mem_file))
        memory = MemoryManager(adapters={"tinydb": adapter})
        team = WSDETeam(name="EDRRTeam")
        team.memory_manager = memory

        team.send_message(
            sender="alice",
            recipients=["bob"],
            message_type="status_update",
            subject="s",
            content="c",
            metadata={"edrr_phase": Phase.EXPAND.value},
        )

        results = memory.query_by_edrr_phase(Phase.EXPAND.value)
>       assert any(r.metadata.get("edrr_phase") == Phase.EXPAND.value for r in
results)
E       assert False
E        +  where False = any(<generator object
test_message_persisted_with_edrr_phase.<locals>.<genexpr> at 0x13fb1f510>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/wsde/test
_wsde_edrr_integration.py:32: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:48,608 -
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB
Memory Adapter initialized
2025-10-28 10:34:48,608 - devsynth.application.memory.memory_manager - INFO -
Memory Manager initialized with adapters: tinydb
2025-10-28 10:34:48,608 - devsynth.application.memory.tiered_cache - INFO -
Tiered cache initialized with max size 50
------------------------------ Captured log call -------------------------------
INFO
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615
TinyDB Memory Adapter initialized
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory
Manager initialized with adapters: tinydb
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered
cache initialized with max size 50
_________________________ test_json_requests_succeeds __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x336a5f3e0>

    @pytest.mark.medium
    def test_json_requests_succeeds(monkeypatch):
        """Test that json requests succeeds.

        ReqID: AGENTAPI-001"""

        TestClient = fastapi.testclient.TestClient

        cli_stub, agentapi = _setup(monkeypatch)
        client = TestClient(agentapi.app)
        resp = client.post("/init", json={"path": "proj"})
        assert resp.status_code == 200
>       assert resp.json() == {"messages": ["init"]}
E       AssertionError: assert {'messages': ...tadata': None} == {'messages':
['init']}
E
E         Omitting 1 identical items, use -vv to show
E         Left contains 1 more item:
E         {'metadata': None}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/test_agentap
i.py:48: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-28 10:34:48,647 - httpx - INFO - HTTP Request: POST
http://testserver/init "HTTP/1.1 200 OK"
------------------------------ Captured log call -------------------------------
INFO     httpx:_client.py:1025 HTTP Request: POST http://testserver/init
"HTTP/1.1 200 OK"
____________________ test_completion_command_outputs_script ____________________

    @pytest.mark.medium
    def test_completion_command_outputs_script():
        runner = CliRunner()
        result = runner.invoke(build_app(), ["completion", "--shell", "bash"])
>       assert result.exit_code == 0
E       assert 1 == 0
E        +  where 1 = <Result NameError("name 'completion_cmd' is not
defined")>.exit_code

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/test_cli_hel
p_and_completion.py:24: AssertionError
=============================== warnings summary ===============================
.venv/lib/python3.12/site-packages/_pytest/config/__init__.py:833
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/config/__init__.py:833: PytestAssertRewriteWarning: Module
already imported so cannot be rewritten; tests.fixtures.optional_deps
    self.import_plugin(import_spec)

.venv/lib/python3.12/site-packages/astor/op_util.py:92
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/op_util.py:92: DeprecationWarning: ast.Num is deprecated and
will be removed in Python 3.14; use ast.Constant instead
    precedence_data = dict((getattr(ast, x, None), z) for x, y, z in op_data)

.venv/lib/python3.12/site-packages/vbuild/__init__.py:33
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:33: DeprecationWarning: 'pkgutil.find_loader' is
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasLess = bool(pkgutil.find_loader("lesscpy"))

.venv/lib/python3.12/site-packages/vbuild/__init__.py:34
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:34: DeprecationWarning: 'pkgutil.find_loader' is
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasSass = bool(pkgutil.find_loader("scss"))

.venv/lib/python3.12/site-packages/vbuild/__init__.py:35
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:35: DeprecationWarning: 'pkgutil.find_loader' is
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasClosure = bool(pkgutil.find_loader("closure"))

.venv/lib/python3.12/site-packages/pytest_bdd/plugin.py:137
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/pytest_bdd/plugin.py:137: PytestUnknownMarkWarning: Unknown
pytest.mark.test-infrastructure - is this a typo?  You can register custom marks
to avoid this warning - for details, see
https://docs.pytest.org/en/stable/how-to/mark.html
    mark = getattr(pytest.mark, tag)

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 2
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 12971 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
tests/integration/general/test_self_analyzer.py: 160971 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:270: DeprecationWarning: ast.Str is deprecated and
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Str") and isinstance(node, getattr(ast, "Str")):

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 2
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 12971 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
tests/integration/general/test_self_analyzer.py: 160971 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:273: DeprecationWarning: ast.Num is deprecated and
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Num") and isinstance(node, getattr(ast, "Num")):

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 3
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 11422 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
tests/integration/general/test_self_analyzer.py: 142017 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:289: DeprecationWarning: ast.NameConstant is
deprecated and will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "NameConstant") and isinstance(

tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer
tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
  <frozen runpy>:128: RuntimeWarning: 'devsynth.cli' found in sys.modules after
import of package 'devsynth', but prior to execution of 'devsynth.cli'; this may
result in unpredictable behaviour

tests/unit/application/code_analysis/test_self_analyzer.py: 1767 warnings
tests/integration/general/test_self_analyzer.py: 22566 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:236: DeprecationWarning: ast.Str is deprecated and
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Str") and isinstance(node, getattr(ast, "Str")):

tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/code_gen.py:599: DeprecationWarning: ast.Str is deprecated and
will be removed in Python 3.14; use ast.Constant instead
    if isinstance(value, ast.Str):

tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/code_gen.py:601: DeprecationWarning: Attribute s is deprecated
and will be removed in Python 3.14; use value instead
    self.write(value.s.replace('{', '{{').replace('}', '}}'))

tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_mvu.
py:56: DeprecationWarning: datetime.datetime.utcnow() is deprecated and
scheduled for removal in a future version. Use timezone-aware objects to
represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    now = datetime.utcnow()

tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test
_api_endpoints.py:452: RuntimeWarning: coroutine 'init_endpoint' was never
awaited
    enhanced_api.init_endpoint(request, init_request, token=None)
  Enable tracemalloc to get traceback where the object was allocated.
  See
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more info.

tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/pytest_asyncio/plugin.py:252: PytestDeprecationWarning: The
configuration option "asyncio_default_fixture_loop_scope" is unset.
  The event loop scope for asynchronous fixtures will default to the fixture
caching scope. Future versions of pytest-asyncio will default the loop scope for
asynchronous fixtures to function scope. Set the default fixture loop scope
explicitly in order to avoid unexpected behavior in the future. Valid fixture
loop scopes are: "function", "class", "module", "package", "session"

    warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))

tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_par
ser.py:484: DeprecationWarning: ast.Str is deprecated and will be removed in
Python 3.14; use ast.Constant instead
    elif isinstance(node, ast.Str):

tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_par
ser.py:487: DeprecationWarning: ast.Num is deprecated and will be removed in
Python 3.14; use ast.Constant instead
    elif isinstance(node, ast.Num):

tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/run_all_tests.py:
24: DeprecationWarning: scripts/run_all_tests.py is deprecated; use 'devsynth
run-tests' instead.
    warnings.warn(

tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning:
  The default value will be `edges="edges" in NetworkX 3.6.

  To make this warning go away, explicitly set the edges kwarg, e.g.:

    nx.node_link_data(G, edges="links") to preserve current behavior, or
    nx.node_link_data(G, edges="edges") for forward compatibility.
    warnings.warn(

tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_text_succeeds
  /opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/
3.12/lib/python3.12/unittest/mock.py:478: RuntimeWarning: coroutine 'sleep' was
never awaited
    _safe_super(NonCallableMock, self).__init__(
  Enable tracemalloc to get traceback where the object was allocated.
  See
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more info.

tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_returns_typed_record
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/me
mory/test_memory_manager.py:66: DeprecationWarning: datetime.datetime.utcnow()
is deprecated and scheduled for removal in a future version. Use timezone-aware
objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    metadata={"edrr_phase": edrr_phase, "seen_at": datetime.utcnow()},

tests/unit/llm/test_openai_provider.py::TestOpenAIProviderStreaming::test_genera
te_stream_without_async_client
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_opena
i_provider.py:347: RuntimeWarning: coroutine 'OpenAIProvider.generate_stream'
was never awaited
    provider.generate_stream("Hello")
  Enable tracemalloc to get traceback where the object was allocated.
  See
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more info.

tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderStreaming::tes
t_generate_stream_without_httpx
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openr
outer_provider.py:315: RuntimeWarning: coroutine
'OpenRouterProvider.generate_stream' was never awaited
    provider.generate_stream("Hello")
  Enable tracemalloc to get traceback where the object was allocated.
  See
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more info.

tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_import
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_import returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_registration
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_registration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_initialization
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_initialization returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_basic.py::test_configuration
_loading
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_basic.py::test_configuration
_loading returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_heal
th_check
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_heal
th_check returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_comprehensive.py::test_lmstu
dio_integration
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_comprehensive.py::test_lmstu
dio_integration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_comprehensive.py::test_mock_
lmstudio_scenario
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_comprehensive.py::test_mock_
lmstudio_scenario returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_standalone.py::test_lmstudio
_integration
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_standalone.py::test_lmstudio
_integration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

tests/integration/general/test_lmstudio_integration_standalone.py::test_mock_lms
tudio_scenario
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions
should return None, but
tests/integration/general/test_lmstudio_integration_standalone.py::test_mock_lms
tudio_scenario returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for
more information.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.12.12-final-0 _______________

Coverage HTML written to dir htmlcov
Coverage JSON written to file test_reports/coverage.json
========================= Test Categorization Summary ==========================
Test Type Distribution:
  Unit Tests: 4008
  Integration Tests: 346
  Behavior Tests: 20

Test Speed Distribution:
  Fast Tests (< 1s): 4336
  Medium Tests (1-5s): 25
  Slow Tests (> 5s): 13
============================= Top 10 Slowest Tests =============================
1.
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_analyze_
devsynth_codebase_succeeds: 14.87s
2.
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_improvem
ent_opportunities_succeeds: 14.36s
3.
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_architec
ture_violations_succeeds: 14.30s
4.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_single_text: 10.49s
5.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_multiple_texts: 10.12s
6.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_basic_text: 10.10s
7.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_api_error: 10.05s
8.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_api_error_handling: 10.05s
9.
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_invalid_parameters: 10.03s
10.
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_with_connection_error_succeeds: 10.01s
=========================== short test summary info ============================
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error
FAILED
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum
FAILED
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.enhanced_analysis_cmd]
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-bat
ch]
FAILED
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-b
atches]
FAILED
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env
FAILED
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset
FAILED
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers
FAILED
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message
FAILED
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message
FAILED
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle
FAILED
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options
FAILED
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
FAILED
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent
FAILED
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled
FAILED
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated
FAILED
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback
FAILED
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary
FAILED
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery
FAILED
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag
FAILED
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success
FAILED
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure
FAILED
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold
FAILED
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression
FAILED
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error
FAILED
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key
FAILED
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models
FAILED
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property
FAILED
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property
FAILED
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic
FAILED
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion
FAILED
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line
FAILED
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence
FAILED
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior
FAILED
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records
FAILED
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task
FAILED
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement
FAILED
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state
FAILED
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
FAILED
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record
FAILED
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple
FAILED
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase
FAILED
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments
FAILED
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument
FAILED
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path
FAILED
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path
FAILED
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory
FAILED
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation
FAILED
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate
FAILED
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code
FAILED
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update
FAILED
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors
FAILED
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry
FAILED
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g
FAILED
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
FAILED
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug
FAILED
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess
FAILED
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests
FAILED
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env
FAILED
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
FAILED
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment
FAILED
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker
FAILED
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url
FAILED
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint
FAILED
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds
FAILED
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds
FAILED
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds
FAILED
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds
FAILED
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory
FAILED
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds
FAILED
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue
FAILED
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code
FAILED
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent
FAILED
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores
FAILED
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds
FAILED
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds
FAILED
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks
FAILED
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches
FAILED
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts
FAILED
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role
FAILED
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus
FAILED
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds
FAILED
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds
FAILED
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds
FAILED
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds
FAILED
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds
FAILED
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds
FAILED
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds
FAILED
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds
FAILED
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds
FAILED
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds
FAILED
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds
FAILED tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds
FAILED tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple
FAILED
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
FAILED
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution
FAILED
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity
FAILED
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted
FAILED
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie
FAILED tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote - Typ...
FAILED tests/unit/general/test_api_health.py::test_health_endpoint_succeeds
FAILED tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds
FAILED
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds
FAILED
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds
FAILED
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds
FAILED
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure
FAILED
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds
FAILED tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled - Attri...
FAILED tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency
FAILED
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected
FAILED
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds
FAILED
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds
FAILED
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction
FAILED
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization
FAILED
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds
FAILED tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success - A...
FAILED tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure - A...
FAILED
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds
FAILED
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds
FAILED
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds
FAILED
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds
FAILED
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds
FAILED
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds
FAILED tests/unit/general/test_speed_option.py::test_speed_option_recognized
FAILED tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds
FAILED tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds
FAILED
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds
FAILED
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds
FAILED
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches
FAILED
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds
FAILED
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error
FAILED
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error
FAILED
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds
FAILED
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds
FAILED
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds
FAILED
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds
FAILED
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds
FAILED
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error
FAILED
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches
FAILED
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds
FAILED tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True]
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False]
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False]
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[absent-3-False]
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance
FAILED
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run
FAILED
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session
FAILED
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets
FAILED
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors
FAILED
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy
FAILED
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents
FAILED
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status
FAILED
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks
FAILED
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks
FAILED
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations
FAILED
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle
FAILED
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec
FAILED
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute
FAILED
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds
FAILED
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[warning-warning]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[success-success]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[info-info]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[unexpected-write]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[# Overview-expected_calls0]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[## Section-expected_calls1]
FAILED
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[### Deep Dive-expected_calls2]
FAILED
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once
FAILED
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta
FAILED
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance
FAILED
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance
FAILED
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time
FAILED
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
FAILED
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
FAILED
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
FAILED
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance
FAILED
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
FAILED
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module
FAILED
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation
FAILED
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds
FAILED
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
FAILED
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors
FAILED
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock
FAILED
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion
FAILED
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error
FAILED
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[error-kwargs0-error]
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[warning-kwargs1-warning]
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[success-kwargs2-success]
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[highlight-kwargs3-info]
FAILED
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats
FAILED
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance
FAILED
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
FAILED
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
FAILED
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
FAILED
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance
FAILED
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling
FAILED
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once
FAILED
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle
FAILED
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir]
FAILED
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[home-absolute]
FAILED
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[non-home-absolute]
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception
FAILED
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type
FAILED
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results
FAILED
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure
FAILED
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results
FAILED
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete
FAILED
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure
FAILED
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations
FAILED
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget
FAILED
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase
FAILED
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
FAILED
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
FAILED
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete
FAILED
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries
FAILED
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called
FAILED
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step
FAILED
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline
FAILED
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted
FAILED
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages
FAILED
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success
FAILED
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure
FAILED
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
FAILED
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid
FAILED
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors
FAILED
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation
FAILED
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics
FAILED
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error
FAILED tests/unit/security/test_policy_audit.py::test_audit_detects_violation
FAILED tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file
FAILED tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy
FAILED
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate
FAILED
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran
FAILED
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches
FAILED
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty
FAILED
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json
FAILED
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change
FAILED
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change
FAILED
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change
FAILED
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call
FAILED
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh
FAILED
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry
FAILED
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch
FAILED
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty
FAILED
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids
FAILED
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty
FAILED
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids
FAILED
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming
FAILED
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow
FAILED
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches
FAILED
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes
FAILED
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout
FAILED
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception
FAILED
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error
FAILED
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker
FAILED
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html
FAILED
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context
FAILED
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success
FAILED
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches
FAILED
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache
FAILED
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins
FAILED
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates
FAILED
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts
FAILED
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile
FAILED
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled
FAILED
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable
FAILED
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success
FAILED
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return
FAILED
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success
FAILED
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression
FAILED
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression
FAILED
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests
FAILED
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates
FAILED
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success
FAILED
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags
FAILED
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message
FAILED
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir
FAILED
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success
FAILED
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache
FAILED
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired
FAILED
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss
FAILED
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime
FAILED
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all
FAILED
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories
FAILED
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl
FAILED
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes
FAILED
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio
FAILED
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters
FAILED
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path
FAILED
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return
FAILED
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips
FAILED
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions
FAILED
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command
FAILED
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command
FAILED
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command
FAILED
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run
FAILED
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto
FAILED
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto
FAILED
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout
FAILED
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once
FAILED
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir
FAILED
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success
FAILED
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips
FAILED
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once
FAILED
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail
FAILED
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection
FAILED
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success
FAILED
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_ports
FAILED
tests/integration/deployment/test_deployment_scripts.py::test_prometheus_exporte
r_refuses_root
FAILED
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_collection
_performance
FAILED
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_analysis_p
erformance
FAILED tests/behavior/test_documentation_generation.py::test_docs_build - Run...
FAILED
tests/unit/adapters/cli/test_typer_adapter.py::test_warn_if_features_disabled_al
l_disabled_succeeds
FAILED tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_succeeds
FAILED tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_table_mode
FAILED
tests/unit/adapters/cli/test_typer_adapter.py::test_parse_args_has_expected
FAILED tests/unit/adapters/cli/test_typer_adapter.py::test_run_cli_succeeds
FAILED
tests/unit/adapters/cli/test_typer_adapter.py::test_dashboard_hook_option_regist
ers
FAILED
tests/unit/adapters/test_kuzu_memory_store.py::test_store_and_search_succeeds
FAILED
tests/unit/adapters/test_kuzu_memory_store.py::test_create_ephemeral_fallback
FAILED
tests/unit/adapters/test_kuzu_memory_store.py::test_create_ephemeral_embedded
FAILED
tests/unit/adapters/test_kuzu_memory_store.py::test_store_failure_raises_memory_
store_error
FAILED
tests/unit/adapters/test_kuzu_memory_store.py::test_delete_returns_false_on_erro
r
FAILED
tests/unit/adapters/test_provider_factory.py::test_create_provider_env_fallback_
has_expected
FAILED
tests/unit/adapters/test_provider_factory.py::test_explicit_openai_missing_key_r
aises_error
FAILED
tests/unit/adapters/test_provider_factory_env_vars.py::test_env_provider_openai_
succeeds
FAILED
tests/unit/adapters/test_provider_factory_env_vars.py::test_env_provider_lmstudi
o_succeeds
FAILED
tests/unit/application/cli/commands/test_help_rendering.py::test_display_command
_help_markdown_renders_markdown
FAILED
tests/unit/application/cli/test_completion_cmd.py::test_completion_cmd_outputs_s
cript
FAILED
tests/unit/application/cli/test_completion_cmd.py::test_completion_cmd_installs_
script
FAILED
tests/unit/application/cli/test_config_validation.py::test_config_warnings_succe
eds
FAILED
tests/unit/application/cli/test_config_validation.py::test_config_success_succee
ds
FAILED
tests/unit/application/cli/test_ingest_phases.py::test_retrospect_phase_succeeds
FAILED
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_creates_config_succee
ds
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_rejects_inval
id_target
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_rejects_inval
id_speed
FAILED
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_reports_disab
led_coverage
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_initialization_succeeds
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_build_consensus_no_conflicts_succeeds
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_build_consensus_with_conflicts_succeeds
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_tie_breaking_strategies_succeeds
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_decision_tracking_and_explanation_succeeds
FAILED
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_associate_subtasks_succeeds
FAILED
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorPr
imusSelection::test_primus_selection_and_consensus_fields_succeeds
FAILED
tests/unit/application/collaboration/test_wsde_phase_transition_and_memory_flush
.py::test_flush_memory_queue_waits_for_sync
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_basic
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_with_error_handling
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_create_micro_cycle
FAILED
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_with_micro_cycles
FAILED
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_progress_to_
phase_runs_succeeds
FAILED
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_execute_expa
nd_phase_succeeds
FAILED
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
initialization_succeeds
FAILED
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
expand_phase_execution_has_expected
FAILED
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
progress_to_phase_has_expected
FAILED
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
full_cycle_succeeds
FAILED
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_progress_to_phase_has_expected
FAILED
tests/unit/application/edrr/test_execute_single_agent_task.py::test_execute_sing
le_agent_task_stores_result_and_calls_agent
FAILED
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_execution_succeeds
FAILED
tests/unit/application/edrr/test_phase_progression.py::test_auto_phase_progressi
on_succeeds
FAILED
tests/unit/application/edrr/test_phase_progression.py::test_result_aggregation_a
fter_full_cycle_has_expected
FAILED
tests/unit/application/edrr/test_progress_recursion.py::test_progress_to_phase_a
uto_recursion_succeeds
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_expand_phase_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_differentiate_phase_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_refine_phase_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_retrospect_phase_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_phase_complete_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_timeout_has_expected
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_no_transition_returns_expected_result
FAILED
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_cycle_updates_parent_results_succeeds
FAILED
tests/unit/application/llm/test_import_without_lmstudio.py::test_import_lmstudio
_provider_without_lmstudio_succeeds
FAILED
tests/unit/application/llm/test_import_without_lmstudio.py::test_factory_missing
_lmstudio_provider_raises_clear_error
FAILED
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_research_artifact_traversal_and_reload
FAILED
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_context_aware_query_succeeds
FAILED
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_query_router_route_succeeds
FAILED
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_store_and_retrieve_with_edrr_phase_has_expected
FAILED
tests/unit/application/memory/test_memory_adapters_regression.py::test_tinydb_se
rializes_unhandled_types
FAILED
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerStore::te
st_store_prefers_graph_for_edrr_succeeds
FAILED
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_succeeds
FAILED
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_with_metadata_succeeds
FAILED
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_store_
vector_succeeds
FAILED
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_get_co
llection_stats_succeeds
FAILED
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_replay
FAILED
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_replay_wi
th_time_range
FAILED
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_recove
r_store
FAILED
tests/unit/application/memory/test_recovery.py::TestWithRecovery::test_successfu
l_execution
FAILED
tests/unit/application/memory/test_recovery.py::TestWithRecovery::test_no_snapsh
ot_creation
FAILED
tests/unit/application/memory/test_sync_wrappers.py::test_cross_store_query_and_
update_wrappers
FAILED
tests/unit/application/test_offline_provider_cli.py::test_generate_does_not_call
_external_succeeds
FAILED
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_aut
o_tuner_initialization_succeeds
FAILED
tests/unit/application/wsde/test_wsde_utils.py::test_run_consensus_falls_back_to
_build
FAILED
tests/unit/behavior/test_wsde_team_extended.py::test_summarize_and_store_consens
us
FAILED
tests/unit/core/test_config_loader.py::test_load_config_normalizes_mvuu_section
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_exact_match_matches_expec
ted
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_partial_match_matches_exp
ected
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_experience_level_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_performance_history_succe
eds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_nested_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_phase_expertise_score_has_expected
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_code_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_doc_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_security_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_rotation_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_unused_priority_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_code_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_doc_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_testing_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_security_task_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_identify_domain_conflicts_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_prioritize_critiques_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_resolve_code_improvement_conflict_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_resolve_content_improvement_conflict_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_code_standards_compliance_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_content_standards_compliance_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_pep8_compliance_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_security_best_practices_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_security_and_performance_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_security_and_usability_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_performance_and_maintainability_succeeds
FAILED
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_generate_detailed_synthesis_reasoning_succeeds
FAILED
tests/unit/domain/test_primus_selection_edge_cases.py::test_documentation_task_p
refers_doc_agents_succeeds
FAILED
tests/unit/domain/test_primus_selection_edge_cases.py::test_edge_case_coverage_s
ucceeds
FAILED
tests/unit/domain/test_wsde_core_methods.py::test_build_consensus_produces_resul
t
FAILED
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_retry_me
trics_and_callback
FAILED
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_conditio
n_callback_records_metrics
FAILED
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_retry_co
ndition_records_metrics
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_init_succeeds
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_spec_succeeds
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_test_succeeds
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_code_succeeds
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_run_succeeds - ...
FAILED tests/unit/general/test_cli.py::TestTyperCLI::test_cli_config_succeeds
FAILED
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_inspect_config_update_suc
ceeds
FAILED
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_inspect_config_prune_succ
eeds
FAILED
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_store_vector_succ
eeds
FAILED
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_get_collection_st
ats_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_suc
cess_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_test_cmd_suc
cess_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_code_cmd_suc
cess_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_run_pipeline
_cmd_success_with_target_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_run_pipeline
_cmd_success_without_target_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_s
et_value_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_g
et_value_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_l
ist_all_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_load_error_displays_error
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_save_error_displays_error
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_nonexistent_feature_creates_feature
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_already_enabled_feature_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_creates
_config_and_commands_use_loader_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_openai_key_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_chromadb_package_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_kuzu_package_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_key_a
utocomplete_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_check_servic
es_warns_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_doctor_cmd_i
nvokes_loader_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_check_cmd_al
ias_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_cmd_wiz
ard_runs_wizard
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_inv
alid_file
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_test_cmd_inv
alid_file
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_code_cmd_no_
tests
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_i
nvalid_key
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_gather_cmd_e
rror_propagates
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_refactor_cmd
_error
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_inspect_cmd_
failure
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_i
nvalid_framework
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_serve_cmd_pa
sses_options
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_f
lask_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_f
astapi_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_e
xisting_dir_without_force_fails
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_e
xisting_dir_with_force_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_invalid_type
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_sqlite_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_mysql_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_mongodb_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_existing_dir_without_force_fails
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_existing_dir_with_force_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_su
ccess_succeeds
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_im
port_error_displays_error
FAILED
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_ru
ntime_error_displays_error
FAILED
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_multi_agent_consensus_succeeds
FAILED
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_agent_failure_continues_fails
FAILED
tests/unit/interface/test_api_endpoints.py::test_health_endpoint_requires_authen
tication_succeeds
FAILED
tests/unit/interface/test_api_endpoints.py::test_metrics_endpoint_requires_authe
ntication_succeeds
FAILED
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_multiple_
subtasks_succeeds
FAILED
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_hea
ding_levels_succeeds
FAILED
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_sma
rt_styling_succeeds
FAILED
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_ric
h_markup_succeeds
FAILED
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_hig
hlight_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_highli
ght_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_error_
succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_warnin
g_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_succes
s_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_headin
g_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_subhea
ding_succeeds
FAILED
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_normal
_succeeds
FAILED
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_escapes_html_
succeeds
FAILED
tests/unit/interface/test_output_sanitization.py::test_webui_sanitizes_output_su
cceeds
FAILED
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_removes_self_
closing_script
FAILED
tests/unit/interface/test_state_access.py::test_get_session_value_with_exception
FAILED
tests/unit/interface/test_state_access.py::test_set_session_value_with_attribute
_exception
FAILED
tests/unit/interface/test_state_access.py::test_set_session_value_with_dict_exce
ption
FAILED
tests/unit/interface/test_state_access.py::test_set_session_value_with_both_exce
ptions
FAILED tests/unit/interface/test_ux_bridge.py::test_function - AssertionError...
FAILED tests/unit/interface/test_uxbridge.py::test_prompt_and_result_consistency
FAILED
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_cli_succeeds
FAILED
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_webui_succeeds
FAILED
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_api_succeeds
FAILED
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_webui_fallback_succeeds
FAILED
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_api_fallback_succeeds
FAILED
tests/unit/interface/test_uxbridge_question_result.py::test_ask_question_and_dis
play_result_consistency[_cli_bridge]
FAILED tests/unit/interface/test_uxbridge_sanitization.py::test_with_clean_state
FAILED
tests/unit/interface/test_uxbridge_sanitization.py::test_webui_sanitizes_display
_result_succeeds
FAILED tests/unit/interface/test_webui_cli_imports.py::test_with_clean_state
FAILED
tests/unit/interface/test_webui_error_handling.py::test_init_cmd_error_handling
FAILED
tests/unit/interface/test_webui_error_handling.py::test_onboarding_page_setup_wi
zard_error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_spec_c
md_error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_inspec
t_cmd_error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_file_n
ot_found_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_analysis_page_inspect_co
de_cmd_error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_synthesis_page_test_cmd_
error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_config_page_load_config_
error_raises_error
FAILED
tests/unit/interface/test_webui_error_handling.py::test_config_page_save_config_
error_raises_error
FAILED
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_start_butto
n_not_clicked
FAILED
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_finish_call
s_gather_requirements
FAILED
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_import_erro
r
FAILED
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_exception
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
initialization_with_state
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
navigation_with_state
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
data_persistence_with_state
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
error_handling_with_state
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
validation_with_state
FAILED
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
start_resets_state
FAILED
tests/unit/interface/test_webui_navigation_and_validation.py::test_navigation_pe
rsists_wizard_state
FAILED
tests/unit/interface/test_webui_navigation_and_validation.py::test_analysis_page
_invalid_path_shows_error
FAILED
tests/unit/interface/test_webui_onboarding.py::test_onboarding_page_succeeds
FAILED
tests/unit/interface/test_webui_onboarding.py::test_onboarding_page_no_submit_su
cceeds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_init_succeeds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_update_succeeds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_succeeds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_cascades_
subtasks
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_add_subtask_succee
ds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_update_subtask_suc
ceeds
FAILED
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_subtask_s
ucceeds
FAILED
tests/unit/interface/test_webui_requirements.py::test_requirements_page_succeeds
FAILED
tests/unit/interface/test_webui_requirements.py::test_requirements_wizard_succee
ds
FAILED
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_in_streamlit_
context
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_get_wizard_state_new
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_get_wizard_state_existin
g
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_va
lid
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_mi
ssing_key
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_in
valid_step
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_mi
smatched_steps
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_reset_wizard_state
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_reset_wizard_state_error
FAILED tests/unit/interface/test_wizard_state_manager.py::test_get_current_step
FAILED tests/unit/interface/test_wizard_state_manager.py::test_go_to_step - R...
FAILED tests/unit/interface/test_wizard_state_manager.py::test_next_step - Ru...
FAILED tests/unit/interface/test_wizard_state_manager.py::test_previous_step
FAILED tests/unit/interface/test_wizard_state_manager.py::test_set_completed
FAILED tests/unit/interface/test_wizard_state_manager.py::test_is_completed
FAILED tests/unit/interface/test_wizard_state_manager.py::test_get_value - Ru...
FAILED tests/unit/interface/test_wizard_state_manager.py::test_set_value - Ru...
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_simulate_wizard_manager_
navigation
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_set_wizard_manager_data
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_gather_wizard_state_mana
ger
FAILED
tests/unit/interface/test_wizard_state_manager.py::test_gather_wizard_workflow
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_basic_text
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_custom_parameters
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_context
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_auto_model_selection
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_invalid_parameters
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderStreaming::test_ge
nerate_stream_with_context
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEmbeddings::test_g
et_embedding_single_text
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEmbeddings::test_g
et_embedding_multiple_texts
FAILED
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_server_connection_error_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_basic_text
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_with_context
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_with_invalid_parameters
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_api_error_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_rate_limit_handling
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderStreaming::test_genera
te_stream_without_async_client
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_single_text
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_multiple_texts
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_api_error
FAILED
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_on_transient_errors
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_basic_text
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_custom_parameters
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_context
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_invalid_parameters
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_api_error_handling
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_rate_limit_handling
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderStreaming::tes
t_generate_stream_without_httpx
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_single_text
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_multiple_texts
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_api_error
FAILED
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_on_transient_errors
FAILED
tests/unit/methodology/test_dialectical_reasoner.py::test_evaluate_change_persis
ts_reasoning_to_memory
FAILED
tests/unit/methodology/test_wsde_edrr_coordinator.py::test_phase_progress_flushe
s_pending_memory_before_and_after
FAILED
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.EXPAND]
FAILED
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.DIFFERENTIATE]
FAILED
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.REFINE]
FAILED
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.RETROSPECT]
FAILED tests/integration/general/test_agent_api.py::test_init_route_succeeds
FAILED tests/integration/general/test_agent_api.py::test_gather_route_succeeds
FAILED
tests/integration/general/test_agent_api.py::test_synthesize_and_status_succeeds
FAILED tests/integration/general/test_agent_api.py::test_spec_route_succeeds
FAILED tests/integration/general/test_agent_api.py::test_test_route_succeeds
FAILED tests/integration/general/test_agent_api.py::test_code_route_succeeds
FAILED tests/integration/general/test_agent_api.py::test_doctor_route_succeeds
FAILED
tests/integration/general/test_agent_api.py::test_edrr_cycle_route_succeeds
FAILED
tests/integration/general/test_agent_api_security.py::test_api_requires_authenti
cation_succeeds
FAILED
tests/integration/general/test_agent_api_security.py::test_api_authentication_di
sabled_succeeds
FAILED
tests/integration/general/test_agent_api_security.py::test_api_health_endpoint_s
ucceeds
FAILED
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_create_team_succeeds
FAILED
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_peer_review_workflow_succeeds
FAILED tests/integration/general/test_agentapi.py::test_init_succeeds - Asser...
FAILED tests/integration/general/test_agentapi.py::test_gather_succeeds - Att...
FAILED
tests/integration/general/test_agentapi.py::test_synthesize_and_status_succeeds
FAILED
tests/integration/general/test_agentapi_routes.py::test_init_route_succeeds
FAILED
tests/integration/general/test_agentapi_routes.py::test_gather_route_succeeds
FAILED
tests/integration/general/test_agentapi_routes.py::test_synthesize_and_status_su
cceeds
FAILED
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_test_command_pipeline_succeeds
FAILED
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_code_command_pipeline_succeeds
FAILED
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_error_handling_in_pipeline_raises_error
FAILED
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_webui_command_pipeline_succeeds
FAILED
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_webui_command_error_handling_succeeds
FAILED
tests/integration/general/test_collaborative_decision_making.py::test_dynamic_ro
le_reassignment_integration_succeeds
FAILED
tests/integration/general/test_collaborative_decision_making.py::test_consensus_
vote_tie_breaker_succeeds
FAILED
tests/integration/general/test_collaborative_voting.py::test_majority_voting_suc
ceeds
FAILED
tests/integration/general/test_collaborative_voting.py::test_weighted_voting_suc
ceeds
FAILED
tests/integration/general/test_collaborative_voting.py::test_voting_result_syncs
_to_memory
FAILED
tests/integration/general/test_complex_workflow.py::TestComplexWorkflow::test_co
mplex_workflow_with_inconsistent_state_succeeds
FAILED
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_project_state_analyzer_with_external_codebase_succeeds
FAILED
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_end_to_end_workflow_succeeds
FAILED
tests/integration/general/test_config_loader.py::test_load_and_save_yaml_config_
succeeds
FAILED
tests/integration/general/test_config_loader.py::test_load_and_save_pyproject_co
nfig_succeeds
FAILED
tests/integration/general/test_delegate_task.py::test_delegate_task_team_consens
us_succeeds
FAILED
tests/integration/general/test_delegate_task_consensus.py::test_delegate_task_te
am_consensus_succeeds
FAILED
tests/integration/general/test_delegate_task_consensus.py::test_critical_decisio
n_tied_vote_falls_back_to_consensus_succeeds
FAILED
tests/integration/general/test_delegate_task_primus_selection.py::test_primus_ro
tation_resets_after_all_have_served_succeeds
FAILED
tests/integration/general/test_delegate_task_workflow.py::test_delegate_task_ful
l_workflow_succeeds
FAILED
tests/integration/general/test_end_to_end_workflow.py::TestEndToEndWorkflow::tes
t_complete_workflow_succeeds
FAILED
tests/integration/general/test_end_to_end_workflow.py::TestEndToEndWorkflow::tes
t_inconsistent_project_workflow_succeeds
FAILED
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_edrr_wsde_integration_raise
s_error
FAILED
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_memory_integration_raises_e
rror
FAILED
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_code_analysis_integration_r
aises_error
FAILED
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_recovery_in_edrr_cycle_raises_error
FAILED
tests/integration/general/test_lmstudio_integration_real.py::test_real_lmstudio_
integration
FAILED
tests/integration/general/test_lmstudio_integration_real.py::test_lmstudio_model
s
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_with_default_config_succeeds
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_with_specified_model_succeeds
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_without_api_key_uses_stub_in_offline
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_integration_succeeds
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_with_context_integration_succeeds
FAILED
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_get_
embedding_integration_succeeds
FAILED
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_devsynth_project_succeeds
FAILED
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_with_missing_files_succeeds
FAILED
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_with_requirements_and_code_succeeds
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_openai_provider_with_different_models_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_openai_provider_with_different_parameters_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_lm_studio_provider_with_different_endpoints_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_lm_studio_provider_with_different_parameters_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_fallback_provider_with_different_configurations_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_provider_system_with_different_default_providers_has_expected
FAILED
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_provider_system_with_context_aware_completion_has_expected
FAILED
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_direct_query
FAILED
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_cross_store_query
FAILED
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_cross_store_query_subset
FAILED
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_federated_query
FAILED
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_analyze_
devsynth_codebase_succeeds
FAILED
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_retrospective_flushes_pending_memory_without_record
FAILED
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_peer
_review_stores_in_memory
FAILED
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_cros
s_store_synchronization
FAILED
tests/integration/generated_tests/test_scaffold_output.py::test_scaffold_integra
tion_tests_creates_files
FAILED
tests/integration/interface/test_bridge_consistency.py::test_bridge_output_consi
stency_succeeds
FAILED
tests/integration/interface/test_multi_agent_collaboration.py::test_cli_and_api_
bridges_multi_agent_consistent
FAILED
tests/integration/interface/test_multi_agent_collaboration.py::test_vote_with_ro
le_reassignment_succeeds
FAILED
tests/integration/interface/test_mvuu_dashboard_report.py::test_dashboard_render
s_from_generated_report
FAILED
tests/integration/interface/test_small_workflow_bridge.py::test_cli_and_api_brid
ges_consistent_succeeds
FAILED
tests/integration/interface/test_webui_cli_lookup.py::test_cli_fallback_to_cli_m
odule
FAILED
tests/integration/live/test_openai_live_smoke.py::test_openai_chat_completion_li
ve_smoke
FAILED
tests/integration/live/test_openai_live_smoke.py::test_openai_embeddings_live_sm
oke
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[offline]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[openrouter]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[openai]
FAILED
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[offline]
FAILED
tests/integration/llm/test_provider_selection_integration.py::test_offline_mode_
returns_offline_provider_for_app_wrapper
FAILED
tests/integration/llm/test_provider_selection_integration.py::test_offline_mode_
returns_offline_provider_for_providers_module
FAILED
tests/integration/llm/test_provider_selection_integration.py::test_openai_withou
t_api_key_raises_validation_error_in_providers
FAILED
tests/integration/llm/test_provider_selection_integration.py::test_openai_withou
t_api_key_raises_connection_error_in_app_wrapper
FAILED tests/integration/mvu/test_report_generation.py::test_report_generation
FAILED
tests/integration/wsde/test_wsde_edrr_integration.py::test_message_persisted_wit
h_edrr_phase
FAILED tests/behavior/test_agentapi.py::test_json_requests_succeeds - Asserti...
FAILED
tests/behavior/test_cli_help_and_completion.py::test_completion_command_outputs_
script
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0
ERROR
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts
ERROR
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true
ERROR
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env
ERROR
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env
ERROR
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled
ERROR
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient
ERROR
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect
ERROR
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights
ERROR
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics
ERROR
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers
ERROR
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results
ERROR
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url
ERROR
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds
ERROR tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup
ERROR
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker
ERROR
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes
ERROR
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key 'api'-key_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-]
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error
ERROR
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error
ERROR
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown
ERROR
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors
ERROR
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information
ERROR
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: File not found: config.yaml-Make sure the
file exists]
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Permission denied: secrets.env-necessary
permissions]
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Invalid value: bad input-Please check your
input]
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Missing key: 'api_key'-Verify that the
referenced key exists]
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Type error: wrong type-Check that all
inputs]
ERROR
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-]
ERROR
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown
ERROR
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once
ERROR
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status
ERROR
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions
ERROR
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router
ERROR
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors
ERROR
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout
ERROR
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router
ERROR
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints
ERROR
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render
ERROR
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once
ERROR
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output
ERROR
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks
ERROR
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection
ERROR
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors
ERROR
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router
ERROR
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed
ERROR
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed_cus
tom
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_recursion_depth_exceeded_s
ucceeds
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_recursion_depth_increments
_succeeds
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_abort_when_should_terminat
e_succeeds
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_store_metadata_and_results
_succeeds
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_parent_aggregates_after_mi
cro_phase_succeeds
ERROR
tests/unit/application/edrr/test_micro_cycle.py::test_create_micro_cycle_from_ma
nifest_dict_succeeds
ERROR tests/unit/interface/test_webui.py::test_onboarding_calls_init_succeeds
ERROR tests/unit/interface/test_webui.py::test_requirements_calls_spec_succeeds
ERROR tests/unit/interface/test_webui.py::test_analysis_calls_analyze_succeeds
ERROR tests/unit/interface/test_webui.py::test_synthesis_buttons_succeeds - I...
ERROR tests/unit/interface/test_webui.py::test_config_update_succeeds - Impor...
ERROR tests/unit/interface/test_webui.py::test_diagnostics_runs_doctor_succeeds
ERROR tests/unit/interface/test_webui.py::test_edrr_cycle_page_succeeds - Imp...
ERROR tests/unit/interface/test_webui.py::test_alignment_page_succeeds - Impo...
ERROR tests/unit/interface/test_webui.py::test_alignment_metrics_page_succeeds
ERROR tests/unit/interface/test_webui.py::test_inspect_config_page_succeeds
ERROR tests/unit/interface/test_webui.py::test_validate_manifest_page_succeeds
ERROR tests/unit/interface/test_webui.py::test_validate_metadata_page_succeeds
ERROR tests/unit/interface/test_webui.py::test_test_metrics_page_succeeds - I...
ERROR tests/unit/interface/test_webui.py::test_docs_generation_page_succeeds
ERROR tests/unit/interface/test_webui.py::test_ingestion_page_succeeds - Impo...
ERROR tests/unit/interface/test_webui.py::test_apispec_page_succeeds - Import...
ERROR tests/unit/interface/test_webui.py::test_refactor_page_succeeds - Impor...
ERROR tests/unit/interface/test_webui.py::test_webapp_page_succeeds - ImportE...
ERROR tests/unit/interface/test_webui.py::test_serve_page_succeeds - ImportEr...
ERROR tests/unit/interface/test_webui.py::test_dbschema_page_succeeds - Impor...
ERROR tests/unit/interface/test_webui.py::test_doctor_page_succeeds - ImportE...
ERROR tests/unit/interface/test_webui.py::test_run_method_renders_pages_succeeds
ERROR
tests/unit/interface/test_webui.py::test_wizard_navigation_helper_clamps_steps
ERROR
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_init_command_pipeline_succeeds
ERROR
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_spec_command_pipeline_succeeds
ERROR
tests/integration/general/test_requirements_gathering.py::test_gather_updates_co
nfig_succeeds
ERROR
tests/integration/general/test_requirements_gathering.py::test_requirements_wiza
rd_persists_priority_succeeds
ERROR
tests/integration/general/test_requirements_gathering.py::test_requirements_wiza
rd_backtracks_priority_succeeds
ERROR
tests/integration/general/test_requirements_gathering.py::test_gather_cmd_loggin
g_exc_info_succeeds
ERROR
tests/integration/general/test_webui_pages.py::test_webui_pages_invoke_commands_
succeeds
ERROR
tests/integration/interface/test_webui_run_navigation.py::test_run_injects_asset
s_and_resets_navigation
ERROR
tests/integration/interface/test_webui_run_navigation.py::test_run_handles_page_
config_errors
= 931 failed, 3248 passed, 195 skipped, 9 deselected, 5 xfailed, 525721
warnings, 232 errors in 402.11s (0:06:42) =
--- Logging error ---
Traceback (most recent call last):
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/_ws_thread.py", line 142, in _log_thread_execution
    await never_set.wait()
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/locks.py", line 212, in wait
    await fut
asyncio.exceptions.CancelledError: Cancelled via cancel scope 16ca8ea50 by <Task
pending name='Task-661' coro=<AsyncTaskManager.run_until_terminated() running at
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/_ws_impl.py:214> cb=[_run_until_complete_cb() at
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/asyncio/base_events.py:181]>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
ValueError: I/O operation on closed file.
Call stack:
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/_ws_thread.py", line 71, in run
    asyncio.run(self._task_manager.run_until_terminated(self._task_target))
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/base_events.py", line 678, in run_until_complete
    self.run_forever()
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/base_events.py", line 645, in run_forever
    self._run_once()
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/base_events.py", line 1999, in _run_once
    handle._run()
  File
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/asyncio/events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/_ws_thread.py", line 149, in _log_thread_execution
    self._logger.info("Websocket thread terminated")
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/_logging.py", line 106, in info
    self._log(logging.INFO, msg, exc_info, stack_info, stacklevel, event_dict)
  File
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/_logging.py", line 62, in _log
    self._stdlib_logger.log(
Message: <lmstudio._logging.StructuredLogEvent object at 0x11fbf6f00>
Arguments: ()

Pytest exited with code 1. Command:
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_usage_hint
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_runtime_hin
t
tests/unit/adapters/cli/test_typer_adapter.py::test_command_help_format_includes
_sections
tests/unit/adapters/issues/test_github_adapter.py::test_fetch_github_issue
tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_normalizes
_mapping
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_without_pa
rameters_returns_none
tests/unit/adapters/llm/test_llm_adapter.py::test_default_factory_delegates_to_g
lobal_registry
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_uses_injected_
factory
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_emits_typed_er
ror_for_unknown_provider
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_maps_registere
d_message
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_propaga
tes_factory_rejection
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_success
tests/unit/adapters/llm/test_llm_adapter.py::test_unknown_llm_provider_error_pre
serves_cause
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_stream
_returns_chunks
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_with_c
ontext_stream_returns_chunks
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_chunk_response_
helper_respects_chunk_size
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_stream_chunks_y
ields_all_segments
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_mock_response_templa
te_serializes
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_round_trip_pr
eserves_defaults
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_matches_cus
tom_template
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_uses_defaul
t_when_no_template_matches
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
coerces_sequences
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
falls_back_to_defaults
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_adapter_initialises_
from_mapping
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_stream_prop
agates_generate_failure
tests/unit/adapters/test_agent_adapter.py::test_factory_initializes_agent_with_c
onfig_payload
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_builds_consensus_p
ayload_from_solutions
tests/unit/adapters/test_agent_adapter.py::test_process_task_without_agents_rais
es_validation_error
tests/unit/adapters/test_agent_adapter.py::test_coerce_task_solutions_filters_in
valid_entries
tests/unit/adapters/test_agent_adapter.py::test_import_agent_falls_back_on_error
tests/unit/adapters/test_agent_adapter.py::test_import_agent_rejects_non_class
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_caches_result
s
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_processing
_failures
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_critical_d
ecisions
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_requires_active_te
am
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_singl
e_agent_flow
tests/unit/adapters/test_agent_adapter.py::test_agent_initialization_payload_han
dles_unknown_type
tests/unit/adapters/test_agent_adapter.py::test_coerce_helpers_normalize_inputs
tests/unit/adapters/test_agent_adapter.py::test_unified_agent_fallback_behaviour
tests/unit/adapters/test_agent_adapter.py::test_load_default_config_uses_yaml_lo
ader
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_uses_future_s
pecs
tests/unit/adapters/test_agent_adapter.py::test_create_team_uses_collaborative_w
hen_memory_manager
tests/unit/adapters/test_agent_adapter.py::test_add_agent_creates_default_team
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_multi
_agent_path
tests/unit/adapters/test_backend_resource_gates.py::test_chromadb_adapter_import
s tests/unit/adapters/test_backend_resource_gates.py::test_kuzu_adapter_imports
tests/unit/adapters/test_backend_resource_gates.py::test_faiss_store_imports_and
_minimal
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_transaction_commit_
and_delete
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_provider_fallback_u
ses_default_embedder
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_store_raises_after_
retries
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_search_handles_empt
y_results
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_commit_failure_mark
s_transaction
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_rollback_transactio
n_states
tests/unit/adapters/test_fake_memory_store.py::test_fake_memory_store_store_retr
ieve_search_delete_and_txn
tests/unit/adapters/test_fake_memory_store.py::test_fake_vector_store_similarity
_and_stats
tests/unit/adapters/test_github_project_adapter.py::test_payload_serialization
tests/unit/adapters/test_github_project_adapter.py::test_graphql_request_payload
_and_helpers
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_creates_colu
mns_and_cards
tests/unit/adapters/test_github_project_adapter.py::test_fetch_and_mutations_wit
h_stub_client
tests/unit/adapters/test_github_project_adapter.py::test_graphql_missing_data_ra
ises
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_skips_existi
ng_items
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_raises_on_gr
aphql_errors
tests/unit/adapters/test_github_project_adapter.py::test_graphql_error_formattin
g_handles_missing_messages
tests/unit/adapters/test_jira_adapter.py::test_create_issue_payload_serializatio
n tests/unit/adapters/test_jira_adapter.py::test_transition_issue_missing_status
tests/unit/adapters/test_jira_adapter.py::test_create_issue_http_error_surfaced
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_i
nit_creates_empty_adapter
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_l
oad_model_sets_session
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_without_loaded_model_raises_error
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_with_loaded_model_calls_session_run
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_multiple_outputs
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_empty_inputs
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_propagates_onnx_exceptions
tests/unit/adapters/test_provider_safe_defaults.py::test_default_safe_falls_back
_to_stub_without_keys_and_lmstudio
tests/unit/adapters/test_provider_safe_defaults.py::test_openai_explicit_without
_key_raises
tests/unit/adapters/test_provider_safe_defaults.py::test_anthropic_implicit_with
out_key_falls_back_safe_default_stub
tests/unit/adapters/test_provider_safe_defaults.py::test_lmstudio_not_attempted_
without_availability_flag
tests/unit/adapters/test_provider_safe_defaults.py::test_disable_providers_retur
ns_null
tests/unit/adapters/test_provider_stub.py::test_stub_provider_complete_and_embed
_are_deterministic
tests/unit/adapters/test_provider_stub.py::test_stub_provider_async_matches_sync
tests/unit/adapters/test_provider_stub.py::test_provider_system_reload_preserves
_settings_import
tests/unit/adapters/test_provider_system.py::test_embed_success_succeeds
tests/unit/adapters/test_provider_system.py::test_embed_error_succeeds
tests/unit/adapters/test_provider_system.py::test_aembed_success_succeeds
tests/unit/adapters/test_provider_system.py::test_aembed_error_succeeds
tests/unit/adapters/test_provider_system.py::test_complete_success_succeeds
tests/unit/adapters/test_provider_system.py::test_complete_error_succeeds
tests/unit/adapters/test_provider_system.py::test_acomplete_success_succeeds
tests/unit/adapters/test_provider_system.py::test_acomplete_error_succeeds
tests/unit/adapters/test_provider_system.py::test_null_provider_complete_raises_
error
tests/unit/adapters/test_provider_system.py::test_null_provider_acomplete_raises
_error
tests/unit/adapters/test_provider_system.py::test_null_provider_embed_raises_err
or
tests/unit/adapters/test_provider_system.py::test_null_provider_aembed_raises_er
ror
tests/unit/adapters/test_provider_system.py::test_null_provider_initialization
tests/unit/adapters/test_provider_system.py::test_provider_factory_create_provid
er_succeeds
tests/unit/adapters/test_provider_system.py::test_get_provider_succeeds
tests/unit/adapters/test_provider_system.py::test_base_provider_methods_succeeds
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[OpenAIProvider-config0]
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[LMStudioProvider-config1]
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_initializati
on_skips_health_check_when_network_guard_active
tests/unit/adapters/test_provider_system.py::test_fallback_provider_succeeds
tests/unit/adapters/test_provider_system.py::test_load_env_file_populates_config
tests/unit/adapters/test_provider_system.py::test_create_tls_config_has_expected
tests/unit/adapters/test_provider_system.py::test_get_env_or_default_succeeds
tests/unit/adapters/test_provider_system.py::test_get_provider_config_has_expect
ed
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_has_e
xpected
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_error
_raises_error
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_retry
_has_expected
tests/unit/adapters/test_provider_system.py::test_openai_provider_acomplete_has_
expected
tests/unit/adapters/test_provider_system.py::test_openai_provider_embed_has_expe
cted
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_complete_has
_expected
tests/unit/adapters/test_provider_system.py::test_fallback_provider_async_method
s_has_expected
tests/unit/adapters/test_provider_system.py::test_provider_with_empty_inputs_has
_expected
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_selects_provider
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_survives_missing_settings
tests/unit/adapters/test_provider_system.py::test_fallback_provider_respects_ord
er
tests/unit/adapters/test_provider_system.py::test_openai_provider_retries_after_
transient_failure
tests/unit/adapters/test_provider_system.py::test_fallback_provider_circuit_brea
ker_blocks_after_failure
tests/unit/adapters/test_provider_system.py::test_complete_falls_back_to_next_pr
ovider
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_re
spects_disable_flag
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_falls_back_to_lmstudio_when_marked_available
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_req
uires_requests_dependency
tests/unit/adapters/test_provider_system_additional.py::test_lmstudio_provider_r
equires_requests_dependency
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_asy
nc_requires_httpx_dependency
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_defaults
_when_settings_missing
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_uses_exp
licit_settings
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_wir
ing
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_emi
ts_metrics_on_retry
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_n
o_valid_providers
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
ync_uses_circuit_breaker
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_failure_opens_breaker
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_respects_open_breaker
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_records_success
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
ll_failures_surface_last_error
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
hort_circuits_after_first_success
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
kips_providers_with_open_breakers
tests/unit/adapters/test_provider_system_additional.py::test_complete_failure_in
crements_metrics
tests/unit/adapters/test_provider_system_additional.py::test_embed_wraps_unexpec
ted_error
tests/unit/adapters/test_provider_system_additional.py::test_acomplete_failure_i
ncrements_metrics
tests/unit/adapters/test_provider_system_additional.py::test_aembed_wraps_unexpe
cted_error
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_uses_next_provider
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_raises_after_exhaustion
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_embed_wraps_une
xpected_exceptions
tests/unit/adapters/test_provider_system_resilience.py::test_base_provider_retry
_harness_records_jitter
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_a
sync_breaker_failure_emits_metrics
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_s
ync_breaker_failure_emits_metrics
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_skips_by_def
ault
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_runs_when_en
abled
tests/unit/adapters/test_storage_adapter_protocol.py::test_storage_adapter_proto
col_shape
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
turns_structure
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
gistered
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_returns_structure
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_registered
tests/unit/agents/test_multi_agent_coordinator.py::test_reach_consensus_majority
_choice
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_returns_structure
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_registered
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_returns_
structure
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_register
ed
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
existing_file
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
missing_file
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
empty_file
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
with_whitespace
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_boundary_value
s_prompt_loaded
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_error_conditio
ns_prompt_loaded
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_with_templates
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_without_templates
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_mixed_availability
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_template_direc
tory_path_construction
tests/unit/agents/test_tool_sandbox.py::test_file_access_restricted
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_blocked
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_allowed
tests/unit/agents/test_tool_sandbox.py::test_sandbox_context_restores_hooks
tests/unit/agents/test_tools.py::test_register_and_get_tool
tests/unit/agents/test_tools.py::test_unknown_tool_returns_none
tests/unit/agents/test_tools.py::test_export_for_openai_formats_tools
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_r
ecords_summary_and_flushes_memory
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_s
upports_primus_rotation_cycle
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict
tests/unit/api/test_public_api_contract.py::test_public_api_imports
tests/unit/api/test_public_api_contract.py::test_deprecated_wrapper_emits_warnin
g
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_initializa
tion_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_no_llm_port_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_no_llm_port_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_process_ab
stract_method_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_create_wsd
e_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_update_wsd
e_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_get_role_p
rompt_succeeds
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_error_raises_error
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_error_raises_error
tests/unit/application/agents/test_test_agent_integration.py::test_process_scaff
olds_tests_from_context
tests/unit/application/agents/test_validation_agent.py::test_process_affirmative
_is_valid_true
tests/unit/application/agents/test_validation_agent.py::test_process_failure_tok
ens_set_invalid
tests/unit/application/agents/test_validation_agent.py::test_process_neutral_tex
t_is_valid
tests/unit/application/agents/test_validation_agent.py::test_is_valid_word_bound
ary_only
tests/unit/application/agents/test_validation_agent.py::test_wsde_contains_agent
_and_role
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[All checks passed; no issues.-True]
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[An error occurred in module A.-False]
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Exception occurred during run.-False]
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Some tests fail on CI.-False]
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Clean run; everything looks good.-True]
tests/unit/application/agents/test_wsde_memory_integration_fast.py::test_store_a
nd_retrieve_dialectical_process
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_displays
_all_config
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_update_k
ey_value_saves_and_reports
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_list_mod
els_displays_models
tests/unit/application/cli/commands/test_config_cmd.py::test_enable_feature_cmd_
updates_and_saves
tests/unit/application/cli/commands/test_doctor_cmd_typed.py::test_doctor_cmd_ac
cepts_path_arguments
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py::test_doctor_cm
d_does_not_import_streamlit_or_nicegui
tests/unit/application/cli/commands/test_ingest_cli_command.py::test_ingest_cli_
command_uses_typed_options
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py::test_
inspect_code_cmd_sanitizes_dynamic_output
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_preserves_alias_after_subtask_rename
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_rebinds_alias_on_multiple_description_updates
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_reports_eta_strings_when_progress_advances
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_records_failure_history_for_diagnostics
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.align_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.alignment_metrics_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.analyze_manifest_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.atomic_rewrite_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.code_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.completion_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dbschema_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.diagnostics_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.doctor_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.documentation_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dpg_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.edrr_cycle_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.enhanced_analysis_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.extra_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.gather_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generate_docs_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generation_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.ingest_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.init_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_code_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_config_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.interface_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.metrics_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_exec_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_init_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_lint_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_report_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_rewrite_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvuu_dashboard_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.pipeline_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.refactor_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.reprioritize_issues_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_pipeline_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_tests_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.security_audit_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.serve_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.spec_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_metrics_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.testing_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_manifest_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_metadata_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validation_cmds]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_chunk_commit_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_fix_rebase_pr_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webapp_cmd]
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webui_cmd]
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_empty_list_returns_empty_dict
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_single_name_defaults_true
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_false_variants
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_true_variants
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
returns_mapping
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
invalid_json_returns_none
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
non_mapping_returns_none
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_allows_requests_
env_default_for_unit
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_smoke_mode_sets_
env_and_disables_parallel
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_feature_flag_map
ping_sets_env
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_marker_passthrou
gh
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_integration_targ
et_retains_cov_when_no_report
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_target_p
rints_error_and_exits
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_speed_pr
ints_error_and_exits
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inner_test_env_d
isables_plugins_and_parallel
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_verbose_and_fast
_timeout_env_behavior
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_report_mode_prin
ts_report_path_message
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_target
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_speed
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_segmented_run_injects_plugins_and_emits_failure_tips
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-bat
ch]
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-b
atches] tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_check_requi
red_env_raises_when_missing_env
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_cmd_happy_path_with_skips
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_runs_when_not_skipped
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_run_secrets
_scan_detects_simple_pattern
tests/unit/application/cli/commands/test_testing_cmd.py::testing_cmd
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_basic_functionality
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_shows_expected_content
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_uses_cli_bridge
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_logging_configuration
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_script_paths_checked
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_output_formatting
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_quick_actions_displayed
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_performance_achievements
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_phase_tasks_completed
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_group_cha
nges_categorizes_and_orders
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_generate_
message_includes_rationale_and_files
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_minimal_returns_text
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_simple_highlight_false_returns_str
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_standard_with_markup_returns_panel_passthrough
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_dict_and_list
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_unsupported_type_falls_back
tests/unit/application/cli/test_command_output_formatter.py::test_format_list_va
riants
tests/unit/application/cli/test_command_output_formatter.py::test_format_code_va
riants
tests/unit/application/cli/test_command_output_formatter.py::test_format_help_va
riants
tests/unit/application/cli/test_command_output_formatter.py::test_display_does_n
ot_raise
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_defaults
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_reads_yaml
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_is_exported
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_import_statement_works
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_protocol_alias_import_statement_works
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_aliases_listed_in_all
tests/unit/application/cli/test_long_running_progress.py::test_update_adapts_int
erval_and_checkpoints
tests/unit/application/cli/test_long_running_progress.py::test_status_history_tr
acks_unique_status_changes
tests/unit/application/cli/test_long_running_progress.py::test_summary_reflects_
fake_timeline_and_sanitizes_descriptions
tests/unit/application/cli/test_long_running_progress.py::test_subtask_updates_r
emap_and_short_circuit
tests/unit/application/cli/test_long_running_progress.py::test_subtask_completio
n_rolls_up_and_freezes_summary
tests/unit/application/cli/test_long_running_progress.py::test_subtask_checkpoin
t_spacing_respects_minimum
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_produces_deterministic_transcript
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_tracks_history_and_alias_renames
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_remains_deterministic_after_reload
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_stays_exported
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_direct_import_succeeds
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_upd
ate_thresholds_with_deterministic_clock
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_sub
task_flow_preserves_mappings_and_progress
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_run
_with_progress_completes_after_exception
tests/unit/application/cli/test_output.py::TestOutputType::test_output_type_valu
es
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
contains_all_types
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
values
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_in
fo
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_su
ccess
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_er
ror
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_info
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_succe
ss
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_error
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_is_concrete_class
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_available_at_import_time
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_protocol_exists
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_inherits_correctly
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_module_reload_preserves_base_class
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_import_from_module_works_after_reload
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_instantiation
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_has_expected_methods
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_deterministic_tests_can_import_base
tests/unit/application/cli/test_requirements_commands.py::test_wizard_cmd_back_n
avigation_succeeds
tests/unit/application/cli/test_requirements_commands.py::test_gather_requiremen
ts_cmd_yaml_succeeds
tests/unit/application/cli/test_requirements_commands.py::test_initialize_servic
es_configures_singletons
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_handles_empty_repository
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_renders_rich_table
tests/unit/application/cli/test_requirements_commands.py::test_create_requiremen
t_invokes_service
tests/unit/application/cli/test_requirements_gathering.py::test_gather_cmd_loggi
ng_exc_info_succeeds
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs
tests/unit/application/cli/test_setup_wizard_textual.py::test_textual_and_cli_pa
yloads_match
tests/unit/application/cli/test_setup_wizard_textual.py::test_requirements_wizar
d_supports_shortcut_navigation
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_planning_cmd_re
turns_structured_plan
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_defaults_when_missing
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_handles_invalid_json
tests/unit/application/code_analysis/test_analyzer.py::test_analyze_code_simple
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_add_docstring_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_complex_transformations_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_extract_function_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_optimize_string_literals_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_remove_unused_imports_and_variables_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_function_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_identifier_no_change
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_parameter_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_variable_succeeds
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_validate_syntax_is_valid
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_complexity_and_readability_metrics_succeeds
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_differentiate_selects_best_option_succeeds
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_expand_implementation_options_succeeds
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_refine_implementation_succeeds
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_retrospect_code_quality_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_initialization_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_index_files_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_detect_languages_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_categorize_file_assigns_lists
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_infer_architecture_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_identify_components_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_requirements_spec_alignment_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_generate_health_report_succeeds
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_analyze_maps_dependencies_and_structure
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_initialization_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_architecture_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_unknown
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_layers_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_layer_dependencies_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_check_architecture_violations_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_code_quality_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_improvement_opportunities_succeeds
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py::test_sel
f_analyzer_analyze_graceful_fallback
tests/unit/application/code_analysis/test_transformer.py::TestAstTransformer::te
st_record_change_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestRedundantAssignmen
tRemover::test_remove_redundant_assignments_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestStringLiteralOptim
izer::test_optimize_string_literals_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestCodeStyleTransform
er::test_improve_code_style_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_code_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_file_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_directory_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_find_python_files_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestSymbolUsageCounter
::test_count_symbol_usage_succeeds
tests/unit/application/code_analysis/test_transformer_basic.py::test_optimize_st
ring_literals_simple
tests/unit/application/code_analysis/test_transformer_helpers.py::test_apply_doc
string_spec_inserts_function_docstring
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_met
hod_from_function_respects_method_type
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_cla
ss_from_functions_wraps_functions
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_to_dict
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_accepts_string_payload
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_cr
eate_team_stores_in_memory
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_consensus_outcome_normalizes
_participants_and_metadata
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_peer_review_consensus_error_
embeds_serialized_outcome
tests/unit/application/collaboration/test_memory_utils_conversion.py::test_task_
round_trip_to_memory_item
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_colla
boration_payload_protocol_support
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_messa
ge_filter_rejects_invalid_input
tests/unit/application/collaboration/test_message_protocol.py::test_message_filt
er_invalid_timestamp_raises
tests/unit/application/collaboration/test_peer_review_store.py::test_store_in_me
mory_persists_peer_review_record
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_returns_review_decisions
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_failure_yields_error_decision
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_wraps_consensus_error_with_serialized_outcome
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_summar
ize_voting_result_persists_summary
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py::test_identify_conflicts_detects_opposing_opinions
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_tie
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_winner
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_consensus_result_methods
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_contradictions
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_different_approaches
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py::tes
t_peer_review_solution_excludes_author
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py::te
st_delegate_subtasks_assigns_best_agent
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_html_documentation_extracts_sections
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_markdown_documentation_respects_heading_levels
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_convert_docstrings_to_chunks_builds_expected_metadata
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_version_key_supports_numeric_sorting_and_literals
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_prefers_vector_results
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_falls_back_to_metadata_items
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_basic_creation
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_phase_context
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_details
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery
tests/unit/application/edrr/test_coordinator.py::test_run_micro_cycles_stops_aft
er_threshold
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::test_enhanced_dec
ide_next_phase_respects_auto_phase
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py::test_collect_ph
ase_metrics_uses_stubbed_helpers
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
missing_memory_manager
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flushes_
on_success
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
errors
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flush_fa
ilure_does_not_raise
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_norma
lizes_outputs
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_missi
ng_manager_returns_empty
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_witho
ut_support_returns_empty
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_stores_context
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_uses_deep_copy
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_ignores_empty
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_enforces_dependencies
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_updates_state
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_consumes_manual_override
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_requires_auto_transitions
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_returns_none_for_final_phase
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ne
xt_phase_rejects_final_phase
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error
tests/unit/application/edrr/test_recursion_termination.py::test_micro_cycle_resp
ects_depth_bounds
tests/unit/application/edrr/test_recursion_termination.py::test_complexity_thres
hold_triggers_termination
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_sp
rint_planning_phase_constant
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_basic
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_empty
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_partial
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_sprint_retrospective_phase_constant
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_basic
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_empty
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_none
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_partial
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_positive_in
t_handles_out_of_range
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_threshold_c
lamps_invalid_values
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_respects_config
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_returns_none_when_missing
tests/unit/application/edrr/test_threshold_helpers.py::test_get_micro_cycle_conf
ig_sanitizes_values
tests/unit/application/ingestion/test_ingestion_pure.py::test_is_artifact_change
d_respects_metadata_differences
tests/unit/application/ingestion/test_ingestion_pure.py::test_identify_improveme
nt_areas_flags_missing_manifest_information
tests/unit/application/ingestion/test_ingestion_pure.py::test_generate_recommend
ations_reflects_project_context
tests/unit/application/ingestion/test_phases.py::test_run_expand_phase_populates
_artifacts
tests/unit/application/ingestion/test_phases.py::test_run_differentiate_phase_us
es_structure
tests/unit/application/llm/test_import_without_openai.py::test_import_openai_pro
vider_without_openai_succeeds
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_in
valid_response_raises_model_error
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_initialization
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_lazy_import
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_caching
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_getattr_delegation
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_initialization
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_call
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_initialization
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_getattr
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_list_downloaded_models
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_configure_default_client
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_success
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_inheritance
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_inheritance
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_message
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_message
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_exists
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_has_expected_attributes
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_prefixes_with_offline
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_with_context_concatenates
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
et_embedding_is_deterministic
tests/unit/application/llm/test_openai_env_key_mock.py::test_openai_provider_use
s_mocked_env_key_without_network
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_succ
ess_offline
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_stre
am_yields_tokens_offline
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_inva
lid_response_raises_model_error
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_not_selected_when_flag_false
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_selected_when_flag_true
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_offlin
e_killswitch_overrides_explicit_selection
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault
tests/unit/application/memory/test_chromadb_store.py::test_store_and_retrieve_wi
th_fallback
tests/unit/application/memory/test_chromadb_store_typed.py::test_search_normaliz
es_serialized_rows
tests/unit/application/memory/test_chromadb_store_typed.py::test_fallback_retrie
ve_uses_serialization_helpers
tests/unit/application/memory/test_circuit_breaker.py::test_circuit_breaker_open
s_after_failures
tests/unit/application/memory/test_circuit_breaker.py::test_registry_returns_sam
e_instance
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_without_vector_extension_falls_back
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_configures_hnsw_when_enabled
tests/unit/application/memory/test_error_logger.py::test_log_error_enforces_max_
errors
tests/unit/application/memory/test_error_logger.py::test_log_error_accepts_neste
d_context
tests/unit/application/memory/test_error_logger.py::test_persist_errors_respects
_toggle
tests/unit/application/memory/test_error_logger.py::test_get_recent_errors_and_s
ummary
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_initialization
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_semantic_robustness_testing
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_get_learning_statistics
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_analyze_behavioral_intent
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior
tests/unit/application/memory/test_faiss_store.py::test_store_and_retrieve_round
_trip_preserves_metadata
tests/unit/application/memory/test_faiss_store.py::test_transaction_commit_persi
sts_changes
tests/unit/application/memory/test_faiss_store.py::test_transaction_rollback_res
tores_snapshot
tests/unit/application/memory/test_faiss_store.py::test_similarity_search_and_st
ats_ignore_deleted_vectors
tests/unit/application/memory/test_fast_in_memory_components.py::test_graph_memo
ry_adapter_in_memory_round_trip
tests/unit/application/memory/test_fast_in_memory_components.py::test_enhanced_g
raph_memory_adapter_edrr_round_trip
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_man
ager_sync_hooks_fire
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
saction_context_commit_and_rollback
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sys
tem_adapter_in_memory_components
tests/unit/application/memory/test_fast_in_memory_components.py::test_fallback_s
tore_falls_back_on_failure
tests/unit/application/memory/test_fast_in_memory_components.py::test_json_file_
store_round_trip
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sna
pshot_save_and_load
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_traverse_graph_depth_and_missing_nodes
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_begin_tran
saction_tracks_and_cleans_up
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_commit_tra
nsaction_persists_explicit_changes
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_rollback_t
ransaction_discards_explicit_changes
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_get_all_it
ems_returns_everything
tests/unit/application/memory/test_memory_manager.py::TestRouteQuery::test_route
_query_normalizes_context_mapping
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_regist
er_and_notify_sync_hook_succeeds
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_sync_h
ook_errors_are_logged
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
disabled_falls_back_to_memory
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
enabled_uses_adapter_and_store
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_various_backends
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_kuzu_init
ialization_and_fallback
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_lmdb_miss
ing_falls_back_to_memory
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_branches_execution
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_cache_and
_transaction_workflow
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_transacti
on_wrappers_raise_without_support
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_round_trip_preserves_metadata
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_from_row_handles_stringified_metadata
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_build
_memory_record_coerces_legacy_mapping
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_analyze_and_segment_task
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_multi_hop_reasoning
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_hybrid_llm_processing
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_contextual_prompts
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_integrate_and_validate_results
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_get_system_status
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_benchmark_against_research
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_validate_system_integrity
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_system_performance
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_memory_graph_integration_check
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execution_learning_integration_check
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_automata_metacognitive_integration_check
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt
tests/unit/application/memory/test_query_router.py::test_direct_query_and_vector
_branch
tests/unit/application/memory/test_query_router.py::test_cross_store_query_group
s_results
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
tests/unit/application/memory/test_query_router.py::test_context_aware_and_route
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_returns_existing_identifier
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_generates_uuid
tests/unit/application/memory/test_rdflib_store_transactions.py::test_transactio
n_methods_are_noops
tests/unit/application/memory/test_search_memory_fallback.py::test_search_memory
_fallback_without_vector_adapter_returns_results
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record
tests/unit/application/memory/test_sync_manager_transactions.py::test_transactio
n_rollback_uses_normalized_snapshots
tests/unit/application/memory/test_tiered_cache_termination.py::test_eviction_lo
op_terminates
tests/unit/application/memory/test_tiered_cache_termination.py::test_preserves_t
yped_values
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_default_
provider_registration
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_optional
_provider_guard
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_edrr_coo
rdinator_delegates_to_helper
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_returns_result
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_logs_consensus_failure
tests/unit/application/promises/test_agent_create_promise.py::test_create_promis
e_sets_metadata_and_parent_relationship
tests/unit/application/promises/test_interface_not_implemented.py::test_promise_
interface_id_not_implemented
tests/unit/application/promises/test_interface_pure.py::test_basic_promise_metad
ata_round_trip
tests/unit/application/promises/test_interface_pure.py::test_then_on_fulfilled_p
romise_invokes_callback_immediately
tests/unit/application/promises/test_interface_pure.py::test_catch_on_rejected_p
romise_yields_handler_result
tests/unit/application/prompts/test_auto_tuning_pure.py::test_success_rate_and_a
verage_feedback_are_computed_from_state
tests/unit/application/prompts/test_auto_tuning_pure.py::test_performance_score_
combines_success_and_feedback
tests/unit/application/prompts/test_auto_tuning_pure.py::test_round_trip_seriali
sation_preserves_variant_fields
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_reaches_consensus
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_logs_consensus_failure
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_failure_stores_retrospect
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_receives_consensus
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_runs_on_failure
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_non_text_response_errors
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_invalid_response_errors
tests/unit/application/requirements/test_dialectical_reasoner.py::test_assess_im
pact_stores_with_phase
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_argument_parsing_consensus_failure_payload_preserved
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_assess_impact_recommendations_payload_preserved
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_requirements_collects_dependencies
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_components_merges_sources
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_asse
ss_risk_level_accounts_for_priority
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_esti
mate_effort_scales_with_affected_entities
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_writes_json
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_cancelled
tests/unit/application/requirements/test_interactions.py::test_gather_requiremen
ts_supports_backtracking
tests/unit/application/requirements/test_requirement_service_dtos.py::test_updat
e_requirement_uses_typed_dto_and_dialectical_hooks
tests/unit/application/requirements/test_requirement_service_dtos.py::test_delet
e_requirement_emits_retrospect_phase
tests/unit/application/requirements/test_wizard.py::test_priority_and_constraint
s_persist_after_navigation
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_each_step
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_exc_info
tests/unit/application/sprint/test_planning.py::test_map_requirements_to_plan_ex
tracts_fields
tests/unit/application/test_documentation_fetcher.py::test_download_success_retu
rns_manifest
tests/unit/application/test_documentation_fetcher.py::test_download_failure_retu
rns_false_manifest
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization_with_memory_port
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_nonexistent
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_info
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_clear_cache
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_operations
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_expiration
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_file_corruption
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_creation
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_as_dict
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_creation
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_with_docstring
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_ttl_configuration
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_unicode_decode_error
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_os_error_handling
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_memory_storage_failure
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_with_extra
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_without_extra
tests/unit/application/utils/test_extras_helper.py::test_require_optional_packag
e_wraps_importerror
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors
tests/unit/cli/test_cli_help.py::test_cli_help_exits_zero_and_shows_summary
tests/unit/cli/test_command_module_loading.py::test_command_modules_register_com
mands_and_build_app
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry
tests/unit/cli/test_command_registry.py::test_enable_feature_not_top_level
tests/unit/cli/test_completion_progress.py::test_completion_cmd_outputs_script_a
nd_progress
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation
tests/unit/cli/test_entry_points_help.py::test_console_scripts_declared
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module
tests/unit/cli/test_help_examples.py::test_get_command_help_includes_examples
tests/unit/cli/test_help_examples.py::test_get_command_help_unknown_command
tests/unit/cli/test_import_gating.py::test_import_devsynth_does_not_import_heavy
_optionals
tests/unit/cli/test_import_gating.py::test_cli_entrypoint_lazy_imports
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_list
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_json
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv0]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv1]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv2]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv3]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv4]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv5]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv6]
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug
tests/unit/cli/test_mvu_commands.py::test_mvu_help_lists_subcommands
tests/unit/cli/test_mvu_commands.py::test_mvu_init_creates_config_and_matches_sc
hema
tests/unit/cli/test_mvuu_command_registration.py::test_mvuu_dashboard_command_re
gistered
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_generat
es_signed_telemetry
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_uses_li
ve_connectors
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_falls_b
ack_on_connector_error
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_force_l
ocal_mode
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests
tests/unit/cli/test_version.py::test_cli_version_option_prints_version_and_exits
_zero
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env
tests/unit/config/test_exception_handling.py::test_is_devsynth_managed_project_i
nvalid_toml_returns_false
tests/unit/config/test_exception_handling.py::test_unified_config_exists_returns
_false_on_invalid_toml
tests/unit/config/test_exception_handling.py::test_load_config_malformed_toml_ra
ises_configuration_error
tests/unit/config/test_exception_handling.py::test_load_config_invalid_values_ra
ises_configuration_error
tests/unit/config/test_exception_handling.py::test_set_default_memory_dir_handle
s_configuration_error
tests/unit/config/test_feature_flag_defaults.py::test_feature_flags_default_off
tests/unit/config/test_feature_flag_defaults.py::test_can_enable_known_feature_f
lag
tests/unit/config/test_provider_env.py::test_parse_bool_truthy_and_falsy_cases
tests/unit/config/test_provider_env.py::test_from_env_defaults_and_with_test_def
aults_sets_stub_and_offline
tests/unit/config/test_provider_env.py::test_apply_to_env_respects_existing_lmst
udio_flag
tests/unit/config/test_provider_env.py::test_as_dict_roundtrip_and_types
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_sets_e
xpected_vars
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_does_n
ot_override_explicit_lmstudio_flag
tests/unit/config/test_provider_env_apply_and_parse.py::test_from_env_reads_curr
ent_environment
tests/unit/config/test_provider_env_behavior.py::test_from_env_defaults_when_uns
et
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_overrid
es_to_safe_when_unset
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_respect
s_explicit_provider
tests/unit/config/test_provider_env_behavior.py::test_apply_to_env_and_as_dict_r
oundtrip
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_parses_
true_and_false_variants
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_unrecog
nized_values_fall_back_to_defaults
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_as_dict_reflects
_values_and_with_test_defaults_sets_openai_key
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_sets_offline_stub_and_openai_key
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_respects_explicit_provider
tests/unit/config/test_unified_loader.py::test_loads_from_pyproject_succeeds
tests/unit/core/mvu/test_api.py::test_get_by_trace_id
tests/unit/core/mvu/test_api.py::test_get_by_affected_path
tests/unit/core/mvu/test_atomic_rewrite.py::test_cluster_commits_by_file
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_valid
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_block
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_bad_traceid
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_issue
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_mvuu_false
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_mvuu
tests/unit/core/mvu/test_mvuu_schema_validation.py::test_mvuu_example_conforms_t
o_schema tests/unit/core/mvu/test_report.py::test_generate_report_markdown
tests/unit/core/mvu/test_report.py::test_generate_report_html
tests/unit/core/mvu/test_storage.py::test_format_mvuu_footer_contains_json
tests/unit/core/mvu/test_storage.py::test_append_mvuu_footer_appends_block
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_accepts_vali
d
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_rejects_bad_
header
tests/unit/core/mvu/test_validator.py::test_validate_affected_files_reports_mism
atches
tests/unit/core/test_config_loader.py::test_core_config_normalizes_mvuu_invalid_
entries
tests/unit/core/test_config_loader_json_types.py::test_load_config_supports_nest
ed_json_resources
tests/unit/core/test_config_loader_json_types.py::test_environment_override_pres
erves_resources
tests/unit/core/test_config_loader_json_types.py::test_core_config_rejects_exces
sively_deep_resources
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
tests/unit/core/test_config_loader_optional_deps.py::test_load_toml_mapping_requ
ires_optional_dependency
tests/unit/core/test_config_loader_optional_deps.py::test_dump_toml_mapping_requ
ires_optional_dependency
tests/unit/core/test_config_loader_optional_deps.py::test_save_global_config_han
dles_missing_yaml
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw0-expected0]
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw1-expected1]
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw2-expected2]
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw3-None]
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[not-a-mapping-None]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload0-expected0]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload1-expected1]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload2-None]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[not-a-mapping-None]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload0-expected0]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload1-expected1]
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload2-expected2]
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories0-True-expected0]
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories1-False-expected1]
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories2-False-expected2]
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[not-a-mapping-False-expected3]
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[15-True]
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[16-False]
tests/unit/core/test_config_loader_validation.py::test_load_yaml_returns_coerced
_core_config_data
tests/unit/core/test_config_loader_validation.py::test_load_toml_returns_coerced
_core_config_data
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[single_override]
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[multiple_fields]
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[ignores_irrelevant_keys]
tests/unit/core/test_config_loader_validation.py::test_load_config_merges_source
s_without_mutating_resources
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[github_only]
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[jira_only]
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[both_providers]
tests/unit/core/test_deterministic_fixtures.py::test_deterministic_seed_sets_env
_and_random_sequence
tests/unit/core/test_deterministic_fixtures.py::test_mock_datetime_fixture_freez
es_time
tests/unit/core/test_deterministic_fixtures.py::test_mock_uuid_fixture_returns_f
ixed_uuid tests/unit/core/test_mvu.py::test_schema_has_required_fields
tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task
tests/unit/deployment/test_deployment_scripts.py::test_bootstrap_script_exists
tests/unit/deployment/test_deployment_scripts.py::test_health_check_script_exist
s
tests/unit/deployment/test_enforcement.py::test_shell_scripts_enforce_non_root_a
nd_env_validation
tests/unit/deployment/test_enforcement.py::test_docker_compose_enforces_user_and
_env_file
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint
tests/unit/deployment/test_scripts_dir.py::test_scripts_bootstrap_exists
tests/unit/deployment/test_scripts_dir.py::test_scripts_health_check_exists
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_noo
p_without_flag
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_rai
ses_for_root
tests/unit/deployment/test_security_hardening.py::test_check_required_env_vars
tests/unit/deployment/test_security_hardening.py::test_apply_secure_umask
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_invokes_he
lpers
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_raises_whe
n_env_missing
tests/unit/devsynth/test_consensus.py::test_build_consensus_majority
tests/unit/devsynth/test_consensus.py::test_build_consensus_no_consensus
tests/unit/devsynth/test_consensus.py::test_build_consensus_tracks_unique_dissen
ting_options
tests/unit/devsynth/test_consensus.py::test_build_consensus_invalid_threshold
tests/unit/devsynth/test_consensus.py::test_build_consensus_empty_votes
tests/unit/devsynth/test_fallback_reliability.py::test_named_condition_callbacks
_record_metrics
tests/unit/devsynth/test_fallback_reliability.py::test_circuit_breaker_open_hook
_and_metrics
tests/unit/devsynth/test_logger.py::test_log_exception_object_normalized
tests/unit/devsynth/test_logger.py::test_log_true_uses_current_exception
tests/unit/devsynth/test_logger.py::test_log_invalid_exc_info_dropped
tests/unit/devsynth/test_metrics.py::test_memory_metrics_increment_and_reset
tests/unit/devsynth/test_metrics.py::test_provider_and_retry_metrics
tests/unit/devsynth/test_metrics.py::test_dashboard_metrics
tests/unit/devsynth/test_metrics.py::test_inc_memory_unhashable_raises_type_erro
r tests/unit/devsynth/test_simple_addition.py::test_add_returns_sum
tests/unit/devsynth/test_simple_addition.py::test_add_raises_type_error_on_non_n
umeric
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_tests_but_
not_docs
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_docs_but_n
ot_tests
tests/unit/domain/interfaces/test_interfaces.py::test_cli_interface_raises_not_i
mplemented
tests/unit/domain/interfaces/test_interfaces.py::test_file_analysis_result_raise
s_not_implemented
tests/unit/domain/interfaces/test_interfaces.py::test_onnx_runtime_raises_not_im
plemented
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_none_values
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_existing_values
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_initial
ization
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_seriali
zation
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_creation
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_content_has
h_generation
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_serializati
on_roundtrip
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_link_manage
ment
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_salience_up
date
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_lifecycle_m
anagement
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_cognitive_t
ype_properties
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_creati
on
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_serial
ization
tests/unit/domain/models/test_project.py::test_project_model_structure_type_defa
ult_standard
tests/unit/domain/models/test_project.py::test_project_model_structure_type_mono
repo
tests/unit/domain/models/test_project.py::test_artifact_metadata_defaults_to_sep
arate_dicts
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_init
ialization_succeeds
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_str_
representation_succeeds
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_repr
_representation_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_project_m
odel_initialization_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_structure_type_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_sta
ndard_model_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_mon
orepo_model_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
act_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
acts_by_type_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_relat
ed_artifacts_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_artifact_type_succeeds
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_to_dict_s
ucceeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_add_agent_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_dialectical_hook_invok
ed_on_add_solution_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_rotate_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_empty_team_
succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_agent_by_role_succ
eeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ws
de_dataclass_initialises_timestamps
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_te
am_post_init_restores_missing_attributes
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_inserts_validation
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_noop_when_already_secure
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_error_hand
ling_wraps_body
tests/unit/domain/models/test_wsde_decision_making.py::test_calculate_idea_simil
arity_overlap
tests/unit/domain/models/test_wsde_decision_making.py::test_evaluate_options_ran
ks_by_weighted_score
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_similar_entries
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_handles_agent_failures
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_limits_count
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_duplicates_with_strict_threshold
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_antithe
sis_returns_typed_draft
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_categorize_criti
ques_by_domain_returns_tuples
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_synthes
is_returns_resolution_plan
tests/unit/domain/models/test_wsde_dialectical_typing.py::test_dialectical_seque
nce_round_trip
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_dialectical_tas
k_serialization_round_trip
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_dynamic_role_reassignment_selects_expert_primus_succeeds
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_categorize_crit
iques_by_domain_groups_terms
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_identify_domain
_conflicts_finds_performance_security
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_generates_synthesis
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_requires_solution
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_combines_solutions
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_requires_solutions
tests/unit/domain/models/test_wsde_knowledge.py::test_get_task_id_uses_existing_
id
tests/unit/domain/models/test_wsde_knowledge.py::test_identify_relevant_knowledg
e_matches_keywords
tests/unit/domain/models/test_wsde_knowledge.py::test_knowledge_graph_insights_p
arses_payload
tests/unit/domain/models/test_wsde_knowledge.py::test_integrate_knowledge_builds
_summary
tests/unit/domain/models/test_wsde_knowledge.py::test_generate_improvement_sugge
stions_deduplicates_entries
tests/unit/domain/models/test_wsde_roles_personas.py::test_enumerate_research_pe
rsonas_includes_overlays
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Synthesizer]
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Contrarian]
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Fact Checker]
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Planner]
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Moderator]
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_s
cores_requirements
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_h
ighlights_gaps
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_identifies_best_solution
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_handles_empty
tests/unit/domain/models/test_wsde_strategies.py::test_weighted_voting_prefers_d
omain_expertise
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores
tests/unit/domain/models/test_wsde_strategies.py::test_multidisciplinary_analysi
s_structures_results
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_add_agent_succeed
s
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_rotate_primus_suc
ceeds
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_empty_
team_succeeds
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_assign_roles_succ
eeds
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds
tests/unit/domain/models/test_wsde_utils.py::test_send_message_invokes_protocol
tests/unit/domain/models/test_wsde_utils.py::test_broadcast_message_excludes_sen
der tests/unit/domain/models/test_wsde_utils.py::test_get_messages_uses_protocol
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_creates_cy
cle
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_collects_f
eedback
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_handles_mi
ssing_peer_review
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_logs_warni
ng_on_failure
tests/unit/domain/models/test_wsde_voting_logic.py::test_deterministic_voting_wi
th_seed
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_determi
nistic_with_seed
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_tie_is_
fair
tests/unit/domain/models/test_wsde_voting_logic.py::test_handle_tied_vote_produc
es_consensus_result
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_analyzer
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_transformer
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_simple_file_analysis
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches
tests/unit/domain/test_wsde_facade.py::test_summarize_consensus_result_outputs_e
xpected_sections
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus
tests/unit/domain/test_wsde_peer_review_workflow.py::test_peer_review_cross_stor
e_sync_succeeds
tests/unit/domain/test_wsde_peer_review_workflow.py::test_mvu_helpers_cover_modu
le
tests/unit/domain/test_wsde_phase_role_rotation.py::test_initial_selection_prefe
rs_unused_agent_succeeds
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds
tests/unit/domain/test_wsde_phase_role_rotation.py::test_assign_roles_for_phase_
rotates_after_all_primus_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_first_time_selection_prior
itizes_unused_agents_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_resets_after_all_
have_served_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_when_all_agents_u
sed_resets_flags_succeeds
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds
tests/unit/domain/test_wsde_team.py::test_select_primus_by_expertise_prefers_doc
umentation_agent_succeeds
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds
tests/unit/domain/test_wsde_team.py::test_rotation_resets_after_all_have_served_
succeeds
tests/unit/domain/test_wsde_team.py::test_select_primus_prefers_doc_expertise_vi
a_config_succeeds
tests/unit/domain/test_wsde_team.py::test_rotate_primus_resets_usage_flags_and_r
ole_map_succeeds
tests/unit/domain/test_wsde_team.py::test_multiple_task_cycles_reset_primus_flag
s_succeeds
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds tests/unit/domain/test_wsde_team.py::test_force_wsde_coverage_succeeds
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie
tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_simple
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_rounds
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_primus_t
ie
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_random
tests/unit/fallback/test_retry_counts.py::test_retry_count_metrics
tests/unit/fallback/test_retry_counts.py::test_retry_only_network_errors
tests/unit/fallback/test_retry_predicates.py::test_retry_predicate_triggers_retr
y
tests/unit/fallback/test_retry_predicates.py::test_integer_predicate_records_met
rics
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_add
_agent_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_agent_type_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_team_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_missing_parameters_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_no_agents_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_type_not_found_succeeds
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_execution_error_raises_error
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_type_enum_s
ucceeds
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_init
ialization_succeeds
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_with
_parameters_succeeds
tests/unit/general/test_agent_models.py::TestAgentModels::test_mvp_capabilities_
succeeds
tests/unit/general/test_agent_system.py::test_agent_state_keys_has_expected
tests/unit/general/test_agent_system.py::test_process_input_node_success_is_vali
d
tests/unit/general/test_agent_system.py::test_process_input_node_empty_input_suc
ceeds
tests/unit/general/test_agent_system.py::test_process_input_node_adds_tool_list
tests/unit/general/test_agent_system.py::test_llm_call_node_success_succeeds
tests/unit/general/test_agent_system.py::test_llm_call_node_llm_failure_fails
tests/unit/general/test_agent_system.py::test_llm_call_node_skip_on_prior_error_
raises_error
tests/unit/general/test_agent_system.py::test_llm_call_node_missing_processed_in
put_succeeds
tests/unit/general/test_agent_system.py::test_parse_output_node_success_is_valid
tests/unit/general/test_agent_system.py::test_parse_output_node_missing_llm_resp
onse_succeeds
tests/unit/general/test_agent_system.py::test_parse_output_node_skip_on_prior_er
ror_raises_error
tests/unit/general/test_agent_system.py::test_base_agent_graph_compiles_raises_e
rror
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error
tests/unit/general/test_api.py::test_verify_token_rejects_invalid_token
tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token
tests/unit/general/test_api_health.py::test_health_endpoint_succeeds
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_help_shows_co
mmand
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_disabled_exit
s_with_guidance
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_enabled_dry_r
un_succeeds
tests/unit/general/test_backend_resource_flags.py::test_backend_flag_mapping_res
pects_env_vars
tests/unit/general/test_backend_resource_flags.py::test_rdflib_env_mapping_disab
les_rdflib
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
handles_partial_spec
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error
tests/unit/general/test_base.py::test_dummy_adapter_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_initiali
zation_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_delete_succee
ds
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_persistence_s
ucceeds
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_exact_
match_matches_expected
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_semant
ic_succeeds
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_store_and_ret
rieve_succeeds
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_token_usage_s
ucceeds
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_lists_comm
ands_succeeds
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_omits_depr
ecated_aliases_succeeds
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_provider_interface_has_expected
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_result_interface_has_expected
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_file_analysis_result_interface_has_expected
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_co
de_analysis_implementation_succeeds
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_fi
le_analysis_implementation_succeeds
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_code_su
cceeds
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_directo
ry_succeeds
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_file_su
cceeds
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_project_structu
re_metrics_succeeds
tests/unit/general/test_config_loader.py::test_load_yaml_config_succeeds
tests/unit/general/test_config_loader.py::test_load_pyproject_toml_succeeds
tests/unit/general/test_config_loader.py::test_autocomplete_succeeds
tests/unit/general/test_config_loader.py::test_save_persists_version_succeeds
tests/unit/general/test_config_loader.py::test_version_mismatch_logs_warning_mat
ches_expected
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_default_values_returns_expected_result
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_from_environment_variables_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_llm_set
tings_returns_expected_result
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[true-True]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[True-True]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[TRUE-True]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[false-False]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[False-False]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[FALSE-False]
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_file_not_found_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_with_dotenv_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_invalid_sec
urity_boolean_raises
tests/unit/general/test_config_settings.py::TestConfigSettings::test_empty_opena
i_api_key_raises
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_defaults_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_from_env_succeeds
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_embedd
ed_attribute_lookup_succeeds
tests/unit/general/test_core_config_loader.py::test_precedence_env_over_project_
over_global_succeeds
tests/unit/general/test_core_config_loader.py::test_load_toml_project_succeeds
tests/unit/general/test_core_config_loader.py::test_save_global_config_yaml_succ
eeds tests/unit/general/test_core_values.py::test_load_core_values_succeeds
tests/unit/general/test_core_values.py::test_find_value_conflicts_succeeds
tests/unit/general/test_core_values.py::test_check_report_for_value_conflicts_su
cceeds
tests/unit/general/test_core_workflows.py::test_filter_args_removes_none_values_
succeeds
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[init_project-init-kwargs0-expected0]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_specs-spec-kwargs1-expected1]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_tests-test-kwargs2-expected2]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_code-code-kwargs3-expected3]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[run_pipeline-run-pipeline-kwargs4-expected4]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs5-expected5]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs6-expected6]
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[inspect_requirements-inspect-kwargs7-expected7]
tests/unit/general/test_core_workflows.py::test_gather_requirements_creates_file
_succeeds
tests/unit/general/test_core_workflows.py::test_workflow_manager_singleton_succe
eds
tests/unit/general/test_delegate_task_disabled.py::test_delegate_task_collaborat
ion_disabled_succeeds
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_c
reate_session_succeeds
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_p
rocess_message_succeeds
tests/unit/general/test_documentation_fetcher.py::test_fetcher_initialization_su
cceeds tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency
tests/unit/general/test_dpg_flag.py::test_dpg_command_enabled
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_no_input_raises_e
rror
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_missing_
raises_error
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_success_
succeeds
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_prompt_success_su
cceeds
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manual_succeeds
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_custom_bridge_has
_expected
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_error_handling_ra
ises_error
tests/unit/general/test_edrr_manifest_string.py::test_start_cycle_from_manifest_
string_succeeds
tests/unit/general/test_exception_logging.py::test_log_exception_emits_error
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_message
_only_succeeds
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_error_c
ode_raises_error
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_details
_raises_error
tests/unit/general/test_exceptions.py::TestDevSynthError::test_to_dict_succeeds
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_validation_erro
r_raises_error
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_configuration_e
rror_raises_error
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_command_error_r
aises_error
tests/unit/general/test_exceptions.py::TestSystemErrors::test_internal_error_rai
ses_error
tests/unit/general/test_exceptions.py::TestSystemErrors::test_resource_exhausted
_error_raises_error
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_error_ra
ises_error
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_timeout_
error_raises_error
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_memory_adapter_er
ror_raises_error
tests/unit/general/test_exceptions.py::TestDomainErrors::test_agent_error_raises
_error
tests/unit/general/test_exceptions.py::TestDomainErrors::test_workflow_error_suc
ceeds
tests/unit/general/test_exceptions.py::TestDomainErrors::test_dialectical_reason
ing_error_raises_error
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_error
_raises_error
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_state
_error_raises_error
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_ingestion_err
or_raises_error
tests/unit/general/test_exceptions.py::TestPortErrors::test_memory_port_error_ra
ises_error
tests/unit/general/test_exceptions.py::TestPortErrors::test_provider_port_error_
raises_error
tests/unit/general/test_exceptions.py::TestPortErrors::test_agent_port_error_rai
ses_error
tests/unit/general/test_fallback_utils.py::test_bulkhead_limits_concurrency
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_defau
lts_succeeds
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_custo
m_manifest_succeeds
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_dry_run_su
cceeds
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_validate_o
nly_is_valid
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_verbose_su
cceeds
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_forwards_a
uto_phase_flag
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_non_intera
ctive_flag_sets_env
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_env_var_en
ables_non_interactive
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_priority_u
pdates_config
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_manifest_e
rror_raises_error
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_ingestion_
error_raises_error
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_success_is_valid
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_file_not_found_is_valid
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_schema_not_found_is_valid
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_validation_failed_fails
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_yaml
_error_raises_error
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_file
_error_raises_error
tests/unit/general/test_ingest_cmd.py::TestPhases::test_expand_phase_has_expecte
d
tests/unit/general/test_ingest_cmd.py::TestPhases::test_differentiate_phase_has_
expected
tests/unit/general/test_ingest_cmd.py::TestPhases::test_refine_phase_has_expecte
d
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected
tests/unit/general/test_ingestion_edrr_integration.py::test_run_ingestion_invoke
s_edrr_phases_succeeds
tests/unit/general/test_ingestion_type_hints.py::test_ingestion_type_hints_raise
s_error
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_update_succee
ds
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_prune_succeed
s
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_no_config_suc
ceeds
tests/unit/general/test_inspect_config_cmd.py::test_analyze_project_structure_re
turns_directories
tests/unit/general/test_inspect_config_cmd.py::test_compare_with_manifest_return
s_differences
tests/unit/general/test_inspect_config_cmd.py::test_update_manifest_adds_directo
ry
tests/unit/general/test_isolation.py::TestIsolation::test_devsynth_dir_isolation
_succeeds
tests/unit/general/test_isolation.py::TestIsolation::test_global_config_isolatio
n_succeeds
tests/unit/general/test_isolation.py::TestIsolation::test_memory_path_isolation_
succeeds
tests/unit/general/test_isolation.py::TestIsolation::test_no_file_logging_preven
ts_directory_creation_succeeds
tests/unit/general/test_isolation.py::TestIsolation::test_path_redirection_in_te
st_environment_succeeds
tests/unit/general/test_isolation.py::TestIsolation::test_comprehensive_isolatio
n_succeeds
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_tmp_p
ath_fixture
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_netwo
rk_keyword
tests/unit/general/test_kuzu_adapter.py::test_store_and_retrieve_vector_succeeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_succeeds
tests/unit/general/test_kuzu_adapter.py::test_persistence_between_instances_succ
eeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_without_numpy_su
cceeds
tests/unit/general/test_kuzu_embedded_missing.py::test_ephemeral_kuzu_store_init
ialises_without_kuzu_embedded
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_creation_succeeds
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_to_dict_succeeds
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_from_dict_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_checkpoint_path_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_exists_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_not_exists_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_put_checkpoint_succeeds
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
create_workflow_succeeds
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
add_step_succeeds
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
execute_workflow_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_save_and_get_workflow_succeeds
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_list_workflows_succeeds
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_registration
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_configuration_loading
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_initialization_with_defaults
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_environment_variable_handling
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_config_file_integration
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url
tests/unit/general/test_logger.py::test_configure_logging_creates_rotating_handl
er tests/unit/general/test_logger.py::test_dev_synth_logger_normalizes_exc_info
tests/unit/general/test_logging_setup.py::test_log_records_include_request_conte
xt_succeeds
tests/unit/general/test_logging_setup.py::test_exc_info_passes_through_succeeds
tests/unit/general/test_logging_setup.py::test_exc_info_true_uses_current_except
ion
tests/unit/general/test_logging_setup.py::test_extra_kwargs_and_reserved_keys_sa
fely_handled
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_idem
potent_no_duplicate_handlers
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_thre
ad_safe
tests/unit/general/test_logging_setup_idempotent.py::test_no_file_logging_toggle
_prevents_file_handler
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_enu
m_succeeds
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_ini
tialization_succeeds
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_wit
h_metadata_succeeds
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_ali
ases
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_typ
e_alias
tests/unit/general/test_memory_store.py::test_memory_store_abstract_methods_succ
eeds
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_delete_succeed
s
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_search_succeed
s
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_store_and_retr
ieve_succeeds
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_delete_succeed
s
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_persistence_su
cceeds
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_search_succeed
s
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_store_and_retr
ieve_succeeds
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_token_usage_su
cceeds
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_add_and
_get_succeeds
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_clear_c
ontext_succeeds
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_get_ful
l_context_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_add
_and_get_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_cle
ar_context_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_full_context_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_relevant_context_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_per
sistence_succeeds
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_tok
en_usage_succeeds
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_file_bas
ed_adapter_succeeds
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_in_memor
y_adapter_succeeds
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_token_us
age_succeeds
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_with_chromadb_succeeds
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_without_vector_store_succeeds
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_memory_and_vector_store_integration_succeeds
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_context_manager_with_chromadb_succeeds
tests/unit/general/test_methodology_logging.py::test_phase_timeout_logs_warning_
succeeds
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_bulk_add_agents_succeeds
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_success
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_failure
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_combines_streams
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_returns_exit_code
tests/unit/general/test_mvu_init_cmd.py::test_mvu_init_cmd_creates_file
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure
tests/unit/general/test_mvuu_dashboard_cli.py::test_mvuu_dashboard_help_succeeds
tests/unit/general/test_mypy_config.py::test_mypy_configuration_raises_error
tests/unit/general/test_mypy_config.py::test_mypy_project_configuration_raises_e
rror
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_ensure_path_exists_respects_no_file_logging_succeeds
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_settings_respects_no_file_logging_succeeds
tests/unit/general/test_onnx_port.py::test_onnx_port_load_and_run_succeeds
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds
tests/unit/general/test_path_restrictions.py::test_configure_logging_within_proj
ect_dir_succeeds
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds
tests/unit/general/test_primus_selection.py::test_highest_expertise_score_become
s_primus_succeeds
tests/unit/general/test_primus_selection.py::test_prioritizes_agents_who_have_no
t_served_as_primus_succeeds
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds
tests/unit/general/test_primus_selection.py::test_rotation_resets_after_all_agen
ts_served_succeeds
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_default_ma
nifest_returned_when_missing_returns_expected_result
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_in
itialization_succeeds
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_di
rect_execution_succeeds
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_execution_succeeds
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_error_raises_error
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_agent_initializ
ation_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_regi
stration_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_and_fulfillment_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_unauthorized_ac
cess_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_not_
found_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_get_available_c
apabilities_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgentMixin::test_mixin_with
_custom_agent_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_initial_state_succe
eds
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_reject_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_then_fulfilled_succ
eeds
tests/unit/general/test_promise_system.py::TestPromise::test_then_rejected_succe
eds tests/unit/general/test_promise_system.py::TestPromise::test_catch_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_chaining_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_error_propagation_r
aises_error
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_value_stati
c_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_reject_with_static_
succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_all_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_all_with_rejection_
succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_race_succeeds
tests/unit/general/test_promise_system.py::TestPromise::test_metadata_succeeds
tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_chat_
models_succeeds
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_diale
ctical_reasoning_model_succeeds
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_impac
t_assessment_model_succeeds
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_change_model_succeeds
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_model_succeeds
tests/unit/general/test_requirement_repository_interface.py::test_requirement_re
pository_interface_crud
tests/unit/general/test_requirement_repository_port_interface.py::test_requireme
nt_repository_port_is_abstract
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
uirement_port_methods_raise_not_implemented
tests/unit/general/test_requirement_service.py::TestRequirementService::test_app
rove_change_succeeds
tests/unit/general/test_requirement_service.py::TestRequirementService::test_cre
ate_requirement_succeeds
tests/unit/general/test_requirement_service.py::TestRequirementService::test_del
ete_requirement_succeeds
tests/unit/general/test_requirement_service.py::TestRequirementService::test_rej
ect_change_succeeds
tests/unit/general/test_requirement_service.py::TestRequirementService::test_upd
ate_requirement_succeeds
tests/unit/general/test_resource_markers.py::test_is_lmstudio_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_codebase_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_resource_available_succeeds
tests/unit/general/test_resource_markers.py::test_with_resource_marker_succeeds
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds
tests/unit/general/test_retry_failure_scenarios.py::test_named_retry_condition_a
borts_and_records_metrics
tests/unit/general/test_retry_failure_scenarios.py::test_circuit_breaker_open_re
cords_abort_metrics
tests/unit/general/test_speed_option.py::test_speed_option_recognized
tests/unit/general/test_sync_manager_persistence.py::test_sync_manager_persists_
to_all_stores
tests/unit/general/test_template_location.py::TestTemplateLocation::test_templat
es_exist_in_temp_location_succeeds
tests/unit/general/test_template_location.py::TestTemplateLocation::test_can_use
_template_to_create_test_succeeds
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_analyz
e_commit_succeeds
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_calcul
ate_metrics_succeeds
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_genera
te_metrics_report_succeeds
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_get_co
mmit_history_succeeds
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_main_s
ucceeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_conversat
ion_tokens_succeeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_message_t
okens_succeeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_tokens_su
cceeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_ensure_token_li
mit_succeeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_fallback_tokeni
zer_succeeds
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_prune_conversat
ion_succeeds
tests/unit/general/test_unified_agent_code_prompt.py::test_process_code_task_inc
ludes_language_and_paradigm_succeeds
tests/unit/general/test_unified_config_loader.py::test_load_from_yaml_succeeds
tests/unit/general/test_unified_config_loader.py::test_load_from_pyproject_succe
eds
tests/unit/general/test_unified_config_loader.py::test_save_and_exists_succeeds
tests/unit/general/test_unified_config_loader.py::test_missing_files_succeeds
tests/unit/general/test_unified_config_loader.py::test_version_mismatch_warning_
succeeds
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_yaml
_succeeds
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_pypr
oject_succeeds tests/unit/general/test_unit_cli_commands.py::test_cmd
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds
tests/unit/general/test_workflow.py::TestWorkflowManager::test_create_workflow_f
or_command_succeeds
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_s
ucceeds
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_f
ailure_fails
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_h
uman_intervention_succeeds
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
atus_enum_succeeds
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
ep_initialization_succeeds
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_in
itialization_succeeds
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_wi
th_steps_succeeds
tests/unit/general/test_wsde_dynamic_roles.py::test_assign_roles_for_phase_selec
ts_primus_by_expertise_has_expected
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_initialization_s
ucceeds
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_with_custom_valu
es_succeeds
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_wsde_team_init
ialization_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_add_agent_succ
eeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_rotate_primus_
succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_primus_suc
ceeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_s
ucceeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
by_expertise_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_autonomous_col
laboration_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds
tests/unit/infrastructure/test_test_infrastructure_sanity.py::test_global_test_i
solation_sets_env_and_dirs
tests/unit/integrations/test_autoresearch_client.py::test_handshake_and_query_su
ccess
tests/unit/integrations/test_autoresearch_client.py::test_handshake_disabled_by_
flag
tests/unit/integrations/test_autoresearch_client.py::test_query_failure_falls_ba
ck
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_initialization
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_record_request
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_within_limit
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_exceeds_limit
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_prune_old_requests
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_multiple_clients
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_health_en
dpoint_exists
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_metrics_e
ndpoint_exists
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_init_requ
est_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_gather_re
quest_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_synthesiz
e_request_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_spec_requ
est_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_code_requ
est_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_doctor_re
quest_model
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_edrr_cycl
e_request_model
tests/unit/interface/test_agentapi_enhanced.py::TestRouter::test_router_exists
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimitingIntegration::tes
t_rate_limiting_logic_integration
tests/unit/interface/test_agentapi_enhanced.py::TestErrorHandling::test_error_re
sponse_structure
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
quest_models_validation
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_answers_a
nd_defaults
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_confirm_c
hoice_coerces_booleans
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_progress_tr
acks_subtasks
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_rate_limit_
blocks_abusive_clients
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_allow
s_after_window
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_raise
s_when_exceeded
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_records_subtasks
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_normalizes_string_advances
tests/unit/interface/test_api_endpoints.py::test_enhanced_rate_limit_state_track
s_buckets
tests/unit/interface/test_api_endpoints.py::test_enhanced_metrics_snapshot_typed
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_sanitize_
output_succeeds
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_ini
t_with_bad_description_uses_fallback
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_upd
ate_with_bad_inputs_uses_fallback
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_sub
tasks_with_bad_inputs_use_fallbacks
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_ask_question_us
es_prompt_toolkit
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_confirm_choice_
uses_prompt_toolkit
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_prompt_fallback
_to_rich
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_noninteractive_re
turns_defaults_and_logs
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches
tests/unit/interface/test_command_output.py::test_format_and_display_message
tests/unit/interface/test_command_output.py::test_format_error_suggestions
tests/unit/interface/test_command_output.py::test_list_and_structured_outputs
tests/unit/interface/test_command_output.py::test_set_console
tests/unit/interface/test_dpg_ui.py::test_all_buttons_trigger_callbacks_and_prog
ress tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog
tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog_error
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_actionable_error_suggestion_str_includes_details
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_format_error_wraps_with_footer
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_defaul
t_file
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_specif
ied_file
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_invokes_strea
mlit tests/unit/interface/test_mvuu_dashboard.py::test_require_streamlit_raises
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_snaps
hot
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_witho
ut_optional_sections
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_with_overlays
_loads_telemetry
tests/unit/interface/test_mvuu_dashboard.py::test_signature_pointer_legacy_env
tests/unit/interface/test_mvuu_dashboard.py::test_signature_secret_falls_back_to
_legacy
tests/unit/interface/test_mvuu_dashboard.py::test_resolve_telemetry_path_prefers
_legacy
tests/unit/interface/test_nicegui_bridge.py::test_session_storage_roundtrip
tests/unit/interface/test_nicegui_bridge.py::test_display_result_notifies_and_re
cords
tests/unit/interface/test_nicegui_bridge.py::test_progress_indicator_updates_and
_completes
tests/unit/interface/test_nicegui_bridge.py::test_display_result_falls_back_with
out_nicegui
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_initialization
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_update
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_complete
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_initialization
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_create_progress
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_main_function
_exists
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_json_yaml_with_and_without_console
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_table_fallback_and_empty_list
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_rich_renderables
tests/unit/interface/test_output_formatter_core_behaviors.py::test_sanitize_outp
ut_delegates_and_handles_edge_cases
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[ERROR: Disk failure-error]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[warning: Low memory-warning]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Task completed successfully-success]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[INFO: FYI-info]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[# Heading-heading]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[-normal]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Routine update-normal]
tests/unit/interface/test_output_formatter_core_behaviors.py::test_format_messag
e_applies_status_styles
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_highl
ight_branch_emits_panel
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_witho
ut_console_raises_value_error
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_format_
message_error_styles_and_escapes_markup
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_markdow
n_branch_sanitizes_hyperlinks
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_table_b
ranch_sanitizes_script_links
tests/unit/interface/test_output_formatter_fallbacks.py::test_table_format_falls
_back_to_text_for_nontabular_inputs
tests/unit/interface/test_output_formatter_fallbacks.py::test_rich_format_select
s_renderables_for_data_shapes
tests/unit/interface/test_output_formatter_fallbacks.py::test_list_of_dicts_tabl
e_renders_missing_and_complex_values
tests/unit/interface/test_output_formatter_fallbacks.py::test_set_format_options
_and_command_output_overrides
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-syntax]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-plain]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-syntax]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-plain]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-dict]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-list]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-empty-list]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-heterogeneous]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-dict]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-list-of-dicts]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-bullet-panel]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-falsy-list]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[text-scalar]
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_mark
down_handles_nested_values
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_to_mark
down_handles_mixed_items
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_tabl
e_serializes_complex_values
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_of_dict
s_to_table_handles_missing_keys
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-value]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-nested]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-list-item]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-value]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-dict-value]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-key]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-value]
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-table-value]
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_table
_and_list_preserve_sanitized_complex_values
tests/unit/interface/test_output_formatter_structured_fast.py::test_command_outp
ut_unknown_extension_and_highlight_panel
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
ed_protocol
tests/unit/interface/test_progress_helpers.py::test_subtask_snapshot_typed_struc
ture
tests/unit/interface/test_progress_utils.py::test_progress_manager_create_get_co
mplete_and_context_manager
tests/unit/interface/test_progress_utils.py::test_progress_manager_track_updates
_on_item_and_slice
tests/unit/interface/test_progress_utils.py::test_progress_indicator_context_man
ager_completes
tests/unit/interface/test_progress_utils.py::test_step_progress_sequencing_and_c
omplete
tests/unit/interface/test_progress_utils.py::test_create_and_track_progress_help
ers_use_manager
tests/unit/interface/test_progress_utils.py::test_progress_tracker_forced_update
_and_complete
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_prefers_di
alog_selection
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_validates_
input
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_multi_select_re
turns_checkbox_choices
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_produces_timeline_snapshot
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_merges_extended_metadata
tests/unit/interface/test_research_telemetry.py::test_merge_extended_metadata_in
to_payload_appends_sections
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_invokes_connectors
tests/unit/interface/test_research_telemetry.py::test_signature_roundtrip_valida
tes
tests/unit/interface/test_research_telemetry.py::test_signature_failure_with_wro
ng_secret
tests/unit/interface/test_textual_ux_bridge.py::test_question_and_display_intera
ctions_are_recorded
tests/unit/interface/test_textual_ux_bridge.py::test_confirm_choice_falls_back_t
o_default
tests/unit/interface/test_textual_ux_bridge.py::test_progress_updates_capture_ne
sted_subtasks
tests/unit/interface/test_textual_ux_bridge.py::test_capabilities_reflect_textua
l_availability
tests/unit/interface/test_textual_ux_bridge.py::test_require_textual_guard
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_enabled
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_disabled
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_backward_compatib
ility_methods
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_handle_error_defa
ult_implementation
tests/unit/interface/test_ux_bridge_coverage.py::test_progress_indicator_context
_manager
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_fallback_i
mport tests/unit/interface/test_uxbridge_aliases.py::test_function
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True]
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False]
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False]
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[absent-3-False]
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants
tests/unit/interface/test_webui_bridge_cli_parity.py::test_webui_bridge_matches_
cli_prompt_defaults
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_helpers_normal
ize_mixed_inputs
tests/unit/interface/test_webui_bridge_fast_suite.py::test_prompt_helpers_echo_d
efaults
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_append
s_documentation_links
tests/unit/interface/test_webui_bridge_fast_suite.py::test_require_streamlit_cac
hes_and_guides
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_clamps_handle_
invalid_inputs
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_and_pr
ogress_use_formatter
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_highli
ght_falls_back_to_write
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_uses
_cached_stub
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_impo
rts_when_missing
tests/unit/interface/test_webui_bridge_handshake.py::test_adjust_wizard_step_han
dles_invalid_inputs
tests/unit/interface/test_webui_bridge_handshake.py::test_normalize_wizard_step_
handles_varied_inputs
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_routes_
messages_and_sanitizes
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_error_b
ranch_records_message
tests/unit/interface/test_webui_bridge_handshake.py::test_bridge_prompt_helpers_
return_defaults
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
handles_varied_inputs
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
invalid_total_defaults_to_zero
tests/unit/interface/test_webui_bridge_normalize.py::test_progress_indicator_rej
ects_missing_parent
tests/unit/interface/test_webui_bridge_normalize.py::test_display_result_routes_
messages_to_streamlit
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_upda
te_paths
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations
tests/unit/interface/test_webui_bridge_progress.py::test_require_streamlit_failu
re
tests/unit/interface/test_webui_bridge_progress.py::test_adjust_wizard_step_edge
s
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_display_re
sult_routes_and_sanitizes
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_session_ac
cess_wrappers
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_prompt_ali
ases_and_progress
tests/unit/interface/test_webui_bridge_progress.py::test_normalize_wizard_step_v
aried_inputs
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_require_stream
lit_raises
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_progress_indic
ator_status_transitions
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_handshake
_routes_to_streamlit
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_error_rou
te_sanitizes_output
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_respects_
sanitization_flag
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_highlight
_routes_to_info
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_success_r
outes_to_success
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_require_streamlit
_missing_dependency_surfaces_install_guidance
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_navigation
_normalization_matches_state_invariants
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_prompt_defaults_a
lign_with_uxbridge_contract
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_display_result_ch
annels_respect_output_formatter_contract
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_get_wiza
rd_manager_uses_session_state
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_session_
helpers_delegate
tests/unit/interface/test_webui_bridge_targeted.py::test_adjust_wizard_step_inva
lid_direction_keeps_bounds
tests/unit/interface/test_webui_bridge_targeted.py::test_normalize_wizard_step_h
andles_strings
tests/unit/interface/test_webui_bridge_targeted.py::test_question_and_confirmati
on_defaults
tests/unit/interface/test_webui_bridge_targeted.py::test_display_result_highligh
t_routes_to_info
tests/unit/interface/test_webui_bridge_targeted.py::test_create_progress_cycles_
statuses
tests/unit/interface/test_webui_bridge_targeted.py::test_session_helpers_delegat
e_to_state_access
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_requ
ires_session_state
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_nested_completion_and_sanitization
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_wizard_na
vigation_and_display_fallback
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_default_s
tatus_thresholds
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_updates_and_completion
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_nested_su
btask_lifecycle
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_display_r
esult_routes_by_type
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_get_wizar
d_manager_and_create
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute
tests/unit/interface/test_webui_commands.py::test_cli_returns_none_when_missing
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_pass_thr
ough
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-File not found]
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Permission denied]
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Invalid value]
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Missing key]
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Type error]
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_generic_
exception
tests/unit/interface/test_webui_commands.py::test_cli_uses_cli_module_when_avail
able
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_reraises
_devsynth_error
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0]
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1]
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2]
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3]
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key 'api'-key_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error]
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-]
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: File not found: config.yaml-Make sure the
file exists]
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Permission denied: secrets.env-necessary
permissions]
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Invalid value: bad input-Please check your
input]
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Missing key: 'api_key'-Verify that the
referenced key exists]
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Type error: wrong type-Check that all
inputs]
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[warning-warning]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[success-success]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[info-info]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[unexpected-write]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[# Overview-expected_calls0]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[## Section-expected_calls1]
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[### Deep Dive-expected_calls2]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error]
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-]
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_normalize_ste
p_logs_warning_on_invalid_value
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_adjust_step_w
arns_on_invalid_direction
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors
tests/unit/interface/test_webui_progress_time.py::test_update_records_time
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_initialization
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_method_existence
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_initialization
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_method_existence
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling
tests/unit/interface/test_webui_rendering_module.py::test_validate_requirements_
step_requires_fields
tests/unit/interface/test_webui_rendering_module.py::test_handle_requirements_na
vigation_cancel_clears_state
tests/unit/interface/test_webui_rendering_module.py::test_save_requirements_clea
rs_temporary_keys
tests/unit/interface/test_webui_rendering_progress.py::test_gather_wizard_render
s_cli_summary
tests/unit/interface/test_webui_rendering_progress.py::test_render_progress_summ
ary_prefers_checkpoint_eta_strings
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist
tests/unit/interface/test_webui_routing.py::test_router_uses_session_state
tests/unit/interface/test_webui_routing.py::test_router_resets_invalid_selection
tests/unit/interface/test_webui_routing.py::test_router_handles_sidebar_exceptio
n
tests/unit/interface/test_webui_routing.py::test_router_surfaces_page_exception
tests/unit/interface/test_webui_routing.py::test_router_requires_pages
tests/unit/interface/test_webui_routing.py::test_router_honors_explicit_default
tests/unit/interface/test_webui_routing.py::test_router_reports_missing_page_han
dler
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_main_block_succeeds
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_simulatio
n_sanitises_nested_tasks
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_require_s
treamlit_guidance
tests/unit/interface/test_webui_state_errors.py::test_clear_reraises_after_loggi
ng
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_require_streamlit_reports_install_guidance
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[error-kwargs0-error]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[warning-kwargs1-warning]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[success-kwargs2-success]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[highlight-kwargs3-info]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[error-kwargs0-error]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[warning-kwargs1-warning]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[success-kwargs2-success]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[highlight-kwargs3-info]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_progre
ss_indicator_nested_lifecycle_and_statuses
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[0-0-Starting...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[10-100-Starting...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[25-100-Processing...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[50-100-Halfway there...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[80-100-Almost done...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[99-100-Finalizing...]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[100-100-Complete]
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_wizard_helpe
rs_clamp_malformed_inputs
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_initialization
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_method_existence
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_initialization
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_method_existence
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_valid_config
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_default_config
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_auto_model_selection
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_custom_port
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_lmstudio_unavailable
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_validation
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_precedence
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_counting_integration
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_limit_validation
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_c
ircuit_breaker_initialization
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_r
etry_logic_configuration
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_valid_config
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_environment_variable
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_without_api_key_raises_error
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_custom_base_url
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_openai_client_unavailable
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_validation
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_with_defaults
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_precedence
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_counting_integration
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_limit_validation
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_circu
it_breaker_initialization
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_logic_configuration
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_metrics_
collection_setup
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_telemetr
y_emission
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_message_args_
and_mappings
tests/unit/logging/test_logging_setup.py::test_redact_filter_property_loop_prese
rves_inputs
tests/unit/logging/test_logging_setup.py::test_request_context_filter_attaches_c
ontext
tests/unit/logging/test_logging_setup.py::test_json_formatter_serializes_request
_context
tests/unit/logging/test_logging_setup.py::test_redaction_in_message_and_payload
tests/unit/logging/test_logging_setup.py::test_request_context_filter_injects_fi
elds_and_clears
tests/unit/logging/test_logging_setup.py::test_jsonformatter_includes_exception_
block
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_no_file_l
ogging
tests/unit/logging/test_logging_setup.py::test_get_log_dir_and_file_use_env_over
rides
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_uses_project_dir_f
or_relative_path
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_under_te
st_project_dir
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_absolute
_outside_home
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_project_d
ir_when_file_logging_disabled
tests/unit/logging/test_logging_setup.py::test_configure_logging_redirects_home_
and_disables_file_handler
tests/unit/logging/test_logging_setup.py::test_short_secret_not_redacted
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_merges_and_fi
lters_kwargs
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_table_normali
zation
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_does_not_muta
te_extra_inputs
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_normalizes_tr
uthy_exc_info
tests/unit/logging/test_logging_setup.py::test_configure_logging_console_only_us
es_caplog
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_secret_tokens
_via_caplog
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_handles_missing_
log_file_path
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_emits_structured
_extras_with_context
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_secrets_f
ilter_masks_values
tests/unit/logging/test_logging_setup_additional_paths.py::test_json_formatter_i
ncludes_context_and_extras
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_respects_project_dir
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_skips_creation_when_disabled
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_warns_when_creation_fails
tests/unit/logging/test_logging_setup_additional_paths.py::test_devsynth_logger_
filters_reserved_extra_keys
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_filter_ma
sks_args_and_payload
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_provis
ions_json_file_handler
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_consol
e_only_mode
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[permission-error]
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[file-not-found]
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_idempo
tent_with_identical_configuration
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_e
xplicit_level_overrides_env
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_j
son_handler_writes_structured_output
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_r
econfigures_console_only_toggle
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_file-logging]
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_console-only]
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_file-logging]
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_console-custom]
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_idempotent_with_identical_settings
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_preserves_filters_on_reconfigure
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_falls_back_to_console_on_file_handler_failure
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_create_dir_guard_preserves_console_only_mode
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle
tests/unit/logging/test_logging_setup_contexts.py::test_cli_context_wires_consol
e_and_json_file_handlers
tests/unit/logging/test_logging_setup_contexts.py::test_test_context_redirects_a
nd_supports_console_only_toggle
tests/unit/logging/test_logging_setup_contexts.py::test_create_dir_toggle_disabl
es_json_file_handler
tests/unit/logging/test_logging_setup_contexts.py::test_console_and_json_handler
s_report_consistent_payloads
tests/unit/logging/test_logging_setup_invariants.py::test_configure_logging_is_i
dempotent_for_handlers
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
masks_known_tokens
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
redacts_payload_and_details
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
survives_mapping_errors
tests/unit/logging/test_logging_setup_invariants.py::test_cli_to_test_context_sw
itch_updates_log_destination
tests/unit/logging/test_logging_setup_invariants.py::test_json_formatter_include
s_structured_extras
tests/unit/logging/test_logging_setup_levels.py::test_configure_logging_honors_e
nv_log_level
tests/unit/logging/test_logging_setup_levels.py::test_json_formatter_captures_re
quest_context
tests/unit/logging/test_logging_setup_levels.py::test_dev_logger_attaches_filter
s_and_handlers
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir]
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env]
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir-disabled]
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env-create-dir-disabled]
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[home-absolute]
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[non-home-absolute]
tests/unit/memory/test_issue3_regression_guard.py::test_issue3_findings_persist
tests/unit/memory/test_layered_cache.py::test_promotes_value_to_higher_layer
tests/unit/memory/test_layered_cache.py::test_hit_ratio_tracking
tests/unit/memory/test_layered_cache.py::test_read_and_write_alias_methods
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_rel
oad_exposes_runtime_protocol
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_protocol_runtime_
checks_accept_custom_layers
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_pro
tocol_remains_runtime_checkable
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_missing_required_store
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_m
emory_store_protocol_runtime_check
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_impor
t_and_construction_succeeds
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_accep
ts_optional_backends
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_still
_requires_primary_backend
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_rejec
ts_unknown_backend_names
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_stub_store_matches
_protocol_runtime
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_memory_store_param
eters_are_runtime_typevars
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_parameterised_memo
ry_store_runtime_is_safe
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_snapshot_alias_pre
serves_runtime_origin
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_value_typevar_iden
tity_is_preserved
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_and_s
napshot_share_runtime_typevar
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_transaction_rolls_
back_typed_stores
tests/unit/memory/test_sync_manager_transaction_failure.py::test_transaction_rol
ls_back_all_stores
tests/unit/memory/test_transaction_lifecycle_failures.py::test_commit_unknown_tr
ansaction_returns_false
tests/unit/memory/test_transaction_lifecycle_failures.py::test_rollback_unknown_
transaction_returns_false
tests/unit/memory/test_transaction_lifecycle_failures.py::test_double_commit_fai
ls_and_state_persists
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_complete
s_with_deterministic_seed
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_tr
ansitions_and_memory_integration
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_imp
ort_accessor_returns_typed_apply
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_seeds_random_and_numpy_modules
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_logs_backoff_and_retry_exhaustion
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_coordinator_records_each_phase
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_exits_when_total_budget_elapsed
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_accepts_dialectical_sequence_payload
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_records_unknown_phase_and_next_phase_fallbacks
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_returns_typed_apply
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_default_path_executes
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_dia
lectical_sequence_records_with_coordinator_fallback
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_tolerates_seed_failures
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_trace_complete
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_configures_seed_providers
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_budget_precheck
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_retries_then_succeeds
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_exhaustion_sets_stop
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_copies_mapping_payload
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_handles_dialectical_sequence
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_raises_for_non_mapping_payload
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_halts_when_result_missing
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_matrix
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_exhausts_retry_budget_and_backoff
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_retries_clamp_sleep_to_remaining_budget
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_stops_retry_when_total_budget_exhausted
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_coordinator_records_phase_transitions
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_records_dialectical_sequences_for_coordinator
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_fallbacks_for_invalid_phase_and_next_phase
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_honors_total_time_budget
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_seeds_random_sources
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_preserves_nonstandard_phase_without_hints
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_handles_extended_phase_transitions
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_enforces_total_time_budget
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retries_until_success
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_fallback_transitions_and_propagation
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_respects_max_iterations_limit
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retry_backoff_respects_remaining_budget
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_honors_phase_and_next_phase_fields
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_clamps_retry_when_budget_consumed
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_rejects_non_mapping_task_payload
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_logs_retry_exhaustion_telemetry
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_exits_when_budget_elapsed_before_iteration
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_retry_sequence_updates_phase_and_coordinator
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_records_results_before_consensus_failure
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
tries_on_transient
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_emits_debug_and_clamps_sleep
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_without_budget_uses_base_backoff
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_clamps_backoff_and_respects_budget
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_remaining_budget_spent
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_retry_exhaustion
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
cords_consensus_failure_via_coordinator
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_consensus_failure_without_coordinator
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_budget_already_exhausted
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_invalid_next
_phase_falls_back_to_transition_map
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_missing_stat
us_relies_on_max_iterations
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_raises_for_non_mapping_results
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_rejects_non_mapping_task_payload
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_handles_seed_failures_gracefully
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_logs_retry_exhaustion
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_import_he
lper_exposes_typed_apply
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_immediate_timeout_skips_apply_invocation
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_respects_total_budget_and_emits_debug
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_uses_fallback_after_invalid_phase
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_stops_after_retry_exhaustion
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_seeds_random_and_numpy
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_applies_synthesis_to_task
tests/unit/methodology/test_adhoc_adapter.py::test_should_start_cycle_true
tests/unit/methodology/test_adhoc_adapter.py::test_should_progress_to_next_phase
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_evaluation
_terminates_with_many_hooks
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_hooks_cont
inue_after_exception
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations
tests/unit/methodology/test_edrr_coordinator.py::test_automate_retrospective_rev
iew_summarizes_results
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs tests/unit/methodology/test_kanban_adapter.py::test_should_start_cycle
tests/unit/methodology/test_kanban_adapter.py::test_progress_respects_wip_limit
tests/unit/methodology/test_milestone_adapter.py::test_should_start_cycle
tests/unit/methodology/test_milestone_adapter.py::test_progress_requires_approva
l_when_configured
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget
tests/unit/methodology/test_sprint_adapter.py::test_calculate_phase_end_time
tests/unit/methodology/test_sprint_adapter.py::test_is_phase_time_exceeded_false
tests/unit/methodology/test_sprint_adapter.py::test_should_progress_when_time_ex
ceeded
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase
tests/unit/methodology/test_sprint_adapter.py::test_before_cycle_provides_contex
t
tests/unit/methodology/test_sprint_adapter.py::test_before_expand_sets_phase_sta
rt_time
tests/unit/methodology/test_sprint_adapter.py::test_after_retrospect_captures_sp
rint_plan
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_failure_br
anch_sets_failed
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step
tests/unit/policies/test_verify_security_policy.py::test_passes_when_all_variabl
es_set
tests/unit/policies/test_verify_security_policy.py::test_fails_when_variable_mis
sing
tests/unit/providers/test_provider_contract.py::test_stub_provider_offline_defau
lts_to_stub
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_uses_
safe_provider
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_null_
provider
tests/unit/providers/test_provider_system_additional.py::test_unknown_provider_f
alls_back
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_us
es_provider_config
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_re
spects_track_metrics_flag
tests/unit/providers/test_provider_system_additional.py::test_stub_provider_dete
rministic_embeddings
tests/unit/providers/test_provider_system_additional.py::test_create_tls_config_
uses_settings
tests/unit/providers/test_provider_system_additional.py::test_provider_factory_p
refers_explicit_tls_config
tests/unit/providers/test_provider_system_additional.py::test_fallback_async_ski
ps_open_circuit
tests/unit/providers/test_provider_system_branches.py::test_factory_honors_disab
le_flag
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_s
tub_safe_default
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_n
ull_when_requested
tests/unit/providers/test_provider_system_branches.py::test_explicit_openai_with
out_key_returns_null
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_availabilit
y_guard_returns_safe_provider
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_fallback_fa
ilure_promotes_safe_provider
tests/unit/providers/test_provider_system_branches.py::test_explicit_anthropic_w
ithout_key_returns_null
tests/unit/providers/test_provider_system_branches.py::test_anthropic_unsupporte
d_error
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_us
es_next_provider_on_failure
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_pr
opagates_failure_when_all_fail
tests/unit/providers/test_provider_system_branches.py::test_fallback_disabled_tr
ies_only_first_provider
tests/unit/providers/test_provider_system_branches.py::test_fallback_initializat
ion_orders_providers_and_records_circuit_results
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_skips
_open_circuit_breaker
tests/unit/providers/test_provider_system_branches.py::test_openai_async_retry_e
mits_telemetry
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_circu
it_breaker_recovery
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[primary-success]
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[secondary-success]
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[all-fail]
tests/unit/providers/test_provider_system_branches.py::test_factory_applies_tls_
and_retry_settings
tests/unit/providers/test_provider_system_branches.py::test_emit_retry_telemetry
_logs_and_counts
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_comp
lete_builds_payload
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_reje
cts_invalid_temperature
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_embe
d_returns_embeddings
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_co
mplete_uses_custom_messages
tests/unit/providers/test_provider_system_branches.py::test_fallback_embed_moves
_to_next_provider
tests/unit/providers/test_provider_system_branches.py::test_fallback_aembed_reco
vers_from_failure
tests/unit/providers/test_provider_system_branches.py::test_get_provider_config_
reads_env_file
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ope
nai_success_path
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ant
hropic_requires_key
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_unk
nown_provider_uses_null
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_lms
tudio_fallback_when_openai_missing
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_asyn
c_paths
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_rea
l_module_branches
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_as
ync_paths
tests/unit/providers/test_provider_system_branches.py::test_complete_helper_incr
ements_metrics_and_propagates_error
tests/unit/providers/test_provider_system_branches.py::test_embed_helper_wraps_n
on_provider_errors
tests/unit/providers/test_provider_system_branches.py::test_acomplete_helper_inc
rements_metrics
tests/unit/providers/test_provider_system_branches.py::test_aembed_helper_promot
es_unexpected_errors
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_components_deterministic
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_requirements_deterministic
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_edrr_phas
e_mapping_on_persist
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_evaluatio
n_hook_invoked_on_consensus_true
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[chromadb]
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[faiss]
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[kuzu]
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[tinydb]
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_file_operations
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_network_calls
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_global_state
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_fixture_usage
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_simple_test_file
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_file_with_isolation_marker
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_handles_syntax_errors
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_generates_recommendations
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages
tests/unit/scripts/test_analyze_test_dependencies.py::TestIntegration::test_end_
to_end_analysis
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_help
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_missing
_test_dir
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_test_execution_script
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_coverage_script
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_validation_script
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_shell_script
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_syntax_errors
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_finds_
testing_scripts
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_analyz
es_overlaps
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_git_us
age_frequency
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_genera
tes_consolidation_recommendations
tests/unit/scripts/test_audit_testing_scripts.py::TestMarkdownGeneration::test_g
enerates_markdown_report
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_help
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_missing_scr
ipts_dir
tests/unit/scripts/test_audit_testing_scripts.py::test_integration_audit_workflo
w
tests/unit/scripts/test_auto_issue_comment.py::test_parse_issue_numbers_extracts
_ids
tests/unit/scripts/test_auto_issue_comment.py::test_dry_run_when_env_missing
tests/unit/scripts/test_auto_issue_comment.py::test_posts_comment_when_env_prese
nt
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_initialization
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_timeout
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_empty
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_with_data
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_generates_recommendations
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_help
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_invalid_
workers
tests/unit/scripts/test_benchmark_test_execution.py::test_integration_benchmark_
workflow
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
valid_anchor
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
missing_anchor
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_component
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_missing_component
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_unit
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
tests/unit/scripts/test_examples_smoke_script.py::test_main_default_examples_suc
ceeds
tests/unit/scripts/test_examples_smoke_script.py::test_main_reports_failure_when
_analyze_raises
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors
tests/unit/scripts/test_gen_ref_pages.py::test_gen_ref_pages_matches_examples
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_with_file
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_without_file
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_property_test_metrics
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_calculate_overall_quality_score
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_generate_quality_recommendations
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics
tests/unit/scripts/test_generate_quality_report.py::test_html_generation
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures
tests/unit/scripts/test_security_ops.py::test_collect_logs_missing_directory
tests/unit/scripts/test_security_ops.py::test_run_audit_calls_security_audit
tests/unit/scripts/test_security_ops.py::test_list_outdated_runs_poetry
tests/unit/scripts/test_security_ops.py::test_apply_updates_runs_poetry
tests/unit/scripts/test_security_scan_script.py::test_main_non_strict_no_tools_r
eturns_ok
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_passes_when_above
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_fails_when_below
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_valid
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_issue
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_mvuu
tests/unit/scripts/test_verify_release_state.py::test_draft_status_missing_tag
tests/unit/scripts/test_verify_release_state.py::test_published_status_without_t
ag
tests/unit/scripts/test_verify_release_state.py::test_published_status_with_tag
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_returns
_fields
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_without
_header
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_missing
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_present
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_when_log_mi
ssing
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_unreso
lved_questions
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_only_r
esolved
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_invali
d_json
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache_i
nvalidation
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_path_fi
lter
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_f
lags_missing_docs
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_p
asses_when_documented
tests/unit/scripts/test_verify_test_markers_cli.py::test_argparser_includes_chan
ged_flag
tests/unit/scripts/test_verify_test_markers_cli.py::test_verify_files_with_temp_
test
tests/unit/scripts/test_verify_test_markers_cross_check.py::test_argparser_inclu
des_cross_check_flag
tests/unit/scripts/test_wsde_edrr_simulation.py::test_simulation_converges
tests/unit/security/test_api_authentication.py::test_verify_token_valid_is_valid
tests/unit/security/test_api_authentication.py::test_verify_token_invalid_is_val
id
tests/unit/security/test_api_authentication.py::test_verify_token_missing_succee
ds
tests/unit/security/test_api_authentication.py::test_verify_token_wrong_format_s
ucceeds
tests/unit/security/test_api_authentication.py::test_verify_token_access_control
_disabled_succeeds
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_password_hasher_parameters_safe_defaults
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_hash_and_verify_roundtrip
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_generate_key_validates_and_encrypts
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_invalid_key_rejected
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_missing_key_env_raises
tests/unit/security/test_authentication_optional_dependency.py::test_authenticat
ion_handles_missing_argon2
tests/unit/security/test_authorization_checks.py::test_require_authorization_all
ows_authorized_action
tests/unit/security/test_authorization_checks.py::test_require_authorization_rai
ses_forbidden
tests/unit/security/test_deployment_coverage.py::test_require_non_root_user_when
_not_required
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_missing_vars
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_all_present
tests/unit/security/test_deployment_coverage.py::test_apply_secure_umask
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_with_requir
ed_env
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_without_req
uired_env
tests/unit/security/test_encryption.py::test_generate_key_returns_expected_resul
t
tests/unit/security/test_encryption.py::test_encrypt_decrypt_roundtrip_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_string_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_bytes_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_env_var_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_no_key_raises_error
tests/unit/security/test_encryption.py::test_encrypt_decrypt_with_env_var_succee
ds
tests/unit/security/test_encryption.py::test_decrypt_invalid_token_raises_error
tests/unit/security/test_encryption.py::test_decrypt_with_wrong_key_raises_error
tests/unit/security/test_logging_redaction.py::test_logging_redacts_openai_api_k
ey
tests/unit/security/test_logging_redaction.py::test_logging_redacts_in_extra_det
ails
tests/unit/security/test_memory_encryption.py::test_json_file_store_encryption_s
ucceeds
tests/unit/security/test_memory_encryption.py::test_lmdb_store_encryption_succee
ds
tests/unit/security/test_memory_encryption.py::test_tinydb_store_encryption_succ
eeds tests/unit/security/test_policy_audit.py::test_audit_detects_violation
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file
tests/unit/security/test_review.py::test_review_due_when_interval_elapsed
tests/unit/security/test_review.py::test_review_not_due_before_interval
tests/unit/security/test_review.py::test_next_review_date_calculation
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_script_suc
ceeds
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_control_ch
ars_succeeds
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_both_succe
eds
tests/unit/security/test_sanitization.py::test_sanitize_input_strips_whitespace_
succeeds
tests/unit/security/test_sanitization.py::test_sanitize_input_no_script_tags_suc
ceeds
tests/unit/security/test_sanitization.py::test_sanitize_input_no_control_chars_s
ucceeds
tests/unit/security/test_sanitization.py::test_sanitize_input_complex_script_tag
s_succeeds
tests/unit/security/test_sanitization.py::test_sanitize_input_multiple_script_ta
gs_succeeds
tests/unit/security/test_sanitization.py::test_validate_safe_input_with_safe_inp
ut_returns_expected_result
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_s
cript_raises_error
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_c
ontrol_chars_raises_error
tests/unit/security/test_security_audit.py::test_run_executes_checks
tests/unit/security/test_security_audit.py::test_run_raises_on_policy_failure
tests/unit/security/test_security_audit.py::test_report_writes_results
tests/unit/security/test_security_audit.py::test_report_records_failure
tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_runs_che
cks
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_respects
_skip_flags
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_register
ed
tests/unit/security/test_security_flags_env.py::test_authentication_disabled_all
ows_any_credentials
tests/unit/security/test_security_flags_env.py::test_authentication_enabled_enfo
rces
tests/unit/security/test_security_flags_env.py::test_authorization_disabled_allo
ws
tests/unit/security/test_security_flags_env.py::test_authorization_enabled_enfor
ces
tests/unit/security/test_security_flags_env.py::test_sanitization_disabled_no_er
ror
tests/unit/security/test_security_flags_env.py::test_sanitization_enabled_raises
tests/unit/security/test_tls_config.py::test_tls_config_timeout_env_override
tests/unit/security/test_tls_config.py::test_tls_config_timeout_explicit_overrid
e
tests/unit/security/test_tls_config.py::test_tls_config_validation_raises_error
tests/unit/security/test_tls_config.py::test_tls_config_validation_partial_raise
s_error
tests/unit/security/test_tls_config.py::test_tls_config_validation_key_only_succ
eeds
tests/unit/security/test_tls_config.py::test_tls_config_validation_cert_only_suc
ceeds
tests/unit/security/test_tls_config.py::test_tls_config_validation_missing_raise
s_error
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_defau
lt_succeeds
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_verif
y_false_succeeds
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
ca_file_has_expected
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_and_key_has_expected
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_only_has_expected
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_ca_fi
le_precedence_succeeds
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_all_p
arams_succeeds
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_valid_string_
is_valid
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[]
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[   ]
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[None]
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_non_string_va
lue_succeeds
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_0]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_1]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[10-10]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[-5--5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[0-0]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[5-1-10-5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[1-1-10-1]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[10-1-10-10]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[-5--10-0--5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[0--10-10-0]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[abc]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[1.5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[None]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value4]
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[0-1]
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[-5-0]
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[5-10]
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[10-5]
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[0--1]
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[100-99]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[a-choices0]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[1-choices1]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[True-choices2]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[None-choices3]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[value-choices4]
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[5-choices5]
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[d-choices0]
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[4-choices1]
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[None-choices2]
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[missing-choices3]
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[20-choices4]
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_option
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_with_no_path
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_run_tests_command
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_standard_cli_fallback
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_missing_run_tests_m
odule
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_cli_import_errors
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_runtime_errors
tests/unit/test_sentinel_speed_markers.py::test_sentinel_fast_bucket_present
tests/unit/test_simple_addition.py::test_add_returns_sum_for_integers
tests/unit/test_simple_addition.py::test_add_accepts_floats_and_mixed_numeric_ty
pes
tests/unit/test_simple_addition.py::test_add_raises_type_error_for_non_numeric_i
nputs
tests/unit/test_verify_test_organization_sentinel.py::test_verify_test_organizat
ion_returns_zero
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran
tests/unit/testing/test_collect_cache_sanitize.py::test_sanitize_node_ids_strips
_line_numbers_only_when_no_function_delimiter
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_union_
reaches_threshold_with_overlap
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_thresh
old_detection_matches_cli_expectations
tests/unit/testing/test_deterministic_seed_fixture.py::test_deterministic_seed_f
ixture_sets_env_vars
tests/unit/testing/test_env_ttl_and_sanitize.py::test_bad_ttl_env_falls_back_to_
default
tests/unit/testing/test_env_ttl_and_sanitize.py::test_sanitize_node_ids_preserve
s_function_qualifier_and_strips_line_numbers
tests/unit/testing/test_failure_tips.py::test_failure_tips_contains_core_guidanc
e
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_can_mutate_addition
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_mutates_addition_to_subtraction
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_cannot_mutate_non_arithmetic
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_can_mutate_equality
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_mutates_equality_to_inequality
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_ca
n_mutate_and_operation
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_mu
tates_and_to_or
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_can_
mutate_not_operation
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_muta
tes_not_by_removal
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_can_mutat
e_boolean_constant
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_t
rue_to_false
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_n
umber_to_zero_and_one
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_mutations_for_simple_code
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_handles
_syntax_errors
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_different_mutation_types
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_initializa
tion
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_killed
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_survived
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
esult_dataclass
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
eport_dataclass
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow
tests/unit/testing/test_run_tests.py::test_sanitize_node_ids_strips_line_numbers
_without_function_delimiter
tests/unit/testing/test_run_tests.py::test_failure_tips_contains_key_guidance_li
nes
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches
tests/unit/testing/test_run_tests.py::test_run_tests_segment_batches
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes
tests/unit/testing/test_run_tests_additional_coverage.py::test_failure_tips_ment
ions_core_troubleshooting_flags
tests/unit/testing/test_run_tests_additional_coverage.py::test_ensure_pytest_cov
_plugin_env_injects_and_skips
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_success
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_missing_json
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_success
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_errors
tests/unit/testing/test_run_tests_additional_coverage.py::test_sanitize_node_ids
_removes_line_numbers
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker
tests/unit/testing/test_run_tests_artifacts.py::test_reset_coverage_artifacts_re
moves_stale_files
tests/unit/testing/test_run_tests_artifacts.py::test_ensure_coverage_artifacts_g
enerates_reports
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_fails_when_pytest
_cov_missing
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_successful_single
_batch
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_h
andles_missing_json
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_r
ejects_invalid_json
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_missing_html
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_s
uccess
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_failure_tips_con
tains_suggestions
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_i
nject_plugins_and_emit_tips
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_failure_tips_surfac
e_cli_remediations
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_batches_fo
llow_segment_size
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_han
dles_resource_marker
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_co
verage_totals
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_skips_placeh
older_artifacts
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_env_passthrough_an
d_coverage_lifecycle
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled
tests/unit/testing/test_run_tests_collection_cache.py::test_sanitize_node_ids_st
rips_trailing_line_without_function_delimiter
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_reset_coverage_art
ifacts_removes_files_and_directories
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_data_missing
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_no_measured_files
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_generates_reports_and_syncs_legacy
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_html_failure_still_writes_json
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_run_tests_writes_m
anifest_with_coverage_reference
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py::test_ensure_c
overage_artifacts_combines_fragment_files
tests/unit/testing/test_run_tests_coverage_short_circuit.py::test_ensure_coverag
e_artifacts_short_circuits_without_measured_files
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_repor
ts_missing_json
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_flags
_invalid_json
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_totals
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_html_index
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_rejec
ts_empty_html
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_succe
ss_path
tests/unit/testing/test_run_tests_coverage_uplift.py::test_failure_tips_formats_
return_code_and_cmd
tests/unit/testing/test_run_tests_coverage_uplift.py::test_reset_coverage_artifa
cts_handles_oserror
tests/unit/testing/test_run_tests_coverage_uplift.py::test_ensure_coverage_artif
acts_handles_unreadable_html
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags
tests/unit/testing/test_run_tests_keyword_exec.py::test_keyword_marker_executes_
matching_node_ids
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success
tests/unit/testing/test_run_tests_logic.py::test_sanitize_node_ids_strips_line_n
umbers_without_function_delimiter
tests/unit/testing/test_run_tests_logic.py::test_failure_tips_contains_key_guida
nce_lines
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_success
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_failure
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_segmented_exe
cution
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_marker_expres
sion_building
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_env_defaults
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exception_han
dling
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exit_code_5_s
uccess
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_dry_run_skips
_execution
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_keyword_filte
r
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_maxfail_optio
n
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_smoke_mode_pl
ugin_injection
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_s
uccess
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_f
rom_existing_cache
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_c
ollection_failure
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_maxfail
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_custom_env
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_execution_failur
e_returns_false
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_no_target_p
ath_raises_error
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories
tests/unit/testing/test_run_tests_marker_fallback.py::test_run_tests_marker_fall
back_skips_segmentation
tests/unit/testing/test_run_tests_marker_fallback.py::test_build_segment_metadat
a_uses_typed_sequences
tests/unit/testing/test_run_tests_marker_merge.py::test_speed_marker_merged_with
_lmstudio_keyword_filter
tests/unit/testing/test_run_tests_marker_merge.py::test_global_marker_with_lmstu
dio_keyword_filter
tests/unit/testing/test_run_tests_module.py::test_sanitize_node_ids_dedup_and_st
rip_line_numbers
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return
tests/unit/testing/test_run_tests_module.py::test_failure_tips_includes_segmenta
tion_guidance
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_missing_file
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_invalid_json
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions
tests/unit/testing/test_run_tests_option_parsing.py::test_parse_pytest_addopts_h
andles_balanced_and_unbalanced_quotes
tests/unit/testing/test_run_tests_option_parsing.py::test_addopts_has_plugin_det
ects_split_and_concatenated_forms
tests/unit/testing/test_run_tests_option_parsing.py::test_coverage_plugin_disabl
ed_detects_common_overrides
tests/unit/testing/test_run_tests_orchestration.py::test_verbose_flag_adds_v_to_
pytest_command
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run
tests/unit/testing/test_run_tests_orchestration.py::test_pytest_addopts_are_pres
erved
tests/unit/testing/test_run_tests_orchestration.py::test_extra_marker_adds_m_fla
g_to_command
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env0-False---no-cov -s]
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env1-True--k smoke -p
pytest_cov]
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env2-False--p
no:pytest_bdd -s]
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env3-True--k feature -p
pytest_bdd.plugin]
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_honors_env_timeout
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_adds_plugin
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_requires_autoload_disable
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_respects_explicit_disables
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_detects_inline_plugin_token
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[--no-cov -s-False---no-cov -s]
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[-k smoke-True--k smoke -p pytest_cov]
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_pytest_cov_support_
status_missing_plugin
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_run_tests_aborts_wh
en_pytest_cov_missing
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_adds_plugin
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_requires_autoload_disable
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_detects_existing_plugin
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_respects_explicit_disable
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-p no:pytest_bdd -s-False--p no:pytest_bdd
-s]
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-k feature-True--k feature -p
pytest_bdd.plugin]
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_strips_tra
iling_line_numbers_without_function_sep
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_keeps_ids_
with_function_sep
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_deduplicat
es_preserving_order
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_single_speed
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_multiple_speeds
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_no_tests_found
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_failure_with_maxfail
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_reuses_cache_and_preserves_environment
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_dry_run_batches_use_typed_requests
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_command_building
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_multiple_node_ids
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_smoke_mode_env
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_no_parallel
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_batches
_execute
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_honors_
keyword_filter
tests/unit/testing/test_run_tests_segmented.py::test_run_segmented_tests_stop_af
ter_maxfail
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_failure_appends_tips
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_segmente
d_failure_surfaces_remediation
tests/unit/testing/test_run_tests_segmented_failures.py::test__run_segmented_tes
ts_aggregates_outputs
tests/unit/testing/test_run_tests_segmented_failures.py::test_segmented_runs_rei
nject_plugins_without_clobbering_addopts
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_single_b
atch_uses_request_object
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_success_invokes_publish
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_failure_skips_graph
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_reports_append_graph
tests/unit/testing/test_run_tests_segmented_report_flag.py::test_run_segmented_t
ests_reports_only_last_segment
tests/unit/testing/test_run_tests_speed_keyword_loop.py::test_speed_loop_uses_ke
yword_filter_and_executes_node_ids
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_merges_fast
_and_medium_collections
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_defaults_to
_fast_and_medium_when_unspecified
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_excludes_gu
i_by_default
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_allows_gui_
when_requested
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_strips_line
_numbers_without_function_selector
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_preserves_o
rder
tests/unit/testing/test_sanitize_node_ids_minimal.py::test_sanitize_node_ids_str
ips_line_when_no_function
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_tuple_e
xc_info
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_invalid
_exc_info
tests/unit/utils/test_logging_coverage.py::test_get_logger_returns_correct_insta
nce
tests/unit/utils/test_logging_coverage.py::test_setup_logging_with_different_log
_levels
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_false_e
xc_info
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_none_ex
c_info
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
baseexception_direct
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
true_with_active_exception
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
none_and_false_explicit
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_invalid_e
xc_info_to_hit_line_48
tests/unit/utils/test_logging_final_coverage.py::test_get_logger_function_direct
_call
tests/unit/utils/test_logging_final_coverage.py::test_setup_logging_function_dir
ect_calls
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_with_kwar
gs
tests/unit/utils/test_logging_utils.py::test_dev_synth_logger_normalizes_exc_inf
o_tuple_and_exception
tests/unit/utils/test_logging_utils.py::test_setup_logging_calls_configure_loggi
ng
tests/unit/utils/test_logging_utils.py::test_get_logger_returns_dev_synth_logger
_instance
tests/unit/utils/test_serialization.py::test_dumps_deterministic_round_trip_simp
le tests/unit/utils/test_serialization.py::test_dump_and_load_file_round_trip
tests/unit/utils/test_serialization.py::test_provider_env_as_dict_deterministic_
serialization
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_with_s
tring_already_having_newline
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_ensure
s_single_newline
tests/unit/utils/test_serialization_coverage.py::test_loads_with_no_trailing_new
line
tests/unit/utils/test_serialization_coverage.py::test_loads_with_trailing_newlin
e
tests/unit/utils/test_serialization_coverage.py::test_dump_to_file_complete_cove
rage
tests/unit/utils/test_serialization_coverage.py::test_load_from_file_complete_co
verage
tests/unit/utils/test_serialization_coverage.py::test_loads_with_multiple_traili
ng_newlines
tests/unit/utils/test_serialization_coverage.py::test_serialization_with_unicode
_characters
tests/unit/utils/test_serialization_coverage.py::test_serialization_edge_cases
tests/unit/utils/test_serialization_coverage.py::test_file_operations_with_speci
al_paths
tests/unit/utils/test_serialization_edges.py::test_loads_tolerates_missing_and_s
ingle_trailing_newline
tests/unit/utils/test_serialization_edges.py::test_dump_to_file_overwrites_and_k
eeps_single_newline
tests/unit/utils/test_serialization_extra.py::test_dumps_and_loads_deterministic
_round_trip_unicode_and_newline
tests/unit/utils/test_serialization_extra.py::test_dump_and_load_file_round_trip
_handles_utf8
tests/unit/utils/test_serialization_extra.py::test_loads_accepts_without_trailin
g_newline
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
direct_line_coverage
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
with_string_that_might_have_newline
tests/unit/utils/test_serialization_final_coverage.py::test_loads_direct_line_co
verage
tests/unit/utils/test_serialization_final_coverage.py::test_dump_to_file_direct_
line_coverage
tests/unit/utils/test_serialization_final_coverage.py::test_load_from_file_direc
t_line_coverage
tests/unit/utils/test_serialization_final_coverage.py::test_serialization_functi
ons_with_mock_to_ensure_coverage
tests/unit/utils/test_serialization_final_coverage.py::test_loads_with_various_n
ewline_scenarios
tests/unit/utils/test_serialization_final_coverage.py::test_file_operations_with
_explicit_paths
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
return_path
tests/unit/utils/test_serialization_final_coverage.py::test_loads_return_path
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_pass
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_failure
tests/integration/agents/test_generation/test_scaffold_generation.py::test_scaff
old_hook_creates_placeholder
tests/integration/agents/test_generation/test_scaffold_generation.py::test_proce
ss_generates_tests_and_scaffolds
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_ports
tests/integration/api/test_api_startup.py::test_agent_openapi_documents_workflow
_models
tests/integration/deployment/test_compose_workflow.py::test_setup_env_refuses_ro
ot
tests/integration/deployment/test_compose_workflow.py::test_check_health_env_per
missions
tests/integration/deployment/test_compose_workflow.py::test_rollback_requires_ta
g
tests/integration/deployment/test_deployment_scripts.py::test_bootstrap_env_refu
ses_root
tests/integration/deployment/test_deployment_scripts.py::test_health_check_valid
ates_url
tests/integration/deployment/test_deployment_scripts.py::test_prometheus_exporte
r_refuses_root
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions[start_stack.sh]
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions[stop_stack.sh]
tests/integration/general/test_complex_workflow.py::test_cmd
tests/integration/general/test_end_to_end_workflow.py::test_cmd
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_registration
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_configuration_loading
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_settings_extraction
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_initialization_with_defaults
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_mock_initialization
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_environment_variable_handling
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_config_file_integration
tests/integration/generated/test_generated_module.py::test_generated_module_work
flow
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_success
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_failure
tests/integration/llm/test_lmstudio_timing_baseline.py::test_timeout_configurati
on_sanity
tests/integration/mvu/test_command_execution.py::test_mvu_exec_runs_command
tests/integration/mvu/test_command_execution.py::test_mvu_exec_propagates_error
tests/integration/utils/test_logging_integration.py::test_setup_logging_returns_
project_logger
tests/integration/utils/test_logging_integration.py::test_log_normalizes_excepti
on
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_results_mu
ltiple_categories
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::tests_with_impr
ovements
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_collection
_performance
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_analysis_p
erformance
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed_cus
tom tests/behavior/test_documentation_generation.py::test_docs_build
tests/behavior/test_marker_auto_injection_guardrail.py::test_behavior_requires_e
xplicit_speed_marker
tests/behavior/test_progress_failover_and_recursion.py::test_long_running_progre
ss_records_telemetry
tests/behavior/test_progress_failover_and_recursion.py::test_run_with_long_runni
ng_progress_completes_on_exception
tests/behavior/test_progress_failover_and_recursion.py::test_provider_factory_fa
lls_back_to_stub_offline
tests/behavior/test_progress_failover_and_recursion.py::test_recursion_terminati
on_at_max_depth
tests/behavior/test_webui_smoke.py::test_webui_layout_config_smoke
tests/unit/adapters/cli/test_typer_adapter.py::test_build_app_returns_expected_r
esult
tests/unit/adapters/cli/test_typer_adapter.py::test_warn_if_features_disabled_al
l_disabled_succeeds
tests/unit/adapters/cli/test_typer_adapter.py::test_warn_if_features_disabled_so
me_enabled_succeeds
tests/unit/adapters/cli/test_typer_adapter.py::test_warn_if_features_disabled_ex
ception_raises_error
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_succeeds
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_table_mode
tests/unit/adapters/cli/test_typer_adapter.py::test_parse_args_has_expected
tests/unit/adapters/cli/test_typer_adapter.py::test_run_cli_succeeds
tests/unit/adapters/cli/test_typer_adapter.py::test_completion_cmd_displays_scri
pt
tests/unit/adapters/cli/test_typer_adapter.py::test_dashboard_hook_option_regist
ers
tests/unit/adapters/issues/test_mvu_enrichment.py::test_get_by_trace_id_enriches
tests/unit/adapters/memory/test_kuzu_adapter.py::test_ephemeral_adapter_cleanup
tests/unit/adapters/providers/test_embeddings.py::test_openai_provider_embed_cal
ls_api_succeeds
tests/unit/adapters/providers/test_embeddings.py::test_openai_provider_aembed_ca
lls_api
tests/unit/adapters/providers/test_embeddings.py::test_lmstudio_provider_embed_c
alls_api_succeeds
tests/unit/adapters/providers/test_embeddings.py::test_embed_function_success_wi
th_lmstudio_succeeds
tests/unit/adapters/providers/test_embeddings.py::test_aembed_function_success_w
ith_lmstudio
tests/unit/adapters/providers/test_embeddings.py::test_aembed_function_error_pro
pagation
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_success_succeeds
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_failure_raises_error
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_jitter_succeeds
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_on_retry_callback_succeeds
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_retryable_exceptions_raises_error
tests/unit/adapters/providers/test_fallback.py::test_retry_with_exponential_back
off_no_jitter_succeeds
tests/unit/adapters/test_kuzu_memory_store.py::test_store_and_search_succeeds
tests/unit/adapters/test_kuzu_memory_store.py::test_create_ephemeral_fallback
tests/unit/adapters/test_kuzu_memory_store.py::test_create_ephemeral_embedded
tests/unit/adapters/test_kuzu_memory_store.py::test_store_failure_raises_memory_
store_error
tests/unit/adapters/test_kuzu_memory_store.py::test_delete_returns_false_on_erro
r
tests/unit/adapters/test_provider_factory.py::test_create_provider_env_fallback_
has_expected
tests/unit/adapters/test_provider_factory.py::test_explicit_openai_missing_key_r
aises_error
tests/unit/adapters/test_provider_factory_env_vars.py::test_env_provider_openai_
succeeds
tests/unit/adapters/test_provider_factory_env_vars.py::test_env_provider_lmstudi
o_succeeds
tests/unit/adapters/test_sync_manager.py::TestSyncManagerCrossStoreQuery::test_c
ross_store_query_returns_results_succeeds
tests/unit/adapters/test_sync_manager.py::TestSyncManagerCrossStoreQuery::test_c
ross_store_query_returns_memory_records
tests/unit/adapters/test_sync_manager.py::TestSyncManagerCrossStoreQuery::test_q
uery_results_cached_succeeds
tests/unit/adapters/test_sync_manager.py::TestSyncManagerCrossStoreQuery::test_c
ross_store_query_async_succeeds
tests/unit/adapters/test_sync_manager.py::TestSyncManagerConcurrentUpdates::test
_queue_updates_from_multiple_tasks_succeeds
tests/unit/adapters/test_sync_manager.py::TestSyncManagerConcurrentUpdates::test
_async_queue_normalizes_records_before_flush
tests/unit/adapters/test_sync_manager.py::TestSyncManagerConcurrentUpdates::test
_conflict_resolution_with_concurrent_updates
tests/unit/agents/test_critique_agent.py::test_feedback_loop
tests/unit/agents/test_critique_agent.py::test_detects_test_failures
tests/unit/agents/test_critique_agent.py::test_warns_on_missing_docstring
tests/unit/agents/test_critique_agent.py::test_accepts_docstring
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_delete_memory_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_retrieve_agent_context_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_retrieve_agent_solutions_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_retrieve_dialectical_reasoning_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_retrieve_memory_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_retrieve_memory_with_context_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_search_memory_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_search_similar_solutions_no_vector_store_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_search_similar_solutions_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_store_agent_context_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_store_agent_solution_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_store_dialectical_reasoning_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_store_memory_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_store_memory_with_context_succeeds
tests/unit/application/agents/test_agent_memory_integration.py::TestAgentMemoryI
ntegration::test_update_memory_succeeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_initializa
tion_succeeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_process_su
cceeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_process_wi
th_empty_inputs_succeeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_get_capabi
lities_succeeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_get_capabi
lities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_code_agent.py::TestCodeAgent::test_process_er
ror_handling_raises_error
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_initia
lization_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_proces
s_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_proces
s_with_empty_inputs_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_proces
s_no_llm_port_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_get_ca
pabilities_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_get_ca
pabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_proces
s_error_handling_raises_error
tests/unit/application/agents/test_critic_agent.py::TestCriticAgent::test_create
_wsde_error_fails
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_init
ialization_succeeds
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_proc
ess_succeeds
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_proc
ess_with_empty_inputs_succeeds
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_get_
capabilities_succeeds
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_get_
capabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_diagram_agent.py::TestDiagramAgent::test_proc
ess_error_handling_raises_error
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_initialization_succeeds
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_process_succeeds
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_process_with_empty_inputs_succeeds
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_get_capabilities_succeeds
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_get_capabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_documentation_agent.py::TestDocumentationAgen
t::test_process_error_handling_raises_error
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_initialization_succeeds
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_supported_languages_succeed[python]
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_supported_languages_succeed[javascript]
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_polyglot_succeeds
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_polyglot_with_invalid_language
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_unsupported_language_raises_error
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_wsde_creation_error_logs_and_returns
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_calls_llm_generate
tests/unit/application/agents/test_multi_language_code.py::TestMultiLanguageCode
Agent::test_process_without_llm_returns_placeholder
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_init
ialization_succeeds
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_proc
ess_succeeds
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_proc
ess_with_empty_inputs_succeeds
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_get_
capabilities_succeeds
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_get_
capabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_planner_agent.py::TestPlannerAgent::test_proc
ess_error_handling_raises_error
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_in
itialization_succeeds
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_pr
ocess_succeeds
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_pr
ocess_with_empty_inputs_succeeds
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_ge
t_capabilities_succeeds
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_ge
t_capabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_pr
ocess_error_handling_code_wsde_fails
tests/unit/application/agents/test_refactor_agent.py::TestRefactorAgent::test_pr
ocess_error_handling_explanation_wsde_fails
tests/unit/application/agents/test_specification_agent.py::TestSpecificationAgen
t::test_process_succeeds
tests/unit/application/agents/test_specification_agent.py::TestSpecificationAgen
t::test_get_capabilities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_test_agent.py::TestTestAgent::test_initializa
tion_succeeds
tests/unit/application/agents/test_test_agent.py::TestTestAgent::test_process_su
cceeds
tests/unit/application/agents/test_test_agent.py::TestTestAgent::test_get_capabi
lities_with_custom_capabilities_succeeds
tests/unit/application/agents/test_test_agent.py::TestTestAgent::test_process_er
ror_handling
tests/unit/application/agents/test_unified_agent_generic.py::test_process_generi
c_task_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_by_concept_type_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_concept_relationships_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_knowledge_for_task_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_knowledge_graph_not_supported_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_knowledge_graph_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_query_related_concepts_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_retrieve_agent_solutions_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_retrieve_dialectical_process_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_retrieve_solutions_by_edrr_phase_has_expected
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_retrieve_team_context_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_search_similar_solutions_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_store_agent_solution_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_store_agent_solution_with_edrr_phase_has_expected
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_store_dialectical_process_succeeds
tests/unit/application/agents/test_wsde_memory_integration.py::TestWSDEMemoryInt
egration::test_store_team_context_succeeds
tests/unit/application/cli/commands/test_completion_cmd_errors.py::test_cli_comp
letion_rejects_invalid_shell
tests/unit/application/cli/commands/test_completion_cmd_errors.py::test_cli_comp
letion_detects_shell_when_unspecified
tests/unit/application/cli/commands/test_completion_cmd_errors.py::test_cli_comp
letion_reports_generation_errors
tests/unit/application/cli/commands/test_completion_cmd_errors.py::test_cli_comp
letion_reports_install_errors
tests/unit/application/cli/commands/test_help_rendering.py::test_display_command
_help_renders_panel
tests/unit/application/cli/commands/test_help_rendering.py::test_display_all_com
mands_help_renders_panel
tests/unit/application/cli/commands/test_help_rendering.py::test_display_command
_table_renders_table
tests/unit/application/cli/commands/test_help_rendering.py::test_display_command
_help_markdown_renders_markdown
tests/unit/application/cli/test_autocomplete.py::test_get_completions_returns_ex
pected_result
tests/unit/application/cli/test_autocomplete.py::test_complete_command_returns_e
xpected_result
tests/unit/application/cli/test_autocomplete.py::test_command_autocomplete_retur
ns_expected_result
tests/unit/application/cli/test_autocomplete.py::test_command_autocomplete_match
es_metadata
tests/unit/application/cli/test_autocomplete.py::test_file_path_autocomplete_ret
urns_expected_result
tests/unit/application/cli/test_autocomplete.py::test_file_path_autocomplete_han
dles_nested_prefixes
tests/unit/application/cli/test_autocomplete.py::test_get_command_help_returns_e
xpected_result
tests/unit/application/cli/test_autocomplete.py::test_get_all_commands_help_retu
rns_expected_result
tests/unit/application/cli/test_autocomplete.py::test_generate_completion_script
_installs_to_target
tests/unit/application/cli/test_autocomplete.py::test_generate_completion_script
_uses_home_directory
tests/unit/application/cli/test_autocomplete.py::test_generate_completion_script
_returns_source
tests/unit/application/cli/test_completion_cmd.py::test_completion_cmd_outputs_s
cript
tests/unit/application/cli/test_completion_cmd.py::test_completion_cmd_installs_
script
tests/unit/application/cli/test_completion_cmd.py::test_cli_supports_install_com
pletion
tests/unit/application/cli/test_config_validation.py::test_config_warnings_succe
eds
tests/unit/application/cli/test_config_validation.py::test_config_success_succee
ds
tests/unit/application/cli/test_help.py::test_get_command_help_returns_expected_
result
tests/unit/application/cli/test_help.py::test_display_command_help_succeeds
tests/unit/application/cli/test_help.py::test_get_all_commands_help_returns_expe
cted_result
tests/unit/application/cli/test_help.py::test_display_all_commands_help_succeeds
tests/unit/application/cli/test_help.py::test_create_command_table_succeeds
tests/unit/application/cli/test_help.py::test_display_command_table_succeeds
tests/unit/application/cli/test_help.py::test_format_command_help_markdown_succe
eds
tests/unit/application/cli/test_help.py::test_display_command_help_markdown_succ
eeds
tests/unit/application/cli/test_help.py::test_get_command_usage_returns_expected
_result
tests/unit/application/cli/test_help.py::test_display_command_usage_succeeds
tests/unit/application/cli/test_help.py::test_get_command_examples_returns_expec
ted_result
tests/unit/application/cli/test_help.py::test_display_command_examples_succeeds
tests/unit/application/cli/test_ingest_cmd.py::test_ingest_cmd_non_interactive_s
kips_prompts
tests/unit/application/cli/test_ingest_cmd.py::test_ingest_cmd_defaults_enable_n
on_interactive
tests/unit/application/cli/test_ingest_phases.py::test_expand_phase_succeeds
tests/unit/application/cli/test_ingest_phases.py::test_differentiate_phase_succe
eds tests/unit/application/cli/test_ingest_phases.py::test_refine_phase_succeeds
tests/unit/application/cli/test_ingest_phases.py::test_retrospect_phase_succeeds
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_creates_config_succee
ds
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_idempotent_succeeds
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_metrics_dashboard_opt
ion
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_wizard_option_invokes
_setup
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_reports_progress
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_non_interactive_flag_
skips_prompts
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_defaults_non_interact
ive_skips_prompts
tests/unit/application/cli/test_init_cmd.py::test_init_cmd_env_non_interactive_s
kips_prompts
tests/unit/application/cli/test_init_cmd.py::test_cli_help_lists_renamed_command
s_succeeds
tests/unit/application/cli/test_metrics_commands.py::test_alignment_metrics_cmd_
success
tests/unit/application/cli/test_metrics_commands.py::test_alignment_metrics_cmd_
failure
tests/unit/application/cli/test_metrics_commands.py::test_test_metrics_cmd_write
s_report
tests/unit/application/cli/test_metrics_commands.py::test_test_metrics_cmd_no_co
mmits
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_rejects_inval
id_target
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_rejects_inval
id_speed
tests/unit/application/cli/test_run_tests_cmd_options.py::test_cli_reports_disab
led_coverage
tests/unit/application/cli/test_serve_cmd.py::test_serve_cmd_missing_uvicorn_suc
ceeds
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_initialization_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_build_consensus_no_conflicts_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_research_persona_assignments_emit_telemetry
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_build_consensus_with_conflicts_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_vote_on_critical_decision_with_expertise_weighting_succeed
s
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_tie_breaking_strategies_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team.py::TestCollab
orativeWSDETeam::test_decision_tracking_and_explanation_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_process_task_non_hierarchica
l_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_process_task_hierarchical_su
cceeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_get_contribution_metrics_suc
ceeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_get_role_history_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_associate_subtasks_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_delegate_subtasks_succeeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_update_subtask_progress_succ
eeds
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_reassign_subtasks_based_on_p
rogress_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorPr
imusSelection::test_primus_selection_and_consensus_fields_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorPr
imusSelection::test_multi_agent_consensus_reached_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorPr
imusSelection::test_role_assignment_and_primus_selection_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorEr
rorPaths::test_missing_agent_type_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorEr
rorPaths::test_critical_decision_invokes_voting_succeeds
tests/unit/application/collaboration/test_coordinator.py::TestAgentCoordinatorEr
rorPaths::test_delegate_task_no_agents_registered_succeeds
tests/unit/application/collaboration/test_delegate_task.py::TestDelegateTask::te
st_team_task_returns_consensus_succeeds
tests/unit/application/collaboration/test_delegate_task.py::TestDelegateTask::te
st_team_task_no_agents_succeeds
tests/unit/application/collaboration/test_delegate_task.py::TestDelegateTask::te
st_invalid_task_format_succeeds
tests/unit/application/collaboration/test_delegate_task.py::TestDelegateTask::te
st_delegate_task_propagates_agent_error_succeeds
tests/unit/application/collaboration/test_delegate_task.py::TestDelegateTask::te
st_delegate_task_role_assignment_error_succeeds
tests/unit/application/collaboration/test_delegate_workflows.py::test_delegate_s
pecific_agent_succeeds
tests/unit/application/collaboration/test_delegate_workflows.py::test_delegate_t
eam_task_succeeds
tests/unit/application/collaboration/test_delegate_workflows.py::test_missing_ag
ent_succeeds
tests/unit/application/collaboration/test_delegate_workflows.py::test_team_task_
no_agents_succeeds
tests/unit/application/collaboration/test_memory_utils_edge_cases.py::test_flush
_memory_queue_handles_missing_manager
tests/unit/application/collaboration/test_memory_utils_edge_cases.py::test_flush
_memory_queue_without_sync_manager
tests/unit/application/collaboration/test_memory_utils_edge_cases.py::test_resto
re_memory_queue_requeues_items_in_order
tests/unit/application/collaboration/test_memory_utils_fallback.py::test_flush_m
emory_queue_falls_back_to_sync_manager
tests/unit/application/collaboration/test_message_protocol.py::test_send_message
_priority_succeeds
tests/unit/application/collaboration/test_message_protocol.py::test_get_messages
_filtered_succeeds
tests/unit/application/collaboration/test_message_protocol.py::test_dto_round_tr
ip_and_deterministic_serialization
tests/unit/application/collaboration/test_message_protocol.py::test_get_messages
_accepts_enum_mapping
tests/unit/application/collaboration/test_message_protocol.py::test_message_stor
e_filters_by_time_and_subject
tests/unit/application/collaboration/test_wsde_phase_transition_and_memory_flush
.py::test_progress_roles_triggers_memory_flush
tests/unit/application/collaboration/test_wsde_phase_transition_and_memory_flush
.py::test_flush_memory_queue_waits_for_sync
tests/unit/application/collaboration/test_wsde_team_extended_consensus.py::test_
build_consensus_enriches_metadata
tests/unit/application/collaboration/test_wsde_team_extended_consensus.py::test_
mark_and_detail_decision_updates_tracking
tests/unit/application/config/test_unified_config_loader.py::test_load_from_yaml
_succeeds
tests/unit/application/config/test_unified_config_loader.py::test_load_from_pypr
oject_succeeds
tests/unit/application/config/test_unified_config_loader.py::test_save_and_exist
s_succeeds
tests/unit/application/config/test_unified_config_loader.py::test_loader_save_fu
nction_yaml_succeeds
tests/unit/application/config/test_unified_config_loader.py::test_loader_save_fu
nction_pyproject_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_initialization_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_markdown_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_text_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_json_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_python_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_html_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_directory_rst_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_url_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_ingest_from_url_error_raises_error
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_markdown_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_text_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_json_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_python_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_html_succeeds
tests/unit/application/documentation/test_documentation_ingestion_manager.py::Te
stDocumentationIngestionManager::test_process_rst_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_api_compatibility_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_class_documentation_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_function_documentation_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_related_functions_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_usage_examples_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_get_usage_patterns_succeeds
tests/unit/application/documentation/test_documentation_manager_utils.py::TestDo
cumentationManagerUtils::test_offline_fetch_uses_cache_returns_expected_result
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_basic
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_with_error_handling
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_create_micro_cycle
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_with_micro_cycles
tests/unit/application/edrr/test_auto_progress.py::test_maybe_auto_progress_loop
s_until_none_succeeds
tests/unit/application/edrr/test_auto_progress.py::test_maybe_auto_progress_disa
bled_succeeds
tests/unit/application/edrr/test_coordinator.py::test_micro_cycle_iterations_unt
il_threshold
tests/unit/application/edrr/test_coordinator.py::test_phase_execution_recovery_h
ook
tests/unit/application/edrr/test_coordinator.py::test_micro_cycle_respects_max_i
terations
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_initialization_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_start_cycle_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_start_cycle_from_manifest_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_start_cycle_from_manifest_string_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_progress_to_phase_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_progress_to_next_phase_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_execute_current_phase_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_generate_report_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_get_execution_traces_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_get_execution_history_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_get_performance_metrics_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_decide_next_phase_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_maybe_auto_progress_succeeds
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_start_cycle_with_invalid_task_is_valid
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_start_cycle_from_manifest_with_invalid_file_is_valid
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_progress_to_phase_without_cycle_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_progress_to_next_phase_without_cycle_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_execute_current_phase_without_cycle_has_expected
tests/unit/application/edrr/test_coordinator_core.py::TestEDRRCoordinatorCore::t
est_generate_report_without_cycle_succeeds
tests/unit/application/edrr/test_coordinator_macro_micro_simulation.py::test_pha
se_transitions_follow_recursive_feature
tests/unit/application/edrr/test_coordinator_macro_micro_simulation.py::test_rec
ursion_depth_limit_matches_feature
tests/unit/application/edrr/test_coordinator_macro_micro_simulation.py::test_fai
led_retry_reports_reason
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_progress_to_
phase_runs_succeeds
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_execute_expa
nd_phase_succeeds
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_execute_diff
erentiate_phase_succeeds
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_execute_refi
ne_phase_succeeds
tests/unit/application/edrr/test_coordinator_phases_simple.py::test_execute_retr
ospect_phase_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
initialization_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
start_cycle_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
expand_phase_execution_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
differentiate_phase_execution_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
refine_phase_execution_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
retrospect_phase_execution_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
generate_final_report_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
execute_current_phase_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
progress_to_phase_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
progress_to_phase_dependency_failure_no_auto_fails
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
full_cycle_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
progress_to_next_phase_has_expected
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
progress_to_next_phase_without_current_fails
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
start_cycle_from_manifest_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
maybe_create_micro_cycles_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
create_micro_cycle_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
micro_cycle_result_aggregation_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
execution_history_logging_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
create_micro_cycle_triggers_termination_succeeds
tests/unit/application/edrr/test_edrr_coordinator.py::TestEDRRCoordinator::test_
safe_retrieve_always_returns_dict_for_list
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_initialization_succeeds
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_progress_to_phase_has_expected
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_enhanced_decide_next_phase_has_expected
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_phase_failure_hook_called
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_enhanced_maybe_auto_progress_succeeds
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_calculate_quality_score_succeeds
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_get_phase_metrics_has_expected
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_get_all_metrics_has_expected
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_get_metrics_history_has_expected
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::TestEnhancedEDRRC
oordinator::test_create_micro_cycle_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_start_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_end_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_all_thresholds_met_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_some_thresholds_not_met_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_missing_metrics_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_too_many_conflicts_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_configure_thresholds_override
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_recovery_hook_recovers
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_should_transition_recovery_hook_fails
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestPhaseTransitionM
etrics::test_failure_hook_invoked_on_unrecovered_failure
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_calculate_enhanced_quality_score_non_dict_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_calculate_enhanced_quality_score_empty_dict_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_calculate_enhanced_quality_score_good_result_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_calculate_enhanced_quality_score_with_errors
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_collect_phase_metrics_expand_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_collect_phase_metrics_differentiate_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_collect_phase_metrics_refine_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestQualityScoring::
test_collect_phase_metrics_retrospect_phase_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_progress_to_phase_collects_metrics_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_decide_next_phase_quality_based_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_decide_next_phase_timeout_based_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_decide_next_phase_no_transition_returns_expected_result
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_maybe_auto_progress_has_expected
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_maybe_auto_progress_reentry_prevention_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_enhanced_maybe_auto_progress_max_iterations_succeeds
tests/unit/application/edrr/test_edrr_phase_transitions.py::TestEnhancedEDRRCoor
dinator::test_calculate_quality_score_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
terminate_recursion_max_depth_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
terminate_recursion_time_based_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
terminate_recursion_memory_usage_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
terminate_recursion_historical_effectiveness_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
not_terminate_recursion_historical_effectiveness_succeeds
tests/unit/application/edrr/test_enhanced_recursion_termination.py::test_should_
terminate_recursion_combined_new_factors_succeeds
tests/unit/application/edrr/test_execute_single_agent_task.py::test_execute_sing
le_agent_task_stores_result_and_calls_agent
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_in
it_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_pa
rse_file_valid_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_pa
rse_file_invalid_json_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_pa
rse_file_not_found_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_pa
rse_string_valid_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_pa
rse_string_invalid_json_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_va
lidate_valid_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_va
lidate_invalid_is_valid
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_instructions_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_instructions_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_instructions_phase_not_found_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_templates_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_templates_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_templates_phase_not_found_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_resources_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_resources_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_resources_phase_not_found_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_id_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_id_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_description_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_description_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_metadata_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_metadata_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_manifest_metadata_no_metadata_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_st
art_execution_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_st
art_execution_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ch
eck_phase_dependencies_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ch
eck_phase_dependencies_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_st
art_phase_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_st
art_phase_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_st
art_phase_dependencies_not_met_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_phase_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_phase_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_phase_not_in_progress_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_execution_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_execution_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_co
mplete_execution_not_all_phases_completed_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_execution_trace_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_execution_trace_no_manifest_succeeds
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_status_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_status_no_manifest_has_expected
tests/unit/application/edrr/test_manifest_parser.py::TestManifestParser::test_ge
t_phase_status_unknown_phase_has_expected
tests/unit/application/edrr/test_micro_cycle.py::test_recursion_depth_exceeded_s
ucceeds
tests/unit/application/edrr/test_micro_cycle.py::test_recursion_depth_increments
_succeeds
tests/unit/application/edrr/test_micro_cycle.py::test_abort_when_should_terminat
e_succeeds
tests/unit/application/edrr/test_micro_cycle.py::test_store_metadata_and_results
_succeeds
tests/unit/application/edrr/test_micro_cycle.py::test_parent_aggregates_after_mi
cro_phase_succeeds
tests/unit/application/edrr/test_micro_cycle.py::test_create_micro_cycle_from_ma
nifest_dict_succeeds
tests/unit/application/edrr/test_micro_cycle_execution.py::test_execute_micro_cy
cle_uses_dialectical_reasoning
tests/unit/application/edrr/test_micro_cycle_execution.py::test_execute_micro_cy
cle_handles_dialectical_errors
tests/unit/application/edrr/test_micro_cycle_execution.py::test_assess_result_qu
ality_from_score
tests/unit/application/edrr/test_micro_cycle_execution.py::test_assess_result_qu
ality_handles_error
tests/unit/application/edrr/test_micro_cycle_hooks.py::test_micro_cycle_hooks_in
voked
tests/unit/application/edrr/test_phase_progression.py::test_auto_phase_progressi
on_succeeds
tests/unit/application/edrr/test_phase_progression.py::test_micro_cycle_result_a
ggregation_succeeds
tests/unit/application/edrr/test_phase_progression.py::test_result_aggregation_a
fter_full_cycle_has_expected
tests/unit/application/edrr/test_phase_recovery_helpers.py::test_recovery_regist
ration_and_threshold_config
tests/unit/application/edrr/test_phase_recovery_helpers.py::test_micro_cycle_inh
erits_recovery_config
tests/unit/application/edrr/test_progress_recursion.py::test_progress_to_phase_a
uto_recursion_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_granularity_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_cost_benefit_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_quality_threshold_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_resource_limit_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_human_override_succeeds[task0-True-human override]
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_human_override_succeeds[task1-False-None]
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_no_factors_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_at_thresholds_succeeds
tests/unit/application/edrr/test_progress_recursion.py::test_should_terminate_re
cursion_combined_factors_fails
tests/unit/application/edrr/test_recovery_hooks.py::test_recovery_hook_success_i
njects_results_and_sets_recovered
tests/unit/application/edrr/test_recovery_hooks.py::test_recovery_retry_when_hoo
ks_do_not_recover
tests/unit/application/edrr/test_recovery_hooks.py::test_recovery_hooks_ignores_
hook_exceptions_and_continues
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_human_override_terminate_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_human_override_continue_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_granularity_threshold_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_cost_benefit_analysis_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_quality_threshold_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_resource_limit_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_complexity_threshold_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_convergence_threshold_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_diminishing_returns
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_parent_phase_compatibility_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestTerminationCondition
s::test_historical_effectiveness_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_process_phase_results_merge_similar_has_expected
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_process_phase_results_prioritize_by_quality_has_expected
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_process_phase_results_handle_conflicts_has_expected
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_merge_cycle_results_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_calculate_similarity_key_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_merge_similar_results_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_merge_dicts_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_merge_lists_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_are_items_similar_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_calculate_quality_score_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_identify_conflicts_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestResultAggregation::t
est_resolve_conflict_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestRecursionMetrics::te
st_calculate_recursion_metrics_no_children_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestRecursionMetrics::te
st_calculate_recursion_metrics_with_children_succeeds
tests/unit/application/edrr/test_recursion_features.py::TestRecursionMetrics::te
st_aggregate_results_with_metrics_contains_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_initialization_with_recursion_support_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_recursion_depth_limit_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_terminated_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_expand_phase_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_differentiate_phase_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_refine_phase_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_edrr_within_retrospect_phase_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_granularity_threshold_check_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_cost_benefit_analysis_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_termination_succeeds[micro_task0]
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_termination_succeeds[micro_task1]
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_quality_threshold_monitoring_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_resource_limits_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_human_judgment_override_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_recursive_execution_tracking_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_auto_micro_cycle_creation_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_max_depth_stop_fails
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_phase_complete_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_timeout_has_expected
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_decide_next_phase_no_transition_returns_expected_result
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_should_terminate_recursion_all_factors_true_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_should_terminate_recursion_all_factors_false_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_get_performance_metrics_total_duration_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_create_micro_cycle_persists_results_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_micro_cycle_updates_parent_results_succeeds
tests/unit/application/edrr/test_recursive_edrr_coordinator.py::TestRecursiveEDR
RCoordinator::test_human_continue_overrides_delimiting_principles_succeeds
tests/unit/application/edrr/test_result_analysis.py::test_extract_key_insights_s
ucceeds
tests/unit/application/edrr/test_result_analysis.py::test_summarize_implementati
on_succeeds
tests/unit/application/edrr/test_result_analysis.py::test_summarize_quality_chec
ks_succeeds
tests/unit/application/edrr/test_result_analysis.py::test_extract_key_learnings_
succeeds
tests/unit/application/edrr/test_result_analysis.py::test_generate_next_steps_su
cceeds
tests/unit/application/edrr/test_result_analysis.py::test_extract_future_conside
rations_succeeds
tests/unit/application/edrr/test_result_analysis.py::test_final_report_includes_
value_conflicts_succeeds
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_template_
definitions_succeeds
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_register_
edrr_templates_succeeds
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_register_
edrr_templates_error_handling_raises_error
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_template_
for_each_phase_has_expected[Phase.EXPAND-\n# Expand Phase for Task:
{task_description}\n\n## Objective\nGenerate diverse ideas, explore broadly, and
gather relevant knowledge for the task.\n\n## Instructions\n1. Consider multiple
approaches to the problem\n2. Explore different perspectives and angles\n3.
Generate a wide range of potential solutions\n4. Identify relevant knowledge and
resources\n5. Don't evaluate or filter ideas yet - focus on quantity and
diversity\n\n## Task Context\n{task_description}\n\n## Questions to Consider\n-
What are all the possible ways to approach this task?\n- What similar problems
have been solved before?\n- What knowledge domains are relevant to this task?\n-
What creative or unconventional approaches might work?\n- What are the key
components or sub-problems within this task?\n]
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_template_
for_each_phase_has_expected[Phase.DIFFERENTIATE-\n# Differentiate Phase for
Task: {task_description}\n\n## Objective\nAnalyze and compare the ideas
generated in the Expand phase,\nevaluate options, and identify trade-offs.\n\n##
Instructions\n1. Compare and contrast different approaches\n2. Evaluate each
option against relevant criteria\n3. Identify strengths and weaknesses of each
approach\n4. Analyze trade-offs between different solutions\n5. Develop decision
criteria for selecting the best approach\n\n## Task
Context\n{task_description}\n\n## Questions to Consider\n- What are the key
differences between the approaches?\n- What criteria are most important for
evaluating solutions?\n- What are the trade-offs between different
approaches?\n- Which approaches best align with the requirements?\n- What are
the risks and benefits of each approach?\n]
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_template_
for_each_phase_has_expected[Phase.REFINE-\n# Refine Phase for Task:
{task_description}\n\n## Objective\nElaborate on the selected approach, develop
a detailed implementation plan,\nand optimize the solution.\n\n##
Instructions\n1. Develop a detailed plan for the selected approach\n2. Elaborate
on implementation details\n3. Optimize the solution for performance,
maintainability, and other relevant factors\n4. Identify potential issues and
develop mitigation strategies\n5. Create quality assurance checks for the
implementation\n\n## Task Context\n{task_description}\n\n## Questions to
Consider\n- What are the specific steps needed to implement this solution?\n-
How can the solution be optimized?\n- What edge cases need to be handled?\n-
What quality checks should be performed?\n- How can the implementation be made
more robust?\n]
tests/unit/application/edrr/test_templates.py::TestEDRRTemplates::test_template_
for_each_phase_has_expected[Phase.RETROSPECT-\n# Retrospect Phase for Task:
{task_description}\n\n## Objective\nExtract learnings, identify patterns,
integrate knowledge,\nand generate improvement suggestions.\n\n##
Instructions\n1. Reflect on the entire process from Expand through Refine\n2.
Extract key learnings and insights\n3. Identify patterns that could be applied
to future tasks\n4. Integrate new knowledge into the existing knowledge base\n5.
Generate suggestions for improving the process and solution\n\n## Task
Context\n{task_description}\n\n## Questions to Consider\n- What worked well in
this process?\n- What challenges were encountered and how were they
addressed?\n- What patterns or principles emerged that could be reused?\n- How
could the process be improved for similar tasks in the future?\n- What new
knowledge was gained that should be preserved?\n]
tests/unit/application/edrr/test_threshold_helpers.py::test_coordinator_register
s_templates
tests/unit/application/edrr/test_threshold_helpers.py::test_assess_phase_quality
_uses_config_threshold
tests/unit/application/edrr/test_threshold_helpers.py::test_micro_cycle_config_s
anitization
tests/unit/application/llm/test_import_without_lmstudio.py::test_import_lmstudio
_provider_without_lmstudio_succeeds
tests/unit/application/llm/test_import_without_lmstudio.py::test_factory_missing
_lmstudio_provider_raises_clear_error
tests/unit/application/llm/test_offline_provider_deps.py::test_load_transformer_
deps_imports_when_available
tests/unit/application/llm/test_offline_provider_deps.py::test_load_transformer_
deps_noop_when_already_loaded
tests/unit/application/llm/test_offline_provider_deps.py::test_load_transformer_
deps_handles_import_error
tests/unit/application/memory/test_basic_crud_adapters.py::test_basic_crud_lifec
ycle[tinydb]
tests/unit/application/memory/test_basic_crud_adapters.py::test_basic_crud_lifec
ycle[lmdb]
tests/unit/application/memory/test_basic_crud_adapters.py::test_basic_crud_lifec
ycle[kuzu]
tests/unit/application/memory/test_basic_crud_adapters.py::test_basic_crud_lifec
ycle[chromadb]
tests/unit/application/memory/test_chromadb_store_delete_versions.py::test_delet
e_also_removes_versions
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_init_s
ucceeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_store_
and_retrieve_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_retrie
ve_nonexistent_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_search
_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_delete
_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_token_
usage_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_persis
tence_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_store_
vector_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_simila
rity_search_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_delete
_vector_succeeds
tests/unit/application/memory/test_duckdb_store.py::TestDuckDBStore::test_get_co
llection_stats_succeeds
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_hnsw_initialization_succeeds
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_custom_hnsw_initialization_succeeds
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_search_returns_typed_records_with_hnsw
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_hnsw_index_creation_succeeds
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_similarity_search_with_hnsw_succeeds
tests/unit/application/memory/test_duckdb_store_hnsw.py::TestDuckDBStoreHNSW::te
st_similarity_search_performance_comparison_succeeds
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_initiali
zation
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_store_pr
imary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_store_pr
imary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_store_al
l_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_retrieve
_primary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_retrieve
_primary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_retrieve
_primary_not_found
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_retrieve
_all_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_search_p
rimary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_search_p
rimary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_search_a
ll_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_delete_p
rimary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_delete_p
rimary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_delete_a
ll_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_get_all_
items_primary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_get_all_
items_primary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_get_all_
items_all_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_begin_tr
ansaction_primary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_begin_tr
ansaction_primary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_begin_tr
ansaction_all_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_commit_t
ransaction_primary_success
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_commit_t
ransaction_primary_failure
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_commit_t
ransaction_all_failures
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_reconcil
e_pending_operations
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_get_stor
e_status
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_get_pend
ing_operations_count
tests/unit/application/memory/test_fallback.py::TestFallbackStore::test_pending_
operation_serialization_round_trip
tests/unit/application/memory/test_fallback.py::test_with_fallback
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_initialization_basic_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_initialization_rdflib_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_store_and_retrieve_basic_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_store_and_retrieve_rdflib_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_research_artifact_traversal_and_reload
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_ingest_helper_generates_hash_and_summary
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_store_with_relationships_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_search_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_delete_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_get_all_relationships_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_add_memory_volatility_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_apply_memory_decay_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_advanced_memory_decay_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_integrate_with_store_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_integrate_with_vector_store_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_save_graph_with_rdflib_store_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_memory_item_triple_creation_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_cascading_query_with_missing_adapter_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_context_aware_query_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_query_router_route_succeeds
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_store_and_retrieve_with_edrr_phase_has_expected
tests/unit/application/memory/test_import_without_duckdb.py::test_init_duckdb_st
ore_without_duckdb_fails
tests/unit/application/memory/test_import_without_tinydb.py::test_import_memory_
with_tinydb_available_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_find_related_items_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_find_items_by_relationship_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_get_item_relationships_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_create_and_delete_relationship_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_query_graph_pattern_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_get_subgraph_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_synchronize_basic_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_synchronize_missing_adapter_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_synchronize_bidirectional_succeeds
tests/unit/application/memory/test_knowledge_graph_utils.py::TestKnowledgeGraphU
tils::test_update_and_queue_succeeds
tests/unit/application/memory/test_kuzu_store.py::test_init_creates_directory
tests/unit/application/memory/test_kuzu_store.py::test_store_and_retrieve_succee
ds
tests/unit/application/memory/test_kuzu_store.py::test_transaction_rollback_rest
ores_snapshot
tests/unit/application/memory/test_kuzu_store.py::test_concurrent_transactions
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_init_succe
eds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_store_and_
retrieve_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_retrieve_n
onexistent_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_search_suc
ceeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_delete_suc
ceeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_token_usag
e_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_persistenc
e_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_close_and_
reopen_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_transactio
n_isolation_succeeds
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_transactio
n_abort_succeeds
tests/unit/application/memory/test_memory_adapters_regression.py::test_store_ret
rieve_search_update[TinyDBMemoryAdapter]
tests/unit/application/memory/test_memory_adapters_regression.py::test_store_ret
rieve_search_update[GraphMemoryAdapter]
tests/unit/application/memory/test_memory_adapters_regression.py::test_vector_ad
apter_operations
tests/unit/application/memory/test_memory_adapters_regression.py::test_tinydb_ad
apter_transaction_support
tests/unit/application/memory/test_memory_adapters_regression.py::test_tinydb_tr
ansaction_optional_id
tests/unit/application/memory/test_memory_adapters_regression.py::test_tinydb_se
rializes_unhandled_types
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerStore::te
st_store_prefers_graph_for_edrr_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerStore::te
st_store_falls_back_to_tinydb_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerStore::te
st_store_falls_back_to_first_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerTyping::t
est_store_with_edrr_phase_coerces_metadata_mapping
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_not_found_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_with_metadata_succeeds
tests/unit/application/memory/test_memory_manager.py::TestMemoryManagerRetrieve:
:test_retrieve_with_edrr_phase_returns_typed_record
tests/unit/application/memory/test_memory_manager.py::TestEmbedText::test_fallba
ck_and_provider_succeeds
tests/unit/application/memory/test_memory_manager_search.py::TestMemoryManagerSe
arch::test_search_returns_similar_results_succeeds
tests/unit/application/memory/test_memory_manager_search.py::TestMemoryManagerSe
arch::test_search_filters_by_metadata_succeeds
tests/unit/application/memory/test_mixed_backend_transactions.py::TestMixedBacke
ndTransactions::test_transaction_across_backends
tests/unit/application/memory/test_mixed_backend_transactions.py::TestMixedBacke
ndTransactions::test_snapshot_rollback_restores_memory_records
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_init_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_short_term_memory_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_episodic_memory_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_semantic_memory_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_unknown_memory_type_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_without_id_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_retrieve_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_retrieve_with_cache_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_store_preserves_typed_metadata
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_cache_round_trip_keeps_metadata_mapping
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_query_returns_metadata_rich_items
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_get_items_by_layer_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_query_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_tiered_cache_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_clear_cache_succeeds
tests/unit/application/memory/test_multi_layered_memory.py::TestMultiLayeredMemo
rySystem::test_clear_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_init_s
ucceeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_store_
and_retrieve_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_retrie
ve_nonexistent_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_search
_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_search
_by_id_and_date_range_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_delete
_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_token_
usage_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_persis
tence_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_store_
vector_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_simila
rity_search_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_delete
_vector_succeeds
tests/unit/application/memory/test_rdflib_store.py::TestRDFLibStore::test_get_co
llection_stats_succeeds
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_snapsho
t_initialization
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_add_ite
m
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_remove_
item
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_get_ite
m
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_snapsho
t_save_and_load
tests/unit/application/memory/test_recovery.py::TestMemorySnapshot::test_snapsho
t_load_invalid_file
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_operation
log_initialization
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_operation
log_log_operation
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_operation
log_save_and_load
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_operation
log_load_invalid_file
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_replay
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_replay_wi
th_time_range
tests/unit/application/memory/test_recovery.py::TestOperationLog::test_replay_fa
ilure
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_recove
ry_manager_initialization
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_create
_snapshot
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_get_op
eration_log
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_recove
ry_manager_log_operation
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_restor
e_from_snapshot
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_restor
e_from_snapshot_no_snapshot
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_restor
e_from_snapshot_failure
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_recove
r_store
tests/unit/application/memory/test_recovery.py::TestRecoveryManager::test_recove
r_store_no_snapshot
tests/unit/application/memory/test_recovery.py::TestWithRecovery::test_successfu
l_execution
tests/unit/application/memory/test_recovery.py::TestWithRecovery::test_execution
_failure_with_recovery
tests/unit/application/memory/test_recovery.py::TestWithRecovery::test_no_snapsh
ot_creation
tests/unit/application/memory/test_recovery.py::test_global_recovery_manager
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_successful_execution
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_retry_on_failure
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_max_retries_exceeded
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_exceptions_to_retry
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_backoff_calculation
tests/unit/application/memory/test_retry.py::TestRetryWithBackoff::test_retry_wi
th_backoff_max_backoff
tests/unit/application/memory/test_retry.py::TestRetryOperation::test_retry_oper
ation_successful_execution
tests/unit/application/memory/test_retry.py::TestRetryOperation::test_retry_oper
ation_retry_on_failure
tests/unit/application/memory/test_retry.py::TestRetryOperation::test_retry_oper
ation_max_retries_exceeded
tests/unit/application/memory/test_retry.py::TestRetryConfig::test_retry_config_
initialization
tests/unit/application/memory/test_retry.py::TestRetryConfig::test_default_retry
_config
tests/unit/application/memory/test_retry.py::TestRetryConfig::test_quick_retry_c
onfig
tests/unit/application/memory/test_retry.py::TestRetryMemoryOperation::test_retr
y_memory_operation_preserves_memory_record
tests/unit/application/memory/test_retry.py::TestRetryMemoryOperation::test_retr
y_memory_operation_condition_receives_payload_union
tests/unit/application/memory/test_retry.py::TestRetryMemoryOperation::test_pers
istent_retry_config
tests/unit/application/memory/test_retry.py::TestRetryMemoryOperation::test_netw
ork_retry_config
tests/unit/application/memory/test_retry.py::TestWithRetry::test_with_retry_defa
ult_config
tests/unit/application/memory/test_retry.py::TestWithRetry::test_with_retry_cust
om_config
tests/unit/application/memory/test_retry_logic.py::TestRetryLogic::test_conditio
n_callback_aborts_retry
tests/unit/application/memory/test_retry_logic.py::TestRetryLogic::test_conditio
n_callback_receives_typed_arguments
tests/unit/application/memory/test_retry_logic.py::TestRetryLogic::test_circuit_
breaker_stops_retries
tests/unit/application/memory/test_s3_memory_adapter.py::test_store_retrieve_del
ete_roundtrip
tests/unit/application/memory/test_s3_memory_adapter.py::test_search_filters_by_
type_and_metadata
tests/unit/application/memory/test_s3_memory_adapter.py::test_transactions_are_u
nsupported
tests/unit/application/memory/test_sync_wrappers.py::test_cross_store_query_and_
update_wrappers
tests/unit/application/memory/test_tiered_cache_validation.py::test_lru_eviction
_matches_reference
tests/unit/application/memory/test_tiered_cache_validation.py::test_hit_miss_cou
nts_match_reference
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_init_s
ucceeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_store_
and_retrieve_succeeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_retrie
ve_nonexistent_succeeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_search
_succeeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_delete
_succeeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_token_
usage_succeeds
tests/unit/application/memory/test_tinydb_store.py::TestTinyDBStore::test_persis
tence_succeeds
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_similari
ty_empty_store
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_similari
ty_zero_norm
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_delete_m
issing
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_collecti
on_stats
tests/unit/application/orchestration/test_workflow_manager.py::test_get_workflow
_status_delegates_to_port_succeeds
tests/unit/application/promises/test_basic_promise.py::test_basic_promise_resolv
e_and_value
tests/unit/application/promises/test_basic_promise.py::test_basic_promise_then_c
hains
tests/unit/application/promises/test_basic_promise.py::test_basic_promise_catch_
handles_rejection
tests/unit/application/promises/test_basic_promise.py::test_access_value_wrong_s
tate_raises
tests/unit/application/promises/test_timeout.py::test_promise_auto_reject_after_
timeout_succeeds
tests/unit/application/promises/test_timeout.py::test_timeout_timer_cancellation
_on_fulfill_succeeds
tests/unit/application/prompts/test_auto_tuning.py::test_temperature_adjustment_
succeeds
tests/unit/application/prompts/test_auto_tuning.py::test_unified_agent_uses_tune
r_succeeds
tests/unit/application/prompts/test_auto_tuning.py::test_prompt_auto_tuner_uses_
typed_collection
tests/unit/application/prompts/test_auto_tuning.py::test_run_tuning_iteration_re
cords_feedback_succeeds
tests/unit/application/prompts/test_auto_tuning.py::test_iterative_prompt_adjust
ment_returns_best_variant_succeeds
tests/unit/application/prompts/test_persistence.py::test_save_and_load_round_tri
p
tests/unit/application/prompts/test_persistence.py::test_load_raises_for_missing
_file
tests/unit/application/prompts/test_persistence.py::test_load_raises_for_invalid
_json
tests/unit/application/test_documentation_fetcher.py::test_fetch_documentation_o
ffline_without_cache_raises_error
tests/unit/application/test_documentation_fetcher.py::test_fetch_documentation_o
ffline_uses_typed_cache
tests/unit/application/test_documentation_fetcher.py::test_fetch_documentation_n
o_source_raises_error
tests/unit/application/test_documentation_fetcher.py::test_get_available_version
s_no_source_raises_error
tests/unit/application/test_documentation_fetcher.py::test_supports_library_retu
rns_false_when_no_source
tests/unit/application/test_documentation_fetcher.py::test_mvu_smoke_coverage
tests/unit/application/test_offline_provider_cli.py::test_generate_does_not_call
_external_succeeds
tests/unit/application/test_offline_provider_unit.py::test_offline_provider_inst
antiation_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_initi
alization_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_succe
ss_rate_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_avera
ge_feedback_score_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_perfo
rmance_score_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_recor
d_usage_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptVariant::test_to_di
ct_and_from_dict_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_aut
o_tuner_initialization_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_reg
ister_template_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_sel
ect_variant_single_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_sel
ect_variant_error_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_sel
ect_variant_performance_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_rec
ord_feedback_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_rec
ord_feedback_error_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_gen
erate_variants_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_mut
ation_methods_succeeds
tests/unit/application/test_prompt_auto_tuning.py::TestPromptAutoTuner::test_sto
rage_succeeds
tests/unit/application/wsde/test_wsde_utils.py::test_reassign_roles_uses_dynamic
_reassignment
tests/unit/application/wsde/test_wsde_utils.py::test_run_consensus_falls_back_to
_build
tests/unit/application/wsde/test_wsde_utils.py::test_run_consensus_no_fallback_w
hen_complete
tests/unit/behavior/test_wsde_team_extended.py::test_summarize_and_store_consens
us
tests/unit/cli/test_exit_codes.py::TestExitCodes::test_run_cli_runtime_error_exi
t_code_1
tests/unit/cli/test_exit_codes.py::TestExitCodes::test_run_cli_usage_error_exit_
code_2
tests/unit/cli/test_exit_codes.py::TestExitCodes::test_parse_args_runtime_error_
exit_code_1
tests/unit/cli/test_exit_codes.py::TestExitCodes::test_parse_args_usage_error_ex
it_code_2
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_invalid_config_type_is_valid
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_invalid_config_range_is_valid
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_invalid_config_syntax_raises_error
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_missing_required_fields_is_valid
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_env_var_override_succeeds
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_config_file_merging_succeeds
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_invalid_feature_flag_is_valid
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_unknown_setting_is_valid
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_config_with_comments_succeeds
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_empty_config_file_succeeds
tests/unit/config/test_config_validation_extended.py::TestConfigValidationExtend
ed::test_config_with_null_values_succeeds
tests/unit/config/test_feature_flags_mvuu_gui.py::test_gui_and_mvuu_dashboard_fl
ags_recognized
tests/unit/config/test_project_config_validation.py::test_valid_project_config_l
oads_succeeds
tests/unit/config/test_project_config_validation.py::test_invalid_project_config
_raises tests/unit/config/test_unified_loader.py::test_loads_from_yaml_succeeds
tests/unit/config/test_unified_loader.py::test_save_round_trip_yaml_succeeds
tests/unit/config/test_unified_loader.py::test_save_round_trip_pyproject_succeed
s tests/unit/core/test_config_loader.py::test_load_from_dev_synth_yaml_succeeds
tests/unit/core/test_config_loader.py::test_load_from_pyproject_toml_succeeds
tests/unit/core/test_config_loader.py::test_yaml_toml_equivalence_succeeds
tests/unit/core/test_config_loader.py::test_load_project_config_yaml_succeeds
tests/unit/core/test_config_loader.py::test_load_project_config_pyproject_succee
ds
tests/unit/core/test_config_loader.py::test_load_config_normalizes_mvuu_section
tests/unit/core/test_core_values.py::test_load_core_values_succeeds
tests/unit/core/test_core_values.py::test_find_value_conflicts_succeeds
tests/unit/core/test_core_values.py::test_check_report_for_value_conflicts_succe
eds
tests/unit/core/test_unified_config_loader.py::test_unified_loader_detects_yaml_
succeeds
tests/unit/core/test_unified_config_loader.py::test_unified_loader_detects_pypro
ject_succeeds
tests/unit/core/test_unified_config_loader.py::test_unified_loader_prefers_pypro
ject_succeeds
tests/unit/core/test_unified_config_loader.py::test_unified_config_save_updates_
pyproject_succeeds
tests/unit/core/test_unified_config_loader.py::test_unified_config_exists_for_bo
th_formats_returns_expected_result
tests/unit/core/test_unified_config_loader.py::test_unified_loader_falls_back_wh
en_pyproject_missing_section_succeeds
tests/unit/core/test_unified_config_loader.py::test_unified_loader_malformed_pyp
roject_fails
tests/unit/core/test_unified_config_loader.py::test_unified_loader_malformed_yam
l_fails
tests/unit/docs/test_feature_matrix.py::test_feature_rows_have_status_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ad
d_agents_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_re
gister_dialectical_hook_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_se
nd_message_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_se
nd_message_updates_memory_manager
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_br
oadcast_message_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ge
t_messages_succeeds
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_co
nduct_peer_review_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_exact_match_matches_expec
ted
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_partial_match_matches_exp
ected
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_experience_level_succeeds

tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_performance_history_succe
eds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_expertise_score_nested_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedExp
ertiseScoring::test_enhanced_calculate_phase_expertise_score_has_expected
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_code_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_doc_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_security_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_rotation_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestEnhancedPri
musSelection::test_enhanced_select_primus_by_expertise_unused_priority_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_code_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_doc_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_testing_task_succeeds
tests/unit/domain/models/test_wsde_context_driven_leadership.py::TestDynamicRole
Reassignment::test_dynamic_role_reassignment_enhanced_security_task_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_categorize_critiques_by_domain_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_identify_domain_conflicts_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_prioritize_critiques_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_resolve_code_improvement_conflict_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_resolve_content_improvement_conflict_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_code_standards_compliance_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_content_standards_compliance_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_pep8_compliance_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_check_security_best_practices_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_security_and_performance_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_security_and_usability_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_balance_performance_and_maintainability_succeeds
tests/unit/domain/models/test_wsde_dialectical_reasoning.py::TestWSDEDialectical
Reasoning::test_generate_detailed_synthesis_reasoning_succeeds
tests/unit/domain/test_memory_type.py::test_memory_type_serialization_deserializ
ation tests/unit/domain/test_memory_type.py::test_working_memory_alias
tests/unit/domain/test_memory_type.py::test_memory_type_members_complete
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[short_te
rm]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[long_ter
m]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[working]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[episodic
]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[solution
]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[dialecti
cal_reasoning]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[team_sta
te]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[knowledg
e_graph]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[relation
ship]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[code_ana
lysis]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[code]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[code_tra
nsformation]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[document
ation]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[context]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[conversa
tion]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[task_his
tory]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[knowledg
e]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[error_lo
g]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[collabor
ation_task]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[collabor
ation_message]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[collabor
ation_team]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[peer_rev
iew]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[expand_r
esults]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[differen
tiate_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[refine_r
esults]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[retrospe
ct_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[final_re
port]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[edrr_cyc
le_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[ingest_e
xpand_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[ingest_d
ifferentiate_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[ingest_r
efine_results]
tests/unit/domain/test_memory_type.py::test_memory_type_lookup_by_value[ingest_r
etrospect_results]
tests/unit/domain/test_primus_selection_edge_cases.py::test_rotation_without_exp
ertise_is_deterministic_succeeds
tests/unit/domain/test_primus_selection_edge_cases.py::test_documentation_task_p
refers_doc_agents_succeeds
tests/unit/domain/test_primus_selection_edge_cases.py::test_has_been_primus_rese
ts_after_full_rotation_succeeds
tests/unit/domain/test_primus_selection_edge_cases.py::test_edge_case_coverage_s
ucceeds
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_dialectical_reasoner_creates_session_and_message
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_inmemory_change_repository
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_inmemory_impact_assessment_repository
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_inmemory_requirement_repository_crud
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_inmemory_requirement_repository_filters
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_simple_reasoner_accepts_explicit_edrr_phase
tests/unit/domain/test_requirement_interfaces.py::TestRequirementInterfaces::tes
t_simple_reasoner_assess_impact_with_phase
tests/unit/domain/test_wsde_core_methods.py::test_assign_roles_sets_roles
tests/unit/domain/test_wsde_core_methods.py::test_select_primus_by_expertise_pre
fers_match
tests/unit/domain/test_wsde_core_methods.py::test_build_consensus_produces_resul
t
tests/unit/fallback/test_circuit_breaker_metrics.py::test_circuit_breaker_state_
metrics
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_condition_callb
ack_prevents_retry
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_prometheus_metr
ics_recorded
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_retry_me
trics_and_callback
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_condition_callb
ack_records_metrics
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_conditio
n_callback_records_metrics
tests/unit/fallback/test_condition_callbacks_prometheus.py::test_memory_retry_co
ndition_records_metrics
tests/unit/fallback/test_retry.py::test_anonymous_retry_condition_records_metric
s tests/unit/fallback/test_retry.py::test_circuit_breaker_open_emits_metrics
tests/unit/fallback/test_retry_condition_metrics.py::test_named_retry_condition_
records_metrics_on_abort
tests/unit/fallback/test_retry_condition_metrics.py::test_named_retry_condition_
allows_retry
tests/unit/fallback/test_retry_condition_metrics.py::test_exception_class_condit
ion_records_metrics
tests/unit/fallback/test_retry_conditions.py::test_should_retry_prevents_retry
tests/unit/fallback/test_retry_conditions.py::test_should_retry_allows_retry_unt
il_success
tests/unit/fallback/test_retry_conditions.py::test_retry_on_result_triggers_retr
y
tests/unit/fallback/test_retry_conditions.py::test_retry_conditions_abort_when_c
ondition_fails
tests/unit/fallback/test_retry_conditions.py::test_retry_conditions_allow_retry
tests/unit/fallback/test_retry_conditions.py::test_string_condition_allows_retry
tests/unit/fallback/test_retry_conditions.py::test_string_condition_aborts_when_
missing
tests/unit/fallback/test_retry_conditions.py::test_class_condition_allows_retry
tests/unit/fallback/test_retry_conditions.py::test_class_condition_aborts_on_mis
match tests/unit/fallback/test_retry_conditions.py::test_exponential_backoff
tests/unit/fallback/test_retry_conditions.py::test_fallback_provider_order
tests/unit/fallback/test_retry_logic.py::test_retry_conditions_respected_with_ci
rcuit_breaker
tests/unit/fallback/test_retry_logic.py::test_circuit_breaker_opens_and_records_
metrics
tests/unit/fallback/test_retry_logic.py::test_retry_stat_prometheus_metrics_reco
rded
tests/unit/fallback/test_retry_logic.py::test_condition_callbacks_receive_typed_
exception_and_attempt
tests/unit/fallback/test_retry_logic.py::test_with_fallback_conditions_and_circu
it_breaker
tests/unit/fallback/test_retry_logic.py::test_error_map_respects_max_retries_wit
h_circuit_breaker
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_success
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_record_stat_counte
rs_for_exponential_backoff
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_failure
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_abort_when_not_ret
ryable
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_invalid_result
tests/unit/fallback/test_retry_metrics.py::test_retry_metrics_success_without_re
tries tests/unit/fallback/test_retry_metrics.py::test_retry_error_metrics
tests/unit/fallback/test_retry_metrics.py::test_retry_error_map_prevents_retry
tests/unit/fallback/test_retry_metrics.py::test_retry_error_map_matches_subclass
tests/unit/fallback/test_retry_metrics.py::test_fallback_handler_predicate_metri
cs
tests/unit/fallback/test_retry_metrics_prometheus_integration.py::test_retry_met
rics_synced_with_prometheus
tests/unit/fallback/test_retry_with_conditions.py::test_error_policy_overrides_m
ax_retries
tests/unit/fallback/test_retry_with_conditions.py::test_error_policy_prevents_re
try
tests/unit/fallback/test_retry_with_conditions.py::test_condition_metrics_track_
trigger_and_suppress
tests/unit/general/test_agent_adapter.py::test_register_agent_type_succeeds
tests/unit/general/test_agent_adapter_delegate.py::test_delegate_task_single_age
nt_succeeds
tests/unit/general/test_agent_adapter_delegate.py::test_delegate_task_multi_agen
t_succeeds
tests/unit/general/test_agent_adapter_delegate.py::test_parse_args_runs_succeeds
tests/unit/general/test_agent_adapter_delegate.py::test_show_help_lists_groups
tests/unit/general/test_agent_adapter_delegate.py::test_show_help_fallback_regis
tered_typers
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_coo
rdinator_initialization_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_add
_agent_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_task_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_sim
plified_agent_factory_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_tea
m_task_phase_notifications_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_task_agent_type_not_found_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_task_multi_agent_consensus_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_task_dynamic_role_assignment_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_task_voting_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_age
nt_adapter_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_bac
kward_compatibility_succeeds
tests/unit/general/test_agent_collaboration.py::TestAgentCollaboration::test_del
egate_calls_select_primus_before_processing_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_show_help_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_init_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_spec_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_test_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_code_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_run_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_config_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_enable_feature_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_edrr_cycle_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_inspect_config_update_suc
ceeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_inspect_config_prune_succ
eeds
tests/unit/general/test_cli.py::TestTyperCLI::test_cli_spec_invalid_file_succeed
s tests/unit/general/test_cli.py::TestTyperCLI::test_cli_config_missing_succeeds
tests/unit/general/test_cli.py::TestTyperCLI::test_parse_args_help_has_new_comma
nds_succeeds
tests/unit/general/test_fallback_utils.py::test_circuit_breaker_concurrent_failu
res
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
find_related_items_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
find_items_by_relationship_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
get_item_relationships_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
create_and_delete_relationship_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
query_graph_pattern_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
get_subgraph_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
synchronize_basic_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
synchronize_missing_adapter_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
synchronize_bidirectional_succeeds
tests/unit/general/test_knowledge_graph_utils.py::TestKnowledgeGraphUtils::test_
update_and_queue_succeeds
tests/unit/general/test_kuzu_project_startup.py::test_project_startup_with_kuzu
tests/unit/general/test_kuzu_store.py::test_store_retrieve_and_versions_succeeds
tests/unit/general/test_kuzu_store.py::test_search_by_metadata_succeeds
tests/unit/general/test_kuzu_store.py::test_store_path_is_absolute
tests/unit/general/test_kuzu_store_fallback.py::test_kuzu_store_falls_back_when_
dependency_missing
tests/unit/general/test_lmstudio_provider_unit.py::test_generate_succeeds
tests/unit/general/test_lmstudio_provider_unit.py::test_generate_with_context_su
cceeds
tests/unit/general/test_lmstudio_provider_unit.py::test_get_embedding_succeeds
tests/unit/general/test_lmstudio_provider_unit.py::test_api_error_handling_raise
s_error
tests/unit/general/test_lmstudio_provider_unit.py::test_circuit_breaker_opens_af
ter_failures_fails
tests/unit/general/test_memory_system_with_kuzu.py::TestMemorySystemWithKuzu::te
st_initialization_with_kuzu_succeeds
tests/unit/general/test_memory_system_with_kuzu.py::TestMemorySystemWithKuzu::te
st_memory_and_vector_store_integration_succeeds
tests/unit/general/test_metrics_dashboard_hook.py::test_dashboard_hook_receives_
events
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_with_timeout_succeeds
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_wait_for_capabi
lity_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_init_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_store_and_retriev
e_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_retrieve_nonexist
ent_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_search_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_search_by_id_and_
date_range_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_delete_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_token_usage_succe
eds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_persistence_succe
eds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_store_vector_succ
eeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_similarity_search
_succeeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_delete_vector_suc
ceeds
tests/unit/general/test_rdflib_store.py::TestRDFLibStore::test_get_collection_st
ats_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_agent_initializ
ation_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_specifi
cation_task_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_test_ta
sk_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_code_ta
sk_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_validat
ion_task_is_valid
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_documen
tation_task_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_project
_initialization_task_succeeds
tests/unit/general/test_unified_agent.py::TestUnifiedAgent::test_process_generic
_task_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_cmd_suc
cess_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_cmd_alr
eady_initialized_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_cmd_exc
eption_raises_error
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_suc
cess_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_test_cmd_suc
cess_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_code_cmd_suc
cess_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_run_pipeline
_cmd_success_with_target_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_run_pipeline
_cmd_success_without_target_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_s
et_value_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_g
et_value_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_l
ist_all_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_u
pdates_yaml_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_yaml_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_u
ses_loader_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_loader_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_load_error_displays_error
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_save_error_displays_error
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_nonexistent_feature_creates_feature
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_already_enabled_feature_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_enable_featu
re_cmd_persists_existing_flags
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_creates
_config_and_commands_use_loader_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_inspect_cmd_
file_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_inspect_cmd_
interactive_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_openai_key_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_chromadb_package_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_mis
sing_kuzu_package_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_key_a
utocomplete_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_check_servic
es_warns_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_doctor_cmd_i
nvokes_loader_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_check_cmd_al
ias_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_gather_cmd_c
reates_file_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_init_cmd_wiz
ard_runs_wizard
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_spec_cmd_inv
alid_file
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_test_cmd_inv
alid_file
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_code_cmd_no_
tests
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_run_pipeline
_cmd_invalid_target
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_config_cmd_i
nvalid_key
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_gather_cmd_e
rror_propagates
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_refactor_cmd
_error
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_inspect_cmd_
failure
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_i
nvalid_framework
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_serve_cmd_pa
sses_options
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_f
lask_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_f
astapi_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_e
xisting_dir_without_force_fails
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webapp_cmd_e
xisting_dir_with_force_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_invalid_type
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_sqlite_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_mysql_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_mongodb_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_existing_dir_without_force_fails
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_dbschema_cmd
_existing_dir_with_force_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_su
ccess_succeeds
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_im
port_error_displays_error
tests/unit/general/test_unit_cli_commands.py::TestCLICommands::test_webui_cmd_ru
ntime_error_displays_error
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
create_team_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
add_agent_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_single_agent_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_multi_agent_consensus_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_critical_decision_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_agent_failure_continues_fails
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_propagates_agent_execution_error_raises_error
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_coverage_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_no_team_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
delegate_task_no_agents_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
get_team_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
set_current_team_succeeds
tests/unit/general/test_wsde_team_coordinator.py::TestWSDETeamCoordinator::test_
set_current_team_nonexistent_succeeds
tests/unit/interface/test_agentapi_class.py::test_agentapi_request_models_reject
_invalid_payloads
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
sponse_models_can_be_instantiated
tests/unit/interface/test_api_endpoints.py::test_health_endpoint_requires_authen
tication_succeeds
tests/unit/interface/test_api_endpoints.py::test_metrics_endpoint_requires_authe
ntication_succeeds
tests/unit/interface/test_api_endpoints.py::test_init_endpoint_initializes_proje
ct_succeeds
tests/unit/interface/test_api_endpoints.py::test_gather_endpoint_collects_requir
ements_succeeds
tests/unit/interface/test_api_endpoints.py::test_synthesize_endpoint_runs_pipeli
ne_succeeds
tests/unit/interface/test_api_endpoints.py::test_spec_endpoint_generates_specifi
cations_succeeds
tests/unit/interface/test_api_endpoints.py::test_test_endpoint_generates_tests_s
ucceeds
tests/unit/interface/test_api_endpoints.py::test_code_endpoint_generates_code_su
cceeds
tests/unit/interface/test_api_endpoints.py::test_doctor_endpoint_runs_diagnostic
s_succeeds
tests/unit/interface/test_api_endpoints.py::test_edrr_cycle_endpoint_runs_cycle_
succeeds
tests/unit/interface/test_api_endpoints.py::test_status_endpoint_returns_message
s_returns_expected_result
tests/unit/interface/test_api_endpoints.py::test_test_endpoint_generates_tests_f
rom_spec_succeeds
tests/unit/interface/test_api_endpoints.py::test_endpoints_handle_errors_properl
y_raises_error tests/unit/interface/test_cli_components.py::test_function
tests/unit/interface/test_cli_components.py::test_CLIProgressIndicator_update_su
cceeds
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_multiple_
subtasks_succeeds
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_hea
ding_levels_succeeds
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_sma
rt_styling_succeeds
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_ric
h_markup_succeeds
tests/unit/interface/test_cli_components.py::test_cliuxbridge_display_result_hig
hlight_succeeds tests/unit/interface/test_cliuxbridge.py::test_function
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_confirm_choice_succee
ds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_highli
ght_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_error_
succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_warnin
g_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_succes
s_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_headin
g_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_subhea
ding_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_rich_m
arkup_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_display_result_normal
_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliuxbridge_ask_question_validate
s_input_succeeds
tests/unit/interface/test_cliuxbridge.py::test_cliprogressindicator_subtasks_suc
ceeds tests/unit/interface/test_dpg_bridge.py::test_ask_question_returns_string
tests/unit/interface/test_dpg_bridge.py::test_confirm_choice_returns_boolean[Yes
-True]
tests/unit/interface/test_dpg_bridge.py::test_confirm_choice_returns_boolean[No-
False]
tests/unit/interface/test_dpg_bridge.py::test_display_result_sanitizes_output
tests/unit/interface/test_dpg_bridge.py::test_create_progress_returns_indicator
tests/unit/interface/test_dpg_bridge.py::test_cancellable_progress_allows_cancel
tests/unit/interface/test_dpg_bridge.py::test_run_cli_command_executes_and_polls
tests/unit/interface/test_dpg_bridge.py::test_run_cli_command_handles_exception
tests/unit/interface/test_dpg_bridge.py::test_run_cli_command_cancellation
tests/unit/interface/test_dpg_bridge.py::test_run_cli_command_propagates_async_e
rror
tests/unit/interface/test_dpg_bridge.py::test_run_cli_command_progress_and_error
_hooks
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_sanitiz
e_output
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_detect_
message_type_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_format_
message_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_display
_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_set_con
sole_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_format_
table_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_format_
list_succeeds
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_format_
structured_json_yaml
tests/unit/interface/test_output_formatter.py::TestOutputFormatter::test_formatt
er_singleton_succeeds
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_escapes_html_
succeeds
tests/unit/interface/test_output_sanitization.py::test_apibridge_sanitizes_outpu
t_succeeds
tests/unit/interface/test_output_sanitization.py::test_webui_sanitizes_output_su
cceeds
tests/unit/interface/test_output_sanitization.py::test_webapp_cmd_error_sanitize
d_raises_error
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_removes_self_
closing_script
tests/unit/interface/test_output_sanitization.py::test_sanitize_output_respects_
env tests/unit/interface/test_state_access.py::test_is_session_state_available
tests/unit/interface/test_state_access.py::test_handle_state_error
tests/unit/interface/test_state_access.py::test_get_session_value_with_none_sess
ion_state
tests/unit/interface/test_state_access.py::test_get_session_value_with_attribute
_access
tests/unit/interface/test_state_access.py::test_get_session_value_with_dict_acce
ss
tests/unit/interface/test_state_access.py::test_get_session_value_with_missing_k
ey
tests/unit/interface/test_state_access.py::test_get_session_value_with_exception
tests/unit/interface/test_state_access.py::test_set_session_value_with_none_sess
ion_state
tests/unit/interface/test_state_access.py::test_set_session_value_with_attribute
_access
tests/unit/interface/test_state_access.py::test_set_session_value_with_dict_acce
ss
tests/unit/interface/test_state_access.py::test_set_session_value_with_attribute
_exception
tests/unit/interface/test_state_access.py::test_set_session_value_with_dict_exce
ption
tests/unit/interface/test_state_access.py::test_set_session_value_with_both_exce
ptions
tests/unit/interface/test_state_access.py::test_integration_with_streamlit
tests/unit/interface/test_ux_bridge.py::test_function
tests/unit/interface/test_uxbridge.py::test_prompt_and_result_consistency
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_apply_uxb
ridge_settings
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_cli_succeeds
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_webui_succeeds
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_api_succeeds
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_webui_fallback_succeeds
tests/unit/interface/test_uxbridge_config.py::TestUXBridgeConfig::test_get_defau
lt_bridge_api_fallback_succeeds
tests/unit/interface/test_uxbridge_question_result.py::test_ask_question_and_dis
play_result_consistency[_cli_bridge]
tests/unit/interface/test_uxbridge_question_result.py::test_ask_question_and_dis
play_result_consistency[_web_bridge]
tests/unit/interface/test_uxbridge_question_result.py::test_ask_question_and_dis
play_result_consistency[_api_bridge]
tests/unit/interface/test_uxbridge_question_result.py::test_ask_question_and_dis
play_result_consistency[_dpg_bridge]
tests/unit/interface/test_uxbridge_sanitization.py::test_with_clean_state
tests/unit/interface/test_uxbridge_sanitization.py::test_apibridge_sanitizes_dis
play_result_succeeds
tests/unit/interface/test_uxbridge_sanitization.py::test_webui_sanitizes_display
_result_succeeds
tests/unit/interface/test_webui.py::test_onboarding_calls_init_succeeds
tests/unit/interface/test_webui.py::test_requirements_calls_spec_succeeds
tests/unit/interface/test_webui.py::test_analysis_calls_analyze_succeeds
tests/unit/interface/test_webui.py::test_synthesis_buttons_succeeds
tests/unit/interface/test_webui.py::test_config_update_succeeds
tests/unit/interface/test_webui.py::test_diagnostics_runs_doctor_succeeds
tests/unit/interface/test_webui.py::test_edrr_cycle_page_succeeds
tests/unit/interface/test_webui.py::test_alignment_page_succeeds
tests/unit/interface/test_webui.py::test_alignment_metrics_page_succeeds
tests/unit/interface/test_webui.py::test_inspect_config_page_succeeds
tests/unit/interface/test_webui.py::test_validate_manifest_page_succeeds
tests/unit/interface/test_webui.py::test_validate_metadata_page_succeeds
tests/unit/interface/test_webui.py::test_test_metrics_page_succeeds
tests/unit/interface/test_webui.py::test_docs_generation_page_succeeds
tests/unit/interface/test_webui.py::test_ingestion_page_succeeds
tests/unit/interface/test_webui.py::test_apispec_page_succeeds
tests/unit/interface/test_webui.py::test_refactor_page_succeeds
tests/unit/interface/test_webui.py::test_webapp_page_succeeds
tests/unit/interface/test_webui.py::test_serve_page_succeeds
tests/unit/interface/test_webui.py::test_dbschema_page_succeeds
tests/unit/interface/test_webui.py::test_doctor_page_succeeds
tests/unit/interface/test_webui.py::test_run_method_renders_pages_succeeds
tests/unit/interface/test_webui.py::test_wizard_navigation_helper_clamps_steps
tests/unit/interface/test_webui_bridge_progress.py::test_display_result_routes_a
nd_message_capture
tests/unit/interface/test_webui_bridge_progress.py::test_create_progress_thresho
lds_use_default_status
tests/unit/interface/test_webui_bridge_progress.py::test_wizard_step_bounds_and_
session_state_validation
tests/unit/interface/test_webui_cli_imports.py::test_with_clean_state
tests/unit/interface/test_webui_error_handling.py::test_init_cmd_error_handling
tests/unit/interface/test_webui_error_handling.py::test_onboarding_page_setup_wi
zard_error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_spec_c
md_error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_inspec
t_cmd_error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_requirements_page_file_n
ot_found_raises_error
tests/unit/interface/test_webui_error_handling.py::test_analysis_page_inspect_co
de_cmd_error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_synthesis_page_test_cmd_
error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_config_page_load_config_
error_raises_error
tests/unit/interface/test_webui_error_handling.py::test_config_page_save_config_
error_raises_error
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_start_butto
n_not_clicked
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_finish_call
s_gather_requirements
tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_import_erro
r tests/unit/interface/test_webui_gather_wizard.py::test_gather_wizard_exception
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
initialization_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
navigation_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
data_persistence_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
completion_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
error_handling_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
cancel_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
validation_with_state
tests/unit/interface/test_webui_gather_wizard_with_state.py::test_gather_wizard_
start_resets_state
tests/unit/interface/test_webui_navigation_and_validation.py::test_navigation_pe
rsists_wizard_state
tests/unit/interface/test_webui_navigation_and_validation.py::test_analysis_page
_invalid_path_shows_error
tests/unit/interface/test_webui_onboarding.py::test_onboarding_page_succeeds
tests/unit/interface/test_webui_onboarding.py::test_onboarding_page_no_submit_su
cceeds
tests/unit/interface/test_webui_progress.py::test_ui_progress_init_succeeds
tests/unit/interface/test_webui_progress.py::test_ui_progress_update_succeeds
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_succeeds
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_cascades_
subtasks
tests/unit/interface/test_webui_progress.py::test_ui_progress_add_subtask_succee
ds
tests/unit/interface/test_webui_progress.py::test_ui_progress_update_subtask_suc
ceeds
tests/unit/interface/test_webui_progress.py::test_ui_progress_complete_subtask_s
ucceeds
tests/unit/interface/test_webui_requirements.py::test_requirements_page_succeeds
tests/unit/interface/test_webui_requirements.py::test_requirements_wizard_succee
ds tests/unit/interface/test_webui_setup.py::test_webui_setup_wizard_runs
tests/unit/interface/test_webui_wizard_state.py::test_function
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_navigation
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_data_persiste
nce
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_completion
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_reset
tests/unit/interface/test_webui_wizard_state.py::test_gather_wizard_state_initia
lization
tests/unit/interface/test_webui_wizard_state.py::test_gather_wizard_workflow
tests/unit/interface/test_webui_wizard_state.py::test_simulate_wizard_navigation
tests/unit/interface/test_webui_wizard_state.py::test_set_wizard_data
tests/unit/interface/test_webui_wizard_state.py::test_manager_clears_temp_state
tests/unit/interface/test_webui_wizard_state.py::test_wizard_state_in_streamlit_
context
tests/unit/interface/test_wizard_state_manager.py::test_wizard_state_manager_ini
tialization
tests/unit/interface/test_wizard_state_manager.py::test_get_wizard_state_new
tests/unit/interface/test_wizard_state_manager.py::test_get_wizard_state_existin
g tests/unit/interface/test_wizard_state_manager.py::test_has_wizard_state
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_va
lid
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_mi
ssing_key
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_in
valid_step
tests/unit/interface/test_wizard_state_manager.py::test_validate_wizard_state_mi
smatched_steps
tests/unit/interface/test_wizard_state_manager.py::test_reset_wizard_state
tests/unit/interface/test_wizard_state_manager.py::test_reset_wizard_state_error
tests/unit/interface/test_wizard_state_manager.py::test_get_current_step
tests/unit/interface/test_wizard_state_manager.py::test_go_to_step
tests/unit/interface/test_wizard_state_manager.py::test_next_step
tests/unit/interface/test_wizard_state_manager.py::test_previous_step
tests/unit/interface/test_wizard_state_manager.py::test_set_completed
tests/unit/interface/test_wizard_state_manager.py::test_is_completed
tests/unit/interface/test_wizard_state_manager.py::test_get_value
tests/unit/interface/test_wizard_state_manager.py::test_set_value
tests/unit/interface/test_wizard_state_manager.py::test_simulate_wizard_manager_
navigation
tests/unit/interface/test_wizard_state_manager.py::test_set_wizard_manager_data
tests/unit/interface/test_wizard_state_manager.py::test_gather_wizard_state_mana
ger
tests/unit/interface/test_wizard_state_manager.py::test_gather_wizard_workflow
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_basic_text
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_custom_parameters
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_context
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_auto_model_selection
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTextGeneration::te
st_generate_with_invalid_parameters
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderStreaming::test_ge
nerate_stream_with_context
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEmbeddings::test_g
et_embedding_single_text
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEmbeddings::test_g
et_embedding_multiple_texts
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_server_connection_error_handling
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_model_error_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_basic_text
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_with_custom_parameters
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_with_context
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_with_invalid_parameters
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_api_error_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTextGeneration::test_g
enerate_rate_limit_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderStreaming::test_genera
te_stream_without_async_client
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_single_text
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_multiple_texts
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEmbeddings::test_get_e
mbedding_api_error
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_ne
twork_timeout_handling
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_on_transient_errors
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_basic_text
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_custom_parameters
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_context
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_with_invalid_parameters
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_api_error_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTextGeneration
::test_generate_rate_limit_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderStreaming::tes
t_generate_stream_without_httpx
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_single_text
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_multiple_texts
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEmbeddings::te
st_get_embedding_api_error
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_network_timeout_handling
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_on_transient_errors
tests/unit/methodology/test_dialectical_reasoner.py::test_evaluate_change_persis
ts_reasoning_to_memory
tests/unit/methodology/test_dialectical_reasoner.py::test_create_session_adds_we
lcome_message
tests/unit/methodology/test_dialectical_reasoner.py::test_process_message_record
s_conversation
tests/unit/methodology/test_dialectical_reasoner_hooks.py::test_hook_receives_co
nsensus_flag
tests/unit/methodology/test_dialectical_reasoner_hooks.py::test_hook_runs_on_fai
lure
tests/unit/methodology/test_dialectical_reasoner_hooks.py::test_hook_exception_s
uppressed
tests/unit/methodology/test_wsde_edrr_coordinator.py::test_phase_progress_flushe
s_pending_memory_before_and_after
tests/unit/providers/test_provider_contract.py::test_openai_live_profile_envs_we
ll_formed
tests/unit/providers/test_provider_contract.py::test_lmstudio_live_profile_envs_
well_formed tests/unit/security/test_argon2_hash.py::test_argon2_hash_roundtrip
tests/unit/security/test_authentication.py::test_hash_and_verify_password_succee
ds
tests/unit/security/test_authentication.py::test_authenticate_success_succeeds
tests/unit/security/test_authentication.py::test_authenticate_failure_succeeds
tests/unit/security/test_authorization.py::test_is_authorized_true_returns_expec
ted_result
tests/unit/security/test_authorization.py::test_is_authorized_false_returns_expe
cted_result
tests/unit/security/test_authorization.py::test_is_authorized_multiple_roles_suc
ceeds
tests/unit/security/test_authorization.py::test_is_authorized_wildcard_succeeds
tests/unit/security/test_authorization.py::test_is_authorized_role_not_in_acl_re
turns_expected_result
tests/unit/security/test_authorization.py::test_is_authorized_empty_roles_return
s_expected_result
tests/unit/security/test_authorization.py::test_is_authorized_empty_acl_returns_
expected_result
tests/unit/security/test_authorization.py::test_is_authorized_case_sensitivity_s
ucceeds
tests/unit/security/test_authorization.py::test_is_authorized_iterable_roles_suc
ceeds
tests/unit/security/test_authorization.py::test_require_authorization_raises
tests/unit/security/test_authorization.py::test_require_authorization_no_excepti
on_raises_error
tests/unit/security/test_authorization.py::test_require_authorization_empty_role
s_raises_error
tests/unit/security/test_authorization.py::test_require_authorization_empty_acl_
raises_error
tests/unit/test_sentinel_speed_markers.py::test_sentinel_medium_bucket_present
tests/integration/adapters/test_github_project_adapter.py::test_github_project_a
dapter_config_loading
tests/integration/adapters/test_jira_adapter.py::test_jira_adapter_config_loadin
g tests/integration/api/test_api_smoke_import.py::test_fastapi_api_import_smoke
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.EXPAND]
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.DIFFERENTIATE]
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.REFINE]
tests/integration/collaboration/test_voting_summary_edrr.py::test_voting_summary
_in_edrr_phases[Phase.RETROSPECT]
tests/integration/config/test_unified_loader.py::test_unified_loader_prefers_pyp
roject_succeeds
tests/integration/config/test_unified_loader.py::test_env_var_override_with_cust
om_path_succeeds
tests/integration/edrr/test_wsde_edrr_integration.py::TestWSDEEDRRIntegration::t
est_phase_specific_role_assignment_has_expected
tests/integration/edrr/test_wsde_edrr_integration.py::TestWSDEEDRRIntegration::t
est_role_rotation_on_phase_transition_has_expected
tests/integration/edrr/test_wsde_edrr_integration.py::TestWSDEEDRRIntegration::t
est_phase_specific_expertise_scoring_succeeds
tests/integration/edrr/test_wsde_edrr_integration.py::TestWSDEEDRRIntegration::t
est_dynamic_role_reassignment_and_consensus
tests/integration/edrr/test_wsde_edrr_integration.py::TestWSDEEDRRIntegration::t
est_dialectical_hooks_invoked
tests/integration/general/test_agent_api.py::test_init_route_succeeds
tests/integration/general/test_agent_api.py::test_gather_route_succeeds
tests/integration/general/test_agent_api.py::test_synthesize_and_status_succeeds
tests/integration/general/test_agent_api.py::test_spec_route_succeeds
tests/integration/general/test_agent_api.py::test_test_route_succeeds
tests/integration/general/test_agent_api.py::test_code_route_succeeds
tests/integration/general/test_agent_api.py::test_doctor_route_succeeds
tests/integration/general/test_agent_api.py::test_edrr_cycle_route_succeeds
tests/integration/general/test_agent_api.py::test_enhanced_metrics_endpoint_repo
rts_requests
tests/integration/general/test_agent_api.py::test_enhanced_rate_limit_returns_st
ructured_error
tests/integration/general/test_agent_api.py::test_enhanced_init_error_payload
tests/integration/general/test_agent_api_security.py::test_api_requires_authenti
cation_succeeds
tests/integration/general/test_agent_api_security.py::test_api_authentication_di
sabled_succeeds
tests/integration/general/test_agent_api_security.py::test_api_error_handling_ra
ises_error
tests/integration/general/test_agent_api_security.py::test_api_validation_is_val
id
tests/integration/general/test_agent_api_security.py::test_api_health_endpoint_s
ucceeds
tests/integration/general/test_agent_api_security.py::test_api_metrics_endpoint_
succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_register_agent_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_create_team_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_create_and_assign_task_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_execute_task_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_send_message_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_execute_workflow_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_message_protocol_persistence_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_peer_review_workflow_succeeds
tests/integration/general/test_agent_collaboration_integration.py::TestAgentColl
aborationSystem::test_peer_review_with_team_builds_consensus_and_rotates_roles
tests/integration/general/test_agent_system_integration.py::test_base_agent_grap
h_successful_invocation_succeeds
tests/integration/general/test_agent_system_integration.py::test_base_agent_grap
h_error_in_input_processing_raises_error
tests/integration/general/test_agent_system_integration.py::test_base_agent_grap
h_error_in_llm_call_raises_error
tests/integration/general/test_agent_system_integration.py::test_base_agent_grap
h_llm_response_is_none_for_parsing_raises_error
tests/integration/general/test_agentapi.py::test_init_succeeds
tests/integration/general/test_agentapi.py::test_gather_succeeds
tests/integration/general/test_agentapi.py::test_synthesize_and_status_succeeds
tests/integration/general/test_agentapi_routes.py::test_init_route_succeeds
tests/integration/general/test_agentapi_routes.py::test_gather_route_succeeds
tests/integration/general/test_agentapi_routes.py::test_synthesize_and_status_su
cceeds
tests/integration/general/test_anthropic_provider.py::TestAnthropicProvider::tes
t_generate_integration_succeeds
tests/integration/general/test_anthropic_provider.py::TestAnthropicProvider::tes
t_generate_with_context_integration_succeeds
tests/integration/general/test_anthropic_provider.py::TestAnthropicProvider::tes
t_get_embedding_integration_succeeds
tests/integration/general/test_chromadb_client_connection.py::test_memory_adapte
r_uses_ephemeral_client_when_no_network
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_init_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_spec_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_test_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_code_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_edrr_cycle_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_error_handling_in_pipeline_raises_error
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_webui_command_pipeline_succeeds
tests/integration/general/test_cli_webui_agentapi_pipeline.py::TestCLIWebUIAgent
APIPipeline::test_webui_command_error_handling_succeeds
tests/integration/general/test_collaborative_decision_making.py::test_dynamic_ro
le_reassignment_integration_succeeds
tests/integration/general/test_collaborative_decision_making.py::test_consensus_
vote_tie_breaker_succeeds
tests/integration/general/test_collaborative_voting.py::test_majority_voting_suc
ceeds
tests/integration/general/test_collaborative_voting.py::test_weighted_voting_suc
ceeds
tests/integration/general/test_collaborative_voting.py::test_voting_result_syncs
_to_memory
tests/integration/general/test_complex_workflow.py::TestComplexWorkflow::test_co
mplex_workflow_with_inconsistent_state_succeeds
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_analyze_external_codebase_succeeds
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_project_state_analyzer_with_external_codebase_succeeds
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_refactor_workflow_with_external_codebase_succeeds
tests/integration/general/test_comprehensive_workflow.py::TestComprehensiveWorkf
low::test_end_to_end_workflow_succeeds
tests/integration/general/test_config_loader.py::test_load_and_save_yaml_config_
succeeds
tests/integration/general/test_config_loader.py::test_load_and_save_pyproject_co
nfig_succeeds
tests/integration/general/test_config_loader.py::test_version_mismatch_warning_y
aml_succeeds
tests/integration/general/test_config_loader.py::test_version_mismatch_warning_p
yproject_succeeds
tests/integration/general/test_config_loader_integration.py::test_load_config_fr
om_yaml_succeeds
tests/integration/general/test_config_loader_integration.py::test_load_config_fr
om_pyproject_succeeds
tests/integration/general/test_config_loader_integration.py::test_pyproject_prec
edence_over_yaml_succeeds
tests/integration/general/test_config_loader_workflow.py::test_load_config_merge
s_defaults_succeeds
tests/integration/general/test_config_loader_workflow.py::test_malformed_yaml_ra
ises
tests/integration/general/test_config_loader_workflow.py::test_malformed_toml_ra
ises
tests/integration/general/test_critique_workflow.py::test_generate_code_todo_tri
ggers_critique_warning
tests/integration/general/test_critique_workflow.py::test_generate_tests_clean_p
asses_critique
tests/integration/general/test_delegate_task.py::test_delegate_task_team_consens
us_succeeds
tests/integration/general/test_delegate_task.py::test_delegate_task_no_agents_su
cceeds
tests/integration/general/test_delegate_task.py::test_delegate_task_invalid_task
_succeeds
tests/integration/general/test_delegate_task_consensus.py::test_delegate_task_te
am_consensus_succeeds
tests/integration/general/test_delegate_task_consensus.py::test_critical_decisio
n_majority_vote_succeeds
tests/integration/general/test_delegate_task_consensus.py::test_critical_decisio
n_tied_vote_falls_back_to_consensus_succeeds
tests/integration/general/test_delegate_task_primus_selection.py::test_delegate_
task_calls_select_primus_by_expertise_and_updates_primus_succeeds
tests/integration/general/test_delegate_task_primus_selection.py::test_primus_ro
tation_resets_after_all_have_served_succeeds
tests/integration/general/test_delegate_task_workflow.py::test_delegate_task_ful
l_workflow_succeeds
tests/integration/general/test_end_to_end_workflow.py::TestEndToEndWorkflow::tes
t_complete_workflow_succeeds
tests/integration/general/test_end_to_end_workflow.py::TestEndToEndWorkflow::tes
t_inconsistent_project_workflow_succeeds
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_edrr_wsde_integration_raise
s_error
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_memory_integration_raises_e
rror
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_provider_integration_raises
_error
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_code_analysis_integration_r
aises_error
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_recovery_in_edrr_cycle_raises_error
tests/integration/general/test_error_handling_at_integration_points.py::TestErro
rHandlingAtIntegrationPoints::test_error_handling_in_cross_component_integration
_raises_error
tests/integration/general/test_feature_flag_integration.py::TestEDRRPhaseTransit
ions::test_auto_transitions_enabled_succeeds
tests/integration/general/test_feature_flag_integration.py::TestEDRRPhaseTransit
ions::test_manual_transitions_when_disabled_succeeds
tests/integration/general/test_feature_flag_integration.py::TestWSDECollaboratio
n::test_collaboration_enabled_succeeds
tests/integration/general/test_feature_flag_integration.py::TestWSDECollaboratio
n::test_collaboration_disabled_succeeds
tests/integration/general/test_feature_flag_integration.py::TestWSDECollaboratio
n::test_collaboration_no_agents_succeeds
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_edrr_cycle_with_graph_memory_succeeds
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_memory_volatility_with_edrr_succeeds
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_query_edrr_phases_from_graph_has_expected
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_relationships_across_edrr_phases_has_expected
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_cross_store_sync_with_memory_manager
tests/integration/general/test_graph_memory_edrr_integration.py::TestGraphMemory
EDRRIntegration::test_transaction_rollback_reverts_changes
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_store_with_invalid_path_raises_permission_error
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_retrieve_nonexistent_item_succeeds
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_delete_nonexistent_item_succeeds
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_search_with_invalid_criteria_returns_empty_list
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_query_related_items_nonexistent_succeeds
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_store_with_corrupted_graph_raises_memory_store_error
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_concurrent_access_succeeds
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_store_and_retrieve_with_special_characters_succeeds
tests/integration/general/test_graph_memory_error_handling.py::TestGraphMemoryEr
rorHandling::test_store_and_retrieve_with_unicode_characters_succeeds
tests/integration/general/test_kuzu_adapter_transactions.py::test_kuzu_adapter_t
ransaction_persistence
tests/integration/general/test_kuzu_memory_fallback.py::test_memory_system_falls
_back_when_kuzu_unavailable
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_memory_vect
or_integration_succeeds[kuzu_store]
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_memory_vect
or_integration_succeeds[kuzu_store_embedded]
tests/integration/general/test_kuzu_memory_integration.py::test_create_for_testi
ng_with_kuzu[kuzu_store]
tests/integration/general/test_kuzu_memory_integration.py::test_create_for_testi
ng_with_kuzu[kuzu_store_embedded]
tests/integration/general/test_kuzu_memory_integration.py::test_ephemeral_store_
cleanup
tests/integration/general/test_kuzu_memory_integration.py::test_ephemeral_store_
startup_respects_env
tests/integration/general/test_kuzu_memory_integration.py::test_configured_path_
usage
tests/integration/general/test_kuzu_memory_integration.py::test_provider_fallbac
k_on_empty_embedding
tests/integration/general/test_kuzu_memory_integration.py::test_create_ephemeral
_embedded_mode
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_embedded_en
v_setting[True]
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_embedded_en
v_setting[False]
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_embedded_mo
dule_export
tests/integration/general/test_kuzu_memory_integration.py::test_kuzu_adapter_eph
emeral_cleanup
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_import
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_registration
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_prov
ider_initialization
tests/integration/general/test_lmstudio_integration_basic.py::test_configuration
_loading
tests/integration/general/test_lmstudio_integration_basic.py::test_lmstudio_heal
th_check
tests/integration/general/test_lmstudio_integration_comprehensive.py::test_lmstu
dio_integration
tests/integration/general/test_lmstudio_integration_comprehensive.py::test_mock_
lmstudio_scenario
tests/integration/general/test_lmstudio_integration_real.py::test_real_lmstudio_
integration
tests/integration/general/test_lmstudio_integration_real.py::test_lmstudio_model
s
tests/integration/general/test_lmstudio_integration_standalone.py::test_lmstudio
_integration
tests/integration/general/test_lmstudio_integration_standalone.py::test_mock_lms
tudio_scenario
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
init_with_default_config_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
init_with_specified_model_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
init_with_connection_error_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
list_available_models_error_fails
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
list_available_models_integration_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
generate_integration_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
generate_with_connection_error_succeeds
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
generate_with_invalid_response_returns_expected_result
tests/integration/general/test_lmstudio_provider.py::TestLMStudioProvider::test_
generate_with_context_integration_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_can_store_and_retrieve_memory_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_can_search_memory_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_can_update_memory_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_can_delete_memory_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_multiple_agents_can_share_memory_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_memory_isolation_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_agent_memory_with_context_succeeds
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_persistent_sync_across_stores
tests/integration/general/test_memory_agent_integration.py::TestMemoryAgentInteg
ration::test_sync_manager_transaction_rolls_back_succeeds
tests/integration/general/test_multi_store_sync_manager.py::test_multi_store_syn
c_and_persistence
tests/integration/general/test_multi_store_sync_manager.py::test_multi_store_tra
nsaction_persistence
tests/integration/general/test_non_hierarchical_decision.py::test_non_hierarchic
al_conflict_resolution_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_with_default_config_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_with_specified_model_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_init
_without_api_key_uses_stub_in_offline
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_with_connection_error_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_with_context_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_get_
embedding_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_integration_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_gene
rate_with_context_integration_succeeds
tests/integration/general/test_openai_provider.py::TestOpenAIProvider::test_get_
embedding_integration_succeeds
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_devsynth_project_succeeds
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_with_missing_files_succeeds
tests/integration/general/test_project_state_analyzer.py::TestProjectStateAnalyz
er::test_analyze_with_requirements_and_code_succeeds
tests/integration/general/test_prompt_auto_tuning.py::test_auto_tuner_disabled_b
y_default_succeeds
tests/integration/general/test_prompt_auto_tuning.py::test_prompt_auto_tuning_fe
edback_loop_succeeds
tests/integration/general/test_provider_system_async.py::test_openai_provider_ac
omplete
tests/integration/general/test_provider_system_async.py::test_acomplete_function
tests/integration/general/test_provider_system_async.py::test_aembed_function
tests/integration/general/test_provider_system_async.py::test_fallback_provider_
acomplete
tests/integration/general/test_provider_system_async.py::test_fallback_provider_
aembed
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_openai_provider_with_different_models_has_expected
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_openai_provider_with_different_parameters_has_expected
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_lm_studio_provider_with_different_endpoints_has_expected
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_lm_studio_provider_with_different_parameters_has_expected
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_fallback_provider_with_different_configurations_has_expected
tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_provider_system_with_different_default_providers_has_expected

tests/integration/general/test_provider_system_configurations.py::TestProviderCo
nfigurations::test_provider_system_with_context_aware_completion_has_expected
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_direct_query
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_cross_store_query
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_cross_store_query_subset
tests/integration/general/test_query_router_integration.py::TestQueryRouterInteg
ration::test_federated_query
tests/integration/general/test_recursive_recovery_flow.py::test_recursive_recove
ry_flow
tests/integration/general/test_refactor_workflow.py::TestRefactorWorkflowManager
::test_analyze_project_state_succeeds
tests/integration/general/test_refactor_workflow.py::TestRefactorWorkflowManager
::test_determine_optimal_workflow_succeeds
tests/integration/general/test_refactor_workflow.py::TestRefactorWorkflowManager
::test_suggest_next_steps_succeeds
tests/integration/general/test_refactor_workflow.py::TestRefactorWorkflowManager
::test_initialize_workflow_succeeds
tests/integration/general/test_refactor_workflow.py::TestRefactorWorkflowManager
::test_execute_refactor_workflow_succeeds
tests/integration/general/test_requirements_gathering.py::test_gather_updates_co
nfig_succeeds
tests/integration/general/test_requirements_gathering.py::test_requirements_wiza
rd_persists_priority_succeeds
tests/integration/general/test_requirements_gathering.py::test_requirements_wiza
rd_backtracks_priority_succeeds
tests/integration/general/test_requirements_gathering.py::test_gather_cmd_loggin
g_exc_info_succeeds
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_analyze_
devsynth_codebase_succeeds
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_architec
ture_violations_succeeds
tests/integration/general/test_self_analyzer.py::TestSelfAnalyzer::test_improvem
ent_opportunities_succeeds
tests/integration/general/test_webui_pages.py::test_webui_pages_invoke_commands_
succeeds
tests/integration/general/test_webui_setup.py::test_guided_setup_button_invokes_
wizard_succeeds
tests/integration/general/test_webui_setup.py::test_offline_toggle_saves_config_
succeeds
tests/integration/general/test_webui_setup.py::test_webui_bridge_error_display_s
ucceeds
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_wsde_team_role_assignment_in_edrr_phases_has_expecte
d
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_wsde_method_calls_in_edrr_phases_has_expected
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_memory_integration_in_edrr_wsde_workflow_succeeds
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_error_handling_in_edrr_wsde_integration_raises_error

tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_phase_progression_flushes_memory_queue
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_memory_sync_hook_receives_events
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_retrospective_flushes_pending_memory_without_record
tests/integration/general/test_wsde_edrr_component_interactions.py::TestWSDEEDRR
ComponentInteractions::test_role_assignment_mapping_is_accessible
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_memo
ry_manager_passed_to_team
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_memo
ry_manager_passed_to_peer_review
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_peer
_review_stores_in_memory
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_full
_peer_review_workflow_with_memory
tests/integration/general/test_wsde_peer_review_memory_integration.py::test_cros
s_store_synchronization
tests/integration/generated_tests/test_scaffold_output.py::test_scaffold_integra
tion_tests_creates_files
tests/integration/interface/test_bridge_consistency.py::test_bridge_output_consi
stency_succeeds
tests/integration/interface/test_bridge_consistency.py::test_bridge_method_signa
tures_succeeds
tests/integration/interface/test_multi_agent_collaboration.py::test_cli_and_api_
bridges_multi_agent_consistent
tests/integration/interface/test_multi_agent_collaboration.py::test_vote_with_ro
le_reassignment_succeeds
tests/integration/interface/test_multi_agent_collaboration.py::test_dynamic_role
_reassignment_changes_primus
tests/integration/interface/test_mvuu_dashboard_report.py::test_dashboard_render
s_from_generated_report
tests/integration/interface/test_small_workflow_bridge.py::test_cli_and_api_brid
ges_consistent_succeeds
tests/integration/interface/test_webui_cli_lookup.py::test_cli_fallback_to_cli_m
odule
tests/integration/interface/test_webui_flow.py::test_webui_navigation_prompt_and
_command
tests/integration/interface/test_webui_flow.py::test_webui_run_resets_invalid_na
vigation
tests/integration/interface/test_webui_flow.py::test_command_error_feedback_surf
aces_actionable_guidance
tests/integration/interface/test_webui_run_navigation.py::test_run_injects_asset
s_and_resets_navigation
tests/integration/interface/test_webui_run_navigation.py::test_run_handles_page_
config_errors
tests/integration/live/test_openai_live_smoke.py::test_openai_chat_completion_li
ve_smoke
tests/integration/live/test_openai_live_smoke.py::test_openai_embeddings_live_sm
oke
tests/integration/llm/test_lmstudio_streaming.py::TestLMStudioStreaming::test_ge
nerate_streaming_returns_expected
tests/integration/llm/test_lmstudio_streaming.py::TestLMStudioStreaming::test_ge
nerate_with_context_streaming_returns_expected
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_has_required_methods[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_method_signatures_consistent[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderInterfaceConsist
ency::test_provider_error_handling_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_authentication_error_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderErrorConsistency
::test_configuration_error_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_standardized_config_acceptance[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderConfigurationCon
sistency::test_environment_variable_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_response_time_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderPerformanceConsi
stency::test_token_usage_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_generation_behavior_consistency[offline]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[openrouter]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[openai]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[lmstudio]
tests/integration/llm/test_provider_consistency.py::TestProviderBehaviorConsiste
ncy::test_context_behavior_consistency[offline]
tests/integration/llm/test_provider_selection_integration.py::test_offline_mode_
returns_offline_provider_for_app_wrapper
tests/integration/llm/test_provider_selection_integration.py::test_offline_mode_
returns_offline_provider_for_providers_module
tests/integration/llm/test_provider_selection_integration.py::test_openai_withou
t_api_key_raises_validation_error_in_providers
tests/integration/llm/test_provider_selection_integration.py::test_openai_withou
t_api_key_raises_connection_error_in_app_wrapper
tests/integration/memory/test_backend_persistence.py::test_multi_store_persisten
ce
tests/integration/memory/test_chromadb_adapter_transactions.py::test_chromadb_tr
ansaction_commit_and_rollback
tests/integration/memory/test_chromadb_fallback.py::test_chromadb_falls_back_to_
ephemeral_client
tests/integration/memory/test_circuit_breaker_integration.py::test_circuit_break
er_resets_after_timeout
tests/integration/memory/test_cross_store_query.py::test_cross_store_query_retur
ns_results
tests/integration/memory/test_cross_store_sync.py::test_store_and_retrieve_round
_trip
tests/integration/memory/test_cross_store_sync.py::test_search_by_memory_type_an
d_metadata_filters
tests/integration/memory/test_cross_store_sync.py::test_delete_removes_item
tests/integration/memory/test_cross_store_sync.py::test_transactions_commit_and_
rollback
tests/integration/memory/test_cross_store_sync.py::test_cross_store_sync_copies_
missing_items
tests/integration/memory/test_cross_store_transactions.py::test_cross_store_tran
saction_commit
tests/integration/memory/test_cross_store_transactions.py::test_cross_store_quer
y_emits_typed_results
tests/integration/memory/test_cross_store_transactions.py::test_cross_store_tran
saction_rollback
tests/integration/memory/test_faiss_transactions.py::test_faiss_transaction_comm
it_and_rollback
tests/integration/memory/test_multistore_transaction_wrapper.py::test_transactio
n_context_commit_and_rollback
tests/integration/memory/test_reverse_sync_retrieval.py::test_bidirectional_pers
istence_and_retrieval
tests/integration/memory/test_sync_manager_core_transactions.py::test_synchroniz
e_core_commit
tests/integration/memory/test_sync_manager_core_transactions.py::test_synchroniz
e_core_rollback
tests/integration/memory/test_transaction_failure_recovery.py::test_commit_failu
re_triggers_rollback
tests/integration/memory/test_transactional_sync.py::test_transactional_sync_rol
lback
tests/integration/mvu/test_atomic_rewrite_round_trip.py::test_atomic_rewrite_rou
nd_trip tests/integration/mvu/test_report_generation.py::test_report_generation
tests/integration/sprint_edrr/test_ceremony_phase_alignment.py::test_default_cer
emony_mapping_aligns_with_edrr
tests/integration/templates/test_generated_module.py::test_generated_module_work
flow
tests/integration/test_real_world_scenarios.py::TestRealWorldScenarios::test_tas
k_manager_cli_workflow
tests/integration/test_real_world_scenarios.py::TestRealWorldScenarios::test_fin
ance_tracker_api_workflow
tests/integration/test_real_world_scenarios.py::TestRealWorldScenarios::test_fil
e_organizer_gui_workflow
tests/integration/test_real_world_scenarios.py::TestRealWorldScenarios::test_edr
r_methodology_validation
tests/integration/test_real_world_scenarios.py::TestDevSynthWorkflowIntegration:
:test_init_command_creates_project_structure
tests/integration/test_real_world_scenarios.py::TestDevSynthWorkflowIntegration:
:test_workflow_artifact_generation
tests/integration/webui/test_mvuu_dashboard_entrypoint.py::test_mvuu_dashboard_e
ntrypoint_loads
tests/integration/webui/test_streamlit_import.py::test_webui_module_import_smoke
_no_side_effects
tests/integration/wsde/test_wsde_edrr_integration.py::test_message_persisted_wit
h_edrr_phase tests/behavior/test_agentapi.py::test_json_requests_succeeds
tests/behavior/test_alignment_metrics_command.py::test_collect_metrics_successfu
lly
tests/behavior/test_alignment_metrics_command.py::test_handle_failure_during_met
rics_collection
tests/behavior/test_cli_help_and_completion.py::test_help_notes_include_completi
on_message
tests/behavior/test_cli_help_and_completion.py::test_completion_command_outputs_
script
tests/behavior/test_cli_help_and_completion.py::test_generate_completion_script_
returns_text tests/behavior/test_cli_webui_parity.py::test_init_parity_succeeds
tests/behavior/test_cli_webui_parity.py::test_spec_parity_succeeds
tests/behavior/test_cli_webui_parity.py::test_code_parity_succeeds
tests/behavior/test_cli_webui_parity.py::test_doctor_parity_succeeds
tests/behavior/test_dialectical_reasoner_termination_behavior.py::test_terminati
on_and_linear_complexity
tests/behavior/test_interactive_workflow.py::test_run_workflow_cli_succeeds
tests/behavior/test_interactive_workflow.py::test_run_workflow_webui_succeeds
tests/behavior/test_sprint_edrr_cycle.py::test_sprint_adapter_phase_hooks
tests/behavior/test_webui_diagnostics_audit.py::test_view_dialectical_audit_log
tests/behavior/test_webui_diagnostics_audit.py::test_audit_log_missing -m not
memory_intensive and (fast or medium) and not gui --cov=src/devsynth
--cov-report=json:test_reports/coverage.json --cov-report=html:htmlcov
--cov-append
Troubleshooting tips:
- Smoke mode: reduce third-party plugin surface to isolate issues:
  poetry run devsynth run-tests --smoke --speed=fast --no-parallel --maxfail=1
- Marker discipline: default is '-m not memory_intensive'.
  Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.
- Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using --smoke;
plugin options may fail otherwise.
- Diagnostics: run 'poetry run devsynth doctor' for a quick environment check.
- Narrow scope: use '-k <expr>' and '-vv' to focus a failure.
- Segment large suites to localize failures and flakes:
  devsynth run-tests --target unit-tests --speed=fast --segment
--segment-size=50
- Limit failures early to speed iteration:
  poetry run devsynth run-tests --target unit-tests --speed=fast --maxfail=1
- Disable parallelism if xdist interaction is suspected:
  devsynth run-tests --target unit-tests --speed=fast --no-parallel
- Generate an HTML report for context (saved under test_reports/):
  devsynth run-tests --target unit-tests --speed=fast --report

Tests failed
