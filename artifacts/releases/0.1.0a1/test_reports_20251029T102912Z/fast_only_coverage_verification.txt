2025-10-28 09:25:06,753 - devsynth.testing.run_tests - INFO - test collection cache hit for target=all-tests (fast)
2025-10-28 09:29:54,614 - devsynth.testing.run_tests - INFO - Coverage data file detected at .coverage (196608 bytes)
2025-10-28 09:30:09,823 - devsynth.testing.run_tests - INFO - Coverage HTML report generated
2025-10-28 09:30:15,142 - devsynth.testing.run_tests - INFO - Coverage JSON report generated
============================= test session starts 
==============================platform darwin -- Python 3.12.12, pytest-8.4.2, 
pluggy-1.6.0benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False
min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 
warmup=False warmup_iterations=100000)rootdir: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynthconfigfile: 
pytest.iniplugins: mock-3.15.1, asyncio-1.2.0, anyio-4.11.0, html-4.1.1, 
xdist-3.8.0, langsmith-0.4.37, metadata-3.1.1, Faker-37.11.0, benchmark-5.1.0, 
hypothesis-6.142.3, bdd-8.1.0, rerunfailures-16.1, cov-7.0.0asyncio: 
mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=functioncollected 2949 
itemstests/unit/adapters/cli/test_typer_adapter.py ....                       [ 
0%]tests/unit/adapters/issues/test_github_adapter.py .                      [  
0%]tests/unit/adapters/issues/test_jira_adapter.py .                        [  
0%]tests/unit/adapters/llm/test_llm_adapter.py .........                    [  
0%]tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py ....          [  
0%]tests/unit/adapters/llm/test_mock_llm_adapter_sync.py ........           [  
0%]tests/unit/adapters/test_agent_adapter.py ...................            [  
1%]tests/unit/adapters/test_backend_resource_gates.py sss                   [  
1%]tests/unit/adapters/test_chromadb_memory_store_unit.py ......            [  
1%]tests/unit/adapters/test_fake_memory_store.py ..                         [  
1%]tests/unit/adapters/test_github_project_adapter.py ........              [  
2%]tests/unit/adapters/test_jira_adapter.py ...                             [  
2%]tests/unit/adapters/test_onnx_runtime_adapter.py sssssss                 [  
2%]tests/unit/adapters/test_provider_safe_defaults.py .....                 [  
2%]tests/unit/adapters/test_provider_stub.py ...                            [  
2%]tests/unit/adapters/test_provider_system.py ............................ [  
3%]..........                                                               [  
4%]tests/unit/adapters/test_provider_system_additional.py .FFF.FFFF........ [  
4%]...........                                                              [  
5%]tests/unit/adapters/test_provider_system_fallbacks_fast.py ...           [  
5%]tests/unit/adapters/test_provider_system_resilience.py ...               [  
5%]tests/unit/adapters/test_resource_gating_seams.py s.                     [  
5%]tests/unit/adapters/test_storage_adapter_protocol.py .                   [  
5%]tests/unit/agents/test_alignment_metrics_tool.py ..                      [  
5%]tests/unit/agents/test_doctor_tool.py ..                                 [  
5%]tests/unit/agents/test_multi_agent_coordinator.py .                      [  
5%]tests/unit/agents/test_run_tests_tool.py ..                              [  
5%]tests/unit/agents/test_security_audit_tool.py ..                         [  
5%]tests/unit/agents/test_test_generator.py ..........                      [  
6%]tests/unit/agents/test_tool_sandbox.py ....                              [  
6%]tests/unit/agents/test_tools.py ...                                      [  
6%]tests/unit/agents/test_wsde_team_coordinator_strict.py ..                [  
6%]tests/unit/api/test_fastapi_testclient_import.py .                       [  
6%]tests/unit/api/test_public_api_contract.py ..                            [  
6%]tests/unit/application/agents/test_base_agent.py ...........             [  
6%]tests/unit/application/agents/test_test_agent_integration.py .           [  
6%]tests/unit/application/agents/test_validation_agent.py .....             [  
6%]tests/unit/application/agents/test_validation_agent_decision.py .....    [  
7%]tests/unit/application/agents/test_wsde_memory_integration_fast.py .     [  
7%]tests/unit/application/cli/commands/test_config_cmd.py ....              [  
7%]tests/unit/application/cli/commands/test_doctor_cmd_typed.py .           [  
7%]tests/unit/application/cli/commands/test_doctor_no_ui_imports.py .       [  
7%]tests/unit/application/cli/commands/test_ingest_cli_command.py .         [  
7%]tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py . [
7%]                                                                         [  
7%]tests/unit/application/cli/commands/test_long_running_progress_timeline_bridg
e.py . [  7%]...                                                                
[  7%]tests/unit/application/cli/commands/test_module_imports.py ............. [
8%].F...................................                                    [  
9%]tests/unit/application/cli/commands/test_parse_feature_options_unit.py . [  
9%]...                                                                      [  
9%]tests/unit/application/cli/commands/test_run_pipeline_cmd.py ...         [  
9%]tests/unit/application/cli/commands/test_run_tests_cmd.py ....F......FFF [ 
10%]                                                                         [ 
10%]tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py EEEE [ 
10%]E                                                                        [ 
10%]tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_in
puts.py . [ 10%].FFFF                                                           
[ 10%]tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py
. [ 10%]FFFFFF                                                                  
[ 
10%]tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds
.py F [ 10%]FF                                                                  
[ 
10%]tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
F [ 10%]FFFFFF                                                                  
[ 10%]tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py EE   [
11%]tests/unit/application/cli/commands/test_run_tests_cmd_features.py EE    [ 
11%]tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py E   [ 
11%]tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py FF   [ 
11%]tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validat
ion.py E [ 11%]EEE                                                              
[ 11%]tests/unit/application/cli/commands/test_run_tests_cmd_markers.py EE     [
11%]tests/unit/application/cli/commands/test_run_tests_cmd_more.py EEE       [ 
11%]tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py 
E [ 11%]E                                                                       
[ 11%]tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py 
F [ 11%]F                                                                       
[ 11%]tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py EE [
11%]EE                                                                       [ 
11%]tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressi
ons.py F [ 11%]FF                                                               
[ 11%]tests/unit/application/cli/commands/test_run_tests_dummy.py .            [
11%]tests/unit/application/cli/commands/test_run_tests_features.py F         [ 
11%]tests/unit/application/cli/commands/test_run_tests_provider_defaults.py F [ 
11%]                                                                         [ 
11%]tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py E [ 
12%]E                                                                        [ 
12%]tests/unit/application/cli/commands/test_run_tests_subprocess.py F       [ 
12%]tests/unit/application/cli/commands/test_run_tests_validation.py FF      [ 
12%]tests/unit/application/cli/commands/test_security_audit_cmd.py ....      [ 
12%]tests/unit/application/cli/commands/test_testing_cmd.py ..........       [ 
12%]tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py ..      [ 
12%]tests/unit/application/cli/test_command_output_formatter.py .........    [ 
12%]tests/unit/application/cli/test_ingest_cmd.py ..                         [ 
13%]tests/unit/application/cli/test_long_running_progress.py .............   [ 
13%]tests/unit/application/cli/test_long_running_progress_deterministic.py . [ 
13%]....                                                                     [ 
13%]tests/unit/application/cli/test_output.py .........                      [ 
13%]tests/unit/application/cli/test_progress.py F                            [ 
14%]tests/unit/application/cli/test_progress_aliasing.py .........           [ 
14%]tests/unit/application/cli/test_requirements_commands.py ......          [ 
14%]tests/unit/application/cli/test_requirements_gathering.py .              [ 
14%]tests/unit/application/cli/test_run_tests_cmd.py FFFFF                   [ 
14%]tests/unit/application/cli/test_run_tests_cmd_options.py FFF             [ 
14%]tests/unit/application/cli/test_run_tests_cmd_smoke.py EEEE              [ 
14%]tests/unit/application/cli/test_setup_wizard.py EEEEEE                   [ 
15%]tests/unit/application/cli/test_setup_wizard_textual.py ..               [ 
15%]tests/unit/application/cli/test_sprint_cmd_types.py ...                  [ 
15%]tests/unit/application/code_analysis/test_analyzer.py .                  [ 
15%]tests/unit/application/code_analysis/test_ast_transformer.py ..........  [ 
15%]tests/unit/application/code_analysis/test_ast_workflow_integration.py .. [ 
15%]...                                                                      [ 
15%]tests/unit/application/code_analysis/test_project_state_analyzer.py .... [ 
16%].....                                                                    [ 
16%]tests/unit/application/code_analysis/test_project_state_analyzer_error_paths
.py F [ 16%]                                                                    
[ 16%]tests/unit/application/code_analysis/test_repo_analyzer.py ..            [
16%]tests/unit/application/code_analysis/test_self_analyzer.py ...........   [ 
16%]tests/unit/application/code_analysis/test_self_analyzer_error_paths.py . [ 
16%]                                                                         [ 
16%]tests/unit/application/code_analysis/test_transformer.py ...........     [ 
17%]tests/unit/application/code_analysis/test_transformer_basic.py .         [ 
17%]tests/unit/application/code_analysis/test_transformer_helpers.py ...     [ 
17%]tests/unit/application/collaboration/test_agent_collaboration_system.py . [ 
17%]..                                                                       [ 
17%]tests/unit/application/collaboration/test_collaborative_wsde_team_task_manag
ement.py . [ 17%].                                                              
[ 17%]tests/unit/application/collaboration/test_memory_utils_conversion.py .   [
17%]tests/unit/application/collaboration/test_message_protocol.py ...        [ 
17%]tests/unit/application/collaboration/test_peer_review_store.py ....      [ 
17%]tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py F.   [ 
17%]tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detec
tion.py . [ 17%]                                                                
[ 17%]tests/unit/application/collaboration/test_wsde_team_consensus_summary.py .
[ 17%]..F                                                                      [
17%]tests/unit/application/collaboration/test_wsde_team_consensus_utils.py . [ 
17%].                                                                        [ 
17%]tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py 
. [ 17%]                                                                        
[ 
17%]tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py
. [ 18%]                                                                        
[ 18%]tests/unit/application/documentation/test_documentation_fetcher_parsing.py
. [ 18%]...                                                                     
[ 18%]tests/unit/application/documentation/test_ingestion_search_variance.py . [
18%].                                                                        [ 
18%]tests/unit/application/edrr/coordinator/test_core.py ...FFFFFFFFFFFFFFFF [ 
18%]FFFF                                                                     [ 
18%]tests/unit/application/edrr/test_coordinator.py .                        [ 
19%]tests/unit/application/edrr/test_coordinator_core.py F                   [ 
19%]tests/unit/application/edrr/test_coordinator_reasoning.py FF             [ 
19%]tests/unit/application/edrr/test_edrr_coordinator_enhanced.py .          [ 
19%]tests/unit/application/edrr/test_edrr_phase_transitions_fast.py .        [ 
19%]tests/unit/application/edrr/test_persistence_module.py ..........        [ 
19%]tests/unit/application/edrr/test_phase_management_module.py ..FF....     [ 
19%]tests/unit/application/edrr/test_reasoning_loop_retries.py F             [ 
19%]tests/unit/application/edrr/test_recursion_termination.py ss             [ 
19%]tests/unit/application/edrr/test_sprint_planning.py ....                 [ 
20%]tests/unit/application/edrr/test_sprint_retrospective.py .....           [ 
20%]tests/unit/application/edrr/test_threshold_helpers.py .....              [ 
20%]tests/unit/application/ingestion/test_ingestion_pure.py ...              [ 
20%]tests/unit/application/ingestion/test_phases.py ..                       [ 
20%]tests/unit/application/llm/test_import_without_openai.py .F              [ 
20%]tests/unit/application/llm/test_lmstudio_health_check.py FF              [ 
20%]tests/unit/application/llm/test_lmstudio_offline_resilience.py ..        [ 
20%]tests/unit/application/llm/test_lmstudio_provider.py ...........FFFFFFFF [ 
21%]FF......                                                                 [ 
21%]tests/unit/application/llm/test_offline_provider.py ...                  [ 
21%]tests/unit/application/llm/test_openai_env_key_mock.py .                 [ 
21%]tests/unit/application/llm/test_openai_offline_resilience.py ....        [ 
21%]tests/unit/application/llm/test_provider_factory.py FF                   [ 
22%]tests/unit/application/llm/test_provider_factory_lmstudio_gating.py ...  [ 
22%]tests/unit/application/llm/test_provider_selection.py FF                 [ 
22%]tests/unit/application/memory/test_chromadb_store.py s                   [ 
22%]tests/unit/application/memory/test_chromadb_store_typed.py ss            [ 
22%]tests/unit/application/memory/test_circuit_breaker.py ..                 [ 
22%]tests/unit/application/memory/test_duckdb_store_schema_flags.py ss       [ 
22%]tests/unit/application/memory/test_error_logger.py ....                  [ 
22%]tests/unit/application/memory/test_execution_learning_integration.py .FF [ 
22%]..FFEEEEEEF.FF                                                           [ 
23%]tests/unit/application/memory/test_faiss_store.py ssss                   [ 
23%]tests/unit/application/memory/test_fast_in_memory_components.py ........ [ 
23%]                                                                         [ 
23%]tests/unit/application/memory/test_graph_memory_adapter.py .             [ 
23%]tests/unit/application/memory/test_lmdb_store.py ....                    [ 
23%]tests/unit/application/memory/test_memory_manager.py ...                 [ 
23%]tests/unit/application/memory/test_memory_system_adapter_unit.py ....... [ 
24%].                                                                        [ 
24%]tests/unit/application/memory/test_metadata_serialization_helpers.py ..F [ 
24%].                                                                        [ 
24%]tests/unit/application/memory/test_phase3_integration_system.py .F...F.. [ 
24%]..F.....EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE                              [ 
25%]tests/unit/application/memory/test_query_router.py ..F.                  [ 
26%]tests/unit/application/memory/test_rdflib_store_transactions.py ...      [ 
26%]tests/unit/application/memory/test_search_memory_fallback.py .           [ 
26%]tests/unit/application/memory/test_sync_manager_transactions.py F.       [ 
26%]tests/unit/application/memory/test_tiered_cache_termination.py ..        [ 
26%]tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py F       [ 
26%]tests/unit/application/memory/test_vector_memory_adapter_extra.py ss     [ 
26%]tests/unit/application/orchestration/test_dialectical_reasoner.py ...    [ 
26%]tests/unit/application/promises/test_agent_create_promise.py .           [ 
26%]tests/unit/application/promises/test_interface_not_implemented.py .      [ 
26%]tests/unit/application/promises/test_interface_pure.py ...               [ 
26%]tests/unit/application/prompts/test_auto_tuning_pure.py ...              [ 
26%]tests/unit/application/requirements/test_dialectical_reasoner.py ..F.... [ 
27%]..FFFF                                                                   [ 
27%]tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloa
ds.py . [ 27%].                                                                 
[ 27%]tests/unit/application/requirements/test_dialectical_reasoner_pure.py .. [
27%]..                                                                       [ 
27%]tests/unit/application/requirements/test_interactions.py ...             [ 
27%]tests/unit/application/requirements/test_requirement_service_dtos.py ..  [ 
27%]tests/unit/application/requirements/test_wizard.py ...                   [ 
27%]tests/unit/application/sprint/test_planning.py .                         [ 
27%]tests/unit/application/test_documentation_fetcher.py ..                  [ 
27%]tests/unit/application/testing/test_enhanced_test_collector.py ..EEE.EEE [ 
28%]E..EEEEE..EF.....F....                                                   [ 
28%]tests/unit/application/utils/test_extras_helper.py ...                   [ 
28%]tests/unit/behavior/test_alignment_metrics_steps_unit.py F               [ 
29%]tests/unit/behavior/test_analyze_commands_steps_unit.py ..               [ 
29%]tests/unit/cli/test_cli_entry.py .                                       [ 
29%]tests/unit/cli/test_cli_error_handling.py F                              [ 
29%]tests/unit/cli/test_cli_help.py .                                        [ 
29%]tests/unit/cli/test_command_module_loading.py .                          [ 
29%]tests/unit/cli/test_command_registry.py F.                               [ 
29%]tests/unit/cli/test_completion_progress.py .                             [ 
29%]tests/unit/cli/test_entry_points_help.py ...                             [ 
29%]tests/unit/cli/test_help_examples.py ..                                  [ 
29%]tests/unit/cli/test_import_gating.py ..                                  [ 
29%]tests/unit/cli/test_init_features_option.py ..                           [ 
29%]tests/unit/cli/test_key_commands_help.py .......                         [ 
29%]tests/unit/cli/test_logging_flags.py FFF                                 [ 
29%]tests/unit/cli/test_mvu_commands.py ..                                   [ 
30%]tests/unit/cli/test_mvuu_command_registration.py .                       [ 
30%]tests/unit/cli/test_mvuu_dashboard_smoke.py F                            [ 
30%]tests/unit/cli/test_mvuu_dashboard_telemetry.py ....                     [ 
30%]tests/unit/cli/test_run_tests_regression.py F                            [ 
30%]tests/unit/cli/test_version.py .                                         [ 
30%]tests/unit/config/test_config_llm_env.py F                               [ 
30%]tests/unit/config/test_exception_handling.py .....                       [ 
30%]tests/unit/config/test_feature_flag_defaults.py ..                       [ 
30%]tests/unit/config/test_provider_env.py ....                              [ 
30%]tests/unit/config/test_provider_env_apply_and_parse.py ...               [ 
30%]tests/unit/config/test_provider_env_behavior.py ....                     [ 
30%]tests/unit/config/test_provider_env_bool_parsing_edges.py ...            [ 
31%]tests/unit/config/test_provider_env_with_test_defaults.py ..             [ 
31%]tests/unit/config/test_unified_loader.py .                               [ 
31%]tests/unit/core/mvu/test_api.py ..                                       [ 
31%]tests/unit/core/mvu/test_atomic_rewrite.py .                             [ 
31%]tests/unit/core/mvu/test_linter.py ......                                [ 
31%]tests/unit/core/mvu/test_mvuu_schema_validation.py .                     [ 
31%]tests/unit/core/mvu/test_report.py ..                                    [ 
31%]tests/unit/core/mvu/test_storage.py ..                                   [ 
31%]tests/unit/core/mvu/test_validator.py ...                                [ 
31%]tests/unit/core/test_config_loader.py .                                  [ 
31%]tests/unit/core/test_config_loader_json_types.py ...                     [ 
31%]tests/unit/core/test_config_loader_mvu.py F                              [ 
31%]tests/unit/core/test_config_loader_optional_deps.py ...                  [ 
32%]tests/unit/core/test_config_loader_validation.py ....................... [ 
32%]....                                                                     [ 
32%]tests/unit/core/test_deterministic_fixtures.py ...                       [ 
33%]tests/unit/core/test_mvu.py ..                                           [ 
33%]tests/unit/deployment/test_bootstrap_script.py FFF                       [ 
33%]tests/unit/deployment/test_deployment_scripts.py ..                      [ 
33%]tests/unit/deployment/test_enforcement.py ..                             [ 
33%]tests/unit/deployment/test_health_check_smoke.py FFFFFF                  [ 
33%]tests/unit/deployment/test_scripts_dir.py ..                             [ 
33%]tests/unit/deployment/test_security_hardening.py ......                  [ 
33%]tests/unit/devsynth/test_consensus.py .....                              [ 
33%]tests/unit/devsynth/test_fallback_reliability.py ..                      [ 
34%]tests/unit/devsynth/test_logger.py ...                                   [ 
34%]tests/unit/devsynth/test_metrics.py ....                                 [ 
34%]tests/unit/devsynth/test_simple_addition.py ..                           [ 
34%]tests/unit/docs/test_dialectical_audit.py ..                             [ 
34%]tests/unit/domain/interfaces/test_interfaces.py ...                      [ 
34%]tests/unit/domain/models/test_agent_coverage.py ..                       [ 
34%]tests/unit/domain/models/test_memetic_unit.py ...........                [ 
34%]tests/unit/domain/models/test_project.py ...                             [ 
35%]tests/unit/domain/models/test_project_model.py ............              [ 
35%]tests/unit/domain/models/test_wsde.py ...F...FFFF                        [ 
35%]tests/unit/domain/models/test_wsde_base_methods.py ..                    [ 
35%]tests/unit/domain/models/test_wsde_code_improvements.py ...              [ 
36%]tests/unit/domain/models/test_wsde_decision_making.py ......             [ 
36%]tests/unit/domain/models/test_wsde_dialectical_helpers.py ...            [ 
36%]tests/unit/domain/models/test_wsde_dialectical_typing.py .               [ 
36%]tests/unit/domain/models/test_wsde_dialectical_workflow.py F.            [ 
36%]tests/unit/domain/models/test_wsde_dynamic_workflows.py .F               [ 
36%]tests/unit/domain/models/test_wsde_enhanced_dialectical.py ......        [ 
36%]tests/unit/domain/models/test_wsde_knowledge.py .....                    [ 
36%]tests/unit/domain/models/test_wsde_roles_personas.py ......              [ 
37%]tests/unit/domain/models/test_wsde_security_checks.py FFF                [ 
37%]tests/unit/domain/models/test_wsde_solution_analysis.py ....             [ 
37%]tests/unit/domain/models/test_wsde_strategies.py .F.                     [ 
37%]tests/unit/domain/models/test_wsde_team.py ..F..F                        [ 
37%]tests/unit/domain/models/test_wsde_utils.py ......F.                     [ 
37%]tests/unit/domain/models/test_wsde_voting_logic.py ....                  [ 
38%]tests/unit/domain/test_code_analysis_interfaces.py ...                   [ 
38%]tests/unit/domain/test_wsde_expertise_score.py F                         [ 
38%]tests/unit/domain/test_wsde_facade.py .F                                 [ 
38%]tests/unit/domain/test_wsde_facade_roles.py FF                           [ 
38%]tests/unit/domain/test_wsde_peer_review_workflow.py ..                   [ 
38%]tests/unit/domain/test_wsde_phase_role_rotation.py .F.                   [ 
38%]tests/unit/domain/test_wsde_primus_selection.py ..FFF.F                  [ 
38%]tests/unit/domain/test_wsde_team.py .FFFF....F.FF                        [ 
39%]tests/unit/domain/test_wsde_voting_logic.py FFFFFFF....                  [ 
39%]tests/unit/fallback/test_retry_counts.py ..                              [ 
39%]tests/unit/fallback/test_retry_predicates.py ..                          [ 
39%]tests/unit/general/test_agent_coordinator.py .......                     [ 
39%]tests/unit/general/test_agent_models.py ....                             [ 
40%]tests/unit/general/test_agent_system.py ............                     [ 
40%]tests/unit/general/test_anthropic_provider_unit.py .....                 [ 
40%]tests/unit/general/test_api.py ..                                        [ 
40%]tests/unit/general/test_api_health.py FF                                 [ 
40%]tests/unit/general/test_atomic_rewrite_cli.py ...                        [ 
40%]tests/unit/general/test_backend_resource_flags.py ...F                   [ 
40%]tests/unit/general/test_base.py .                                        [ 
40%]tests/unit/general/test_chroma_db_adapter.py .FFFFFF                     [ 
41%]tests/unit/general/test_chromadb_store.py ssssss                         [ 
41%]tests/unit/general/test_cli_commands.py ..                               [ 
41%]tests/unit/general/test_code_analysis_interface.py ...                   [ 
41%]tests/unit/general/test_code_analysis_models.py ..                       [ 
41%]tests/unit/general/test_code_analyzer.py ....                            [ 
41%]tests/unit/general/test_config_loader.py .....                           [ 
41%]tests/unit/general/test_config_settings.py .................             [ 
42%]tests/unit/general/test_core_config_loader.py ...                        [ 
42%]tests/unit/general/test_core_values.py ...                               [ 
42%]tests/unit/general/test_core_workflows.py ...........                    [ 
43%]tests/unit/general/test_delegate_task_disabled.py .                      [ 
43%]tests/unit/general/test_dialectical_reasoner.py F.FF.                    [ 
43%]tests/unit/general/test_documentation_fetcher.py .                       [ 
43%]tests/unit/general/test_dpg_flag.py FFs                                  [ 
43%]tests/unit/general/test_edrr_cycle_cmd.py .......                        [ 
43%]tests/unit/general/test_edrr_manifest_string.py .                        [ 
43%]tests/unit/general/test_exception_logging.py .                           [ 
43%]tests/unit/general/test_exceptions.py .....................              [ 
44%]tests/unit/general/test_fallback_utils.py .                              [ 
44%]tests/unit/general/test_ingest_cmd.py .....................F             [ 
45%]tests/unit/general/test_ingestion_edrr_integration.py .                  [ 
45%]tests/unit/general/test_ingestion_type_hints.py s                        [ 
45%]tests/unit/general/test_inspect_config_cmd.py ......                     [ 
45%]tests/unit/general/test_isolation.py ......                              [ 
45%]tests/unit/general/test_isolation_auto_marking.py ..                     [ 
45%]tests/unit/general/test_kuzu_adapter.py ssss                             [ 
45%]tests/unit/general/test_kuzu_embedded_missing.py .                       [ 
45%]tests/unit/general/test_langgraph_adapter.py ............                [ 
46%]tests/unit/general/test_llm_provider_selection.py FF                     [ 
46%]tests/unit/general/test_lmstudio_integration_regression.py ..F.F..       [ 
46%]tests/unit/general/test_lmstudio_service.py s                            [ 
46%]tests/unit/general/test_logger.py ..                                     [ 
46%]tests/unit/general/test_logging_setup.py ....                            [ 
46%]tests/unit/general/test_logging_setup_idempotent.py ...                  [ 
47%]tests/unit/general/test_memory_models.py .....                           [ 
47%]tests/unit/general/test_memory_store.py .                                [ 
47%]tests/unit/general/test_memory_system.py ....................            [ 
47%]tests/unit/general/test_memory_system_with_chromadb.py ssss              [ 
48%]tests/unit/general/test_methodology_logging.py .                         [ 
48%]tests/unit/general/test_multi_agent_adapter_workflow.py F.               [ 
48%]tests/unit/general/test_mvu_exec_cli.py ..                               [ 
48%]tests/unit/general/test_mvu_exec_cmd.py ..                               [ 
48%]tests/unit/general/test_mvu_init_cmd.py .                                [ 
48%]tests/unit/general/test_mvu_lint_cli.py FF                               [ 
48%]tests/unit/general/test_mvuu_dashboard_cli.py .                          [ 
48%]tests/unit/general/test_mypy_config.py ..                                [ 
48%]tests/unit/general/test_no_devsynth_dir_creation.py ..                   [ 
48%]tests/unit/general/test_onnx_port.py .                                   [ 
48%]tests/unit/general/test_path_restrictions.py F.                          [ 
48%]tests/unit/general/test_ports_with_fixtures.py E                         [ 
48%]tests/unit/general/test_primus_selection.py ..FF.F                       [ 
48%]tests/unit/general/test_project_yaml.py .....                            [ 
49%]tests/unit/general/test_promise_agent.py ...........                     [ 
49%]tests/unit/general/test_promise_system.py ..............                 [ 
49%]tests/unit/general/test_provider_logging.py ss                           [ 
49%]tests/unit/general/test_requirement_models.py .....                      [ 
50%]tests/unit/general/test_requirement_repository_interface.py .            [ 
50%]tests/unit/general/test_requirement_repository_port_interface.py ..      [ 
50%]tests/unit/general/test_requirement_service.py .....                     [ 
50%]tests/unit/general/test_resource_markers.py ..F.sF                       [ 
50%]tests/unit/general/test_retry_failure_scenarios.py ..                    [ 
50%]tests/unit/general/test_speed_option.py .                                [ 
50%]tests/unit/general/test_sync_manager_persistence.py .                    [ 
50%]tests/unit/general/test_template_location.py ..                          [ 
50%]tests/unit/general/test_test_first_metrics.py .....                      [ 
51%]tests/unit/general/test_token_tracker.py ......                          [ 
51%]tests/unit/general/test_unified_agent_code_prompt.py .                   [ 
51%]tests/unit/general/test_unified_config_loader.py .......                 [ 
51%]tests/unit/general/test_unit_cli_commands.py .                           [ 
51%]tests/unit/general/test_ux_bridge.py FF                                  [ 
51%]tests/unit/general/test_workflow.py F.F...                               [ 
51%]tests/unit/general/test_workflow_models.py ....                          [ 
51%]tests/unit/general/test_wsde_dynamic_roles.py .                          [ 
51%]tests/unit/general/test_wsde_model.py ..                                 [ 
52%]tests/unit/general/test_wsde_role_mapping.py F                           [ 
52%]tests/unit/general/test_wsde_team_extended.py .....F.F.FFFFFFFFFFFFFF    [ 
52%]tests/unit/general/test_wsde_team_voting_invalid.py FF                   [ 
52%]tests/unit/general/test_wsde_voting.py FFFF                              [ 
53%]tests/unit/general/test_wsde_voting_mechanisms.py FFFFFF                 [ 
53%]tests/unit/infrastructure/test_test_infrastructure_sanity.py .           [ 
53%]tests/unit/integrations/test_autoresearch_client.py ...                  [ 
53%]tests/unit/interface/test_agent_api_fastapi_guard.py .                   [ 
53%]tests/unit/interface/test_agentapi_enhanced.py ...................       [ 
54%]tests/unit/interface/test_agentapi_enhanced_bridge.py ssss               [ 
54%]tests/unit/interface/test_agentapi_rate_limit_progress.py ssss           [ 
54%]tests/unit/interface/test_api_endpoints.py ..F                           [ 
54%]tests/unit/interface/test_cli_components.py .                            [ 
54%]tests/unit/interface/test_cli_progress_indicator.py ...                  [ 
54%]tests/unit/interface/test_cli_prompt_toolkit_bridge.py ...               [ 
54%]tests/unit/interface/test_cli_uxbridge_noninteractive.py ..              [ 
54%]tests/unit/interface/test_command_output.py ....                         [ 
54%]tests/unit/interface/test_dpg_ui.py ...                                  [ 
54%]tests/unit/interface/test_enhanced_error_handler.py ..                   [ 
55%]tests/unit/interface/test_mvuu_dashboard.py ..........                   [ 
55%]tests/unit/interface/test_nicegui_bridge.py ....                         [ 
55%]tests/unit/interface/test_nicegui_webui.py ......                        [ 
55%]tests/unit/interface/test_output_formatter_command_options_fast.py ...   [ 
55%]tests/unit/interface/test_output_formatter_core_behaviors.py ........... [ 
56%]                                                                         [ 
56%]tests/unit/interface/test_output_formatter_error_rendering_fast.py ...   [ 
56%]tests/unit/interface/test_output_formatter_fallbacks.py ....             [ 
56%]tests/unit/interface/test_output_formatter_structured_fast.py .......... [ 
56%].................                                                        [ 
57%]tests/unit/interface/test_output_sanitization.py .                       [ 
57%]tests/unit/interface/test_progress_helpers.py ..                         [ 
57%]tests/unit/interface/test_progress_utils.py ......                       [ 
57%]tests/unit/interface/test_prompt_toolkit_adapter.py ...                  [ 
57%]tests/unit/interface/test_research_telemetry.py ......                   [ 
57%]tests/unit/interface/test_textual_ux_bridge.py ....s                     [ 
58%]tests/unit/interface/test_ux_bridge_coverage.py .......                  [ 
58%]tests/unit/interface/test_uxbridge_aliases.py .F                         [ 
58%]tests/unit/interface/test_webui_behavior_checklist_fast.py FFFFFFFFFFFFF [ 
58%]FFFFFFFFFFFFF                                                            [ 
59%]tests/unit/interface/test_webui_bootstrap_fast.py FFF                    [ 
59%]tests/unit/interface/test_webui_bridge_aa_coverage.py FF                 [ 
59%]tests/unit/interface/test_webui_bridge_cli_parity.py .                   [ 
59%]tests/unit/interface/test_webui_bridge_fast_suite.py FF.......           [ 
59%]tests/unit/interface/test_webui_bridge_handshake.py ....FF...            [ 
60%]tests/unit/interface/test_webui_bridge_normalize.py ....                 [ 
60%]tests/unit/interface/test_webui_bridge_progress.py .F..F....             [ 
60%]tests/unit/interface/test_webui_bridge_require_streamlit.py ..           [ 
60%]tests/unit/interface/test_webui_bridge_routing.py .....                  [ 
60%]tests/unit/interface/test_webui_bridge_spec_alignment.py .F.F..          [ 
61%]tests/unit/interface/test_webui_bridge_state_fast.py .F.                 [ 
61%]tests/unit/interface/test_webui_bridge_targeted.py ......F.              [ 
61%]tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py ....... [ 
61%]                                                                         [ 
61%]tests/unit/interface/test_webui_commands.py F..........                  [ 
61%]tests/unit/interface/test_webui_dashboard_toggles_fast.py EE             [ 
62%]tests/unit/interface/test_webui_display_and_layout.py FEEEEEEEEEEEEEEEEE [ 
62%]EEEEEEEEEEEEEEEEEEEE                                                     [ 
63%]tests/unit/interface/test_webui_display_guidance.py EEEE                 [ 
63%]tests/unit/interface/test_webui_enhanced.py FFFFFFFFF                    [ 
63%]tests/unit/interface/test_webui_handle_command_errors.py EEEEEEE         [ 
64%]tests/unit/interface/test_webui_layout_and_display_branching.py FFFFFFFF [ 
64%]FFFFFF                                                                   [ 
64%]tests/unit/interface/test_webui_layout_and_messaging.py EEEEEEEEEEEEEEEE [ 
65%]EEE                                                                      [ 
65%]tests/unit/interface/test_webui_lazy_loader_fast.py FFF                  [ 
65%]tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py EFEE   [ 
65%]tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py FE..        [ 
65%]tests/unit/interface/test_webui_progress.py FFFFF                        [ 
65%]tests/unit/interface/test_webui_progress_cascade_fast.py EEEEEE          [ 
65%]tests/unit/interface/test_webui_progress_time.py s                       [ 
65%]tests/unit/interface/test_webui_rendering.py ....F..F..F..F.......F..... [ 
66%].                                                                        [ 
66%]tests/unit/interface/test_webui_rendering_module.py sss                  [ 
66%]tests/unit/interface/test_webui_rendering_progress.py ..                 [ 
67%]tests/unit/interface/test_webui_require_streamlit.py FF                  [ 
67%]tests/unit/interface/test_webui_requirements_wizard.py FFFFFFFF          [ 
67%]tests/unit/interface/test_webui_routing.py .......                       [ 
67%]tests/unit/interface/test_webui_run_edge_cases.py FFFFFFF.               [ 
67%]tests/unit/interface/test_webui_run_fast.py E                            [ 
67%]tests/unit/interface/test_webui_simulations_fast.py FFFF.F.              [ 
68%]tests/unit/interface/test_webui_state_errors.py .                        [ 
68%]tests/unit/interface/test_webui_streamlit_free_progress_fast.py EEE      [ 
68%]tests/unit/interface/test_webui_streamlit_free_regressions.py F.FFFF.... [ 
68%]........F.                                                               [ 
68%]tests/unit/interface/test_webui_streamlit_stub.py EFEEEE                 [ 
69%]tests/unit/interface/test_webui_targeted_branches.py EEEEEEEEE           [ 
69%]tests/unit/interface/webui/test_rendering.py ....F..F..F..F.......F..... [ 
70%].                                                                        [ 
70%]tests/unit/llm/test_lmstudio_provider.py .....FFF.F.....FFFFF            [ 
71%]tests/unit/llm/test_openai_provider.py ..sF..FF.........FFFFF            [ 
71%]tests/unit/llm/test_openrouter_provider.py ..F.F.FF.F........FFFF        [ 
72%]tests/unit/logging/test_logging_setup.py .......................         [ 
73%]tests/unit/logging/test_logging_setup_additional_paths.py .......        [ 
73%]tests/unit/logging/test_logging_setup_branches.py .....                  [ 
73%]tests/unit/logging/test_logging_setup_configuration.py ...               [ 
73%]tests/unit/logging/test_logging_setup_configure_logging.py .....F...F    [ 
74%]tests/unit/logging/test_logging_setup_contexts.py ....                   [ 
74%]tests/unit/logging/test_logging_setup_invariants.py ......               [ 
74%]tests/unit/logging/test_logging_setup_levels.py ...                      [ 
74%]tests/unit/logging/test_logging_setup_retention.py F...FF                [ 
74%]tests/unit/memory/test_issue3_regression_guard.py .                      [ 
74%]tests/unit/memory/test_layered_cache.py ...                              [ 
75%]tests/unit/memory/test_layered_cache_runtime_protocol.py ...             [ 
75%]tests/unit/memory/test_sync_manager_protocol.py F.FFFFFF.F               [ 
75%]tests/unit/memory/test_sync_manager_protocol_runtime.py ...........      [ 
75%]tests/unit/memory/test_sync_manager_transaction_failure.py .             [ 
75%]tests/unit/memory/test_transaction_lifecycle_failures.py ...             [ 
75%]tests/unit/methodology/edrr/test_reasoning_loop.py ..                    [ 
76%]tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py . [ 
76%]......                                                                   [ 
76%]tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py . [ 
76%].............                                                            [ 
76%]tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py ........ [ 
77%]                                                                         [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py ..    [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_invariants.py .........  [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_regressions.py ...       [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_retry.py .........       [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py ....       [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py ..     [ 
77%]tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py ....... [ 
78%]                                                                         [ 
78%]tests/unit/methodology/test_adhoc_adapter.py ..                          [ 
78%]tests/unit/methodology/test_dialectical_reasoner_termination.py ..       [ 
78%]tests/unit/methodology/test_dialectical_reasoning.py FFF                 [ 
78%]tests/unit/methodology/test_dialectical_reasoning_loop.py FFF            [ 
78%]tests/unit/methodology/test_edrr_coordinator.py ..                       [ 
78%]tests/unit/methodology/test_kanban_adapter.py ..                         [ 
78%]tests/unit/methodology/test_milestone_adapter.py ..                      [ 
78%]tests/unit/methodology/test_reasoning_loop_time_budget.py F              [ 
78%]tests/unit/methodology/test_sprint_adapter.py ...F...                    [ 
79%]tests/unit/methodology/test_sprint_hooks.py FF                           [ 
79%]tests/unit/orchestration/test_graph_transitions_and_controls.py F.FFF    [ 
79%]tests/unit/policies/test_verify_security_policy.py ..                    [ 
79%]tests/unit/providers/test_provider_contract.py .                         [ 
79%]tests/unit/providers/test_provider_stub_offline.py F                     [ 
79%]tests/unit/providers/test_provider_system_additional.py .........        [ 
79%]tests/unit/providers/test_provider_system_branches.py .................. [ 
80%]....................                                                     [ 
81%]tests/unit/providers/test_resource_gating_meta.py ..                     [ 
81%]tests/unit/requirements/test_dialectical_reasoner_determinism.py ..F..   [ 
81%]tests/unit/retrieval/test_backend_gating_smoke.py sss.                   [ 
81%]tests/unit/scripts/test_analyze_test_dependencies.py ........F...        [ 
81%]tests/unit/scripts/test_audit_testing_scripts.py .............           [ 
82%]tests/unit/scripts/test_auto_issue_comment.py ...                        [ 
82%]tests/unit/scripts/test_benchmark_test_execution.py .F.F......           [ 
82%]tests/unit/scripts/test_check_internal_links.py ..                       [ 
82%]tests/unit/scripts/test_enhanced_test_parser.py ...                      [ 
82%]tests/unit/scripts/test_enhanced_test_parser_marker_parity.py F          [ 
82%]tests/unit/scripts/test_examples_smoke_script.py ..                      [ 
82%]tests/unit/scripts/test_find_syntax_errors.py FF                         [ 
83%]tests/unit/scripts/test_gen_ref_pages.py .                               [ 
83%]tests/unit/scripts/test_generate_quality_report.py .....FF.              [ 
83%]tests/unit/scripts/test_run_all_tests_wrapper.py ...                     [ 
83%]tests/unit/scripts/test_security_ops.py ....                             [ 
83%]tests/unit/scripts/test_security_scan_script.py .                        [ 
83%]tests/unit/scripts/test_verify_coverage_threshold.py ..                  [ 
83%]tests/unit/scripts/test_verify_mvuu_references.py ....                   [ 
83%]tests/unit/scripts/test_verify_release_state.py ...........              [ 
84%]tests/unit/scripts/test_verify_test_markers.py .F....                    [ 
84%]tests/unit/scripts/test_verify_test_markers_cli.py ..                    [ 
84%]tests/unit/scripts/test_verify_test_markers_cross_check.py .             [ 
84%]tests/unit/scripts/test_wsde_edrr_simulation.py .                        [ 
84%]tests/unit/security/test_api_authentication.py .....                     [ 
84%]tests/unit/security/test_auth_and_encryption_defaults.py .....           [ 
84%]tests/unit/security/test_authentication_optional_dependency.py .         [ 
84%]tests/unit/security/test_authorization_checks.py ..                      [ 
84%]tests/unit/security/test_deployment_coverage.py ......                   [ 
85%]tests/unit/security/test_encryption.py ..........                        [ 
85%]tests/unit/security/test_logging_redaction.py ..                         [ 
85%]tests/unit/security/test_memory_encryption.py ...                        [ 
85%]tests/unit/security/test_policy_audit.py FF                              [ 
85%]tests/unit/security/test_review.py ...                                   [ 
85%]tests/unit/security/test_sanitization.py ...........                     [ 
86%]tests/unit/security/test_security_audit.py ....F                         [ 
86%]tests/unit/security/test_security_audit_cmd.py ...                       [ 
86%]tests/unit/security/test_security_flags_env.py ......                    [ 
86%]tests/unit/security/test_tls_config.py ..............                    [ 
87%]tests/unit/security/test_validation.py ................................. [ 
88%].....                                                                    [ 
88%]tests/unit/specifications/test_mvuu_config_schema_validation.py F        [ 
88%]tests/unit/test_cli.py ..s.ss.                                           [ 
88%]tests/unit/test_sentinel_speed_markers.py .                              [ 
88%]tests/unit/test_simple_addition.py ...                                   [ 
88%]tests/unit/test_verify_test_organization_sentinel.py .                   [ 
88%]tests/unit/testing/test_collect_behavior_fallback.py F                   [ 
88%]tests/unit/testing/test_collect_cache_sanitize.py .F                     [ 
88%]tests/unit/testing/test_collect_synthesize_on_empty.py F                 [ 
89%]tests/unit/testing/test_collect_tests_cache_bad_json.py F                [ 
89%]tests/unit/testing/test_collect_tests_cache_invalidation.py FFF          [ 
89%]tests/unit/testing/test_collect_tests_cache_ttl.py FF                    [ 
89%]tests/unit/testing/test_collect_tests_with_cache_additional_paths.py FFF [ 
89%]F                                                                        [ 
89%]tests/unit/testing/test_collect_tests_with_cache_fallback.py FF          [ 
89%]tests/unit/testing/test_coverage_segmentation_simulation.py ..           [ 
89%]tests/unit/testing/test_deterministic_seed_fixture.py .                  [ 
89%]tests/unit/testing/test_env_ttl_and_sanitize.py ..                       [ 
89%]tests/unit/testing/test_failure_tips.py .                                [ 
89%]tests/unit/testing/test_html_report_artifacts.py F                       [ 
89%]tests/unit/testing/test_mutation_testing.py ....................F        [ 
90%]tests/unit/testing/test_run_tests.py ..F.F                               [ 
90%]tests/unit/testing/test_run_tests_additional_coverage.py .......F        [ 
90%]tests/unit/testing/test_run_tests_additional_error_paths.py FFF          [ 
90%]tests/unit/testing/test_run_tests_artifacts.py .......F.F                [ 
91%]tests/unit/testing/test_run_tests_benchmark_warning.py F                 [ 
91%]tests/unit/testing/test_run_tests_cache_prune_and_tips.py .F             [ 
91%]tests/unit/testing/test_run_tests_cache_pruning.py F                     [ 
91%]tests/unit/testing/test_run_tests_cli_helpers_focus.py .FFFF.            [ 
91%]tests/unit/testing/test_run_tests_cli_invocation.py FF.F.FF...FFF        [ 
92%]tests/unit/testing/test_run_tests_collection_cache.py .                  [ 
92%]tests/unit/testing/test_run_tests_coverage_artifacts.py ....F..          [ 
92%]tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py .      [ 
92%]tests/unit/testing/test_run_tests_coverage_short_circuit.py .            [ 
92%]tests/unit/testing/test_run_tests_coverage_status.py ......              [ 
92%]tests/unit/testing/test_run_tests_coverage_uplift.py ...                 [ 
92%]tests/unit/testing/test_run_tests_extra.py FF                            [ 
92%]tests/unit/testing/test_run_tests_extra_marker.py FF                     [ 
92%]tests/unit/testing/test_run_tests_extra_marker_passthrough.py F          [ 
92%]tests/unit/testing/test_run_tests_extra_paths.py FFF                     [ 
92%]tests/unit/testing/test_run_tests_failure_tips.py F                      [ 
92%]tests/unit/testing/test_run_tests_keyword_exec.py .                      [ 
93%]tests/unit/testing/test_run_tests_keyword_filter.py FF                   [ 
93%]tests/unit/testing/test_run_tests_keyword_filter_empty.py F              [ 
93%]tests/unit/testing/test_run_tests_logic.py ..FFFFF                       [ 
93%]tests/unit/testing/test_run_tests_main_function.py ...........           [ 
93%]tests/unit/testing/test_run_tests_main_logic.py ....FF..FF.FFFFF.FF      [ 
94%]tests/unit/testing/test_run_tests_marker_fallback.py ..                  [ 
94%]tests/unit/testing/test_run_tests_marker_merge.py ..                     [ 
94%]tests/unit/testing/test_run_tests_module.py .FFFFFF.F..                  [ 
94%]tests/unit/testing/test_run_tests_no_xdist_assertions.py F               [ 
94%]tests/unit/testing/test_run_tests_option_parsing.py ...                  [ 
95%]tests/unit/testing/test_run_tests_orchestration.py .FFFF..               [ 
95%]tests/unit/testing/test_run_tests_parallel_flags.py F                    [ 
95%]tests/unit/testing/test_run_tests_parallel_no_cov.py F                   [ 
95%]tests/unit/testing/test_run_tests_plugin_env.py ....                     [ 
95%]tests/unit/testing/test_run_tests_plugin_timeouts.py F.                  [ 
95%]tests/unit/testing/test_run_tests_pytest_cov_plugin.py ........          [ 
95%]tests/unit/testing/test_run_tests_pytest_plugins_bdd.py ......F          [ 
96%]tests/unit/testing/test_run_tests_report.py F                            [ 
96%]tests/unit/testing/test_run_tests_returncode5_success.py F               [ 
96%]tests/unit/testing/test_run_tests_sanitize_node_ids.py ...               [ 
96%]tests/unit/testing/test_run_tests_segmentation.py F                      [ 
96%]tests/unit/testing/test_run_tests_segmentation_helpers.py ............   [ 
96%]tests/unit/testing/test_run_tests_segmented.py ...                       [ 
96%]tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py F [ 
96%]                                                                         [ 
96%]tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py F       [ 
96%]tests/unit/testing/test_run_tests_segmented_empty_node_ids.py F          [ 
96%]tests/unit/testing/test_run_tests_segmented_failure_paths.py .F          [ 
96%]tests/unit/testing/test_run_tests_segmented_failures.py ....             [ 
97%]tests/unit/testing/test_run_tests_segmented_orchestration.py ...         [ 
97%]tests/unit/testing/test_run_tests_segmented_report_flag.py .             [ 
97%]tests/unit/testing/test_run_tests_speed_keyword_loop.py .                [ 
97%]tests/unit/testing/test_run_tests_speed_selection.py ....                [ 
97%]tests/unit/testing/test_sanitize_node_ids.py ..                          [ 
97%]tests/unit/testing/test_sanitize_node_ids_minimal.py .                   [ 
97%]tests/unit/utils/test_logging_coverage.py ......                         [ 
97%]tests/unit/utils/test_logging_final_coverage.py .......                  [ 
97%]tests/unit/utils/test_logging_utils.py ...                               [ 
97%]tests/unit/utils/test_serialization.py ...                               [ 
98%]tests/unit/utils/test_serialization_coverage.py ..........               [ 
98%]tests/unit/utils/test_serialization_edges.py ..                          [ 
98%]tests/unit/utils/test_serialization_extra.py ...                         [ 
98%]tests/unit/utils/test_serialization_final_coverage.py ..........         [ 
98%]tests/integration/agents/test_generation/test_run_generated_tests.py ..  [ 
99%]tests/integration/agents/test_generation/test_scaffold_generation.py ..  [ 
99%]tests/integration/api/test_api_startup.py F.                             [ 
99%]tests/integration/deployment/test_compose_workflow.py ...                [ 
99%]tests/integration/deployment/test_deployment_scripts.py .....            [ 
99%]tests/integration/general/test_complex_workflow.py .                     [ 
99%]tests/integration/general/test_end_to_end_workflow.py .                  [ 
99%]tests/integration/general/test_lmstudio_integration_regression.py ssssss [ 
99%]s                                                                        [ 
99%]tests/integration/generated/test_generated_module.py s                   [ 
99%]tests/integration/generated/test_run_generated_tests.py ..               [ 
99%]tests/integration/llm/test_lmstudio_timing_baseline.py s                 [ 
99%]tests/integration/mvu/test_command_execution.py ..                       [ 
99%]tests/integration/utils/test_logging_integration.py ..                   
[100%]==================================== ERRORS 
====================================________________ ERROR at setup of 
test_cli_marker_passthrough _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bd9a0&amp;gt;    
@pytest.fixture    def cli_app(        monkeypatch: pytest.MonkeyPatch,    ) 
-&amp;gt; tuple[ModuleType, Typer, RecordingBridge]:        
&amp;quot;&amp;quot;&amp;quot;Load the CLI command with a stub bridge and 
patched coverage helpers.&amp;quot;&amp;quot;&amp;quot;            app, module =
build_minimal_cli_app(monkeypatch)            bridge = RecordingBridge()&amp;gt;
monkeypatch.setattr(module, &amp;quot;bridge&amp;quot;, bridge)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;bridge&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py:56: 
AttributeError___________ ERROR at setup of 
test_cli_feature_flags_set_environment ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bdd30&amp;gt;    
@pytest.fixture    def cli_app(        monkeypatch: pytest.MonkeyPatch,    ) 
-&amp;gt; tuple[ModuleType, Typer, RecordingBridge]:        
&amp;quot;&amp;quot;&amp;quot;Load the CLI command with a stub bridge and 
patched coverage helpers.&amp;quot;&amp;quot;&amp;quot;            app, module =
build_minimal_cli_app(monkeypatch)            bridge = RecordingBridge()&amp;gt;
monkeypatch.setattr(module, &amp;quot;bridge&amp;quot;, bridge)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;bridge&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py:56: 
AttributeError_________ ERROR at setup of 
test_cli_segmentation_arguments_forwarded __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bff50&amp;gt;    
@pytest.fixture    def cli_app(        monkeypatch: pytest.MonkeyPatch,    ) 
-&amp;gt; tuple[ModuleType, Typer, RecordingBridge]:        
&amp;quot;&amp;quot;&amp;quot;Load the CLI command with a stub bridge and 
patched coverage helpers.&amp;quot;&amp;quot;&amp;quot;            app, module =
build_minimal_cli_app(monkeypatch)            bridge = RecordingBridge()&amp;gt;
monkeypatch.setattr(module, &amp;quot;bridge&amp;quot;, bridge)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;bridge&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py:56: 
AttributeError____________ ERROR at setup of 
test_cli_inventory_mode_exports_json ____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bcef0&amp;gt;    
@pytest.fixture    def cli_app(        monkeypatch: pytest.MonkeyPatch,    ) 
-&amp;gt; tuple[ModuleType, Typer, RecordingBridge]:        
&amp;quot;&amp;quot;&amp;quot;Load the CLI command with a stub bridge and 
patched coverage helpers.&amp;quot;&amp;quot;&amp;quot;            app, module =
build_minimal_cli_app(monkeypatch)            bridge = RecordingBridge()&amp;gt;
monkeypatch.setattr(module, &amp;quot;bridge&amp;quot;, bridge)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;bridge&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py:56: 
AttributeError___________ ERROR at setup of 
test_cli_failure_propagates_exit_code ____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bd580&amp;gt;    
@pytest.fixture    def cli_app(        monkeypatch: pytest.MonkeyPatch,    ) 
-&amp;gt; tuple[ModuleType, Typer, RecordingBridge]:        
&amp;quot;&amp;quot;&amp;quot;Load the CLI command with a stub bridge and 
patched coverage helpers.&amp;quot;&amp;quot;&amp;quot;            app, module =
build_minimal_cli_app(monkeypatch)            bridge = RecordingBridge()&amp;gt;
monkeypatch.setattr(module, &amp;quot;bridge&amp;quot;, bridge)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;bridge&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py:56: 
AttributeError_____ ERROR at setup of 
test_inner_test_env_tightening_forces_no_parallel ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115e67d40&amp;gt;    
@pytest.fixture(autouse=True)    def _clean_env(monkeypatch: 
pytest.MonkeyPatch):        # Ensure a clean slate for env vars we mutate       
keys = [            &amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;,         
&amp;quot;PYTEST_ADDOPTS&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;,            
&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,        ]        for k 
in keys:            monkeypatch.delenv(k, raising=False)&amp;gt;       
monkeypatch.setattr(rtc, &amp;quot;enforce_coverage_threshold&amp;quot;, lambda 
*a, **k: 100.0)E       AttributeError: &amp;lt;function run_tests_cmd at 
0x1146f18a0&amp;gt; has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_env_pa
ths.py:31: AttributeError_ ERROR at setup of 
test_unit_tests_sets_allow_requests_by_default_and_respects_existing 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e653a0&amp;gt;    @pytest.fixture(autouse=True)    def 
_clean_env(monkeypatch: pytest.MonkeyPatch):        # Ensure a clean slate for 
env vars we mutate        keys = [            
&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;,            
&amp;quot;PYTEST_ADDOPTS&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;,            
&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,        ]        for k 
in keys:            monkeypatch.delenv(k, raising=False)&amp;gt;       
monkeypatch.setattr(rtc, &amp;quot;enforce_coverage_threshold&amp;quot;, lambda 
*a, **k: 100.0)E       AttributeError: &amp;lt;function run_tests_cmd at 
0x1146f18a0&amp;gt; has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_env_pa
ths.py:31: AttributeError_______ ERROR at setup of 
test_feature_flags_set_env_and_success_message _______obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e64650&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_features.py:11: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError________ ERROR at setup of 
test_marker_option_is_passed_as_extra_marker ________obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e66f60&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_features.py:11: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError_____ ERROR at setup of 
test_inner_test_mode_disables_plugins_and_parallel _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115677260&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_inner_
test.py:21: AttributeError_______ ERROR at setup of 
test_inventory_mode_exports_json_and_skips_run _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115675310&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_invent
ory_and_validation.py:28: AttributeError______ ERROR at setup of 
test_inventory_mode_handles_collection_failures _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115674440&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_invent
ory_and_validation.py:28: AttributeError__________ ERROR at setup of 
test_invalid_target_exits_with_help_text __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115676c90&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_invent
ory_and_validation.py:28: AttributeError_________ ERROR at setup of 
test_marker_option_is_forwarded_to_runner __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115675ca0&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_invent
ory_and_validation.py:28: AttributeError_______ ERROR at setup of 
test_marker_anding_passthrough_multiple_speeds _______obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115674e60&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_markers.py:13: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError________ ERROR at setup of 
test_invalid_marker_expression_exits_cleanly ________obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1156774d0&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_markers.py:13: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError______________ ERROR at setup of test_speed_and_marker_forwarding
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x123a91b20&amp;gt;    @pytest.fixture(autouse=True)    def 
_clean_env(monkeypatch: pytest.MonkeyPatch):        keys = [            
&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;,            
&amp;quot;PYTEST_ADDOPTS&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;,            
&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_EXPA&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_FEATURE_B&amp;quot;,        ]        for k in keys:  
monkeypatch.delenv(k, raising=False)&amp;gt;       monkeypatch.setattr(rtc, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_more.p
y:31: AttributeError_________ ERROR at setup of 
test_report_true_prints_output_and_success _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1240198e0&amp;gt;    
@pytest.fixture(autouse=True)    def _clean_env(monkeypatch: 
pytest.MonkeyPatch):        keys = [            
&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;,            
&amp;quot;PYTEST_ADDOPTS&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;,            
&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_EXPA&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_FEATURE_B&amp;quot;,        ]        for k in keys:  
monkeypatch.delenv(k, raising=False)&amp;gt;       monkeypatch.setattr(rtc, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_more.p
y:31: AttributeError_____________ ERROR at setup of 
test_observability_and_error_path ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1151150d0&amp;gt;    
@pytest.fixture(autouse=True)    def _clean_env(monkeypatch: 
pytest.MonkeyPatch):        keys = [            
&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;,            
&amp;quot;PYTEST_ADDOPTS&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;,            
&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;,            
&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_EXPA&amp;quot;,            
&amp;quot;DEVSYNTH_FEATURE_FEATURE_B&amp;quot;,        ]        for k in keys:  
monkeypatch.delenv(k, raising=False)&amp;gt;       monkeypatch.setattr(rtc, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_more.p
y:31: AttributeError_______ ERROR at setup of 
test_provider_defaults_are_applied_when_unset ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x123a92540&amp;gt;    
@pytest.fixture(autouse=True)    def _clean_env(monkeypatch: 
pytest.MonkeyPatch):        keys = [            
&amp;quot;DEVSYNTH_PROVIDER&amp;quot;,            
&amp;quot;DEVSYNTH_OFFLINE&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,        ]        for k 
in keys:            monkeypatch.delenv(k, raising=False)&amp;gt;       
monkeypatch.setattr(rtc, &amp;quot;enforce_coverage_threshold&amp;quot;, lambda 
*a, **k: 100.0)E       AttributeError: &amp;lt;function run_tests_cmd at 
0x1146f18a0&amp;gt; has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_provid
er_defaults.py:26: AttributeError______ ERROR at setup of 
test_provider_defaults_do_not_override_existing _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x120000ec0&amp;gt;    
@pytest.fixture(autouse=True)    def _clean_env(monkeypatch: 
pytest.MonkeyPatch):        keys = [            
&amp;quot;DEVSYNTH_PROVIDER&amp;quot;,            
&amp;quot;DEVSYNTH_OFFLINE&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,        ]        for k 
in keys:            monkeypatch.delenv(k, raising=False)&amp;gt;       
monkeypatch.setattr(rtc, &amp;quot;enforce_coverage_threshold&amp;quot;, lambda 
*a, **k: 100.0)E       AttributeError: &amp;lt;function run_tests_cmd at 
0x1146f18a0&amp;gt; has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_provid
er_defaults.py:26: AttributeError___ ERROR at setup of 
test_report_flag_with_missing_directory_prints_warning ___obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115f8c140&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_report_path.py:12: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError_______ ERROR at setup of 
test_smoke_mode_sets_env_and_disables_parallel _______obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115f7e840&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_report_path.py:12: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError________________ ERROR at setup of test_no_parallel_maps_to_n0 
_________________obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x123534d10&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_report_path.py:12: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError_______ ERROR at setup of 
test_emit_coverage_messages_reports_artifacts ________obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x123535f70&amp;gt;    @pytest.fixture(autouse=True)    def 
_patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -&amp;gt; None:&amp;gt; 
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_thres
hold&amp;quot;,            lambda *a, **k: 100.0,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_report_path.py:12: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;enforce_coverage_threshold&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92:
 AttributeError_______ ERROR at setup of 
test_run_tests_cli_report_option_forwards_true _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1235350d0&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_reporting_
and_env.py:21: AttributeError_____ ERROR at setup of 
test_run_tests_cmd_respects_explicit_provider_env ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x123534f80&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_reporting_
and_env.py:21: AttributeError__ ERROR at setup of 
test_smoke_mode_sets_pytest_disable_plugin_autoload_env ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115114f20&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_smoke.py:17: 
AttributeError___ ERROR at setup of 
test_smoke_mode_skips_coverage_gate_when_cov_disabled ____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115117b90&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_smoke.py:17: 
AttributeError_______ ERROR at setup of 
test_smoke_mode_cli_imports_fastapi_testclient _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115114ef0&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_smoke.py:17: 
AttributeError___ ERROR at setup of 
test_smoke_mode_skips_coverage_gate_when_instrumented ____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115117500&amp;gt;    
@pytest.fixture(autouse=True)    def _patch_coverage_helper(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:&amp;gt;       monkeypatch.setattr(module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *a, **k: 100.0)E       
AttributeError: &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; has no 
attribute 
&amp;#x27;enforce_coverage_threshold&amp;#x27;/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_smoke.py:17: 
AttributeError__________ ERROR at setup of 
test_setup_wizard_instantiation_succeeds __________name = 
&amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115116240&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError________ ERROR at setup of 
test_wizard_prompts_via_cli_bridge_succeeds _________name = 
&amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115114a70&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_______________ ERROR at setup of test_setup_wizard_run_succeeds 
_______________name = &amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: 
str) -&amp;gt; object:        &amp;quot;&amp;quot;&amp;quot;Lazily expose CLI 
command callables when requested.&amp;quot;&amp;quot;&amp;quot;            if ( 
name            in {                &amp;quot;config_app&amp;quot;,             
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115114950&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError______________ ERROR at setup of test_setup_wizard_abort_succeeds 
______________name = &amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: 
str) -&amp;gt; object:        &amp;quot;&amp;quot;&amp;quot;Lazily expose CLI 
command callables when requested.&amp;quot;&amp;quot;&amp;quot;            if ( 
name            in {                &amp;quot;config_app&amp;quot;,             
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115e670e0&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError____ ERROR at setup of 
test_prompt_features_uses_prompt_toolkit_multiselect ____name = 
&amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115e66d80&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError___________ ERROR at setup of 
test_setup_wizard_accepts_typed_inputs ___________name = 
&amp;#x27;setup_wizard&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above 
exception was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115e66180&amp;gt;    
@pytest.fixture(autouse=True)    def disable_prompt_toolkit(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Disable prompt-toolkit integration for legacy 
wizard tests.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter&amp;q
uot;, lambda: None        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/test_setup_wizard.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;setup_wizard&amp;#x27;, ann = 
&amp;#x27;devsynth.application.cli.setup_wizard&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.setup_wizard has 
no attribute 
&amp;#x27;setup_wizard&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError____ ERROR at setup of 
TestExecutionTrajectoryCollector.test_initialization ____self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043e2a0&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError_ ERROR at setup 
of TestExecutionTrajectoryCollector.test_analyze_code_structure _self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043e750&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError_ ERROR at setup 
of TestExecutionTrajectoryCollector.test_extract_execution_patterns _self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043c740&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError_ ERROR at setup 
of TestExecutionTrajectoryCollector.test_create_memetic_units_from_trajectories 
_self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043ed50&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError_ ERROR at setup 
of TestExecutionTrajectoryCollector.test_get_execution_insights _self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043f290&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError_ ERROR at setup 
of TestExecutionTrajectoryCollector.test_validate_trajectory_quality _self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionTrajectoryCollector object at 0x12043f7d0&amp;gt;    def 
setup_method(self):        &amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       self.collector = 
ExecutionTrajectoryCollector(sandbox_enabled=False)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ExecutionTrajectoryCollector&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_execution_learning_integration.py:210: NameError____ ERROR at 
setup of TestEnhancedGraphRAGQueryEngine.test_initialization _____self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205a1400&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,355 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,355 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_process_complex_query _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205acce0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,367 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,367 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized__ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_parse_query_intent ___self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205ad220&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,391 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,391 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized___ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_extract_entities ____self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205ad760&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,404 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,404 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_extract_relationships _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205adca0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,417 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,417 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_calculate_required_hops _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205ae1e0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,430 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,430 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized___ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_resolve_entities ____self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205ae720&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,455 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,455 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_plan_multi_hop_traversal _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205aec60&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,472 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,472 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_execute_semantic_traversal _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestEnhance
dGraphRAGQueryEngine object at 0x1205af1a0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.enhanced_graph = 
EnhancedKnowledgeGraph()        self.execution_learning = Mock()&amp;gt;       
self.query_engine = EnhancedGraphRAGQueryEngine(self.enhanced_graph, 
self.execution_learning)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
NameError: name &amp;#x27;EnhancedGraphRAGQueryEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:329: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,499 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,499 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized______ ERROR at setup of 
TestAutomataSynthesisEngine.test_initialization _______self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205af3b0&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_ ERROR at setup of 
TestAutomataSynthesisEngine.test_synthesize_automata_from_exploration _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205af860&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_ ERROR at setup of 
TestAutomataSynthesisEngine.test_generate_task_segmentation _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205c0320&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_ ERROR at setup of 
TestAutomataSynthesisEngine.test_validate_automata_quality _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205c0860&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_ ERROR at setup of 
TestAutomataSynthesisEngine.test_create_memetic_units_from_automata _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205afe30&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_ ERROR at setup of 
TestAutomataSynthesisEngine.test_get_task_segmentation_for_query _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestAutomat
aSynthesisEngine object at 0x1205afda0&amp;gt;    def setup_method(self):       
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.automata_engine = 
AutomataSynthesisEngine(self.execution_learning)                               
^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;AutomataSynthesisEngine&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:475: NameError_______ ERROR at setup
of TestHybridLLMArchitecture.test_initialization ________self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c0f50&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError_ ERROR at setup of 
TestHybridLLMArchitecture.test_process_complex_reasoning_task _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c1490&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError_ ERROR at setup of 
TestHybridLLMArchitecture.test_get_optimal_provider_for_task _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c19d0&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError_ ERROR at setup of 
TestHybridLLMArchitecture.test_benchmark_hybrid_vs_individual _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c1f10&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError________ ERROR at 
setup of TestHybridLLMArchitecture.test_add_provider _________self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c2450&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError_ ERROR at setup of 
TestHybridLLMArchitecture.test_get_architecture_statistics _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestHybridL
LMArchitecture object at 0x1205c2990&amp;gt;    def setup_method(self):        
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.hybrid_llm = 
HybridLLMArchitecture(self.execution_learning)                          
^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;HybridLLMArchitecture&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:579: NameError____ ERROR at setup of
TestMetacognitiveTrainingSystem.test_initialization _____self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205c2ba0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_start_think_aloud_session _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205c3050&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_record_verbalization __self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205c3590&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_end_think_aloud_session _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205cc050&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_get_metacognitive_insights _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205cc590&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_apply_metacognitive_improvements _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205ccad0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_generate_self_monitoring_report _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestMetacog
nitiveTrainingSystem object at 0x1205c3ad0&amp;gt;    def setup_method(self):   
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)                            
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;MetacognitiveTrainingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:689: NameError_____ ERROR at setup 
of TestContextualPromptingSystem.test_initialization ______self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205ccf50&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_create_contextual_prompt _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205cd490&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_engineer_contextual_prompt _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205cd9d0&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_add_behavioral_directive _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205cdf10&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_add_environmental_constraint _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205ce450&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_get_prompt_performance_analytics _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205ce990&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestContextualPromptingSystem.test_create_agent_specific_prompt _self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestContext
ualPromptingSystem object at 0x1205ceed0&amp;gt;    def setup_method(self):     
&amp;quot;&amp;quot;&amp;quot;Set up test 
fixtures.&amp;quot;&amp;quot;&amp;quot;        self.execution_learning = 
Mock()&amp;gt;       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)                              
^^^^^^^^^^^^^^^^^^^^^^^^^E       NameError: name 
&amp;#x27;ContextualPromptingSystem&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:806: NameError_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_unit _tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category0/test_project/tests/be
havior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, 
buffering = -1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = 
None    def open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None, 
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_by_category0/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_integration _tmp_path =
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category1&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category1/test_project/tests/be
havior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, 
buffering = -1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = 
None    def open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None, 
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_by_category1/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_behavior _tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category2&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_by_category2/test_project/tests/be
havior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, 
buffering = -1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = 
None    def open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None, 
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_by_category2/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_all_categories _tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_all_categor0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_all_categor0/test_project/tests/be
havior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, 
buffering = -1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = 
None    def open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None, 
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_all_categor0/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError___ ERROR at setup of 
TestEnhancedTestCollector.test_get_tests_with_markers ____tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_get_tests_with_markers0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_get_tests_with_markers0/test_project/tests/behav
ior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering =
-1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_get_tests_with_markers0/test_project/tests/behavior/featur
es/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/
Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError____ ERROR at setup of 
TestEnhancedTestCollector.test_caching_functionality ____tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_caching_functionality0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_caching_functionality0/test_project/tests/behavi
or/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = 
-1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_caching_functionality0/test_project/tests/behavior/feature
s/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/P
ython.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_____ ERROR at setup of 
TestEnhancedTestCollector.test_force_refresh_cache _____tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_force_refresh_cache0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_force_refresh_cache0/test_project/tests/behavior
/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = 
-1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_force_refresh_cache0/test_project/tests/behavior/features/
example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pyt
hon.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_____ ERROR at setup of 
TestEnhancedTestCollector.test_memory_integration ______tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_memory_integration0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_memory_integration0/test_project/tests/behavior/
features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = -1,
encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_memory_integration0/test_project/tests/behavior/features/e
xample.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pyth
on.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_____ ERROR at setup of 
TestEnhancedTestCollector.test_is_valid_test_file ______tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_is_valid_test_file0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_is_valid_test_file0/test_project/tests/behavior/
features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = -1,
encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_is_valid_test_file0/test_project/tests/behavior/features/e
xample.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pyth
on.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_____ ERROR at setup of 
TestEnhancedTestCollector.test_contains_test_code ______tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_contains_test_code0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_contains_test_code0/test_project/tests/behavior/
features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = -1,
encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_contains_test_code0/test_project/tests/behavior/features/e
xample.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pyth
on.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError_______ ERROR at setup of 
TestEnhancedTestCollector.test_test_has_marker _______tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_test_has_marker0&amp;#x27;)    @pytest.fixture  
def temp_project(tmp_path):        &amp;quot;&amp;quot;&amp;quot;Create a 
temporary test project.&amp;quot;&amp;quot;&amp;quot;        project_dir = 
tmp_path / &amp;quot;test_project&amp;quot;            # Create test directories
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_test_has_marker0/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = -1, 
encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_test_has_marker0/test_project/tests/behavior/features/exam
ple.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.
framework/Versions/3.12/lib/python3.12/pathlib.py:1013: FileNotFoundError_______
ERROR at setup of TestEnhancedTestCollector.test_analyze_markers _______tmp_path
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_analyze_markers0&amp;#x27;)    @pytest.fixture  
def temp_project(tmp_path):        &amp;quot;&amp;quot;&amp;quot;Create a 
temporary test project.&amp;quot;&amp;quot;&amp;quot;        project_dir = 
tmp_path / &amp;quot;test_project&amp;quot;            # Create test directories
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_analyze_markers0/test_project/tests/behavior/fea
tures/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering = -1, 
encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    def 
open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,             
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_analyze_markers0/test_project/tests/behavior/features/exam
ple.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.
framework/Versions/3.12/lib/python3.12/pathlib.py:1013: FileNotFoundError__ 
ERROR at setup of TestEnhancedTestCollector.test_store_collection_results 
___tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_store_collection_results0&amp;#x27;)    
@pytest.fixture    def temp_project(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Create a temporary test 
project.&amp;quot;&amp;quot;&amp;quot;        project_dir = tmp_path / 
&amp;quot;test_project&amp;quot;            # Create test directories        
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot;).mkdir(parents=True)
(project_dir / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True)            # Create unit tests
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot; / 
&amp;quot;test_example.py&amp;quot;).write_text(            &amp;#x27;import 
pytest\n\n@pytest.mark.fast\ndef test_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test example 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert 1 + 1 == 
2\n\n@pytest.mark.medium\ndef test_another_example():\n    
&amp;quot;&amp;quot;&amp;quot;Test another 
function.&amp;quot;&amp;quot;&amp;quot;\n    assert True is True&amp;#x27;      
)            # Create integration tests        (project_dir / 
&amp;quot;tests&amp;quot; / &amp;quot;integration&amp;quot; / 
&amp;quot;test_integration.py&amp;quot;).write_text(            &amp;#x27;import
pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
&amp;quot;&amp;quot;&amp;quot;Test integration 
functionality.&amp;quot;&amp;quot;&amp;quot;\n    assert 
&amp;quot;integration&amp;quot; in &amp;quot;integration 
test&amp;quot;&amp;#x27;        )            # Create behavior tests&amp;gt;    
(project_dir / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;features&amp;quot; / &amp;quot;example.feature&amp;quot;).write_text( 
&amp;#x27;Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen&amp;#x27;  
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/tes
ting/test_enhanced_test_collector.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text    with 
self.open(mode=&amp;#x27;w&amp;#x27;, encoding=encoding, errors=errors, 
newline=newline) as f:         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_store_collection_results0/test_project/tests/beh
avior/features/example.feature&amp;#x27;)mode = &amp;#x27;w&amp;#x27;, buffering
= -1, encoding = &amp;#x27;locale&amp;#x27;, errors = None, newline = None    
def open(self, mode=&amp;#x27;r&amp;#x27;, buffering=-1, encoding=None,         
errors=None, newline=None):        &amp;quot;&amp;quot;&amp;quot;        Open 
the file pointed to by this path and return a file object, as        the 
built-in open() function does.        &amp;quot;&amp;quot;&amp;quot;        if 
&amp;quot;b&amp;quot; not in mode:            encoding = 
io.text_encoding(encoding)&amp;gt;       return io.open(self, mode, buffering, 
encoding, errors, newline)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
FileNotFoundError: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_store_collection_results0/test_project/tests/behavior/feat
ures/example.feature&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framework
s/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013: 
FileNotFoundError________________ ERROR at setup of test_ports_fixtures_succeeds
________________fixturedef = &amp;lt;FixtureDef 
argname=&amp;#x27;llm_port&amp;#x27; scope=&amp;#x27;function&amp;#x27; 
baseid=&amp;#x27;&amp;#x27;&amp;gt;request = &amp;lt;SubRequest 
&amp;#x27;llm_port&amp;#x27; for &amp;lt;Function 
test_ports_fixtures_succeeds&amp;gt;&amp;gt;    @pytest.hookimpl(wrapper=True)  
def pytest_fixture_setup(fixturedef: FixtureDef, request) -&amp;gt; object | 
None:        asyncio_mode = _get_asyncio_mode(request.config)        if not 
_is_asyncio_fixture_function(fixturedef.func):            if asyncio_mode == 
Mode.STRICT:                # Ignore async fixtures without explicit asyncio 
mark in strict mode                # This applies to pytest_trio fixtures, for 
example&amp;gt;               return (yield)                        
^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/s
ite-packages/pytest_asyncio/plugin.py:733: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     @pytest.fixture    def llm_port():
&amp;quot;&amp;quot;&amp;quot;Provide an LLMPort using the 
MockLLMAdapter.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       port = 
LLMPort(_MockProviderFactory())                       ^^^^^^^^^^^^^^^^^^^^E     
NameError: name &amp;#x27;_MockProviderFactory&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/ports
.py:92: NameError_____ ERROR at setup of 
test_webui_layout_breakpoints_toggle_between_modes _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531ecf0&amp;gt;    
@pytest.fixture    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
Iterator[Tuple]:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a deterministic Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = _mock_streamlit()        
st.markdown.reset_mock()        st.error.reset_mock()            stub_argon2 = 
ModuleType(&amp;quot;argon2&amp;quot;)        stub_argon2.PasswordHasher = 
MagicMock()        stub_exceptions = 
ModuleType(&amp;quot;argon2.exceptions&amp;quot;)            class 
_VerifyMismatchError(Exception):            pass            
stub_exceptions.VerifyMismatchError = _VerifyMismatchError        
stub_argon2.exceptions = stub_exceptions        monkeypatch.setitem(sys.modules,
&amp;quot;argon2&amp;quot;, stub_argon2)        monkeypatch.setitem(sys.modules,
&amp;quot;argon2.exceptions&amp;quot;, stub_exceptions)            stub_config =
ModuleType(&amp;quot;devsynth.config&amp;quot;)        
stub_config.load_project_config = MagicMock(return_value={})        
stub_config.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, stub_config)            stub_yaml = 
ModuleType(&amp;quot;yaml&amp;quot;)        stub_yaml.safe_dump = 
MagicMock(return_value=&amp;quot;{}&amp;quot;)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, stub_yaml)           
stub_output_formatter = 
ModuleType(&amp;quot;devsynth.interface.output_formatter&amp;quot;)            
class _Formatter:            def __init__(self, *_args, **_kwargs) -&amp;gt; 
None:                pass                def format_message(                
self,                message: str,                *,                
message_type: str | None = None,                highlight: bool = False,        
) -&amp;gt; str:                return message            
stub_output_formatter.OutputFormatter = _Formatter        monkeypatch.setitem(  
sys.modules, &amp;quot;devsynth.interface.output_formatter&amp;quot;, 
stub_output_formatter        )            stub_shared_bridge = 
ModuleType(&amp;quot;devsynth.interface.shared_bridge&amp;quot;)            
class _SharedBridgeMixin:            def __init__(self, *args, **kwargs) 
-&amp;gt; None:                super().__init__(*args, **kwargs)  # type: ignore
self.formatter = _Formatter(None)                def _format_for_output(        
self,                message: str,                *,                highlight: 
bool = False,                message_type: str | None = None,            ) 
-&amp;gt; str:                return message            
stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.interface.shared_bridge&amp;quot;, stub_shared_bridge        
)            monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)
&amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_dashboard_toggles_fast.py:88: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_webui_error_guidance_surfaces_suggestions_and_docs ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531ecc0&amp;gt;    
@pytest.fixture    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
Iterator[Tuple]:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a deterministic Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = _mock_streamlit()        
st.markdown.reset_mock()        st.error.reset_mock()            stub_argon2 = 
ModuleType(&amp;quot;argon2&amp;quot;)        stub_argon2.PasswordHasher = 
MagicMock()        stub_exceptions = 
ModuleType(&amp;quot;argon2.exceptions&amp;quot;)            class 
_VerifyMismatchError(Exception):            pass            
stub_exceptions.VerifyMismatchError = _VerifyMismatchError        
stub_argon2.exceptions = stub_exceptions        monkeypatch.setitem(sys.modules,
&amp;quot;argon2&amp;quot;, stub_argon2)        monkeypatch.setitem(sys.modules,
&amp;quot;argon2.exceptions&amp;quot;, stub_exceptions)            stub_config =
ModuleType(&amp;quot;devsynth.config&amp;quot;)        
stub_config.load_project_config = MagicMock(return_value={})        
stub_config.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, stub_config)            stub_yaml = 
ModuleType(&amp;quot;yaml&amp;quot;)        stub_yaml.safe_dump = 
MagicMock(return_value=&amp;quot;{}&amp;quot;)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, stub_yaml)           
stub_output_formatter = 
ModuleType(&amp;quot;devsynth.interface.output_formatter&amp;quot;)            
class _Formatter:            def __init__(self, *_args, **_kwargs) -&amp;gt; 
None:                pass                def format_message(                
self,                message: str,                *,                
message_type: str | None = None,                highlight: bool = False,        
) -&amp;gt; str:                return message            
stub_output_formatter.OutputFormatter = _Formatter        monkeypatch.setitem(  
sys.modules, &amp;quot;devsynth.interface.output_formatter&amp;quot;, 
stub_output_formatter        )            stub_shared_bridge = 
ModuleType(&amp;quot;devsynth.interface.shared_bridge&amp;quot;)            
class _SharedBridgeMixin:            def __init__(self, *args, **kwargs) 
-&amp;gt; None:                super().__init__(*args, **kwargs)  # type: ignore
self.formatter = _Formatter(None)                def _format_for_output(        
self,                message: str,                *,                highlight: 
bool = False,                message_type: str | None = None,            ) 
-&amp;gt; str:                return message            
stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.interface.shared_bridge&amp;quot;, stub_shared_bridge        
)            monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)
&amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_dashboard_toggles_fast.py:88: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_get_layout_config_breakpoints[500-expected0] ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531e060&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_get_layout_config_breakpoints[800-expected1] ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531f110&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_get_layout_config_breakpoints[1200-expected2] _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531e6f0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_get_layout_config_breakpoints[None-expected3] _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531ef60&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______ ERROR at setup of 
test_display_result_renders_markup_and_sanitizes ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531f860&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ ERROR at setup of 
test_display_result_highlight_uses_info ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14531f0e0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_display_result_routes_message_types_and_plain_write __monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f3bc0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_______ ERROR at setup of 
test_display_result_error_suggestions_and_docs _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f15b0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___ ERROR at setup of 
test_display_result_error_prefix_without_message_type ____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f2570&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_display_result_heading_routes_to_header ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f2510&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ ERROR at setup of 
test_display_result_additional_headings ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f3170&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[File not found: missing.yaml-file_not_found] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1453f35c0&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Permission denied when opening-permission_denied] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1453f2180&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Invalid parameter --foo-invalid_parameter] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1453f2240&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Invalid format provided-invalid_format] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1453f1ee0&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Missing key &amp;#x27;api&amp;#x27;-key_error] 
__monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145401730&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Type error while casting-type_error] _monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f2360&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Configuration error detected-config_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145387ef0&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Connection error occurred-connection_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14542f920&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_get_error_type_mappings[API error status-api_error] __monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542f0b0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Validation error raised-validation_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14542ede0&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Syntax error unexpected token-syntax_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14542f110&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_mappings[Import error for module-import_error] _monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542f410&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______ ERROR at setup of 
test_get_error_type_mappings[Unrelated message-] ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542c230&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________________ ERROR at setup of 
test_error_helper_defaults _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542fb60&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ ERROR at setup of 
test_render_traceback_uses_expander _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542f290&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________________ ERROR at setup of 
test_format_error_message __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542fce0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_ensure_router_caches_instance _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14542f4a0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ ERROR at setup of 
test_run_configures_streamlit_and_router __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f3050&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_run_handles_page_config_error _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f2390&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_run_handles_components_error ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145475250&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_ui_progress_updates_emit_eta ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145474ec0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_______________ ERROR at setup of 
test_ui_progress_subtask_flow ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145220ec0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ ERROR at setup of 
test_webui_ensure_router_caches_instance __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145222b40&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_webui_run_configures_layout_and_router _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145220e00&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ ERROR at setup of 
test_webui_run_handles_page_config_error __________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452201d0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___________ ERROR at setup of 
test_webui_run_handles_component_error ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452a22d0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a lightweight 
Streamlit substitute.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_display_result_translates_markup_to_markdown ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145402f00&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch):        
streamlit_mod, state = _make_streamlit_stub()        
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, streamlit_mod)  
monkeypatch.setitem(sys.modules, &amp;quot;chromadb&amp;quot;, MagicMock())     
monkeypatch.setitem(sys.modules, &amp;quot;uvicorn&amp;quot;, MagicMock())      
security_pkg = ModuleType(&amp;quot;devsynth.security&amp;quot;)        
security_pkg.__path__ = []  # type: ignore        validation_stub = 
ModuleType(&amp;quot;devsynth.security.validation&amp;quot;)        
validation_stub.parse_bool_env = lambda _name, default=True: default        
sanitization_stub = 
ModuleType(&amp;quot;devsynth.security.sanitization&amp;quot;)        
sanitization_stub.sanitize_input = lambda text: text        auth_stub = 
ModuleType(&amp;quot;devsynth.security.authentication&amp;quot;)        
auth_stub.authenticate = MagicMock(return_value=True)        
auth_stub.hash_password = MagicMock(return_value=&amp;quot;hash&amp;quot;)      
auth_stub.verify_password = MagicMock(return_value=True)        
monkeypatch.setitem(sys.modules, &amp;quot;devsynth.security&amp;quot;, 
security_pkg)        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.validation&amp;quot;, validation_stub)        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.security.sanitization&amp;quot;, sanitization_stub        )  
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.authentication&amp;quot;, auth_stub)        
security_pkg.validation = validation_stub        security_pkg.sanitization = 
sanitization_stub        security_pkg.authentication = auth_stub        
config_stub = ModuleType(&amp;quot;devsynth.config&amp;quot;)        
config_stub.load_project_config = MagicMock(return_value={})        
config_stub.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, config_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, MagicMock())        
rich_module = ModuleType(&amp;quot;rich&amp;quot;)        rich_box = 
ModuleType(&amp;quot;rich.box&amp;quot;)        rich_box.ROUNDED = MagicMock()  
rich_box.Box = MagicMock()        rich_console = 
ModuleType(&amp;quot;rich.console&amp;quot;)            class _Console:         
def __init__(self, *args, **kwargs):                self.print_calls: 
list[tuple[tuple, dict]] = []                def print(self, *args, **kwargs):  
self.print_calls.append((args, kwargs))            rich_console.Console = 
_Console        rich_markdown = ModuleType(&amp;quot;rich.markdown&amp;quot;)   
class _Markdown:            def __init__(self, text: str, **kwargs: object) 
-&amp;gt; None:                self.text = text                self.kwargs = 
kwargs            rich_markdown.Markdown = _Markdown        rich_panel = 
ModuleType(&amp;quot;rich.panel&amp;quot;)            class _Panel:            
def __init__(self, renderable: object, **kwargs: object) -&amp;gt; None:        
self.renderable = renderable                self.kwargs = kwargs            
rich_panel.Panel = _Panel        rich_style = 
ModuleType(&amp;quot;rich.style&amp;quot;)            class _Style:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs            rich_style.Style
= _Style        rich_syntax = ModuleType(&amp;quot;rich.syntax&amp;quot;)       
class _Syntax:            def __init__(                self, code: str, lexer: 
str | None = None, **kwargs: object            ) -&amp;gt; None:                
self.code = code                self.lexer = lexer                self.kwargs = 
kwargs            rich_syntax.Syntax = _Syntax        rich_table = 
ModuleType(&amp;quot;rich.table&amp;quot;)            class _Table:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs                
self.columns: list[tuple[str, dict]] = []                self.rows: 
list[tuple[tuple, dict]] = []                def add_column(self, name: str, 
**kwargs: object) -&amp;gt; None:                self.columns.append((name, 
kwargs))                def add_row(self, *cells: object, **kwargs: object) 
-&amp;gt; None:                self.rows.append((cells, kwargs))            
rich_table.Table = _Table        rich_text = 
ModuleType(&amp;quot;rich.text&amp;quot;)            class _Text(str):          
def __new__(cls, text: str, *args: object, **kwargs: object):  # type: ignore   
obj = str.__new__(cls, text)                obj._args = args  # type: ignore    
obj._kwargs = kwargs  # type: ignore                return obj            
rich_text.Text = _Text        rich_module.box = rich_box        
rich_module.console = rich_console        rich_module.markdown = rich_markdown  
rich_module.panel = rich_panel        rich_module.style = rich_style        
rich_module.syntax = rich_syntax        rich_module.table = rich_table        
rich_module.text = rich_text        monkeypatch.setitem(sys.modules, 
&amp;quot;rich&amp;quot;, rich_module)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.box&amp;quot;, rich_box)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.console&amp;quot;, rich_console)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.markdown&amp;quot;, 
rich_markdown)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.panel&amp;quot;, rich_panel)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.style&amp;quot;, rich_style)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.syntax&amp;quot;, rich_syntax)  
monkeypatch.setitem(sys.modules, &amp;quot;rich.table&amp;quot;, rich_table)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.text&amp;quot;, rich_text)      
sys.modules.pop(&amp;quot;devsynth.interface.webui&amp;quot;, None)&amp;gt;     
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_guidance.py:272: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___ ERROR at setup of 
test_display_result_surfaces_guidance_for_file_errors ____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f3c50&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch):        
streamlit_mod, state = _make_streamlit_stub()        
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, streamlit_mod)  
monkeypatch.setitem(sys.modules, &amp;quot;chromadb&amp;quot;, MagicMock())     
monkeypatch.setitem(sys.modules, &amp;quot;uvicorn&amp;quot;, MagicMock())      
security_pkg = ModuleType(&amp;quot;devsynth.security&amp;quot;)        
security_pkg.__path__ = []  # type: ignore        validation_stub = 
ModuleType(&amp;quot;devsynth.security.validation&amp;quot;)        
validation_stub.parse_bool_env = lambda _name, default=True: default        
sanitization_stub = 
ModuleType(&amp;quot;devsynth.security.sanitization&amp;quot;)        
sanitization_stub.sanitize_input = lambda text: text        auth_stub = 
ModuleType(&amp;quot;devsynth.security.authentication&amp;quot;)        
auth_stub.authenticate = MagicMock(return_value=True)        
auth_stub.hash_password = MagicMock(return_value=&amp;quot;hash&amp;quot;)      
auth_stub.verify_password = MagicMock(return_value=True)        
monkeypatch.setitem(sys.modules, &amp;quot;devsynth.security&amp;quot;, 
security_pkg)        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.validation&amp;quot;, validation_stub)        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.security.sanitization&amp;quot;, sanitization_stub        )  
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.authentication&amp;quot;, auth_stub)        
security_pkg.validation = validation_stub        security_pkg.sanitization = 
sanitization_stub        security_pkg.authentication = auth_stub        
config_stub = ModuleType(&amp;quot;devsynth.config&amp;quot;)        
config_stub.load_project_config = MagicMock(return_value={})        
config_stub.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, config_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, MagicMock())        
rich_module = ModuleType(&amp;quot;rich&amp;quot;)        rich_box = 
ModuleType(&amp;quot;rich.box&amp;quot;)        rich_box.ROUNDED = MagicMock()  
rich_box.Box = MagicMock()        rich_console = 
ModuleType(&amp;quot;rich.console&amp;quot;)            class _Console:         
def __init__(self, *args, **kwargs):                self.print_calls: 
list[tuple[tuple, dict]] = []                def print(self, *args, **kwargs):  
self.print_calls.append((args, kwargs))            rich_console.Console = 
_Console        rich_markdown = ModuleType(&amp;quot;rich.markdown&amp;quot;)   
class _Markdown:            def __init__(self, text: str, **kwargs: object) 
-&amp;gt; None:                self.text = text                self.kwargs = 
kwargs            rich_markdown.Markdown = _Markdown        rich_panel = 
ModuleType(&amp;quot;rich.panel&amp;quot;)            class _Panel:            
def __init__(self, renderable: object, **kwargs: object) -&amp;gt; None:        
self.renderable = renderable                self.kwargs = kwargs            
rich_panel.Panel = _Panel        rich_style = 
ModuleType(&amp;quot;rich.style&amp;quot;)            class _Style:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs            rich_style.Style
= _Style        rich_syntax = ModuleType(&amp;quot;rich.syntax&amp;quot;)       
class _Syntax:            def __init__(                self, code: str, lexer: 
str | None = None, **kwargs: object            ) -&amp;gt; None:                
self.code = code                self.lexer = lexer                self.kwargs = 
kwargs            rich_syntax.Syntax = _Syntax        rich_table = 
ModuleType(&amp;quot;rich.table&amp;quot;)            class _Table:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs                
self.columns: list[tuple[str, dict]] = []                self.rows: 
list[tuple[tuple, dict]] = []                def add_column(self, name: str, 
**kwargs: object) -&amp;gt; None:                self.columns.append((name, 
kwargs))                def add_row(self, *cells: object, **kwargs: object) 
-&amp;gt; None:                self.rows.append((cells, kwargs))            
rich_table.Table = _Table        rich_text = 
ModuleType(&amp;quot;rich.text&amp;quot;)            class _Text(str):          
def __new__(cls, text: str, *args: object, **kwargs: object):  # type: ignore   
obj = str.__new__(cls, text)                obj._args = args  # type: ignore    
obj._kwargs = kwargs  # type: ignore                return obj            
rich_text.Text = _Text        rich_module.box = rich_box        
rich_module.console = rich_console        rich_module.markdown = rich_markdown  
rich_module.panel = rich_panel        rich_module.style = rich_style        
rich_module.syntax = rich_syntax        rich_module.table = rich_table        
rich_module.text = rich_text        monkeypatch.setitem(sys.modules, 
&amp;quot;rich&amp;quot;, rich_module)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.box&amp;quot;, rich_box)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.console&amp;quot;, rich_console)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.markdown&amp;quot;, 
rich_markdown)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.panel&amp;quot;, rich_panel)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.style&amp;quot;, rich_style)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.syntax&amp;quot;, rich_syntax)  
monkeypatch.setitem(sys.modules, &amp;quot;rich.table&amp;quot;, rich_table)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.text&amp;quot;, rich_text)      
sys.modules.pop(&amp;quot;devsynth.interface.webui&amp;quot;, None)&amp;gt;     
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_guidance.py:272: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________ ERROR at setup of 
test_display_result_highlights_information _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453fb3e0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch):        
streamlit_mod, state = _make_streamlit_stub()        
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, streamlit_mod)  
monkeypatch.setitem(sys.modules, &amp;quot;chromadb&amp;quot;, MagicMock())     
monkeypatch.setitem(sys.modules, &amp;quot;uvicorn&amp;quot;, MagicMock())      
security_pkg = ModuleType(&amp;quot;devsynth.security&amp;quot;)        
security_pkg.__path__ = []  # type: ignore        validation_stub = 
ModuleType(&amp;quot;devsynth.security.validation&amp;quot;)        
validation_stub.parse_bool_env = lambda _name, default=True: default        
sanitization_stub = 
ModuleType(&amp;quot;devsynth.security.sanitization&amp;quot;)        
sanitization_stub.sanitize_input = lambda text: text        auth_stub = 
ModuleType(&amp;quot;devsynth.security.authentication&amp;quot;)        
auth_stub.authenticate = MagicMock(return_value=True)        
auth_stub.hash_password = MagicMock(return_value=&amp;quot;hash&amp;quot;)      
auth_stub.verify_password = MagicMock(return_value=True)        
monkeypatch.setitem(sys.modules, &amp;quot;devsynth.security&amp;quot;, 
security_pkg)        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.validation&amp;quot;, validation_stub)        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.security.sanitization&amp;quot;, sanitization_stub        )  
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.authentication&amp;quot;, auth_stub)        
security_pkg.validation = validation_stub        security_pkg.sanitization = 
sanitization_stub        security_pkg.authentication = auth_stub        
config_stub = ModuleType(&amp;quot;devsynth.config&amp;quot;)        
config_stub.load_project_config = MagicMock(return_value={})        
config_stub.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, config_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, MagicMock())        
rich_module = ModuleType(&amp;quot;rich&amp;quot;)        rich_box = 
ModuleType(&amp;quot;rich.box&amp;quot;)        rich_box.ROUNDED = MagicMock()  
rich_box.Box = MagicMock()        rich_console = 
ModuleType(&amp;quot;rich.console&amp;quot;)            class _Console:         
def __init__(self, *args, **kwargs):                self.print_calls: 
list[tuple[tuple, dict]] = []                def print(self, *args, **kwargs):  
self.print_calls.append((args, kwargs))            rich_console.Console = 
_Console        rich_markdown = ModuleType(&amp;quot;rich.markdown&amp;quot;)   
class _Markdown:            def __init__(self, text: str, **kwargs: object) 
-&amp;gt; None:                self.text = text                self.kwargs = 
kwargs            rich_markdown.Markdown = _Markdown        rich_panel = 
ModuleType(&amp;quot;rich.panel&amp;quot;)            class _Panel:            
def __init__(self, renderable: object, **kwargs: object) -&amp;gt; None:        
self.renderable = renderable                self.kwargs = kwargs            
rich_panel.Panel = _Panel        rich_style = 
ModuleType(&amp;quot;rich.style&amp;quot;)            class _Style:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs            rich_style.Style
= _Style        rich_syntax = ModuleType(&amp;quot;rich.syntax&amp;quot;)       
class _Syntax:            def __init__(                self, code: str, lexer: 
str | None = None, **kwargs: object            ) -&amp;gt; None:                
self.code = code                self.lexer = lexer                self.kwargs = 
kwargs            rich_syntax.Syntax = _Syntax        rich_table = 
ModuleType(&amp;quot;rich.table&amp;quot;)            class _Table:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs                
self.columns: list[tuple[str, dict]] = []                self.rows: 
list[tuple[tuple, dict]] = []                def add_column(self, name: str, 
**kwargs: object) -&amp;gt; None:                self.columns.append((name, 
kwargs))                def add_row(self, *cells: object, **kwargs: object) 
-&amp;gt; None:                self.rows.append((cells, kwargs))            
rich_table.Table = _Table        rich_text = 
ModuleType(&amp;quot;rich.text&amp;quot;)            class _Text(str):          
def __new__(cls, text: str, *args: object, **kwargs: object):  # type: ignore   
obj = str.__new__(cls, text)                obj._args = args  # type: ignore    
obj._kwargs = kwargs  # type: ignore                return obj            
rich_text.Text = _Text        rich_module.box = rich_box        
rich_module.console = rich_console        rich_module.markdown = rich_markdown  
rich_module.panel = rich_panel        rich_module.style = rich_style        
rich_module.syntax = rich_syntax        rich_module.table = rich_table        
rich_module.text = rich_text        monkeypatch.setitem(sys.modules, 
&amp;quot;rich&amp;quot;, rich_module)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.box&amp;quot;, rich_box)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.console&amp;quot;, rich_console)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.markdown&amp;quot;, 
rich_markdown)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.panel&amp;quot;, rich_panel)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.style&amp;quot;, rich_style)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.syntax&amp;quot;, rich_syntax)  
monkeypatch.setitem(sys.modules, &amp;quot;rich.table&amp;quot;, rich_table)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.text&amp;quot;, rich_text)      
sys.modules.pop(&amp;quot;devsynth.interface.webui&amp;quot;, None)&amp;gt;     
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_guidance.py:272: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_ui_progress_tracks_status_and_subtasks _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453f9b20&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch):        
streamlit_mod, state = _make_streamlit_stub()        
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, streamlit_mod)  
monkeypatch.setitem(sys.modules, &amp;quot;chromadb&amp;quot;, MagicMock())     
monkeypatch.setitem(sys.modules, &amp;quot;uvicorn&amp;quot;, MagicMock())      
security_pkg = ModuleType(&amp;quot;devsynth.security&amp;quot;)        
security_pkg.__path__ = []  # type: ignore        validation_stub = 
ModuleType(&amp;quot;devsynth.security.validation&amp;quot;)        
validation_stub.parse_bool_env = lambda _name, default=True: default        
sanitization_stub = 
ModuleType(&amp;quot;devsynth.security.sanitization&amp;quot;)        
sanitization_stub.sanitize_input = lambda text: text        auth_stub = 
ModuleType(&amp;quot;devsynth.security.authentication&amp;quot;)        
auth_stub.authenticate = MagicMock(return_value=True)        
auth_stub.hash_password = MagicMock(return_value=&amp;quot;hash&amp;quot;)      
auth_stub.verify_password = MagicMock(return_value=True)        
monkeypatch.setitem(sys.modules, &amp;quot;devsynth.security&amp;quot;, 
security_pkg)        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.validation&amp;quot;, validation_stub)        
monkeypatch.setitem(            sys.modules, 
&amp;quot;devsynth.security.sanitization&amp;quot;, sanitization_stub        )  
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.security.authentication&amp;quot;, auth_stub)        
security_pkg.validation = validation_stub        security_pkg.sanitization = 
sanitization_stub        security_pkg.authentication = auth_stub        
config_stub = ModuleType(&amp;quot;devsynth.config&amp;quot;)        
config_stub.load_project_config = MagicMock(return_value={})        
config_stub.save_config = MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.config&amp;quot;, config_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;yaml&amp;quot;, MagicMock())        
rich_module = ModuleType(&amp;quot;rich&amp;quot;)        rich_box = 
ModuleType(&amp;quot;rich.box&amp;quot;)        rich_box.ROUNDED = MagicMock()  
rich_box.Box = MagicMock()        rich_console = 
ModuleType(&amp;quot;rich.console&amp;quot;)            class _Console:         
def __init__(self, *args, **kwargs):                self.print_calls: 
list[tuple[tuple, dict]] = []                def print(self, *args, **kwargs):  
self.print_calls.append((args, kwargs))            rich_console.Console = 
_Console        rich_markdown = ModuleType(&amp;quot;rich.markdown&amp;quot;)   
class _Markdown:            def __init__(self, text: str, **kwargs: object) 
-&amp;gt; None:                self.text = text                self.kwargs = 
kwargs            rich_markdown.Markdown = _Markdown        rich_panel = 
ModuleType(&amp;quot;rich.panel&amp;quot;)            class _Panel:            
def __init__(self, renderable: object, **kwargs: object) -&amp;gt; None:        
self.renderable = renderable                self.kwargs = kwargs            
rich_panel.Panel = _Panel        rich_style = 
ModuleType(&amp;quot;rich.style&amp;quot;)            class _Style:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs            rich_style.Style
= _Style        rich_syntax = ModuleType(&amp;quot;rich.syntax&amp;quot;)       
class _Syntax:            def __init__(                self, code: str, lexer: 
str | None = None, **kwargs: object            ) -&amp;gt; None:                
self.code = code                self.lexer = lexer                self.kwargs = 
kwargs            rich_syntax.Syntax = _Syntax        rich_table = 
ModuleType(&amp;quot;rich.table&amp;quot;)            class _Table:            
def __init__(self, *args: object, **kwargs: object) -&amp;gt; None:             
self.args = args                self.kwargs = kwargs                
self.columns: list[tuple[str, dict]] = []                self.rows: 
list[tuple[tuple, dict]] = []                def add_column(self, name: str, 
**kwargs: object) -&amp;gt; None:                self.columns.append((name, 
kwargs))                def add_row(self, *cells: object, **kwargs: object) 
-&amp;gt; None:                self.rows.append((cells, kwargs))            
rich_table.Table = _Table        rich_text = 
ModuleType(&amp;quot;rich.text&amp;quot;)            class _Text(str):          
def __new__(cls, text: str, *args: object, **kwargs: object):  # type: ignore   
obj = str.__new__(cls, text)                obj._args = args  # type: ignore    
obj._kwargs = kwargs  # type: ignore                return obj            
rich_text.Text = _Text        rich_module.box = rich_box        
rich_module.console = rich_console        rich_module.markdown = rich_markdown  
rich_module.panel = rich_panel        rich_module.style = rich_style        
rich_module.syntax = rich_syntax        rich_module.table = rich_table        
rich_module.text = rich_text        monkeypatch.setitem(sys.modules, 
&amp;quot;rich&amp;quot;, rich_module)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.box&amp;quot;, rich_box)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.console&amp;quot;, rich_console)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.markdown&amp;quot;, 
rich_markdown)        monkeypatch.setitem(sys.modules, 
&amp;quot;rich.panel&amp;quot;, rich_panel)        
monkeypatch.setitem(sys.modules, &amp;quot;rich.style&amp;quot;, rich_style)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.syntax&amp;quot;, rich_syntax)  
monkeypatch.setitem(sys.modules, &amp;quot;rich.table&amp;quot;, rich_table)    
monkeypatch.setitem(sys.modules, &amp;quot;rich.text&amp;quot;, rich_text)      
sys.modules.pop(&amp;quot;devsynth.interface.webui&amp;quot;, None)&amp;gt;     
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_guidance.py:272: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___________ ERROR at setup of 
test_handle_command_errors_passthrough ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452fac90&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_handle_command_errors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: File 
not found: config.yaml-Make sure the file exists] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144fca600&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_handle_command_errors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: 
Permission denied: secrets.env-necessary permissions] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446c7950&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_handle_command_errors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: 
Invalid value: bad input-Please check your input] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446c6900&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_handle_command_errors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: 
Missing key: &amp;#x27;api_key&amp;#x27;-Verify that the referenced key exists] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1446c4860&amp;gt;    @pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_handle_command_errors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Type 
error: wrong type-Check that all inputs] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ab2150&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_handle_command_errors_generic_exception ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ab0050&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Reload the WebUI module with a mocked Streamlit 
dependency.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock() 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_handle_command_errors.py:26: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_get_layout_config_respects_breakpoints _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a3230&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_ask_question_and_confirm_choice_use_streamlit_controls _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1454ddca0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____ ERROR at setup of 
test_display_result_message_types_provide_guidance _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a34d0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_______ ERROR at setup of 
test_display_result_markup_and_keyword_routing _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a07a0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[File not found-file_not_found] _monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451a8d10&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Permission denied-permission_denied] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14545b200&amp;gt;    @pytest.fixture    def webui_module(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload
``devsynth.interface.webui`` with a fresh Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock()       
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Invalid parameter-invalid_parameter] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1451ced50&amp;gt;    @pytest.fixture    def webui_module(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload
``devsynth.interface.webui`` with a fresh Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock()       
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Invalid format-invalid_format] _monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452c4cb0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Missing key-key_error] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ffaa50&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Type error-type_error] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452512b0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[TypeError-type_error] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145251b20&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Configuration error-config_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145252390&amp;gt;    @pytest.fixture    def webui_module(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload
``devsynth.interface.webui`` with a fresh Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock()       
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Connection error-connection_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145565b20&amp;gt;    @pytest.fixture    def webui_module(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload
``devsynth.interface.webui`` with a fresh Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock()       
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[API error-api_error] __monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145566960&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Validation error-validation_error] 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14501d040&amp;gt;    @pytest.fixture    def webui_module(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload
``devsynth.interface.webui`` with a fresh Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = make_streamlit_mock()       
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Syntax error-syntax_error] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145701b20&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Import error-import_error] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457002c0&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_get_error_type_matches_keywords[Completely different-] _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145701910&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_error_suggestions_and_docs_cover_known_and_unknown ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457b9400&amp;gt;    
@pytest.fixture    def webui_module(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
Tuple:        &amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` 
with a fresh Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            st = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_messaging.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___________ ERROR at setup of 
test_lazy_streamlit_proxy_imports_once ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14584fc50&amp;gt;    
@pytest.fixture()    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; Iterator[_HarnessStreamlit]:        
&amp;quot;&amp;quot;&amp;quot;Install the harness as the cached Streamlit module
for the duration.&amp;quot;&amp;quot;&amp;quot;            original_import = 
importlib.import_module        stub = _HarnessStreamlit()            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py:116: 
AttributeError___ ERROR at setup of 
test_progress_indicator_emits_eta_and_sanitized_status ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14584e870&amp;gt;    
@pytest.fixture()    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; Iterator[_HarnessStreamlit]:        
&amp;quot;&amp;quot;&amp;quot;Install the harness as the cached Streamlit module
for the duration.&amp;quot;&amp;quot;&amp;quot;            original_import = 
importlib.import_module        stub = _HarnessStreamlit()            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py:116: 
AttributeError______ ERROR at setup of 
test_permission_denied_error_renders_suggestions ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14584f7d0&amp;gt;    
@pytest.fixture()    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; Iterator[_HarnessStreamlit]:        
&amp;quot;&amp;quot;&amp;quot;Install the harness as the cached Streamlit module
for the duration.&amp;quot;&amp;quot;&amp;quot;            original_import = 
importlib.import_module        stub = _HarnessStreamlit()            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py:116: 
AttributeError_______ ERROR at setup of 
test_display_result_translates_markup_to_html ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145843920&amp;gt;    
@pytest.fixture    def reloaded_webui(monkeypatch: pytest.MonkeyPatch):        
&amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` with a fresh 
Streamlit stub.&amp;quot;&amp;quot;&amp;quot;            streamlit_stub = 
make_streamlit_mock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, streamlit_stub)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_lazy_streamlit_and_wizard.py:23: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_progress_complete_cascades_with_sanitized_fallback ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457cdca0&amp;gt;    
@pytest.fixture()    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; _StubStreamlit:        stub = _StubStreamlit()&amp;gt;       
monkeypatch.setattr(webui, &amp;quot;st&amp;quot;, stub)E       AttributeError: 
&amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: 
AttributeError__________ ERROR at setup of 
test_webui_layout_and_display_behaviors ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1452fbc50&amp;gt;    
@pytest.fixture()    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; _StubStreamlit:        stub = _StubStreamlit()&amp;gt;       
monkeypatch.setattr(webui, &amp;quot;st&amp;quot;, stub)E       AttributeError: 
&amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: AttributeError________ 
ERROR at setup of test_ui_progress_status_transitions_and_eta 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458a9040&amp;gt;    @pytest.fixture()    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; _StubStreamlit:        stub = 
_StubStreamlit()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: 
AttributeError_____________ ERROR at setup of test_ensure_router_caches_instance
_____________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458a8b60&amp;gt;    @pytest.fixture()    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; _StubStreamlit:        stub = 
_StubStreamlit()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: AttributeError________ 
ERROR at setup of test_webui_run_configures_layout_and_router 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458aab70&amp;gt;    @pytest.fixture()    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; _StubStreamlit:        stub = 
_StubStreamlit()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: 
AttributeError__________ ERROR at setup of 
test_webui_run_handles_streamlit_errors ___________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458a9130&amp;gt;    
@pytest.fixture()    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; _StubStreamlit:        stub = _StubStreamlit()&amp;gt;       
monkeypatch.setattr(webui, &amp;quot;st&amp;quot;, stub)E       AttributeError: 
&amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_progress_cascade_fast.py:249: AttributeError_ ERROR 
at setup of test_webui_run_injects_resize_script_and_configures_layout 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14498ed50&amp;gt;    @pytest.fixture    def streamlit_router_stub(monkeypatch:
pytest.MonkeyPatch):        &amp;quot;&amp;quot;&amp;quot;Provide a stubbed 
Streamlit module and Router replacement.&amp;quot;&amp;quot;&amp;quot;          
from devsynth.interface import webui as webui_module            
previous_streamlit = sys.modules.get(&amp;quot;streamlit&amp;quot;)        # 
Import Router from the webui package submodule&amp;gt;       from 
devsynth.interface.webui import 
Router/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/
test_webui_run_fast.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___ ERROR at setup of 
test_webui_run_configures_dashboard_and_invokes_router ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ef7dd0&amp;gt;    
@pytest.fixture    def streamlit_free_webui(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; Iterator[Tuple[ModuleType, ModuleType, 
dict]]:        &amp;quot;&amp;quot;&amp;quot;Reload 
:mod:`devsynth.interface.webui` with a deterministic Streamlit 
stub.&amp;quot;&amp;quot;&amp;quot;            st = _mock_streamlit()        
st.sidebar.radio = MagicMock(return_value=&amp;quot;Summary&amp;quot;)        
st.session_state.screen_width = 860        st.session_state.screen_height = 600 
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_streamlit_free_progress_fast.py:31: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_ ERROR at setup of 
test_progress_updates_emit_telemetry_and_sanitize_checkpoints _monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ef68a0&amp;gt;    
@pytest.fixture    def progress_webui(        monkeypatch: pytest.MonkeyPatch,  
) -&amp;gt; Iterator[Tuple[ModuleType, ModuleType, MagicMock, MagicMock, 
MagicMock]]:        &amp;quot;&amp;quot;&amp;quot;Provide a reloaded WebUI 
module with deterministic progress containers.&amp;quot;&amp;quot;&amp;quot;    
st = _mock_streamlit()        status_container = 
MagicMock(name=&amp;quot;status_container&amp;quot;)        time_container = 
MagicMock(name=&amp;quot;time_container&amp;quot;)        bar_container = 
MagicMock(name=&amp;quot;bar_container&amp;quot;)            st.empty = 
MagicMock(side_effect=)        st.progress = 
MagicMock(return_value=bar_container)            
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_streamlit_free_progress_fast.py:62: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____ ERROR at setup of 
test_display_result_sanitizes_message_before_render _____monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144e0e8d0&amp;gt;    
@pytest.fixture    def sanitized_webui(        monkeypatch: pytest.MonkeyPatch, 
) -&amp;gt; Iterator[Tuple[ModuleType, ModuleType]]:        
&amp;quot;&amp;quot;&amp;quot;Yield a reloaded WebUI module for sanitization 
assertions.&amp;quot;&amp;quot;&amp;quot;            st = _mock_streamlit()     
monkeypatch.setitem(sys.modules, &amp;quot;streamlit&amp;quot;, st)    &amp;gt; 
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_streamlit_free_progress_fast.py:82: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;
Web UI module for DevSynth.        This module provides web interface components
for DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering 
import (        LifecyclePages,        OperationsPages,        PageRenderer,    
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_lazy_loader_imports_streamlit_stub_once ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144689430&amp;gt;    
@pytest.fixture    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
StreamlitStub:        &amp;quot;&amp;quot;&amp;quot;Install a deterministic 
Streamlit stub for the duration of a test.&amp;quot;&amp;quot;&amp;quot;        
stub = StreamlitStub()        original_import = importlib.import_module         
def fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:227: 
AttributeError_________ ERROR at setup of 
test_display_result_sanitizes_error_output _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444ddf10&amp;gt;    
@pytest.fixture    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
StreamlitStub:        &amp;quot;&amp;quot;&amp;quot;Install a deterministic 
Streamlit stub for the duration of a test.&amp;quot;&amp;quot;&amp;quot;        
stub = StreamlitStub()        original_import = importlib.import_module         
def fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:227: AttributeError________
ERROR at setup of test_ui_progress_tracks_status_and_subtasks 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1444de060&amp;gt;    @pytest.fixture    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; StreamlitStub:        
&amp;quot;&amp;quot;&amp;quot;Install a deterministic Streamlit stub for the 
duration of a test.&amp;quot;&amp;quot;&amp;quot;            stub = 
StreamlitStub()        original_import = importlib.import_module            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:227: AttributeError____ 
ERROR at setup of test_router_run_uses_default_and_persists_selection 
_____monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a5d460&amp;gt;    @pytest.fixture    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; StreamlitStub:        
&amp;quot;&amp;quot;&amp;quot;Install a deterministic Streamlit stub for the 
duration of a test.&amp;quot;&amp;quot;&amp;quot;            stub = 
StreamlitStub()        original_import = importlib.import_module            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:227: AttributeError________
ERROR at setup of test_webui_run_configures_router_and_layout 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a5ef60&amp;gt;    @pytest.fixture    def streamlit_stub(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; StreamlitStub:        
&amp;quot;&amp;quot;&amp;quot;Install a deterministic Streamlit stub for the 
duration of a test.&amp;quot;&amp;quot;&amp;quot;            stub = 
StreamlitStub()        original_import = importlib.import_module            def 
fake_import(name: str, package: str | None = None):            if name == 
&amp;quot;streamlit&amp;quot;:                stub.import_requests += 1         
return stub            return original_import(name, package)    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:227: AttributeError________
ERROR at setup of test_ask_question_selectbox_indexes_default 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a5f5f0&amp;gt;    @pytest.fixture    def webui_under_test(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; SimpleNamespace:        
&amp;quot;&amp;quot;&amp;quot;Reload ``devsynth.interface.webui`` with a rich 
Streamlit double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_ask_question_text_input_when_no_choices ________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14445da90&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________ ERROR at setup of 
test_confirm_choice_returns_checkbox_value _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14464fe90&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__ ERROR at setup of 
test_display_result_error_surfaces_suggestions_and_docs ___monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14464c860&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________ ERROR at setup of 
test_render_traceback_expander_renders_code _________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14464e660&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_ui_progress_sanitizes_updates _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a92e0&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ ERROR at setup of 
test_ensure_router_memoizes_instance ____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145098530&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ ERROR at setup of 
test_run_handles_page_config_errors _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145098410&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ ERROR at setup of 
test_run_renders_layout_and_router _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451b5430&amp;gt;    
@pytest.fixture    def webui_under_test(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; SimpleNamespace:        &amp;quot;&amp;quot;&amp;quot;Reload 
``devsynth.interface.webui`` with a rich Streamlit 
double.&amp;quot;&amp;quot;&amp;quot;            from 
tests.unit.interface.test_webui_enhanced import _mock_streamlit            
fake_streamlit = _mock_streamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_targeted_branches.py:24: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError=================================== 
FAILURES ===================================_____________ 
test_provider_factory_offline_uses_stub_safe_default _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x11540a9f0&amp;gt;    
@pytest.mark.fast    def test_provider_factory_offline_uses_stub_safe_default(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Return the stub provider when offline guard is 
enabled.            ReqID: N/A        &amp;quot;&amp;quot;&amp;quot;            
config = _make_provider_config(openai_key=&amp;quot;token&amp;quot;)        
_install_factory_config(monkeypatch, config)            
monkeypatch.setenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, 
&amp;quot;true&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
&amp;quot;stub&amp;quot;)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
raising=False)            provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)&amp;gt;       assert
isinstance(provider, StubProvider)E       assert FalseE        +  where False = 
isinstance(&amp;lt;devsynth.adapters.provider_system.StubProvider object at 
0x115fc73e0&amp;gt;, 
StubProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:251: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,300 - 
devsynth.adapters.provider_system - INFO - Falling back to Stub provider: 
DEVSYNTH_OFFLINE active; using safe provider----------------------------- 
Captured stderr call -------------------------------- Logging error ---Traceback
(most recent call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 250, in 
test_provider_factory_offline_uses_stub_safe_default    provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 311, in create_provider    return 
_safe_provider(&amp;quot;DEVSYNTH_OFFLINE active; using safe provider&amp;quot;)
File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 300, in _safe_provider    
logger.info(&amp;quot;Falling back to Stub provider: %s&amp;quot;, reason)  File
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 623, in info    self._log(logging.INFO, msg, *args,
**kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Falling back to Stub provider: 
%s&amp;#x27;Arguments: (&amp;#x27;DEVSYNTH_OFFLINE active; using safe 
provider&amp;#x27;,)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.provider_system:logging_setup.py:615 Falling back to Stub 
provider: DEVSYNTH_OFFLINE active; using safe provider_____________ 
test_provider_factory_offline_uses_null_safe_default _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x11540a960&amp;gt;    
@pytest.mark.fast    def test_provider_factory_offline_uses_null_safe_default(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Return the null provider when offline guard favors
null safe defaults.            ReqID: N/A        &amp;quot;&amp;quot;&amp;quot; 
config = _make_provider_config(openai_key=&amp;quot;token&amp;quot;)        
_install_factory_config(monkeypatch, config)            
monkeypatch.setenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, &amp;quot;1&amp;quot;) 
monkeypatch.setenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
&amp;quot;null&amp;quot;)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
raising=False)            provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)&amp;gt;       assert
isinstance(provider, NullProvider)E       assert FalseE        +  where False = 
isinstance(&amp;lt;devsynth.adapters.provider_system.NullProvider object at 
0x115fc58b0&amp;gt;, 
NullProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:272: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,615 - 
devsynth.adapters.provider_system - INFO - Falling back to Null provider: 
DEVSYNTH_OFFLINE active; using safe provider----------------------------- 
Captured stderr call -------------------------------- Logging error ---Traceback
(most recent call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 271, in 
test_provider_factory_offline_uses_null_safe_default    provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 311, in create_provider    return 
_safe_provider(&amp;quot;DEVSYNTH_OFFLINE active; using safe provider&amp;quot;)
File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 305, in _safe_provider    
logger.info(&amp;quot;Falling back to Null provider: %s&amp;quot;, reason)  File
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 623, in info    self._log(logging.INFO, msg, *args,
**kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Falling back to Null provider: 
%s&amp;#x27;Arguments: (&amp;#x27;DEVSYNTH_OFFLINE active; using safe 
provider&amp;#x27;,)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.provider_system:logging_setup.py:615 Falling back to Null 
provider: DEVSYNTH_OFFLINE active; using safe provider_ 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable _monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x11540acf0&amp;gt;    @pytest.mark.fast    def 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable(        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:      
&amp;quot;&amp;quot;&amp;quot;Use the safe default provider when LM Studio 
fallback is not permitted.            ReqID: N/A        
&amp;quot;&amp;quot;&amp;quot;            config = 
_make_provider_config(openai_key=None)        
_install_factory_config(monkeypatch, config)            
monkeypatch.setenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
&amp;quot;null&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
&amp;quot;false&amp;quot;)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)            provider = ProviderFactory.create_provider()&amp;gt;  
assert isinstance(provider, NullProvider)E       assert FalseE        +  where 
False = isinstance(&amp;lt;devsynth.adapters.provider_system.NullProvider object
at 0x115e6d100&amp;gt;, 
NullProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:294: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,636 - 
devsynth.adapters.provider_system - INFO - Falling back to Null provider: No 
OPENAI_API_KEY; LM Studio not marked available. Hint: export OPENAI_API_KEY, 
export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set DEVSYNTH_PROVIDER=stub 
for offline runs.----------------------------- Captured stderr call 
-------------------------------- Logging error ---Traceback (most recent call 
last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 293, in 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable    provider = ProviderFactory.create_provider()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 349, in create_provider    return 
_safe_provider(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 305, in _safe_provider    
logger.info(&amp;quot;Falling back to Null provider: %s&amp;quot;, reason)  File
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 623, in info    self._log(logging.INFO, msg, *args,
**kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Falling back to Null provider: 
%s&amp;#x27;Arguments: (&amp;#x27;No OPENAI_API_KEY; LM Studio not marked 
available. Hint: export OPENAI_API_KEY, export 
DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set DEVSYNTH_PROVIDER=stub for 
offline runs.&amp;#x27;,)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.provider_system:logging_setup.py:615 Falling back to Null 
provider: No OPENAI_API_KEY; LM Studio not marked available. Hint: export 
OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set 
DEVSYNTH_PROVIDER=stub for offline runs._ 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default 
__monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115fa9a30&amp;gt;    @pytest.mark.fast    def 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default(    
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Return a null provider when LM Studio 
instantiation raises an exception.            ReqID: N/A        
&amp;quot;&amp;quot;&amp;quot;            config = _make_provider_config()      
_install_factory_config(monkeypatch, config)            
monkeypatch.setenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
&amp;quot;null&amp;quot;)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
raising=False)            monkeypatch.setattr(            provider_system,      
&amp;quot;LMStudioProvider&amp;quot;,            
MagicMock(side_effect=RuntimeError(&amp;quot;kaboom&amp;quot;)),        )       
provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)&amp;gt;       
assert isinstance(provider, NullProvider)E       assert FalseE        +  where 
False = isinstance(&amp;lt;devsynth.adapters.provider_system.NullProvider object
at 0x115e586e0&amp;gt;, 
NullProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:351: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,671 - 
devsynth.adapters.provider_system - WARNING - Unknown provider type 
&amp;#x27;ProviderType.LMSTUDIO&amp;#x27;, falling back to safe 
default2025-10-28 09:25:17,672 - devsynth.adapters.provider_system - INFO - 
Falling back to Null provider: Unknown provider 
type----------------------------- Captured stderr call 
-------------------------------- Logging error ---Traceback (most recent call 
last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 350, in 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default    
provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 432, in create_provider    
logger.warning(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 627, in warning    self._log(logging.WARNING, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;quot;Unknown provider type &amp;#x27;%s&amp;#x27;, 
falling back to safe default&amp;quot;Arguments: (&amp;lt;ProviderType.LMSTUDIO:
&amp;#x27;lmstudio&amp;#x27;&amp;gt;,)--- Logging error ---Traceback (most 
recent call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 350, in 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default    
provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 436, in create_provider    return 
_safe_provider(&amp;quot;Unknown provider type&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 305, in _safe_provider    
logger.info(&amp;quot;Falling back to Null provider: %s&amp;quot;, reason)  File
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 623, in info    self._log(logging.INFO, msg, *args,
**kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Falling back to Null provider: 
%s&amp;#x27;Arguments: (&amp;#x27;Unknown provider 
type&amp;#x27;,)------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider type 
&amp;#x27;ProviderType.LMSTUDIO&amp;#x27;, falling back to safe defaultINFO     
devsynth.adapters.provider_system:logging_setup.py:615 Falling back to Null 
provider: Unknown provider type_______ 
test_provider_factory_openai_explicit_missing_key_surfaces_error 
_______monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115faa8a0&amp;gt;    @pytest.mark.fast    def 
test_provider_factory_openai_explicit_missing_key_surfaces_error(        
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Surface explicit OpenAI credential errors via null
provider.            ReqID: N/A        &amp;quot;&amp;quot;&amp;quot;           
config = _make_provider_config(openai_key=None)        
_install_factory_config(monkeypatch, config)            
monkeypatch.delenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
raising=False)            provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)&amp;gt;       assert
isinstance(provider, NullProvider)E       assert FalseE        +  where False = 
isinstance(&amp;lt;devsynth.adapters.provider_system.NullProvider object at 
0x115f66450&amp;gt;, 
NullProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:372: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,695 - 
devsynth.adapters.provider_system - ERROR - OpenAI API key is missing for 
explicitly requested OpenAI provider. Set OPENAI_API_KEY or choose a safe 
provider via DEVSYNTH_PROVIDER=stub.2025-10-28 09:25:17,696 - 
devsynth.adapters.provider_system - ERROR - Failed to create provider openai: 
Missing OPENAI_API_KEY for OpenAI provider. Set OPENAI_API_KEY or use 
DEVSYNTH_PROVIDER=stub.----------------------------- Captured stderr call 
-------------------------------- Logging error ---Traceback (most recent call 
last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 371, in 
test_provider_factory_openai_explicit_missing_key_surfaces_error    provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 318, in create_provider    logger.error(
File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 631, in error    self._log(logging.ERROR, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;OpenAI API key is missing for explicitly 
requested OpenAI provider. Set OPENAI_API_KEY or choose a safe provider via 
DEVSYNTH_PROVIDER=stub.&amp;#x27;Arguments: ()--- Logging error ---Traceback 
(most recent call last):  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 324, in create_provider    raise 
ProviderError(devsynth.exceptions.ProviderError: Missing OPENAI_API_KEY for 
OpenAI provider. Set OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.During 
handling of the above exception, another exception occurred:Traceback (most 
recent call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 371, in 
test_provider_factory_openai_explicit_missing_key_surfaces_error    provider = 
ProviderFactory.create_provider(&amp;quot;openai&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 438, in create_provider    
logger.error(f&amp;quot;Failed to create provider {provider_type}: 
{e}&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 631, in error    self._log(logging.ERROR, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Failed to create provider openai: Missing 
OPENAI_API_KEY for OpenAI provider. Set OPENAI_API_KEY or use 
DEVSYNTH_PROVIDER=stub.&amp;#x27;Arguments: ()------------------------------ 
Captured log call -------------------------------ERROR    
devsynth.adapters.provider_system:logging_setup.py:615 OpenAI API key is missing
for explicitly requested OpenAI provider. Set OPENAI_API_KEY or choose a safe 
provider via DEVSYNTH_PROVIDER=stub.ERROR    
devsynth.adapters.provider_system:logging_setup.py:615 Failed to create provider
openai: Missing OPENAI_API_KEY for OpenAI provider. Set OPENAI_API_KEY or use 
DEVSYNTH_PROVIDER=stub.__________ 
test_provider_factory_anthropic_missing_key_surfaces_error __________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115fa9550&amp;gt;    
@pytest.mark.fast    def 
test_provider_factory_anthropic_missing_key_surfaces_error(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Surface explicit Anthropic credential errors via 
null provider.            ReqID: N/A        &amp;quot;&amp;quot;&amp;quot;      
config = _make_provider_config()        _install_factory_config(monkeypatch, 
config)            monkeypatch.delenv(&amp;quot;ANTHROPIC_API_KEY&amp;quot;, 
raising=False)        monkeypatch.delenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)            provider = 
ProviderFactory.create_provider(&amp;quot;anthropic&amp;quot;)&amp;gt;       
assert isinstance(provider, NullProvider)E       assert FalseE        +  where 
False = isinstance(&amp;lt;devsynth.adapters.provider_system.NullProvider object
at 0x115efc830&amp;gt;, 
NullProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:393: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,724 - 
devsynth.adapters.provider_system - ERROR - Anthropic API key is missing for 
explicitly requested Anthropic provider2025-10-28 09:25:17,725 - 
devsynth.adapters.provider_system - ERROR - Failed to create provider anthropic:
Anthropic API key is required for Anthropic 
provider----------------------------- Captured stderr call 
-------------------------------- Logging error ---Traceback (most recent call 
last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 392, in 
test_provider_factory_anthropic_missing_key_surfaces_error    provider = 
ProviderFactory.create_provider(&amp;quot;anthropic&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 387, in create_provider    logger.error(
File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 631, in error    self._log(logging.ERROR, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Anthropic API key is missing for explicitly 
requested Anthropic provider&amp;#x27;Arguments: ()--- Logging error 
---Traceback (most recent call last):  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 391, in create_provider    raise 
ProviderError(devsynth.exceptions.ProviderError: Anthropic API key is required 
for Anthropic providerDuring handling of the above exception, another exception 
occurred:Traceback (most recent call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 392, in 
test_provider_factory_anthropic_missing_key_surfaces_error    provider = 
ProviderFactory.create_provider(&amp;quot;anthropic&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 438, in create_provider    
logger.error(f&amp;quot;Failed to create provider {provider_type}: 
{e}&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 631, in error    self._log(logging.ERROR, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Failed to create provider anthropic: Anthropic 
API key is required for Anthropic provider&amp;#x27;Arguments: 
()------------------------------ Captured log call 
-------------------------------ERROR    
devsynth.adapters.provider_system:logging_setup.py:615 Anthropic API key is 
missing for explicitly requested Anthropic providerERROR    
devsynth.adapters.provider_system:logging_setup.py:615 Failed to create provider
anthropic: Anthropic API key is required for Anthropic provider_______________ 
test_provider_factory_accepts_provider_type_enum _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115fab080&amp;gt;    
@pytest.mark.fast    def test_provider_factory_accepts_provider_type_enum(      
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Interpret ``ProviderType`` enum values when 
selecting providers.            ReqID: N/A        &amp;quot;&amp;quot;&amp;quot;
config = _make_provider_config(default_provider=&amp;quot;stub&amp;quot;)       
_install_factory_config(monkeypatch, config)            
monkeypatch.delenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_DISABLE_PROVIDERS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_SAFE_DEFAULT_PROVIDER&amp;quot;, 
raising=False)            provider = 
ProviderFactory.create_provider(ProviderType.STUB)&amp;gt;       assert 
isinstance(provider, StubProvider)E       assert FalseE        +  where False = 
isinstance(&amp;lt;devsynth.adapters.provider_system.StubProvider object at 
0x111b9f5c0&amp;gt;, 
StubProvider)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/ada
pters/test_provider_system_additional.py:414: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:25:17,753 - 
devsynth.adapters.provider_system - WARNING - Unknown provider type 
&amp;#x27;ProviderType.STUB&amp;#x27;, falling back to safe default2025-10-28 
09:25:17,754 - devsynth.adapters.provider_system - INFO - Falling back to Stub 
provider: Unknown provider type----------------------------- Captured stderr 
call -------------------------------- Logging error ---Traceback (most recent 
call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 413, in 
test_provider_factory_accepts_provider_type_enum    provider = 
ProviderFactory.create_provider(ProviderType.STUB)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 432, in create_provider    
logger.warning(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 627, in warning    self._log(logging.WARNING, msg, 
*args, **kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;quot;Unknown provider type &amp;#x27;%s&amp;#x27;, 
falling back to safe default&amp;quot;Arguments: (&amp;lt;ProviderType.STUB: 
&amp;#x27;stub&amp;#x27;&amp;gt;,)--- Logging error ---Traceback (most recent 
call last):  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 73, in emit    
if self.shouldRollover(record):       ^^^^^^^^^^^^^^^^^^^^^^^^^^^  File 
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/handlers.py&amp;quot;, line 191, in 
shouldRollover    self.stream = self._open()                  ^^^^^^^^^^^^  File
&amp;quot;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/V
ersions/3.12/lib/python3.12/logging/__init__.py&amp;quot;, line 1263, in _open  
return open_func(self.baseFilename, self.mode,           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^FileNotFoundError: [Errno 2] No such file
or directory: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log
&amp;#x27;Call stack:  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, 
line 198, in _run_module_as_main  File &amp;quot;&amp;lt;frozen 
runpy&amp;gt;&amp;quot;, line 88, in _run_code  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pytest/__main__.py&amp;quot;, line 9, in 
&amp;lt;module&amp;gt;    raise SystemExit(pytest.console_main())  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 201, in 
console_main    code = main()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/config/__init__.py&amp;quot;, line 175, in main    
ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 336, in pytest_cmdline_main   
return wrap_session(config, _main)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 289, in wrap_session    
session.exitstatus = doit(config, session) or 0  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 343, in _main    
config.hook.pytest_runtestloop(session=session)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/main.py&amp;quot;, line 367, in pytest_runtestloop    
item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 117, in 
pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 136, in runtestprotocol    
reports.append(call_and_report(item, &amp;quot;call&amp;quot;, log))  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 245, in call_and_report    
call = CallInfo.from_call(  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 344, in from_call    result:
TResult | None = func()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 246, in 
&amp;lt;lambda&amp;gt;    lambda: runtest_hook(item=item, **kwds), when=when, 
reraise=reraise  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/runner.py&amp;quot;, line 178, in pytest_runtest_call 
item.runtest()  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 1671, in runtest    
self.ihook.pytest_pyfunc_call(pyfuncitem=self)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_hooks.py&amp;quot;, line 512, in __call__    return 
self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_manager.py&amp;quot;, line 120, in _hookexec    return
self._inner_hookexec(hook_name, methods, kwargs, firstresult)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/pluggy/_callers.py&amp;quot;, line 121, in _multicall    res =
hook_impl.function(*args)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3
.12/site-packages/_pytest/python.py&amp;quot;, line 157, in pytest_pyfunc_call  
result = testfunction(**testargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapte
rs/test_provider_system_additional.py&amp;quot;, line 413, in 
test_provider_factory_accepts_provider_type_enum    provider = 
ProviderFactory.create_provider(ProviderType.STUB)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 436, in create_provider    return 
_safe_provider(&amp;quot;Unknown provider type&amp;quot;)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/provider_system.py&amp;quot;, line 300, in _safe_provider    
logger.info(&amp;quot;Falling back to Stub provider: %s&amp;quot;, reason)  File
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 623, in info    self._log(logging.INFO, msg, *args,
**kwargs)  File 
&amp;quot;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;quot;, line 615, in _log    self.logger.log(level, msg, *args, 
**log_kwargs)Message: &amp;#x27;Falling back to Stub provider: 
%s&amp;#x27;Arguments: (&amp;#x27;Unknown provider 
type&amp;#x27;,)------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider type 
&amp;#x27;ProviderType.STUB&amp;#x27;, falling back to safe defaultINFO     
devsynth.adapters.provider_system:logging_setup.py:615 Falling back to Stub 
provider: Unknown provider type_ test_command_module_import _module_name = 
&amp;#x27;devsynth.application.cli.commands.enhanced_analysis_cmd&amp;#x27;    
@pytest.mark.parametrize(&amp;quot;module_name&amp;quot;, MODULE_NAMES)    def 
test_command_module_import(module_name: str) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Each CLI command module should be 
importable.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
importlib.import_module(module_name)/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/application/cli/commands/test_module_imports.py:28: _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;    Enhanced Analysis Command        This 
module provides a comprehensive command that runs all enhanced analysis    
capabilities including test infrastructure analysis, quality assurance,    
security validation, and requirements traceability.        Key features:    - 
Unified command interface for all enhanced analysis tools    - Configurable 
analysis scope and depth    - Integration with DevSynth&amp;#x27;s EDRR workflow
system    - Comprehensive reporting with multiple output formats    - Memory 
system integration for persistent analysis results    
&amp;quot;&amp;quot;&amp;quot;        import json    import os    import time   
from pathlib import Path    from typing import Any, Dict, List, Optional        
import typer    from rich.console import Console    from rich.progress import 
Progress, SpinnerColumn, TextColumn    from rich.table import Table        from 
devsynth.application.testing.enhanced_test_collector import 
EnhancedTestCollector&amp;gt;   from 
devsynth.application.testing.test_isolation_analyzer import 
TestIsolationAnalyzerE   ModuleNotFoundError: No module named 
&amp;#x27;devsynth.application.testing.test_isolation_analyzer&amp;#x27;/Users/c
aitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cli/comman
ds/enhanced_analysis_cmd.py:28: ModuleNotFoundError_________________________ 
test_inventory_exports_file __________________________obj = &amp;lt;function 
run_tests_cmd at 0x1146f18a0&amp;gt;, name = 
&amp;#x27;collect_tests_with_cache&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;function&amp;#x27; 
object has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x11510e510&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_inventory_exports_file0&amp;#x27;)tmp_path_facto
ry = TempPathFactory(_given_basetemp=None, 
_trace=&amp;lt;pluggy._tracing.TagTracerSub object at 0x114e5db50&amp;gt;, 
_basetemp=PosixPath...lders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitly
n/pytest-1428&amp;#x27;), _retention_count=3, 
_retention_policy=&amp;#x27;all&amp;#x27;)    @pytest.mark.fast    def 
test_inventory_exports_file(monkeypatch, tmp_path, tmp_path_factory) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;ReqID: CLI-RT-08  inventory mode 
exports JSON and skips run.&amp;quot;&amp;quot;&amp;quot;            # Return 
deterministic collections        def fake_collect(tgt: str, spd: str | None = 
None) -&amp;gt; list:            return 
[&amp;quot;tests/unit/test_example.py::test_ok&amp;quot;]            def 
fake_run_tests(*args, **kwargs) -&amp;gt; tuple:            
pytest.fail(&amp;quot;run_tests should not be called in inventory 
mode&amp;quot;)    &amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_cac
he&amp;quot;,            fake_collect,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd.py:191: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;function run_tests_cmd 
at 0x1146f18a0&amp;gt;, name = &amp;#x27;collect_tests_with_cache&amp;#x27;ann =
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;function&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError__________________ test_failed_run_surfaces_maxfail_guidance 
___________________obj = &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt;, 
name = &amp;#x27;run_tests&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;function&amp;#x27; 
object has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e332f0&amp;gt;    @pytest.mark.fast    def 
test_failed_run_surfaces_maxfail_guidance(monkeypatch) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: CLI-RT-17  Failed runs surface maxfail 
troubleshooting tips.&amp;quot;&amp;quot;&amp;quot;            failure_output = 
(            &amp;quot;Pytest exited with code 1. Command: python -m pytest 
--maxfail=2 tests/unit\n&amp;quot;            &amp;quot;Troubleshooting 
tips:\n&amp;quot;            &amp;quot;- Segment large suites to localize 
failures.\n&amp;quot;        )            def fake_run_tests(*args, **kwargs) 
-&amp;gt; tuple:  # noqa: ANN001            return False, failure_output    
&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.run_tests&amp;quot;,  
fake_run_tests,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd.py:403: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;function run_tests_cmd 
at 0x1146f18a0&amp;gt;, name = &amp;#x27;run_tests&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;function&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_______________ test_run_tests_cmd_exits_when_pytest_cov_missing 
_______________obj = &amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt;name =
&amp;#x27;pytest_cov_support_status&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;function&amp;#x27; 
object has no attribute 
&amp;#x27;pytest_cov_support_status&amp;#x27;/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e310a0&amp;gt;    @pytest.mark.fast    def 
test_run_tests_cmd_exits_when_pytest_cov_missing(monkeypatch) -&amp;gt; None:   
&amp;quot;&amp;quot;&amp;quot;Missing pytest-cov triggers an actionable 
remediation banner.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_sta
tus&amp;quot;,            lambda env=None: (                False,              
run_tests_cmd_module.PYTEST_COV_PLUGIN_MISSING_MESSAGE,            ),        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd.py:437: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;function run_tests_cmd 
at 0x1146f18a0&amp;gt;name = &amp;#x27;pytest_cov_support_status&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;function&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;pytest_cov_support_status&amp;#x27;/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError___________ 
test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov ___________obj = 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt;name = 
&amp;#x27;pytest_cov_support_status&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;function&amp;#x27; 
object has no attribute 
&amp;#x27;pytest_cov_support_status&amp;#x27;/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1029bd100&amp;gt;    @pytest.mark.fast    def 
test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov(monkeypatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Autoload blocking pytest-cov halts 
execution for standard runs.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_sta
tus&amp;quot;,            lambda env=None: (                False,              
run_tests_cmd_module.PYTEST_COV_AUTOLOAD_DISABLED_MESSAGE,            ),        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd.py:483: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;function run_tests_cmd 
at 0x1146f18a0&amp;gt;name = &amp;#x27;pytest_cov_support_status&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;function&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;pytest_cov_support_status&amp;#x27;/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_____________ test_cli_runner_inventory_handles_collection_errors 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x11540a390&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_runner_inventory_hand0&amp;#x27;)    
@pytest.mark.fast    def test_cli_runner_inventory_handles_collection_errors(   
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Inventory exports tolerate collection errors and 
still succeed.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            calls: list[tuple] = []            def 
fake_collect(target: str, speed: str | None) -&amp;gt; list:            
calls.append((target, speed))            if target == 
&amp;quot;integration-tests&amp;quot; and speed == &amp;quot;medium&amp;quot;:  
raise RuntimeError(&amp;quot;collection failed&amp;quot;)            suffix = 
speed or &amp;quot;all&amp;quot;            return             app, cli_module =
_build_minimal_app(monkeypatch)&amp;gt;       monkeypatch.setattr(cli_module, 
&amp;quot;collect_tests_with_cache&amp;quot;, fake_collect)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runn
er_invalid_inputs.py:311: AttributeError_____________ 
test_cli_runner_failed_run_surfaces_maxfail_guidance _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1029bfd10&amp;gt;    
@pytest.mark.fast    def test_cli_runner_failed_run_surfaces_maxfail_guidance(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Failed Typer invocation should surface the maxfail
troubleshooting tip.&amp;quot;&amp;quot;&amp;quot;            app, cli_module = 
_build_minimal_app(monkeypatch)            def fake_run_tests(*args: object, 
**kwargs: object) -&amp;gt; tuple:            cmd = [&amp;quot;python&amp;quot;,
&amp;quot;-m&amp;quot;, &amp;quot;pytest&amp;quot;, 
&amp;quot;--maxfail&amp;quot;, &amp;quot;2&amp;quot;]            tips = 
run_tests_module._failure_tips(1, cmd)            return False, 
&amp;quot;segment error\n&amp;quot; + tips    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, fake_run_tests)E 
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inpu
ts.py:340: AttributeError____________ 
test_cli_runner_inventory_write_failure_exits_nonzero _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115fcf860&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_runner_inventory_writ0&amp;#x27;)    
@pytest.mark.fast    def test_cli_runner_inventory_write_failure_exits_nonzero( 
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,    ) -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;Disk errors while exporting inventory should exit 
with code 1.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            def fake_collect(target: str, speed: str 
| None) -&amp;gt; list:            suffix = speed or &amp;quot;all&amp;quot;    
return             app, cli_module = _build_minimal_app(monkeypatch)&amp;gt;    
monkeypatch.setattr(cli_module, &amp;quot;collect_tests_with_cache&amp;quot;, 
fake_collect)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runn
er_invalid_inputs.py:365: AttributeError_____________ 
test_cli_runner_maxfail_option_propagates_to_runner ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115916660&amp;gt;    
@pytest.mark.fast    def test_cli_runner_maxfail_option_propagates_to_runner(   
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;The Typer surface must forward --maxfail to 
run_tests.&amp;quot;&amp;quot;&amp;quot;            app, cli_module = 
_build_minimal_app(monkeypatch)            received: dict = {}            def 
fake_run_tests(*args: object, **kwargs: object) -&amp;gt; tuple:            
received[&amp;quot;args&amp;quot;] = args            
received[&amp;quot;kwargs&amp;quot;] = dict(kwargs)            return True, 
&amp;quot;pytest ok&amp;quot;    &amp;gt;       monkeypatch.setattr(cli_module, 
&amp;quot;run_tests&amp;quot;, fake_run_tests)E       AttributeError: 
&amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inpu
ts.py:395: AttributeError________________ 
test_cli_inventory_mode_exports_json_via_typer ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115917a70&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_inventory_mode_export1&amp;#x27;)    
@pytest.mark.fast    def test_cli_inventory_mode_exports_json_via_typer(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Inventory-only mode collects all targets and 
writes JSON via the CLI.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            calls: list[tuple] = []            def 
fake_collect(target: str, speed: str | None) -&amp;gt; list:            
calls.append((target, speed))            # Encode target/speed in the node id so
assertions are deterministic            suffix = speed or 
&amp;quot;all&amp;quot;            return             app, cli_module = 
build_minimal_cli_app(monkeypatch)&amp;gt;       monkeypatch.setattr(cli_module,
&amp;quot;collect_tests_with_cache&amp;quot;, fake_collect)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runn
er_paths.py:112: AttributeError____________________ 
test_cli_smoke_dry_run_invokes_preview ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115917ce0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_smoke_dry_run_invokes0&amp;#x27;)    
@pytest.mark.fast    def test_cli_smoke_dry_run_invokes_preview(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Smoke dry-run previews pytest invocation without 
executing suites.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
sys.modules.pop(&amp;quot;devsynth.config&amp;quot;, None)        
sys.modules.pop(&amp;quot;devsynth.config.settings&amp;quot;, None)        
sys.modules.pop(&amp;quot;devsynth.config.provider_env&amp;quot;, None)        
monkeypatch.delenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
raising=False)        monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_TEST_TIMEOUT_SECONDS&amp;quot;, 
raising=False)        app, cli_module = build_minimal_cli_app(monkeypatch)      
calls: list[dict] = []            def fake_run_tests(*args: Any, **kwargs: Any) 
-&amp;gt; tuple:            calls.append({&amp;quot;args&amp;quot;: args, 
&amp;quot;kwargs&amp;quot;: kwargs})            return (                True,   
&amp;quot;Dry run: pytest invocation prepared but not executed.\n&amp;quot;     
&amp;quot;Command: python -m pytest 
tests/unit/test_example.py::test_case\n&amp;quot;,            )            
monkeypatch.setattr(run_tests_module, &amp;quot;run_tests&amp;quot;, 
fake_run_tests)&amp;gt;       monkeypatch.setattr(cli_module, 
&amp;quot;run_tests&amp;quot;, fake_run_tests)E       AttributeError: 
&amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py:159
: AttributeError_____________ 
test_cli_enforces_coverage_threshold_via_cli_runner ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1159163c0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_enforces_coverage_thr0&amp;#x27;)    
@pytest.mark.fast    def test_cli_enforces_coverage_threshold_via_cli_runner(   
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Successful Typer invocation enforces coverage 
thresholds and emits tips.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            core_stub = 
ModuleType(&amp;quot;devsynth.core.config_loader&amp;quot;)        
core_stub.ConfigSearchResult = object  # type: ignore        
core_stub.load_config = lambda *args, **kwargs: object()        
core_stub._find_project_config = lambda *args, **kwargs: None        
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.core.config_loader&amp;quot;, core_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;tinydb&amp;quot;, 
ModuleType(&amp;quot;tinydb&amp;quot;))            for module_name, attr in [   
(&amp;quot;devsynth.application.cli.commands.edrr_cycle_cmd&amp;quot;, 
&amp;quot;edrr_cycle_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.align_cmd&amp;quot;, 
&amp;quot;align_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.analyze_manifest_cmd&amp;quot;,     
&amp;quot;analyze_manifest_cmd&amp;quot;,            ),            
(&amp;quot;devsynth.application.cli.commands.generate_docs_cmd&amp;quot;, 
&amp;quot;generate_docs_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.ingest_cmd&amp;quot;, 
&amp;quot;ingest_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.doctor_cmd&amp;quot;, 
&amp;quot;doctor_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.validate_manifest_cmd&amp;quot;,    
&amp;quot;validate_manifest_cmd&amp;quot;,            ),            (           
&amp;quot;devsynth.application.cli.commands.validate_metadata_cmd&amp;quot;,    
&amp;quot;validate_metadata_cmd&amp;quot;,            ),        ]:            
stub_module = ModuleType(module_name)            setattr(stub_module, attr, 
lambda *args, **kwargs: None)            monkeypatch.setitem(sys.modules, 
module_name, stub_module)            app, cli_module = 
build_minimal_cli_app(monkeypatch)            runner = CliRunner()    &amp;gt;  
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *args, 
**kwargs: (True, &amp;quot;ok&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py:239
: AttributeError___________ 
test_cli_smoke_mode_reports_coverage_skip_and_artifacts ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115914ec0&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_smoke_mode_reports_co0&amp;#x27;)    
@pytest.mark.fast    def 
test_cli_smoke_mode_reports_coverage_skip_and_artifacts(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Smoke mode prints diagnostic skip notice while 
still surfacing artifacts.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            core_stub = 
ModuleType(&amp;quot;devsynth.core.config_loader&amp;quot;)        
core_stub.ConfigSearchResult = object  # type: ignore        
core_stub.load_config = lambda *args, **kwargs: object()        
core_stub._find_project_config = lambda *args, **kwargs: None        
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.core.config_loader&amp;quot;, core_stub)        
monkeypatch.setitem(sys.modules, &amp;quot;tinydb&amp;quot;, 
ModuleType(&amp;quot;tinydb&amp;quot;))            for module_name, attr in [   
(&amp;quot;devsynth.application.cli.commands.edrr_cycle_cmd&amp;quot;, 
&amp;quot;edrr_cycle_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.align_cmd&amp;quot;, 
&amp;quot;align_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.analyze_manifest_cmd&amp;quot;,     
&amp;quot;analyze_manifest_cmd&amp;quot;,            ),            
(&amp;quot;devsynth.application.cli.commands.generate_docs_cmd&amp;quot;, 
&amp;quot;generate_docs_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.ingest_cmd&amp;quot;, 
&amp;quot;ingest_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.doctor_cmd&amp;quot;, 
&amp;quot;doctor_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.validate_manifest_cmd&amp;quot;,    
&amp;quot;validate_manifest_cmd&amp;quot;,            ),            (           
&amp;quot;devsynth.application.cli.commands.validate_metadata_cmd&amp;quot;,    
&amp;quot;validate_metadata_cmd&amp;quot;,            ),        ]:            
stub_module = ModuleType(module_name)            setattr(stub_module, attr, 
lambda *args, **kwargs: None)            monkeypatch.setitem(sys.modules, 
module_name, stub_module)            app, cli_module = 
build_minimal_cli_app(monkeypatch)            recorded_args: list[tuple[tuple, 
dict]] = []            def fake_run_tests(*args: object, **kwargs: object) 
-&amp;gt; tuple:            recorded_args.append((args, kwargs))            
return True, &amp;quot;pytest output&amp;quot;    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, fake_run_tests)E 
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py:340
: AttributeError_______________ test_cli_exits_when_autoload_disables_pytest_cov
_______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115914380&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_exits_when_autoload_d0&amp;#x27;)    
@pytest.mark.fast    def test_cli_exits_when_autoload_disables_pytest_cov(      
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Fail fast when plugin autoloading disables 
pytest-cov instrumentation.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            app, cli_module = 
build_minimal_cli_app(monkeypatch)    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *_, **__: 
(True, &amp;quot;&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py:397
: AttributeError_____________ 
test_cli_exits_when_pytest_cov_disabled_via_autoload _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115917b00&amp;gt;    
@pytest.mark.fast    def test_cli_exits_when_pytest_cov_disabled_via_autoload(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Typer run surfaces remediation tips when 
pytest-cov is disabled.&amp;quot;&amp;quot;&amp;quot;            app, cli_module
= build_minimal_cli_app(monkeypatch)            runner = CliRunner()    &amp;gt;
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *args, 
**kwargs: (True, &amp;quot;&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py:444
: AttributeError_________________ test_cli_reports_coverage_artifacts_success 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x115914530&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_reports_coverage_arti0&amp;#x27;)    
@pytest.mark.fast    def test_cli_reports_coverage_artifacts_success(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Successful CLI run emits artifact locations after 
enforcing thresholds.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            app, cli_module = 
build_minimal_cli_app(monkeypatch)            html_dir = tmp_path / 
&amp;quot;htmlcov&amp;quot;        html_dir.mkdir()        html_index = html_dir
/ &amp;quot;index.html&amp;quot;        
html_index.write_text(&amp;quot;&amp;lt;html&amp;gt;coverage&amp;lt;/html&amp;gt
;&amp;quot;)            json_path = tmp_path / &amp;quot;test_reports&amp;quot; 
/ &amp;quot;coverage.json&amp;quot;        json_path.parent.mkdir()        
json_path.write_text(&amp;quot;{}&amp;quot;)            
monkeypatch.setattr(cli_module, &amp;quot;COVERAGE_HTML_DIR&amp;quot;, html_dir)
monkeypatch.setattr(cli_module, &amp;quot;COVERAGE_JSON_PATH&amp;quot;, 
json_path)    &amp;gt;       monkeypatch.setattr(cli_module, 
&amp;quot;run_tests&amp;quot;, lambda *_, **__: (True, &amp;quot;pytest 
ok&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.p
y:35: AttributeError________________ 
test_cli_exits_when_coverage_artifacts_missing ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115914740&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_exits_when_coverage_a0&amp;#x27;)    
@pytest.mark.fast    def test_cli_exits_when_coverage_artifacts_missing(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Missing coverage artifacts trigger an exit with 
remediation guidance.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            app, cli_module = 
build_minimal_cli_app(monkeypatch)    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *_, **__: 
(True, &amp;quot;&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.p
y:82: AttributeError__________________ 
test_cli_surfaces_threshold_runtime_errors __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115914b60&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_surfaces_threshold_ru0&amp;#x27;)    
@pytest.mark.fast    def test_cli_surfaces_threshold_runtime_errors(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Runtime errors from threshold enforcement bubble 
up through the CLI.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            app, cli_module = 
build_minimal_cli_app(monkeypatch)    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *_, **__: 
(True, &amp;quot;&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.p
y:125: AttributeError_______________ 
test_smoke_command_generates_coverage_artifacts ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115916300&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_smoke_command_generates_c0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x124091730&amp;gt;    
@pytest.mark.fast    def test_smoke_command_generates_coverage_artifacts(       
monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Smoke profile writes coverage artifacts even with 
plugin autoload disabled.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
raising=False)            popen_envs, combine_calls = 
_install_pytest_stubs(monkeypatch)            runner = CliRunner()&amp;gt;      
app = _build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:173: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError_________________ 
test_smoke_command_injects_pytest_bdd_plugin _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115f8c380&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_smoke_command_injects_pyt0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x115f8d430&amp;gt;    
@pytest.mark.fast    def test_smoke_command_injects_pytest_bdd_plugin(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Smoke profile explicitly loads pytest-bdd when 
plugin autoload is disabled.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, raising=False)        
monkeypatch.delenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
raising=False)            popen_envs, combine_calls = 
_install_pytest_stubs(monkeypatch)            runner = CliRunner()&amp;gt;      
app = _build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:209: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError_ 
test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115f8dcd0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_fast_medium_command_gener0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x115e54f80&amp;gt;    
@pytest.mark.fast    def 
test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled(   
monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Fast+medium aggregate run preserves coverage 
artifacts when autoload is disabled.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, raising=False)           
popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)            runner
= CliRunner()&amp;gt;       app = _build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:248: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError______________ 
test_fast_medium_preserves_existing_cov_fail_under ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115f8dbb0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_fast_medium_preserves_exi0&amp;#x27;)    
@pytest.mark.fast    def test_fast_medium_preserves_existing_cov_fail_under(    
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Custom fail-under thresholds survive coverage 
plugin injection.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, 
&amp;quot;--cov-fail-under=95&amp;quot;)            popen_envs, _ = 
_install_pytest_stubs(monkeypatch)            runner = CliRunner()&amp;gt;      
app = _build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:288: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError______________ 
test_fast_medium_command_handles_empty_collection _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x115f8dbe0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_fast_medium_command_handl0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x1214728a0&amp;gt;    
@pytest.mark.fast    def test_fast_medium_command_handles_empty_collection(     
monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Fallback to marker execution when collection 
returns no node identifiers.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, raising=False)           
popen_envs, combine_calls = _install_pytest_stubs(            monkeypatch, 
collect_stdout_sequence=[&amp;quot;&amp;quot;, &amp;quot;&amp;quot;]        )   
runner = CliRunner()&amp;gt;       app = _build_minimal_app(monkeypatch)        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:323: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError_________ 
test_fast_profile_generates_coverage_and_exits_successfully 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e66ba0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_fast_profile_generates_co0&amp;#x27;)    
@pytest.mark.fast    def 
test_fast_profile_generates_coverage_and_exits_successfully(        monkeypatch:
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Default fast profile produces coverage artifacts 
and a zero exit code.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.delenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, raising=False)           
popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)            runner
= CliRunner()&amp;gt;       app = _build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:362: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError______ 
test_fast_profile_missing_coverage_artifacts_returns_exit_code_one 
______monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115e651c0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_fast_profile_missing_cove0&amp;#x27;)    
@pytest.mark.fast    def 
test_fast_profile_missing_coverage_artifacts_returns_exit_code_one(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Missing coverage artifacts in fast profile should 
exit with code 1.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            runner = CliRunner()&amp;gt;       app = 
_build_minimal_app(monkeypatch)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py
:399: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app    
cli_module = _load_cli_module(monkeypatch)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py:7
5: in _load_cli_module    return 
importlib.import_module(&amp;quot;devsynth.application.cli.commands.run_tests_cm
d&amp;quot;)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/h
omebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib
/python3.12/importlib/__init__.py:90: in import_module    return 
_bootstrap._gcd_import(name, package, level)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1387: in _gcd_import    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1360: in _find_and_load    ???&amp;lt;frozen 
importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap_external&amp;gt;:999: in exec_module    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:488: in _call_with_frames_removed
???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_     &amp;quot;&amp;quot;&amp;quot;CLI command to run DevSynth tests.        
Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`    
command. This command mirrors the options exposed by the underlying helper.     
Example:        `devsynth run-tests --target unit-tests --speed fast`    
&amp;quot;&amp;quot;&amp;quot;        from __future__ import annotations        
import os    import shlex    from typing import cast        import typer        
# Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``    # still affects the symbol used by 
this CLI command. Accessing attributes through    # the module avoids stale 
references when tests replace ``run_tests``.    import 
devsynth.testing.run_tests as run_tests_module        # Ensure sitecustomize is 
loaded for Python 3.12+ compatibility patches    import sitecustomize  # noqa: 
F401&amp;gt;   from devsynth.config.provider_env import ProviderEnvE   
ModuleNotFoundError: No module named 
&amp;#x27;devsynth.config.provider_env&amp;#x27;; 
&amp;#x27;devsynth.config&amp;#x27; is not a 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/cli/commands/run_tests_cmd.py:25: ModuleNotFoundError______________ 
test_inventory_mode_writes_file_and_prints_message ______________obj = 
&amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;collect_tests_with_cache&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; has no 
attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115675610&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_inventory_mode_writes_fil0&amp;#x27;)capsys = 
&amp;lt;_pytest.capture.CaptureFixture object at 0x115bf7500&amp;gt;    
@pytest.mark.fast    def 
test_inventory_mode_writes_file_and_prints_message(monkeypatch, tmp_path, 
capsys):        &amp;quot;&amp;quot;&amp;quot;ReqID: TR-CLI-03  Inventory mode 
writes JSON and prints message.            Validates that running with 
--inventory writes test_reports/test_inventory.json        and prints a 
user-facing message with the path.        &amp;quot;&amp;quot;&amp;quot;        
# run in temporary cwd        monkeypatch.chdir(tmp_path)            # Return 
deterministic collections        def fake_collect(target: str, speed: str):  # 
noqa: ARG001            return             
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_TEST_ALLOW_REQUESTS&amp;quot;, 
&amp;quot;true&amp;quot;)    &amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_cac
he&amp;quot;,            fake_collect,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_cmd_inventory.py:29: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt;name = 
&amp;#x27;collect_tests_with_cache&amp;#x27;ann = 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError___________________ test_inventory_handles_collection_errors 
___________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x115675670&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_inventory_handles_collect0&amp;#x27;)    
@pytest.mark.fast    def test_inventory_handles_collection_errors(monkeypatch, 
tmp_path):        &amp;quot;&amp;quot;&amp;quot;ReqID: TR-CLI-04  Inventory 
mode handles collection errors.            When collection raises, ensure JSON 
still includes empty lists for all            targets and speeds.        
&amp;quot;&amp;quot;&amp;quot;        monkeypatch.chdir(tmp_path)            def
flaky_collect(target: str, speed: str):  # noqa: ARG001            raise 
RuntimeError(&amp;quot;boom&amp;quot;)            app, cli_module = 
build_minimal_cli_app(monkeypatch)&amp;gt;       monkeypatch.setattr(cli_module,
&amp;quot;collect_tests_with_cache&amp;quot;, flaky_collect)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;collect_tests_with_cache&amp;#x27;/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_inventor
y.py:60: AttributeError______________ 
test_cli_report_flag_warns_when_directory_missing _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1238bef60&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_report_flag_warns_whe0&amp;#x27;)    
@pytest.mark.fast    def test_cli_report_flag_warns_when_directory_missing(     
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Successful runs with --report mention missing 
directories.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            # Provide minimal config loader stubs so 
CLI bootstraps cleanly.        core_stub = 
ModuleType(&amp;quot;devsynth.core.config_loader&amp;quot;)        
core_stub.ConfigSearchResult = object  # type: ignore        
core_stub.load_config = lambda *args, **kwargs: object()        
core_stub._find_project_config = lambda *args, **kwargs: None        
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.core.config_loader&amp;quot;, core_stub)            
tinydb_stub = ModuleType(&amp;quot;tinydb&amp;quot;)        tinydb_stub.Query = 
object  # type: ignore        tinydb_stub.TinyDB = object  # type: ignore       
monkeypatch.setitem(sys.modules, &amp;quot;tinydb&amp;quot;, tinydb_stub)       
for module_name, attr in [            
(&amp;quot;devsynth.application.cli.commands.edrr_cycle_cmd&amp;quot;, 
&amp;quot;edrr_cycle_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.align_cmd&amp;quot;, 
&amp;quot;align_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.analyze_manifest_cmd&amp;quot;,     
&amp;quot;analyze_manifest_cmd&amp;quot;,            ),            (            
&amp;quot;devsynth.application.cli.commands.generate_docs_cmd&amp;quot;,        
&amp;quot;generate_docs_cmd&amp;quot;,            ),            
(&amp;quot;devsynth.application.cli.commands.ingest_cmd&amp;quot;, 
&amp;quot;ingest_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.doctor_cmd&amp;quot;, 
&amp;quot;doctor_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.validate_manifest_cmd&amp;quot;,    
&amp;quot;validate_manifest_cmd&amp;quot;,            ),            (           
&amp;quot;devsynth.application.cli.commands.validate_metadata_cmd&amp;quot;,    
&amp;quot;validate_metadata_cmd&amp;quot;,            ),        ]:            
stub_module = ModuleType(module_name)            setattr(stub_module, attr, 
lambda *args, **kwargs: None)            monkeypatch.setitem(sys.modules, 
module_name, stub_module)            app, cli_module = 
build_minimal_cli_app(monkeypatch)            runner = CliRunner()    &amp;gt;  
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, lambda *_, **__: 
(True, &amp;quot;pytest ok&amp;quot;))E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py:68: 
AttributeError____________ test_cli_segment_option_failure_surfaces_failure_tips
_____________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115f8dcd0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_segment_option_failur0&amp;#x27;)    
@pytest.mark.fast    def test_cli_segment_option_failure_surfaces_failure_tips( 
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Segmentation failures bubble remediation guidance 
and exit non-zero.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            core_stub = 
ModuleType(&amp;quot;devsynth.core.config_loader&amp;quot;)        
core_stub.ConfigSearchResult = object  # type: ignore        
core_stub.load_config = lambda *args, **kwargs: object()        
core_stub._find_project_config = lambda *args, **kwargs: None        
monkeypatch.setitem(sys.modules, 
&amp;quot;devsynth.core.config_loader&amp;quot;, core_stub)        tinydb_stub =
ModuleType(&amp;quot;tinydb&amp;quot;)        tinydb_stub.Query = object  # 
type: ignore        tinydb_stub.TinyDB = object  # type: ignore        
monkeypatch.setitem(sys.modules, &amp;quot;tinydb&amp;quot;, tinydb_stub)       
for module_name, attr in [            
(&amp;quot;devsynth.application.cli.commands.edrr_cycle_cmd&amp;quot;, 
&amp;quot;edrr_cycle_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.align_cmd&amp;quot;, 
&amp;quot;align_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.analyze_manifest_cmd&amp;quot;,     
&amp;quot;analyze_manifest_cmd&amp;quot;,            ),            (            
&amp;quot;devsynth.application.cli.commands.generate_docs_cmd&amp;quot;,        
&amp;quot;generate_docs_cmd&amp;quot;,            ),            
(&amp;quot;devsynth.application.cli.commands.ingest_cmd&amp;quot;, 
&amp;quot;ingest_cmd&amp;quot;),            
(&amp;quot;devsynth.application.cli.commands.doctor_cmd&amp;quot;, 
&amp;quot;doctor_cmd&amp;quot;),            (                
&amp;quot;devsynth.application.cli.commands.validate_manifest_cmd&amp;quot;,    
&amp;quot;validate_manifest_cmd&amp;quot;,            ),            (           
&amp;quot;devsynth.application.cli.commands.validate_metadata_cmd&amp;quot;,    
&amp;quot;validate_metadata_cmd&amp;quot;,            ),        ]:            
stub_module = ModuleType(module_name)            setattr(stub_module, attr, 
lambda *args, **kwargs: None)            monkeypatch.setitem(sys.modules, 
module_name, stub_module)            app, cli_module = 
build_minimal_cli_app(monkeypatch)            captured_args: tuple = ()        
captured_kwargs: dict = {}            def failing_run_tests(*args: object, 
**kwargs: object) -&amp;gt; tuple:            nonlocal captured_args            
captured_args = args            captured_kwargs.update(kwargs)            return
False, &amp;quot;pytest failure output\n&amp;quot; + SEGMENTATION_FAILURE_TIPS  
&amp;gt;       monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, 
failing_run_tests)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py:152:
 AttributeError____________ 
test_segmented_cli_failure_emits_tips_and_reinjection _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1235356d0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_cli_failure_emi0&amp;#x27;)    
@pytest.mark.fast    def test_segmented_cli_failure_emits_tips_and_reinjection( 
monkeypatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Segmented runs surface remediation tips and 
reinjection notices once.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;)    
app, cli_module = build_minimal_cli_app(monkeypatch)            cov_calls: 
list[dict] = []        bdd_calls: list[dict] = []            def 
cov_wrapper(env: dict) -&amp;gt; bool:            cov_calls.append(env.copy())  
return run_tests_module.ensure_pytest_cov_plugin_env(env)            def 
bdd_wrapper(env: dict) -&amp;gt; bool:            bdd_calls.append(env.copy())  
return run_tests_module.ensure_pytest_bdd_plugin_env(env)    &amp;gt;       
monkeypatch.setattr(cli_module, 
&amp;quot;ensure_pytest_cov_plugin_env&amp;quot;, cov_wrapper)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;ensure_pytest_cov_plugin_env&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_tests_cmd_segm
entation_regressions.py:47: AttributeError_ 
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate _monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1235359d0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_cli_failure_rep0&amp;#x27;)failed_batc
hes = [&amp;#x27;one&amp;#x27;]    @pytest.mark.fast    
@pytest.mark.parametrize(        &amp;quot;failed_batches&amp;quot;,        [   
pytest.param([&amp;quot;one&amp;quot;], id=&amp;quot;single-batch&amp;quot;),   
pytest.param([&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;, 
&amp;quot;three&amp;quot;], id=&amp;quot;multiple-batches&amp;quot;),        ], 
)    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(     
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,        failed_batches: 
list,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Remediation 
banners surface once per failed segment and 
aggregate.&amp;quot;&amp;quot;&amp;quot;            monkeypatch.chdir(tmp_path) 
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;)    
app, cli_module = build_minimal_cli_app(monkeypatch)            
observed_batches: list = []            def raising_segment(label: str) -&amp;gt;
None:            observed_batches.append(label)            raise 
RuntimeError(f&amp;quot;segment {label} crashed&amp;quot;)            
monkeypatch.setattr(            run_tests_module,            
&amp;quot;_segment_batches&amp;quot;,            raising_segment,            
raising=False,        )            received_kwargs: dict = {}            def 
fake_run_tests(*_: object, **kwargs: object) -&amp;gt; tuple:            
received_kwargs.update(kwargs)            batch_outputs: list = []            
for index, batch in enumerate(failed_batches, start=1):                try:     
run_tests_module._segment_batches(batch)  # type: ignore                except 
RuntimeError as exc:  # pragma: no cover - exercised via test logic             
batch_outputs.append(_build_batch_output(str(index), exc))            
batch_outputs.append(                &amp;quot;Aggregate segmentation 
failure\n&amp;quot; + SEGMENTATION_FAILURE_TIPS            )            return 
False, &amp;quot;\n&amp;quot;.join(batch_outputs)    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, fake_run_tests)E 
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regression
s.py:139: AttributeError_ 
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate _monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x123534e90&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_cli_failure_rep1&amp;#x27;)failed_batc
hes = [&amp;#x27;one&amp;#x27;, &amp;#x27;two&amp;#x27;, 
&amp;#x27;three&amp;#x27;]    @pytest.mark.fast    @pytest.mark.parametrize(    
&amp;quot;failed_batches&amp;quot;,        [            
pytest.param([&amp;quot;one&amp;quot;], id=&amp;quot;single-batch&amp;quot;),   
pytest.param([&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;, 
&amp;quot;three&amp;quot;], id=&amp;quot;multiple-batches&amp;quot;),        ], 
)    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(     
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,        failed_batches: 
list,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Remediation 
banners surface once per failed segment and 
aggregate.&amp;quot;&amp;quot;&amp;quot;            monkeypatch.chdir(tmp_path) 
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;)    
app, cli_module = build_minimal_cli_app(monkeypatch)            
observed_batches: list = []            def raising_segment(label: str) -&amp;gt;
None:            observed_batches.append(label)            raise 
RuntimeError(f&amp;quot;segment {label} crashed&amp;quot;)            
monkeypatch.setattr(            run_tests_module,            
&amp;quot;_segment_batches&amp;quot;,            raising_segment,            
raising=False,        )            received_kwargs: dict = {}            def 
fake_run_tests(*_: object, **kwargs: object) -&amp;gt; tuple:            
received_kwargs.update(kwargs)            batch_outputs: list = []            
for index, batch in enumerate(failed_batches, start=1):                try:     
run_tests_module._segment_batches(batch)  # type: ignore                except 
RuntimeError as exc:  # pragma: no cover - exercised via test logic             
batch_outputs.append(_build_batch_output(str(index), exc))            
batch_outputs.append(                &amp;quot;Aggregate segmentation 
failure\n&amp;quot; + SEGMENTATION_FAILURE_TIPS            )            return 
False, &amp;quot;\n&amp;quot;.join(batch_outputs)    &amp;gt;       
monkeypatch.setattr(cli_module, &amp;quot;run_tests&amp;quot;, fake_run_tests)E 
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;run_tests&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regression
s.py:139: AttributeError___________________ 
test_run_tests_cli_feature_flags_set_env ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x123535eb0&amp;gt;    
@pytest.mark.fast    def test_run_tests_cli_feature_flags_set_env(monkeypatch): 
# Ensure a clean env for the feature vars        
monkeypatch.delenv(&amp;quot;DEVSYNTH_FEATURE_EXPERIMENTAL&amp;quot;, 
raising=False)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_FEATURE_LOGGING&amp;quot;, raising=False) 
runner = CliRunner()&amp;gt;       with patch(            
&amp;quot;devsynth.application.cli.commands.run_tests_cmd.run_tests&amp;quot;,  
return_value=(True, &amp;quot;&amp;quot;),        
):/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cl
i/commands/test_run_tests_features.py:19: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x1212dc530&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module 
&amp;#x27;devsynth.application.cli.commands.run_tests_cmd&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/commands/run_tests_cmd.py&amp;#x27;&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError_________ 
test_run_tests_cmd_applies_stub_offline_defaults_when_unset 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x123535250&amp;gt;    @pytest.mark.fast    def 
test_run_tests_cmd_applies_stub_offline_defaults_when_unset(        monkeypatch:
pytest.MonkeyPatch,    ) -&amp;gt; None:        # Ensure variables are unset    
for key in [            &amp;quot;DEVSYNTH_PROVIDER&amp;quot;,            
&amp;quot;DEVSYNTH_OFFLINE&amp;quot;,            
&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;,        ]:            
monkeypatch.delenv(key, raising=False)    &amp;gt;       with 
patch.object(module, &amp;quot;run_tests&amp;quot;, return_value=(True, 
&amp;quot;&amp;quot;)):             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Project
s/github.com/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_test
s_provider_defaults.py:33: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x1212f71a0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError__________ 
test_run_tests_command_succeeds_without_optional_providers __________    
@pytest.mark.fast    def 
test_run_tests_command_succeeds_without_optional_providers() -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;``devsynth run-tests`` should exit 0 without 
external providers.            ReqID: FR-22        
&amp;quot;&amp;quot;&amp;quot;            if 
os.environ.get(&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;) == 
&amp;quot;1&amp;quot;:            pytest.skip(&amp;quot;inner run&amp;quot;)    
env = os.environ.copy()        
env[&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;] = 
&amp;quot;false&amp;quot;        env[&amp;quot;DEVSYNTH_INNER_TEST&amp;quot;] = 
&amp;quot;1&amp;quot;        env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = 
&amp;quot;-k test_dummy&amp;quot;        result = subprocess.run(            [  
&amp;quot;devsynth&amp;quot;,                &amp;quot;run-tests&amp;quot;,     
&amp;quot;--speed&amp;quot;,                &amp;quot;fast&amp;quot;,           
],            capture_output=True,            text=True,            env=env,    
cwd=Path(__file__).resolve().parents[5],        )&amp;gt;       assert 
result.returncode == 0, result.stdout + result.stderrE       AssertionError: 
2025-10-28 09:25:21,868 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentationE         
2025-10-28 09:25:21,868 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooksE         
2025-10-28 09:25:21,868 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportE       
2025-10-28 09:25:21,868 - devsynth.application.cli.commands.run_tests_cmd - INFO
- CLI appended -p pytest_cov to PYTEST_ADDOPTS to enforce coverage 
instrumentationE         -p pytest_cov appended to PYTEST_ADDOPTS because plugin
autoloading is disabledE         2025-10-28 09:25:21,868 - 
devsynth.application.cli.commands.run_tests_cmd - INFO - CLI appended -p 
pytest_bdd.plugin to PYTEST_ADDOPTS to preserve pytest-bdd hooksE         -p 
pytest_bdd.plugin appended to PYTEST_ADDOPTS because plugin autoloading is E    
disabledE         2025-10-28 09:25:21,869 - 
devsynth.application.cli.commands.run_tests_cmd - INFO - CLI appended -p 
pytest_asyncio.plugin to PYTEST_ADDOPTS to preserve async test supportE         
-p pytest_asyncio.plugin appended to PYTEST_ADDOPTS because plugin autoloading E
is disabledE         2025-10-28 09:25:21,869 - devsynth.testing.run_tests - INFO
- Injected -p pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test 
supportE         2025-10-28 09:25:21,894 - devsynth.testing.run_tests - INFO - 
test collection cache hit for target=all-tests (fast)E         2025-10-28 
09:25:54,103 - devsynth.testing.run_tests - INFO - Coverage data file detected 
at .coverage (188416 bytes)E         2025-10-28 09:26:08,463 - 
devsynth.testing.run_tests - INFO - Coverage HTML report generatedE         
2025-10-28 09:26:13,470 - devsynth.testing.run_tests - INFO - Coverage JSON 
report generatedE         ============================= test session starts E   
==============================platform darwin -- Python 3.12.12, pytest-8.4.2, E
pluggy-1.6.0rootdir: E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynthconfigfile: E         
pytest.iniplugins: cov-7.0.0asyncio: mode=Mode.STRICT, debug=False, E         
asyncio_default_fixture_loop_scope=None, E         
asyncio_default_test_loop_scope=functioncollected 2949 items / 2943 deselected 
/E         6 selectedtests/unit/application/cli/commands/test_run_tests_dummy.py
.         E         [ 
16%]tests/unit/application/memory/test_fast_in_memory_components.py .        [E 
33%]tests/unit/general/test_base.py .                                        [ E
50%]tests/unit/general/test_requirement_repository_port_interface.py .       [ E
66%]tests/unit/interface/test_progress_helpers.py .                          [ E
83%]tests/unit/interface/test_ux_bridge_coverage.py .                        E  
[100%]=============================== warnings summary E         
===============================.venv/lib/python3.12/site-packages/_pytest/config
E         /__init__.py:833  E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
E         ackages/_pytest/config/__init__.py:833: PytestAssertRewriteWarning: 
Module E         already imported so cannot be rewritten; 
tests.fixtures.optional_deps    E         
self.import_plugin(import_spec).venv/lib/python3.12/site-packages/astor/op_util.
E         py:92  E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
E         ackages/astor/op_util.py:92: DeprecationWarning: ast.Num is deprecated
and will E         be removed in Python 3.14; use ast.Constant instead    
precedence_data = E         dict((getattr(ast, x, None), z) for x, y, z in E    
op_data).venv/lib/python3.12/site-packages/vbuild/__init__.py:33  E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
E         ackages/vbuild/__init__.py:33: DeprecationWarning: E         
&amp;amp;amp;#x27;pkgutil.find_loader&amp;amp;amp;#x27; is deprecated and slated
for removal in E         Python 3.14; use importlib.util.find_spec() instead    
hasLess = E         
bool(pkgutil.find_loader(&amp;amp;amp;quot;lesscpy&amp;amp;amp;quot;)).venv/lib/
python3.12/site-E         packages/vbuild/__init__.py:34  E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
E         ackages/vbuild/__init__.py:34: DeprecationWarning: E         
&amp;amp;amp;#x27;pkgutil.find_loader&amp;amp;amp;#x27; is deprecated and slated
for removal in E         Python 3.14; use importlib.util.find_spec() instead    
hasSass = E         
bool(pkgutil.find_loader(&amp;amp;amp;quot;scss&amp;amp;amp;quot;)).venv/lib/pyt
hon3.12/site-pacE         kages/vbuild/__init__.py:35  E         
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
E         ackages/vbuild/__init__.py:35: DeprecationWarning: E         
&amp;amp;amp;#x27;pkgutil.find_loader&amp;amp;amp;#x27; is deprecated and slated
for removal in E         Python 3.14; use importlib.util.find_spec() instead    
hasClosure = E         
bool(pkgutil.find_loader(&amp;amp;amp;quot;closure&amp;amp;amp;quot;))-- Docs: E
https://docs.pytest.org/en/stable/how-to/capture-warnings.html==================
E         ============== tests coverage 
================================______________ E         coverage: platform 
darwin, python 3.12.12-final-0 _______________Coverage HTML E         written to
dir htmlcovCoverage JSON written to file E         
test_reports/coverage.json========================= Test Categorization Summary 
E         ==========================Test Type Distribution:  Unit Tests: 6  
Integration E         Tests: 0  Behavior Tests: 0Test Speed Distribution:  Fast 
Tests (&amp;amp;amp;lt; 1s): 6E         Medium Tests (1-5s): 0  Slow Tests 
(&amp;amp;amp;gt; 5s): 0=============================E         Top 10 Slowest 
Tests =============================1. E         
tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy: 
0.04s2.E         
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
E         uirement_port_methods_raise_not_implemented: 0.00s3. E         
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
E         saction_context_commit_and_rollback: 0.00s4. E         
tests/unit/general/test_base.py::test_dummy_adapter_succeeds: 0.00s5. E         
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
E         ed_protocol: 0.00s6. E         
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods: E 
0.00s=============== 6 passed, 2943 deselected, 5 warnings in 27.32s E         
================E         Tests completed successfullyE         2025-10-28 
09:26:13,535 - devsynth.testing.run_tests - ERROR - Coverage 23.81% is below the
required 70.00%E         Coverage 23.81% is below the required 70.00%E         E
assert 1 == 0E        +  where 1 = 
CompletedProcess(args=[&amp;#x27;devsynth&amp;#x27;, 
&amp;#x27;run-tests&amp;#x27;, &amp;#x27;--speed&amp;#x27;, 
&amp;#x27;fast&amp;#x27;], returncode=1, stdout=&amp;#x27;2025-10-28 
09:25:21,868 - de...sts - ERROR - Coverage 23.81% is below the required 
70.00%\nCoverage 23.81% is below the required 70.00%\n&amp;#x27;, 
stderr=&amp;#x27;&amp;#x27;).returncode/Users/caitlyn/Projects/github.com/raveno
ak/devsynth/tests/unit/application/cli/commands/test_run_tests_subprocess.py:34:
 AssertionError________________ test_invalid_target_exits_with_helpful_message 
________________    @pytest.mark.fast    def 
test_invalid_target_exits_with_helpful_message() -&amp;gt; None:        runner =
CliRunner()        app = build_app()&amp;gt;       result = runner.invoke(app, 
[&amp;quot;run-tests&amp;quot;, &amp;quot;--target&amp;quot;, 
&amp;quot;weird-tests&amp;quot;])  # nosec                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Proje
cts/github.com/ravenoak/devsynth/tests/unit/application/cli/commands/test_run_te
sts_validation.py:16: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke    use_cli = _get_command(app)           
^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/
python3.12/site-packages/typer/main.py:352: in get_command    click_command: 
click.Command = get_group(typer_instance)                                   
^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.v
env/lib/python3.12/site-packages/typer/main.py:334: in get_group    group = 
get_group_from_info(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/l
ib/python3.12/site-packages/typer/main.py:479: in get_group_from_info    
sub_group = get_group_from_info(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ group_info = &amp;lt;typer.models.TyperInfo 
object at 0x120c3a5d0&amp;gt;    def get_group_from_info(        group_info: 
TyperInfo,        *,        pretty_exceptions_short: bool,        
rich_markup_mode: MarkupMode,    ) -&amp;gt; TyperGroup:&amp;gt;       assert 
group_info.typer_instance, (               ^^^^^^^^^^^^^^^^^^^^^^^^^            
&amp;quot;A Typer instance is needed to generate a Click Group&amp;quot;        
)E       AssertionError: A Typer instance is needed to generate a Click 
Group/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/s
ite-packages/typer/main.py:466: AssertionError________________ 
test_invalid_speed_exits_with_helpful_message _________________    
@pytest.mark.fast    def test_invalid_speed_exits_with_helpful_message() 
-&amp;gt; None:        runner = CliRunner()        app = build_app()        # 
Include one valid and one invalid to ensure detection&amp;gt;       result = 
runner.invoke(            app,            [                
&amp;quot;run-tests&amp;quot;,                &amp;quot;--target&amp;quot;,     
&amp;quot;unit-tests&amp;quot;,                &amp;quot;--speed&amp;quot;,     
&amp;quot;fast&amp;quot;,                &amp;quot;--speed&amp;quot;,           
&amp;quot;warp&amp;quot;,            ],        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli
/commands/test_run_tests_validation.py:28: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke    use_cli = _get_command(app)           
^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/
python3.12/site-packages/typer/main.py:352: in get_command    click_command: 
click.Command = get_group(typer_instance)                                   
^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.v
env/lib/python3.12/site-packages/typer/main.py:334: in get_group    group = 
get_group_from_info(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/l
ib/python3.12/site-packages/typer/main.py:479: in get_group_from_info    
sub_group = get_group_from_info(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ group_info = &amp;lt;typer.models.TyperInfo 
object at 0x123535a30&amp;gt;    def get_group_from_info(        group_info: 
TyperInfo,        *,        pretty_exceptions_short: bool,        
rich_markup_mode: MarkupMode,    ) -&amp;gt; TyperGroup:&amp;gt;       assert 
group_info.typer_instance, (               ^^^^^^^^^^^^^^^^^^^^^^^^^            
&amp;quot;A Typer instance is needed to generate a Click Group&amp;quot;        
)E       AssertionError: A Typer instance is needed to generate a Click 
Group/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/s
ite-packages/typer/main.py:466: AssertionError___________________ 
test_progress_manager_handles_lifecycle ____________________self = 
&amp;lt;MagicMock name=&amp;#x27;mock.update&amp;#x27; 
id=&amp;#x27;4663445408&amp;#x27;&amp;gt;, args = ()kwargs = 
{&amp;#x27;advance&amp;#x27;: 1, &amp;#x27;description&amp;#x27;: &amp;#x27;step
one&amp;#x27;}expected = call(advance=1, description=&amp;#x27;step 
one&amp;#x27;)actual = call(advance=1, description=&amp;#x27;step one&amp;#x27;,
status=None)_error_message = &amp;lt;function 
NonCallableMock.assert_called_with.&amp;lt;locals&amp;gt;._error_message at 
0x114d6fb00&amp;gt;cause = None    def assert_called_with(self, /, *args, 
**kwargs):        &amp;quot;&amp;quot;&amp;quot;assert that the last call was 
made with the specified arguments.            Raises an AssertionError if the 
args and keyword args passed in are        different to the last call to the 
mock.&amp;quot;&amp;quot;&amp;quot;        if self.call_args is None:           
expected = self._format_mock_call_signature(args, kwargs)            actual = 
&amp;#x27;not called.&amp;#x27;            error_message = (&amp;#x27;expected 
call not found.\nExpected: %s\n  Actual: %s&amp;#x27;                    % 
(expected, actual))            raise AssertionError(error_message)            
def _error_message():            msg = self._format_mock_failure_message(args, 
kwargs)            return msg        expected = self._call_matcher(_Call((args, 
kwargs), two=True))        actual = self._call_matcher(self.call_args)        if
actual != expected:            cause = expected if isinstance(expected, 
Exception) else None&amp;gt;           raise AssertionError(_error_message()) 
from causeE           AssertionError: expected call not found.E           
Expected: update(advance=1, description=&amp;#x27;step one&amp;#x27;)E          
Actual: update(advance=1, description=&amp;#x27;step one&amp;#x27;, 
status=None)/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework
/Versions/3.12/lib/python3.12/unittest/mock.py:949: AssertionErrorDuring 
handling of the above exception, another exception occurred:self = 
&amp;lt;MagicMock name=&amp;#x27;mock.update&amp;#x27; 
id=&amp;#x27;4663445408&amp;#x27;&amp;gt;, args = ()kwargs = 
{&amp;#x27;advance&amp;#x27;: 1, &amp;#x27;description&amp;#x27;: &amp;#x27;step
one&amp;#x27;}    def assert_called_once_with(self, /, *args, **kwargs):        
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called exactly once and 
that that call was        with the specified 
arguments.&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:    
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to be called once. Called %s 
times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))            raise AssertionError(msg)&amp;gt;       return 
self.assert_called_with(*args, **kwargs)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: expected call 
not found.E       Expected: update(advance=1, description=&amp;#x27;step 
one&amp;#x27;)E         Actual: update(advance=1, description=&amp;#x27;step 
one&amp;#x27;, status=None)E       E       pytest introspection follows:E       
E       Kwargs:E       assert {&amp;#x27;advance&amp;#x27;: 
1...status&amp;#x27;: None} == {&amp;#x27;advance&amp;#x27;: 1...&amp;#x27;: 
&amp;#x27;step one&amp;#x27;}E         E         Omitting 2 identical items, use
-vv to showE         Left contains 1 more item:E         
{&amp;#x27;status&amp;#x27;: None}E         Use -v to get more 
diff/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Version
s/3.12/lib/python3.12/unittest/mock.py:961: AssertionErrorDuring handling of the
above exception, another exception occurred:    def 
test_progress_manager_handles_lifecycle():        bridge = DummyBridge()        
manager = ProgressManager(bridge)  # type: ignore            indicator = 
manager.create_progress(&amp;quot;task&amp;quot;, &amp;quot;Task&amp;quot;, 
total=2)        manager.update_progress(&amp;quot;task&amp;quot;, 
description=&amp;quot;step one&amp;quot;)&amp;gt;       
bridge.indicator.update.assert_called_once_with(advance=1, 
description=&amp;quot;step one&amp;quot;)E       AssertionError: expected call 
not found.E       Expected: update(advance=1, description=&amp;#x27;step 
one&amp;#x27;)E         Actual: update(advance=1, description=&amp;#x27;step 
one&amp;#x27;, status=None)E       E       pytest introspection follows:E       
E       Kwargs:E       assert {&amp;#x27;advance&amp;#x27;: 
1...status&amp;#x27;: None} == {&amp;#x27;advance&amp;#x27;: 1...&amp;#x27;: 
&amp;#x27;step one&amp;#x27;}E         E         Omitting 2 identical items, use
-vv to showE         Left contains 1 more item:E         
{&amp;#x27;status&amp;#x27;: None}E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/
cli/test_progress.py:24: AssertionError__________________________ 
test_parse_feature_options __________________________    @pytest.mark.fast    
def test_parse_feature_options() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`_parse_feature_options` converts option values to
booleans.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       result = 
module._parse_feature_options([&amp;quot;a&amp;quot;, 
&amp;quot;b=false&amp;quot;, &amp;quot;c=1&amp;quot;])                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;function&amp;#x27; object has no attribute 
&amp;#x27;_parse_feature_options&amp;#x27;/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/application/cli/test_run_tests_cmd.py:16: 
AttributeError________________________ test_cli_accepts_feature_flags 
________________________    @pytest.mark.fast    def 
test_cli_accepts_feature_flags() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;CLI invocation with ``--feature`` delegates to 
`run_tests`.&amp;quot;&amp;quot;&amp;quot;            runner = 
CliRunner()&amp;gt;       with patch.object(module, 
&amp;quot;run_tests&amp;quot;, return_value=(True, &amp;quot;&amp;quot;)) as 
mock_run:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Project
s/github.com/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd.py:
25: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x1154d3410&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError______________________ test_cli_reports_coverage_percent 
_______________________    @pytest.mark.fast    def 
test_cli_reports_coverage_percent() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Successful runs print the measured coverage 
percentage.&amp;quot;&amp;quot;&amp;quot;            runner = CliRunner()       
app = build_app()            with (&amp;gt;           patch.object(module, 
&amp;quot;run_tests&amp;quot;, return_value=(True, &amp;quot;&amp;quot;)),      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            
patch.object(module, &amp;quot;_configure_optional_providers&amp;quot;, 
return_value=None),            patch.object(module, 
&amp;quot;_emit_coverage_artifact_messages&amp;quot;, return_value=None),       
patch.object(                module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, return_value=92.5            ) 
as mock_enforce,            patch.object(                module, 
&amp;quot;_coverage_instrumentation_status&amp;quot;, return_value=(True, None) 
),            patch.object(module, 
&amp;quot;coverage_artifacts_status&amp;quot;, return_value=(True, None)),      
patch.object(module, &amp;quot;increment_counter&amp;quot;, return_value=None), 
):/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cl
i/test_run_tests_cmd.py:42: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x115ee8440&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError____________________ test_cli_errors_when_plugins_disabled 
_____________________    @pytest.mark.fast    def 
test_cli_errors_when_plugins_disabled() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;CLI fails fast when pytest-cov was disabled by 
plugin autoload settings.&amp;quot;&amp;quot;&amp;quot;            runner = 
CliRunner()        app = build_app()            with (&amp;gt;           
patch.object(module, &amp;quot;run_tests&amp;quot;, return_value=(True, 
&amp;quot;&amp;quot;)),            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            
patch.object(module, &amp;quot;_configure_optional_providers&amp;quot;, 
return_value=None),            patch.object(module, 
&amp;quot;_emit_coverage_artifact_messages&amp;quot;, return_value=None),       
patch.object(module, &amp;quot;enforce_coverage_threshold&amp;quot;, 
return_value=95.0),            patch.object(                module,             
&amp;quot;_coverage_instrumentation_status&amp;quot;,                
return_value=(                    False,                    &amp;quot;pytest 
plugin autoload disabled without -p pytest_cov&amp;quot;,                ),     
),            patch.object(                module,                
&amp;quot;coverage_artifacts_status&amp;quot;,                
return_value=(False, &amp;quot;Coverage JSON missing at 
test_reports/coverage.json&amp;quot;),            ),            
patch.object(module, &amp;quot;increment_counter&amp;quot;, return_value=None), 
):/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cl
i/test_run_tests_cmd.py:69: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x115fcf1d0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError____________________ test_cli_errors_when_artifacts_missing 
____________________    @pytest.mark.fast    def 
test_cli_errors_when_artifacts_missing() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;CLI reports actionable remediation when coverage 
artifacts are absent.&amp;quot;&amp;quot;&amp;quot;            runner = 
CliRunner()        app = build_app()            with (&amp;gt;           
patch.object(module, &amp;quot;run_tests&amp;quot;, return_value=(True, 
&amp;quot;&amp;quot;)),            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            
patch.object(module, &amp;quot;_configure_optional_providers&amp;quot;, 
return_value=None),            patch.object(module, 
&amp;quot;_emit_coverage_artifact_messages&amp;quot;, return_value=None),       
patch.object(                module, 
&amp;quot;enforce_coverage_threshold&amp;quot;, return_value=95.0            ) 
as mock_enforce,            patch.object(                module, 
&amp;quot;_coverage_instrumentation_status&amp;quot;, return_value=(True, None) 
),            patch.object(                module,                
&amp;quot;coverage_artifacts_status&amp;quot;,                
return_value=(False, &amp;quot;Coverage JSON missing at 
test_reports/coverage.json&amp;quot;),            ),            
patch.object(module, &amp;quot;increment_counter&amp;quot;, return_value=None), 
):/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cl
i/test_run_tests_cmd.py:102: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x1235fa9f0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError______________________ test_feature_flags_set_environment 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1154d2780&amp;gt;    @pytest.mark.fast    def 
test_feature_flags_set_environment(monkeypatch) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;--feature name[=bool] should set 
DEVSYNTH_FEATURE_&amp;lt;NAME&amp;gt; env vars.&amp;quot;&amp;quot;&amp;quot;   
# Ensure clean env        
monkeypatch.delenv(&amp;quot;DEVSYNTH_FEATURE_ALPHA&amp;quot;, raising=False)   
monkeypatch.delenv(&amp;quot;DEVSYNTH_FEATURE_BETA&amp;quot;, raising=False)    
runner = CliRunner()&amp;gt;       with patch.object(module, 
&amp;quot;run_tests&amp;quot;, return_value=(True, &amp;quot;&amp;quot;)) as 
mock_run:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Project
s/github.com/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_opt
ions.py:23: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x124052d20&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError__________________ test_no_parallel_flag_is_passed_to_runner 
___________________    @pytest.mark.fast    def 
test_no_parallel_flag_is_passed_to_runner() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;--no-parallel should result in parallel=False in 
run_tests call.&amp;quot;&amp;quot;&amp;quot;        runner = 
CliRunner()&amp;gt;       with patch.object(module, 
&amp;quot;run_tests&amp;quot;, return_value=(True, &amp;quot;&amp;quot;)) as 
mock_run:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Project
s/github.com/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_opt
ions.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x1151167e0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError_____________________ test_segment_options_are_propagated 
______________________    @pytest.mark.fast    def 
test_segment_options_are_propagated() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;--segment and --segment-size should be passed 
through to run_tests.&amp;quot;&amp;quot;&amp;quot;        runner = 
CliRunner()&amp;gt;       with patch.object(module, 
&amp;quot;run_tests&amp;quot;, return_value=(True, &amp;quot;&amp;quot;)) as 
mock_run:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Project
s/github.com/ravenoak/devsynth/tests/unit/application/cli/test_run_tests_cmd_opt
ions.py:59: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x115915760&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;function run_tests_cmd at 0x1146f18a0&amp;gt; does not have the 
attribute 
&amp;#x27;run_tests&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks
/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError____________ test_project_state_analyzer_analyze_graceful_fallback
_____________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x115edbec0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_project_state_analyzer_an0&amp;#x27;)    
@pytest.mark.fast    def 
test_project_state_analyzer_analyze_graceful_fallback(monkeypatch, tmp_path: 
Path):        &amp;quot;&amp;quot;&amp;quot;ReqID: T-ANALYZER-ERR-002        
When any internal step raises, ProjectStateAnalyzer.analyze should not raise    
and must return a safe dictionary shape with default values.        It must also
avoid creating files outside the provided environment.        
&amp;quot;&amp;quot;&amp;quot;            # Force an early method to raise to 
cover the error path        def boom(self, *args: Any, **kwargs: Any) -&amp;gt; 
None:  # noqa: ARG002            raise RuntimeError(&amp;quot;boom&amp;quot;)   
monkeypatch.setattr(ProjectStateAnalyzer, &amp;quot;_index_files&amp;quot;, 
boom)            before = set(os.listdir(tmp_path))            analyzer = 
ProjectStateAnalyzer(str(tmp_path))            # Act        result: Dict = 
analyzer.analyze()            after = set(os.listdir(tmp_path))            # 
Assert: safe shape        assert set(result.keys()) == {            
&amp;quot;files&amp;quot;,            &amp;quot;languages&amp;quot;,            
&amp;quot;architecture&amp;quot;,            &amp;quot;components&amp;quot;,    
&amp;quot;health_report&amp;quot;,        }            assert 
result[&amp;quot;files&amp;quot;] == {}        assert 
result[&amp;quot;languages&amp;quot;] == {}        assert 
isinstance(result[&amp;quot;architecture&amp;quot;], dict)        assert 
result[&amp;quot;architecture&amp;quot;].get(&amp;quot;components&amp;quot;, [])
== []        assert result[&amp;quot;components&amp;quot;] == []            hr =
result[&amp;quot;health_report&amp;quot;]        assert isinstance(hr, 
dict)&amp;gt;       assert hr.get(&amp;quot;status&amp;quot;) == 
&amp;quot;unknown&amp;quot;E       AssertionError: assert None == 
&amp;#x27;unknown&amp;#x27;E        +  where None = &amp;lt;built-in method get 
of dict object at 0x11575edc0&amp;gt;(&amp;#x27;status&amp;#x27;)E        +    
where &amp;lt;built-in method get of dict object at 0x11575edc0&amp;gt; = 
{&amp;#x27;architecture&amp;#x27;: {&amp;#x27;components&amp;#x27;: [], 
&amp;#x27;confidence&amp;#x27;: 0.0, &amp;#x27;type&amp;#x27;: 
&amp;#x27;unknown&amp;#x27;}, &amp;#x27;code_count&amp;#x27;: 0, 
&amp;#x27;file_count&amp;#x27;: 0, &amp;#x27;health_score&amp;#x27;: 0.0, 
...}.get/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicat
ion/code_analysis/test_project_state_analyzer_error_paths.py:52: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:15,477 - 
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting 
project analysis for 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_project_state_analyzer_an02025-10-28 09:26:15,477 - 
devsynth.application.code_analysis.project_state_analyzer - ERROR - 
ProjectStateAnalyzer.analyze failed: boom------------------------------ Captured
log call -------------------------------INFO     
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615 
Starting project analysis for 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_project_state_analyzer_an0ERROR    
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615 
ProjectStateAnalyzer.analyze failed: boom_______________ 
test_build_consensus_stores_decision_and_summary _______________memory_manager =
&amp;lt;devsynth.application.memory.memory_manager.MemoryManager object at 
0x142b30230&amp;gt;    @pytest.mark.fast    def 
test_build_consensus_stores_decision_and_summary(        memory_manager: 
MemoryManager,    ) -&amp;gt; None:        team = DummyTeam(memory_manager)     
task = {&amp;quot;id&amp;quot;: &amp;quot;t1&amp;quot;, 
&amp;quot;title&amp;quot;: &amp;quot;Test&amp;quot;}&amp;gt;       
team.build_consensus(task)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/application/collaboration/test_wsde_memory_sync_hooks.py:125: _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:60: in build_consensus    return 
self._build_consensus_inner(task, phase)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;tests.unit.application.collaboration.test_wsde_memory_sync_hooks.DummyTe
am object at 0x142b302c0&amp;gt;task = {&amp;#x27;id&amp;#x27;: 
&amp;#x27;t1&amp;#x27;, &amp;#x27;title&amp;#x27;: &amp;#x27;Test&amp;#x27;}, 
phase = None    def _build_consensus_inner(        self, task: Dict, phase: 
Optional[Phase] = None    ) -&amp;gt; ConsensusOutcome:        
&amp;quot;&amp;quot;&amp;quot;Internal implementation of consensus 
building.&amp;quot;&amp;quot;&amp;quot;        if &amp;quot;id&amp;quot; not in 
task:            task[&amp;quot;id&amp;quot;] = str(uuid.uuid4())            
self.logger.info(            f&amp;quot;Building consensus for task 
{task[&amp;#x27;id&amp;#x27;]}: {task.get(&amp;#x27;title&amp;#x27;, 
&amp;#x27;Untitled&amp;#x27;)}&amp;quot;        )            task_text = (      
(task.get(&amp;quot;description&amp;quot;, &amp;quot;&amp;quot;) or 
&amp;quot;&amp;quot;) + &amp;quot; &amp;quot; + 
(task.get(&amp;quot;title&amp;quot;, &amp;quot;&amp;quot;) or 
&amp;quot;&amp;quot;)        )        keywords = 
set(re.findall(r&amp;quot;\b\w+\b&amp;quot;, task_text.lower()))            
agent_opinions = self._collect_agent_opinion_records(task, keywords=keywords)   
if not agent_opinions:            self._generate_agent_opinions(task)           
agent_opinions = self._collect_agent_opinion_records(                task, 
keywords=keywords            )            consensus_id = 
str(uuid.uuid4())&amp;gt;       conflicts = self._identify_conflicts(task, 
agent_opinions)                    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: 
DummyTeam._identify_conflicts() takes 2 positional arguments but 3 were 
given/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applicati
on/collaboration/wsde_team_consensus.py:86: 
TypeError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:19,315 - 
devsynth.application.memory.memory_manager - INFO - Memory Manager initialized 
with adapters: tinydb, graph2025-10-28 09:26:19,315 - 
devsynth.application.memory.tiered_cache - INFO - Tiered cache initialized with 
max size 50------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.memory_manager:logging_setup.py:615 Memory Manager 
initialized with adapters: tinydb, graphINFO     
devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered cache 
initialized with max size 50----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:19,316 - dummy - INFO - Building 
consensus for task t1: Test------------------------------ Captured log call 
-------------------------------INFO     dummy:wsde_team_consensus.py:69 Building
consensus for task t1: Test______________ 
test_consensus_outcome_round_trip_orders_conflicts ______________    
@pytest.mark.fast    def test_consensus_outcome_round_trip_orders_conflicts() 
-&amp;gt; None:        mixin = DummyTeam()        payload = {            
&amp;quot;dto_type&amp;quot;: &amp;quot;ConsensusOutcome&amp;quot;,            
&amp;quot;consensus_id&amp;quot;: &amp;quot;c3&amp;quot;,            
&amp;quot;task_id&amp;quot;: &amp;quot;t1&amp;quot;,            
&amp;quot;method&amp;quot;: &amp;quot;conflict_resolution_synthesis&amp;quot;,  
&amp;quot;agent_opinions&amp;quot;: [                {                    
&amp;quot;dto_type&amp;quot;: &amp;quot;AgentOpinionRecord&amp;quot;,           
&amp;quot;agent_id&amp;quot;: &amp;quot;beta&amp;quot;,                    
&amp;quot;opinion&amp;quot;: &amp;quot;no&amp;quot;,                    
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-02T00:00:00&amp;quot;,         
},                {                    &amp;quot;dto_type&amp;quot;: 
&amp;quot;AgentOpinionRecord&amp;quot;,                    
&amp;quot;agent_id&amp;quot;: &amp;quot;alpha&amp;quot;,                    
&amp;quot;opinion&amp;quot;: &amp;quot;yes&amp;quot;,                    
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-01T00:00:00&amp;quot;,         
},            ],            &amp;quot;conflicts&amp;quot;: [                {   
&amp;quot;dto_type&amp;quot;: &amp;quot;ConflictRecord&amp;quot;,               
&amp;quot;conflict_id&amp;quot;: &amp;quot;c2&amp;quot;,                    
&amp;quot;task_id&amp;quot;: &amp;quot;t1&amp;quot;,                    
&amp;quot;agent_a&amp;quot;: &amp;quot;beta&amp;quot;,                    
&amp;quot;agent_b&amp;quot;: &amp;quot;alpha&amp;quot;,                    
&amp;quot;opinion_a&amp;quot;: &amp;quot;no&amp;quot;,                    
&amp;quot;opinion_b&amp;quot;: &amp;quot;yes&amp;quot;,                },       
{                    &amp;quot;dto_type&amp;quot;: 
&amp;quot;ConflictRecord&amp;quot;,                    
&amp;quot;conflict_id&amp;quot;: &amp;quot;c1&amp;quot;,                    
&amp;quot;task_id&amp;quot;: &amp;quot;t1&amp;quot;,                    
&amp;quot;agent_a&amp;quot;: &amp;quot;alpha&amp;quot;,                    
&amp;quot;agent_b&amp;quot;: &amp;quot;beta&amp;quot;,                    
&amp;quot;opinion_a&amp;quot;: &amp;quot;yes&amp;quot;,                    
&amp;quot;opinion_b&amp;quot;: &amp;quot;no&amp;quot;,                },        
],            &amp;quot;conflicts_identified&amp;quot;: 0,            
&amp;quot;synthesis&amp;quot;: {                &amp;quot;dto_type&amp;quot;: 
&amp;quot;SynthesisArtifact&amp;quot;,                &amp;quot;text&amp;quot;: 
&amp;quot;resolved&amp;quot;,                &amp;quot;key_points&amp;quot;: 
[&amp;quot;compromise&amp;quot;],                
&amp;quot;expertise_weights&amp;quot;: {&amp;quot;alpha&amp;quot;: 0.6, 
&amp;quot;beta&amp;quot;: 0.4},                
&amp;quot;readability_score&amp;quot;: {&amp;quot;flesch_reading_ease&amp;quot;:
65.0},            },            &amp;quot;timestamp&amp;quot;: 
&amp;quot;2025-01-01T00:00:00&amp;quot;,        }    &amp;gt;       outcome = 
ConsensusOutcome.from_dict(payload)                  
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/d
evsynth/tests/unit/application/collaboration/test_wsde_team_consensus_summary.py
:124: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:270: in from_dict    return cls(**kwargs)           
^^^^^^^^^^^^^&amp;lt;string&amp;gt;:18: in __init__    
???/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application
/collaboration/dto.py:340: in __post_init__    sorted(_ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ opinion = 
{&amp;#x27;agent_id&amp;#x27;: &amp;#x27;beta&amp;#x27;, 
&amp;#x27;dto_type&amp;#x27;: &amp;#x27;AgentOpinionRecord&amp;#x27;, 
&amp;#x27;opinion&amp;#x27;: &amp;#x27;no&amp;#x27;, 
&amp;#x27;timestamp&amp;#x27;: &amp;#x27;2025-01-02T00:00:00&amp;#x27;}&amp;gt; 
key=lambda opinion: (opinion.agent_id or &amp;quot;&amp;quot;, opinion.timestamp
or &amp;quot;&amp;quot;),                                 ^^^^^^^^^^^^^^^^      
)    )E   AttributeError: &amp;#x27;dict&amp;#x27; object has no attribute 
&amp;#x27;agent_id&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/collaboration/dto.py:342: AttributeError__ 
TestEDRRCoordinatorInitialization.test_coordinator_initialization_defaults 
__self = &amp;lt;test_core.TestEDRRCoordinatorInitialization object at 
0x117dbb170&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpqi29p5sd
&amp;#x27;)    @pytest.mark.fast    def 
test_coordinator_initialization_defaults(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test coordinator initialization with default 
values.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       with patch(            
&amp;quot;devsynth.application.edrr.coordinator.core.get_llm_settings&amp;quot; 
) as 
mock_get_settings:/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/application/edrr/coordinator/test_core.py:55: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142b2aff0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module &amp;#x27;devsynth.application.edrr.coordinator.core&amp;#x27; 
from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/edrr/coordinator/core.py&amp;#x27;&amp;gt; does not have the attribute 
&amp;#x27;get_llm_settings&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Fra
meworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError_ 
TestEDRRCoordinatorInitialization.test_coordinator_initialization_custom_config 
_self = &amp;lt;test_core.TestEDRRCoordinatorInitialization object at 
0x117dbad50&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphgfrg16o
&amp;#x27;)    @pytest.mark.fast    def 
test_coordinator_initialization_custom_config(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test coordinator initialization with custom 
configuration.&amp;quot;&amp;quot;&amp;quot;        custom_config = {           
&amp;quot;max_recursion_depth&amp;quot;: 5,            
&amp;quot;granularity_threshold&amp;quot;: 0.3,            
&amp;quot;cost_benefit_ratio&amp;quot;: 0.6,        }    &amp;gt;       
coordinator = EDRRCoordinator(custom_config)                      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: EDRRCoordinator.__init__() 
missing 5 required positional arguments: &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:89: 
TypeError_ 
TestEDRRCoordinatorInitialization.test_coordinator_dependencies_initialization 
_self = &amp;lt;test_core.TestEDRRCoordinatorInitialization object at 
0x117dbb8c0&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpsv2egte3
&amp;#x27;)    @pytest.mark.fast    def 
test_coordinator_dependencies_initialization(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test that coordinator initializes all dependencies
correctly.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:98: 
TypeError_______ 
TestEDRRCoordinatorPhaseExecution.test_start_cycle_from_manifest _______self = 
&amp;lt;test_core.TestEDRRCoordinatorPhaseExecution object at 
0x117dd8c20&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp0rezeblx
&amp;#x27;)    @pytest.mark.fast    def test_start_cycle_from_manifest(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test cycle execution from
manifest file.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:165: 
TypeError___ 
TestEDRRCoordinatorRecursion.test_should_terminate_recursion_depth_limit ___self
= &amp;lt;test_core.TestEDRRCoordinatorRecursion object at 
0x117dd8d10&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpiy7rdh_q
&amp;#x27;)    @pytest.mark.fast    def 
test_should_terminate_recursion_depth_limit(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test recursion termination based on depth 
limit.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator({&amp;quot;max_recursion_depth&amp;quot;: 2})                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 5 required positional arguments: 
&amp;#x27;wsde_team&amp;#x27;, &amp;#x27;code_analyzer&amp;#x27;, 
&amp;#x27;ast_transformer&amp;#x27;, &amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:198: 
TypeError___ 
TestEDRRCoordinatorRecursion.test_should_terminate_recursion_granularity ___self
= &amp;lt;test_core.TestEDRRCoordinatorRecursion object at 
0x117dd9160&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp6fxb05ci
&amp;#x27;)    @pytest.mark.fast    def 
test_should_terminate_recursion_granularity(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test recursion termination based on granularity 
threshold.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:211: 
TypeError__ 
TestEDRRCoordinatorRecursion.test_should_terminate_recursion_cost_benefit 
___self = &amp;lt;test_core.TestEDRRCoordinatorRecursion object at 
0x117dd9640&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpjdf82ren
&amp;#x27;)    @pytest.mark.fast    def 
test_should_terminate_recursion_cost_benefit(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test recursion termination based on cost-benefit 
ratio.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:227: 
TypeError_ 
TestEDRRCoordinatorRecursion.test_should_terminate_recursion_resource_limit 
__self = &amp;lt;test_core.TestEDRRCoordinatorRecursion object at 
0x117dd9b20&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp_u4flgxp
&amp;#x27;)    @pytest.mark.fast    def 
test_should_terminate_recursion_resource_limit(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test recursion termination based on resource 
limits.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:243: 
TypeError_ 
TestEDRRCoordinatorRecursion.test_should_not_terminate_recursion_good_metrics 
_self = &amp;lt;test_core.TestEDRRCoordinatorRecursion object at 
0x117dda000&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp00rkseyr
&amp;#x27;)    @pytest.mark.fast    def 
test_should_not_terminate_recursion_good_metrics(self, tmp_project_dir):        
&amp;quot;&amp;quot;&amp;quot;Test that recursion continues when metrics are 
good.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:259: 
TypeError________ TestEDRRCoordinatorMicroCycles.test_register_micro_cycle_hook 
_________self = &amp;lt;test_core.TestEDRRCoordinatorMicroCycles object at 
0x117ddaa50&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp26b3p2by
&amp;#x27;)    @pytest.mark.fast    def test_register_micro_cycle_hook(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test micro-cycle hook 
registration.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:301: 
TypeError_________ TestEDRRCoordinatorMicroCycles.test_invoke_micro_cycle_hooks 
_________self = &amp;lt;test_core.TestEDRRCoordinatorMicroCycles object at 
0x117ddaf30&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpizyepqa7
&amp;#x27;)    @pytest.mark.fast    def test_invoke_micro_cycle_hooks(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test micro-cycle hook 
invocation.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:314: 
TypeError_______________ TestEDRRCoordinatorHooks.test_register_sync_hook 
_______________self = &amp;lt;test_core.TestEDRRCoordinatorHooks object at 
0x117ddb500&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbcc9_8yl
&amp;#x27;)    @pytest.mark.fast    def test_register_sync_hook(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test sync hook 
registration.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:345: 
TypeError_______________ TestEDRRCoordinatorHooks.test_invoke_sync_hooks 
________________self = &amp;lt;test_core.TestEDRRCoordinatorHooks object at 
0x117dd9fd0&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp4hcc14ez
&amp;#x27;)    @pytest.mark.fast    def test_invoke_sync_hooks(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test sync hook 
invocation.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:358: 
TypeError_____________ TestEDRRCoordinatorHooks.test_register_recovery_hook 
_____________self = &amp;lt;test_core.TestEDRRCoordinatorHooks object at 
0x117ddb6e0&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp6cl_ywjz
&amp;#x27;)    @pytest.mark.fast    def test_register_recovery_hook(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test recovery hook 
registration.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:380: 
TypeError_____________ TestEDRRCoordinatorHooks.test_execute_recovery_hooks 
_____________self = &amp;lt;test_core.TestEDRRCoordinatorHooks object at 
0x117df40e0&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpl_3i2zw7
&amp;#x27;)    @pytest.mark.fast    def test_execute_recovery_hooks(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test recovery hook 
execution.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:393: 
TypeError______ 
TestEDRRCoordinatorPhaseManagement.test_set_manual_phase_override _______self = 
&amp;lt;test_core.TestEDRRCoordinatorPhaseManagement object at 
0x117df4200&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnxju0_hg
&amp;#x27;)    @pytest.mark.fast    def test_set_manual_phase_override(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test manual phase 
override setting.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:425: 
TypeError_____ 
TestEDRRCoordinatorPhaseManagement.test_get_phase_quality_threshold ______self =
&amp;lt;test_core.TestEDRRCoordinatorPhaseManagement object at 
0x117df4650&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpze8sene9
&amp;#x27;)    @pytest.mark.fast    def test_get_phase_quality_threshold(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test phase quality 
threshold retrieval.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:438: 
TypeError_________ TestEDRRCoordinatorUtilityMethods.test_sanitize_positive_int 
_________self = &amp;lt;test_core.TestEDRRCoordinatorUtilityMethods object at 
0x117df4c20&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpimdiyok_
&amp;#x27;)    @pytest.mark.fast    def test_sanitize_positive_int(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test positive integer 
sanitization.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:456: 
TypeError__________ TestEDRRCoordinatorUtilityMethods.test_sanitize_threshold 
___________self = &amp;lt;test_core.TestEDRRCoordinatorUtilityMethods object at 
0x117df5070&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpg5snilge
&amp;#x27;)    @pytest.mark.fast    def test_sanitize_threshold(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test threshold 
sanitization.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:481: 
TypeError________ TestEDRRCoordinatorIntegration.test_edrr_cycle_error_recovery 
_________self = &amp;lt;test_core.TestEDRRCoordinatorIntegration object at 
0x117df5f40&amp;gt;tmp_project_dir = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp1cpt6v3c
&amp;#x27;)    @pytest.mark.fast    def test_edrr_cycle_error_recovery(self, 
tmp_project_dir):        &amp;quot;&amp;quot;&amp;quot;Test EDRR cycle with 
error recovery.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       coordinator = 
EDRRCoordinator()                      ^^^^^^^^^^^^^^^^^E       TypeError: 
EDRRCoordinator.__init__() missing 6 required positional arguments: 
&amp;#x27;memory_manager&amp;#x27;, &amp;#x27;wsde_team&amp;#x27;, 
&amp;#x27;code_analyzer&amp;#x27;, &amp;#x27;ast_transformer&amp;#x27;, 
&amp;#x27;prompt_manager&amp;#x27;, and 
&amp;#x27;documentation_manager&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/application/edrr/coordinator/test_core.py:574: 
TypeError____________________ test_maybe_auto_progress_respects_flag 
____________________self = &amp;lt;MagicMock 
name=&amp;#x27;_decide_next_phase&amp;#x27; 
id=&amp;#x27;5413963920&amp;#x27;&amp;gt;    def assert_called_once(self):      
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called only once.        
&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:            
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to have been called once. 
Called %s times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;_decide_next_phase&amp;#x27; to have been 
called once. Called 10 times.E           Calls: 
./opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/unittest/mock.py:928: AssertionErrorDuring handling of the 
above exception, another exception occurred:    @pytest.mark.fast    def 
test_maybe_auto_progress_respects_flag():        
&amp;quot;&amp;quot;&amp;quot;Auto progress runs only when enabled.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        core = EDRRCoordinatorCore(     
memory_manager=MagicMock(spec=MemoryManager),            
wsde_team=MagicMock(spec=WSDETeam),            
code_analyzer=MagicMock(spec=CodeAnalyzer),            
ast_transformer=MagicMock(spec=AstTransformer),            
prompt_manager=MagicMock(spec=PromptManager),            
documentation_manager=MagicMock(spec=DocumentationManager),        )            
core.current_phase = Phase.EXPAND        core.task = {&amp;quot;name&amp;quot;: 
&amp;quot;task&amp;quot;}            core.config = 
{&amp;quot;auto_progress&amp;quot;: True}        with patch.object(            
core, &amp;quot;_decide_next_phase&amp;quot;, return_value=Phase.DIFFERENTIATE  
) as decide:            with patch.object(core, 
&amp;quot;progress_to_phase&amp;quot;) as progress:                
core._maybe_auto_progress()&amp;gt;       decide.assert_called_once()E       
AssertionError: Expected &amp;#x27;_decide_next_phase&amp;#x27; to have been 
called once. Called 10 times.E       Calls: 
./Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edr
r/test_coordinator_core.py:575: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:26:19,927 - 
devsynth.application.edrr.coordinator_core - INFO - Initialized EDRR Coordinator
(cycle_id: e72d8d4d-4009-47d7-8406-3a7b224411c2)2025-10-28 09:26:19,927 - 
devsynth.application.edrr.coordinator_core - INFO - Auto-progressing to next 
phase2025-10-28 09:26:19,927 - devsynth.application.edrr.coordinator_core - INFO
- Auto-progressing to next phase2025-10-28 09:26:19,927 - 
devsynth.application.edrr.coordinator_core - INFO - Auto-progressing to next 
phase2025-10-28 09:26:19,928 - devsynth.application.edrr.coordinator_core - INFO
- Auto-progressing to next phase2025-10-28 09:26:19,928 - 
devsynth.application.edrr.coordinator_core - INFO - Auto-progressing to next 
phase2025-10-28 09:26:19,928 - devsynth.application.edrr.coordinator_core - INFO
- Auto-progressing to next phase2025-10-28 09:26:19,928 - 
devsynth.application.edrr.coordinator_core - INFO - Auto-progressing to next 
phase2025-10-28 09:26:19,928 - devsynth.application.edrr.coordinator_core - INFO
- Auto-progressing to next phase2025-10-28 09:26:19,928 - 
devsynth.application.edrr.coordinator_core - INFO - Auto-progressing to next 
phase2025-10-28 09:26:19,928 - devsynth.application.edrr.coordinator_core - INFO
- Auto-progressing to next phase------------------------------ Captured log call
-------------------------------INFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Initialized EDRR
Coordinator (cycle_id: e72d8d4d-4009-47d7-8406-3a7b224411c2)INFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phaseINFO     
devsynth.application.edrr.coordinator_core:logging_setup.py:615 Auto-progressing
to next phase___________________ test_apply_dialectical_reasoning_success 
___________________coordinator = 
&amp;lt;devsynth.application.edrr.coordinator.core.EDRRCoordinator object at 
0x142b1b140&amp;gt;    @pytest.mark.fast    def 
test_apply_dialectical_reasoning_success(coordinator: EDRRCoordinator) -&amp;gt;
None:        &amp;quot;&amp;quot;&amp;quot;ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;            final = 
{&amp;quot;status&amp;quot;: &amp;quot;completed&amp;quot;, 
&amp;quot;synthesis&amp;quot;: &amp;quot;done&amp;quot;}        with patch(     
&amp;quot;devsynth.application.edrr.coordinator.core.reasoning_loop&amp;quot;,  
return_value=[{&amp;quot;synthesis&amp;quot;: &amp;quot;next&amp;quot;}, final],
) as rl:            result = coordinator.apply_dialectical_reasoning(           
{&amp;quot;solution&amp;quot;: &amp;quot;initial&amp;quot;}, MagicMock()        
)        rl.assert_called_once()        
coordinator.memory_manager.flush_updates.assert_called_once()&amp;gt;       
assert isinstance(result, DialecticalSequence)E       AssertionError: assert 
FalseE        +  where False = isinstance({&amp;#x27;status&amp;#x27;: 
&amp;#x27;completed&amp;#x27;, &amp;#x27;synthesis&amp;#x27;: 
&amp;#x27;done&amp;#x27;}, 
DialecticalSequence)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/application/edrr/test_coordinator_reasoning.py:51: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:19,983 - 
devsynth.application.edrr.templates - INFO - Registered expand_phase 
template2025-10-28 09:26:19,983 - devsynth.application.edrr.templates - INFO - 
Registered differentiate_phase template2025-10-28 09:26:19,983 - 
devsynth.application.edrr.templates - INFO - Registered refine_phase 
template2025-10-28 09:26:19,983 - devsynth.application.edrr.templates - INFO - 
Registered retrospect_phase template2025-10-28 09:26:19,983 - 
devsynth.application.edrr.manifest_parser - INFO - Manifest Parser initialized 
with enhanced traceability2025-10-28 09:26:19,983 - 
devsynth.application.edrr.coordinator.core - INFO - EDRR coordinator initialized
(recursion depth: 0)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.edrr.templates:logging_setup.py:615 Registered expand_phase
templateINFO     devsynth.application.edrr.templates:logging_setup.py:615 
Registered differentiate_phase templateINFO     
devsynth.application.edrr.templates:logging_setup.py:615 Registered refine_phase
templateINFO     devsynth.application.edrr.templates:logging_setup.py:615 
Registered retrospect_phase templateINFO     
devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest Parser 
initialized with enhanced traceabilityINFO     
devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR coordinator
initialized (recursion depth: 0)----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:26:19,984 - 
devsynth.application.edrr.coordinator.core - INFO - EDRRCoordinator invoking 
dialectical reasoning------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRRCoordinator 
invoking dialectical reasoning______________ 
test_apply_dialectical_reasoning_consensus_failure ______________coordinator = 
&amp;lt;devsynth.application.edrr.coordinator.core.EDRRCoordinator object at 
0x142b42060&amp;gt;caplog = &amp;lt;_pytest.logging.LogCaptureFixture object at 
0x1154dd130&amp;gt;    @pytest.mark.fast    def 
test_apply_dialectical_reasoning_consensus_failure(        coordinator: 
EDRRCoordinator, caplog: pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A&amp;quot;&amp;quot;&amp;quot;          
with patch(            
&amp;quot;devsynth.application.edrr.coordinator.core.reasoning_loop&amp;quot;,  
return_value=[],        ):            with caplog.at_level(logging.WARNING):    
result = coordinator.apply_dialectical_reasoning(                    
{&amp;quot;solution&amp;quot;: &amp;quot;initial&amp;quot;}, MagicMock()        
)&amp;gt;       assert isinstance(result, DialecticalSequence)E       assert 
FalseE        +  where False = isinstance({}, 
DialecticalSequence)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/application/edrr/test_coordinator_reasoning.py:70: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:20,003 - 
devsynth.application.edrr.templates - INFO - Registered expand_phase 
template2025-10-28 09:26:20,003 - devsynth.application.edrr.templates - INFO - 
Registered differentiate_phase template2025-10-28 09:26:20,003 - 
devsynth.application.edrr.templates - INFO - Registered refine_phase 
template2025-10-28 09:26:20,003 - devsynth.application.edrr.templates - INFO - 
Registered retrospect_phase template2025-10-28 09:26:20,004 - 
devsynth.application.edrr.manifest_parser - INFO - Manifest Parser initialized 
with enhanced traceability2025-10-28 09:26:20,004 - 
devsynth.application.edrr.coordinator.core - INFO - EDRR coordinator initialized
(recursion depth: 0)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.edrr.templates:logging_setup.py:615 Registered expand_phase
templateINFO     devsynth.application.edrr.templates:logging_setup.py:615 
Registered differentiate_phase templateINFO     
devsynth.application.edrr.templates:logging_setup.py:615 Registered refine_phase
templateINFO     devsynth.application.edrr.templates:logging_setup.py:615 
Registered retrospect_phase templateINFO     
devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest Parser 
initialized with enhanced traceabilityINFO     
devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR coordinator
initialized (recursion depth: 0)----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:26:20,005 - 
devsynth.application.edrr.coordinator.core - WARNING - Consensus failure during 
dialectical reasoning------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.edrr.coordinator.core:logging_setup.py:615 Consensus 
failure during dialectical reasoning______________ 
test_decide_next_phase_respects_quality_threshold _______________coordinator = 
&amp;lt;tests.unit.application.edrr.test_phase_management_module.StubCoordinator
object at 0x142b39d60&amp;gt;    def 
test_decide_next_phase_respects_quality_threshold(        coordinator: 
StubCoordinator,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A&amp;quot;&amp;quot;&amp;quot;          
coordinator.current_phase = Phase.EXPAND        coordinator.results = 
{&amp;quot;EXPAND&amp;quot;: {&amp;quot;quality_score&amp;quot;: 0.2}}        
coordinator._quality_thresholds = {Phase.EXPAND: 0.5}        assert 
coordinator._decide_next_phase() is None            
coordinator.results[&amp;quot;EXPAND&amp;quot;][&amp;quot;quality_score&amp;quot
;] = 0.8        coordinator._phase_start_times[Phase.EXPAND] = datetime.now() - 
timedelta(            seconds=10        )        
coordinator.phase_transition_timeout = 1&amp;gt;       assert 
coordinator._decide_next_phase() == Phase.DIFFERENTIATEE       AssertionError: 
assert None == &amp;lt;Phase.DIFFERENTIATE: 
&amp;#x27;differentiate&amp;#x27;&amp;gt;E        +  where None = 
_decide_next_phase()E        +    where _decide_next_phase = 
&amp;lt;tests.unit.application.edrr.test_phase_management_module.StubCoordinator
object at 0x142b39d60&amp;gt;._decide_next_phaseE        +  and   
&amp;lt;Phase.DIFFERENTIATE: &amp;#x27;differentiate&amp;#x27;&amp;gt; = 
Phase.DIFFERENTIATE/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/application/edrr/test_phase_management_module.py:108: 
AssertionError_________________ test_maybe_auto_progress_invokes_progression 
_________________self = &amp;lt;MagicMock 
id=&amp;#x27;5414072640&amp;#x27;&amp;gt;, args = (&amp;lt;Phase.REFINE: 
&amp;#x27;refine&amp;#x27;&amp;gt;,)kwargs = {}, msg = &amp;quot;Expected 
&amp;#x27;mock&amp;#x27; to be called once. Called 0 times.&amp;quot;    def 
assert_called_once_with(self, /, *args, **kwargs):        
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called exactly once and 
that that call was        with the specified 
arguments.&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:    
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to be called once. Called %s 
times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;mock&amp;#x27; to be called once. Called 0 
times./opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versi
ons/3.12/lib/python3.12/unittest/mock.py:960: AssertionErrorDuring handling of 
the above exception, another exception occurred:coordinator = 
&amp;lt;tests.unit.application.edrr.test_phase_management_module.StubCoordinator
object at 0x142cfe660&amp;gt;    def 
test_maybe_auto_progress_invokes_progression(coordinator: StubCoordinator) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;            coordinator.auto_phase_transitions 
= True        coordinator.wsde_team.elaborate_details = MagicMock()        
coordinator._decide_next_phase = MagicMock(side_effect=[Phase.REFINE, None])    
coordinator.progress_to_phase = MagicMock()            
coordinator._maybe_auto_progress()    &amp;gt;       
coordinator.progress_to_phase.assert_called_once_with(Phase.REFINE)E       
AssertionError: Expected &amp;#x27;mock&amp;#x27; to be called once. Called 0 
times./Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicatio
n/edrr/test_phase_management_module.py:121: AssertionError________________ 
test_reasoning_loop_retries_on_transient_error ________________obj = 
&amp;lt;module &amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt;name = 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;ann = 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:&amp;gt;           obj = getattr(obj, name)                  
^^^^^^^^^^^^^^^^^^E           AttributeError: module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;. Did you mean: 
&amp;#x27;ApplyDialecticalReasoning&amp;#x27;?/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90:
 AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x142b618b0&amp;gt;    @pytest.mark.fast    @pytest.mark.unit    def 
test_reasoning_loop_retries_on_transient_error(monkeypatch):        calls = 
{&amp;quot;count&amp;quot;: 0}            def flaky_apply(team, task, critic, 
memory):  # signature mirrors underlying call            
calls[&amp;quot;count&amp;quot;] += 1            # First call fails with a 
transient error; second returns success            if 
calls[&amp;quot;count&amp;quot;] == 1:                raise 
RuntimeError(&amp;quot;transient&amp;quot;)            return {                
&amp;quot;status&amp;quot;: &amp;quot;completed&amp;quot;,                
&amp;quot;phase&amp;quot;: &amp;quot;refine&amp;quot;,                
&amp;quot;synthesis&amp;quot;: task.get(&amp;quot;solution&amp;quot;),          
}            # Patch the internal alias used by the reasoning loop&amp;gt;      
monkeypatch.setattr(            
&amp;quot;devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoning&
amp;quot;,            flaky_apply,            raising=True,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edr
r/test_reasoning_loop_retries.py:33: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt;name = 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;ann = 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.methodology.edrr.reasoning_loop 
has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:9
2: AttributeError____________________ test_openai_provider_requires_api_key 
_____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x142c0bf50&amp;gt;    @pytest.mark.fast    def 
test_openai_provider_requires_api_key(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Missing API key should raise a clear 
error.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.delenv(&amp;quot;OPENAI_API_KEY&amp;quot;, raising=False)           
# Import the module (it should handle missing OpenAI gracefully)        module =
importlib.import_module(&amp;quot;devsynth.application.llm.openai_provider&amp;q
uot;)            # Test that instantiation fails with a clear error when no API 
key is provided        with pytest.raises(module.OpenAIConnectionError) as 
exc:&amp;gt;           
module.OpenAIProvider({})/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/application/llm/test_import_without_openai.py:29: _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x142b246b0&amp;gt;config = {}    def __init__(self, config: Dict = None):      
&amp;quot;&amp;quot;&amp;quot;Initialize the OpenAI provider.            Args:  
config: Configuration dictionary with the following keys:                - 
api_key: OpenAI API key (default: from config)                - model: Model 
name to use (default: from config)                - max_tokens: Maximum tokens 
for responses (default: from config)                - temperature: Temperature 
for generation (default: from config)                - api_base: Base URL for 
the OpenAI API (default: from config)            Raises:            
OpenAIConnectionError: If no API key is provided or available in environment    
&amp;quot;&amp;quot;&amp;quot;        # Get default settings from configuration 
from ...config.settings import get_llm_settings            default_settings = 
get_llm_settings()            # Initialize with default settings, overridden by 
provided config        self.config = {**default_settings, **(config or {})}     
# Set instance variables from config using standardized parameter names        
self.api_key = self.config.get(&amp;quot;api_key&amp;quot;) or self.config.get( 
&amp;quot;openai_api_key&amp;quot;        )  # Support both old and new names   
self.model = (            self.config.get(&amp;quot;model&amp;quot;)            
or self.config.get(&amp;quot;openai_model&amp;quot;)            or 
&amp;quot;gpt-3.5-turbo&amp;quot;        )        self.max_tokens = 
self.config.get(&amp;quot;max_tokens&amp;quot;) or 1024        self.temperature 
= self.config.get(&amp;quot;temperature&amp;quot;) or 0.7        self.api_base =
self.config.get(&amp;quot;base_url&amp;quot;) or self.config.get(            
&amp;quot;api_base&amp;quot;        )  # Support both old and new names        
self.timeout = self.config.get(&amp;quot;timeout&amp;quot;) or 60        
self.max_retries = self.config.get(&amp;quot;max_retries&amp;quot;, 3)        
self.circuit_breaker = CircuitBreaker(            
failure_threshold=self.config.get(&amp;quot;failure_threshold&amp;quot;, 3),    
recovery_timeout=self.config.get(&amp;quot;recovery_timeout&amp;quot;, 60),     
)        # Deterministic per-call timeout (seconds)        # Env var precedence:
OPENAI_HTTP_TIMEOUT (docs/tasks.md Task 70)        try:            timeout_env =
os.environ.get(&amp;quot;OPENAI_HTTP_TIMEOUT&amp;quot;)            
self.call_timeout = (                float(timeout_env)                if 
timeout_env is not None                else 
float(self.config.get(&amp;quot;call_timeout&amp;quot;, 15))            )       
except (TypeError, ValueError):            self.call_timeout = 15.0            #
Check for API key in config or environment        if not self.api_key and 
&amp;quot;OPENAI_API_KEY&amp;quot; in os.environ:            self.api_key = 
os.environ[&amp;quot;OPENAI_API_KEY&amp;quot;]            # Initialize token 
tracker        self.token_tracker = TokenTracker()            # Require API key 
explicitly for this provider; tests enforce clear error        if not 
self.api_key:&amp;gt;           raise OpenAIAuthenticationError(                
&amp;quot;OpenAI API key is required. Set OPENAI_API_KEY or provide 
&amp;#x27;api_key&amp;#x27; in config.&amp;quot;            )E           
devsynth.application.llm.openai_provider.OpenAIAuthenticationError: OpenAI API 
key is required. Set OPENAI_API_KEY or provide &amp;#x27;api_key&amp;#x27; in 
config./Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/llm/openai_provider.py:131: 
OpenAIAuthenticationError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:20,362 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token 
counting------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token counting____________ 
test_health_check_succeeds_when_sync_api_lists_models _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142c0a570&amp;gt;    
@pytest.mark.fast    def 
test_health_check_succeeds_when_sync_api_lists_models(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;ReqID: LMSTUDIO-HC-1        When 
sync_api.list_downloaded_models returns, health_check should be True.        
&amp;quot;&amp;quot;&amp;quot;        # Ensure resource flag is enabled so 
health_check runs        
monkeypatch.setenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
&amp;quot;true&amp;quot;)            from 
devsynth.application.llm.lmstudio_provider import LMStudioProvider            
provider = LMStudioProvider({&amp;quot;auto_select_model&amp;quot;: False})     
# Patch list_downloaded_models to return non-empty list quickly        with 
patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.list_down
loaded_models&amp;quot;,            return_value=,        ):&amp;gt;           
assert provider.health_check() is TrueE           assert False is TrueE         
+  where False = health_check()E            +    where health_check = 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142c0bb90&amp;gt;.health_check/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/tests/unit/application/llm/test_lmstudio_health_check.py:25: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:20,383 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:20,386 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:20,386 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:20,386 - devsynth.application.llm.lmstudio_provider - INFO - Using default
model: qwen/qwen3-4b-25072025-10-28 09:26:20,890 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio health_check 
failed within budget: Network access disabled during 
tests------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionINFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using default 
model: qwen/qwen3-4b-2507INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
health_check failed within budget: Network access disabled during tests_________
test_health_check_bounded_retry_and_returns_false_on_failure 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x142c5b020&amp;gt;    @pytest.mark.fast    def 
test_health_check_bounded_retry_and_returns_false_on_failure(monkeypatch):      
&amp;quot;&amp;quot;&amp;quot;ReqID: LMSTUDIO-HC-2        If 
sync_api.list_downloaded_models keeps failing, health_check returns False within
~5s budget.        &amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
&amp;quot;true&amp;quot;)        # Keep retries small and timeout small to speed
up        monkeypatch.setenv(&amp;quot;DEVSYNTH_LMSTUDIO_RETRIES&amp;quot;, 
&amp;quot;3&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_LMSTUDIO_TIMEOUT_SECONDS&amp;quot;, 
&amp;quot;0.4&amp;quot;)            from 
devsynth.application.llm.lmstudio_provider import LMStudioProvider            
provider = LMStudioProvider({&amp;quot;auto_select_model&amp;quot;: False})     
call_count = {&amp;quot;n&amp;quot;: 0}            def _boom(kind: str):  # 
noqa: ARG001            call_count[&amp;quot;n&amp;quot;] += 1            raise 
RuntimeError(&amp;quot;unreachable&amp;quot;)            with (            
patch(                
&amp;quot;devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.list_down
loaded_models&amp;quot;,                side_effect=_boom,            ),        
patch(                
&amp;quot;devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.configure
_default_client&amp;quot;,                return_value=None,            ),      
):            t0 = time.perf_counter()            ok = provider.health_check()  
duration = time.perf_counter() - t0            assert ok is False            
assert duration &amp;lt;= 5.5  # bounded by implementation budget&amp;gt;       
assert call_count[&amp;quot;n&amp;quot;] &amp;gt;= 1E           assert 0 
&amp;gt;= 
1/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm
/test_lmstudio_health_check.py:63: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:26:20,910 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:20,912 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:20,912 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:20,912 - devsynth.application.llm.lmstudio_provider - INFO - Using default
model: qwen/qwen3-4b-25072025-10-28 09:26:24,430 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio health_check 
failed within budget: Network access disabled during 
tests------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionINFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using default 
model: qwen/qwen3-4b-2507INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
health_check failed within budget: Network access disabled during 
tests____________ TestRequireLMStudio.test_require_lmstudio_import_error 
____________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestRequireLMStudio 
object at 0x1203611c0&amp;gt;    @pytest.mark.fast    def 
test_require_lmstudio_import_error(self):        
&amp;quot;&amp;quot;&amp;quot;Test error handling when lmstudio module is not 
available.&amp;quot;&amp;quot;&amp;quot;        with 
patch.dict(&amp;quot;sys.modules&amp;quot;, {}, clear=True):            with 
patch(                &amp;quot;builtins.__import__&amp;quot;, 
side_effect=ImportError(&amp;quot;Module not found&amp;quot;)            ):     
with pytest.raises(DevSynthError) as exc_info:&amp;gt;                   
_require_lmstudio()/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/application/llm/test_lmstudio_provider.py:214: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:42: in _require_lmstudio    import lmstudio as _lmstudio 
# type: ignore    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/.venv/lib/python3.12/site-packages/lmstudio/__init__.py:16: in 
&amp;lt;module&amp;gt;    from .sdk_api import 
*/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/lmstudio/sdk_api.py:3: in &amp;lt;module&amp;gt;    from contextlib 
import AsyncContextDecorator, 
ContextDecorator/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.frame
work/Versions/3.12/lib/python3.12/contextlib.py:3: in &amp;lt;module&amp;gt;    
import os&amp;lt;frozen importlib._bootstrap&amp;gt;:1360: in _find_and_load    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:1331: in _find_and_load_unlocked 
???&amp;lt;frozen importlib._bootstrap&amp;gt;:935: in _load_unlocked    
???&amp;lt;frozen importlib._bootstrap&amp;gt;:1176: in exec_module    ???_ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&amp;gt;   ???E   AttributeError: module &amp;#x27;sys&amp;#x27; has no 
attribute &amp;#x27;builtin_module_names&amp;#x27;&amp;lt;frozen os&amp;gt;:33: 
AttributeError_______ 
TestLMStudioProvider.test_provider_initialization_default_config _______self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120362330&amp;gt;    @pytest.mark.fast    def 
test_provider_initialization_default_config(self):        
&amp;quot;&amp;quot;&amp;quot;Test provider initialization with default 
configuration.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       with patch(          
&amp;quot;devsynth.application.llm.lmstudio_provider.get_llm_settings&amp;quot; 
) as 
mock_get_settings:/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/application/llm/test_lmstudio_provider.py:226: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142cadac0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module &amp;#x27;devsynth.application.llm.lmstudio_provider&amp;#x27; 
from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/lmstudio_provider.py&amp;#x27;&amp;gt; does not have the attribute 
&amp;#x27;get_llm_settings&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Fra
meworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError_______ 
TestLMStudioProvider.test_provider_initialization_custom_config ________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x1203627e0&amp;gt;    @pytest.mark.fast    def 
test_provider_initialization_custom_config(self):        
&amp;quot;&amp;quot;&amp;quot;Test provider initialization with custom 
configuration.&amp;quot;&amp;quot;&amp;quot;        custom_config = {           
&amp;quot;api_base&amp;quot;: &amp;quot;http://custom:8080&amp;quot;,           
&amp;quot;model&amp;quot;: &amp;quot;custom-model&amp;quot;,            
&amp;quot;max_tokens&amp;quot;: 2000,            
&amp;quot;temperature&amp;quot;: 0.8,        }            provider = 
LMStudioProvider(custom_config)            assert provider.api_base == 
&amp;quot;http://custom:8080&amp;quot;&amp;gt;       assert provider.model == 
&amp;quot;custom-model&amp;quot;E       AssertionError: assert 
&amp;#x27;qwen/qwen3-4b-2507&amp;#x27; == &amp;#x27;custom-model&amp;#x27;E     
E         - custom-modelE         + 
qwen/qwen3-4b-2507/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/application/llm/test_lmstudio_provider.py:262: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,492 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,495 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,540 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b325a0&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;))2025-10-28 09:26:31,540
- devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;custom-model&amp;#x27;: Failed to connect to LM 
Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b325a0&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;)), falling back to 
auto-selection2025-10-28 09:26:31,544 - 
devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b32f30&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;))2025-10-28 09:26:31,545
- devsynth.application.llm.lmstudio_provider - WARNING - Could not connect to LM
Studio: Failed to connect to LM Studio: 
HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max retries 
exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b32f30&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;)). Using fallback: 
qwen/qwen3-4b-2507------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, 
port=8080): Max retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b325a0&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;))WARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;custom-model&amp;#x27;: Failed to connect to LM 
Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b325a0&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;)), falling back to 
auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, 
port=8080): Max retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b32f30&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;))WARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: 
HTTPConnectionPool(host=&amp;#x27;custom&amp;#x27;, port=8080): Max retries 
exceeded with url: /api/v0/models (Caused by 
NameResolutionError(&amp;quot;&amp;lt;urllib3.connection.HTTPConnection object 
at 0x142b32f30&amp;gt;: Failed to resolve &amp;#x27;custom&amp;#x27; ([Errno 8] 
nodename nor servname provided, or not known)&amp;quot;)). Using fallback: 
qwen/qwen3-4b-2507______________ 
TestLMStudioProvider.test_provider_complete_method ______________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120362c90&amp;gt;    @pytest.mark.fast    def 
test_provider_complete_method(self):        &amp;quot;&amp;quot;&amp;quot;Test 
the complete method functionality.&amp;quot;&amp;quot;&amp;quot;        provider
= LMStudioProvider()    &amp;gt;       with patch.object(provider, 
&amp;quot;_get_client&amp;quot;) as mock_get_client:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/application/llm/test_lmstudio_provider.py:271: _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142ba4440&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142cad7f0&amp;gt; does not have the attribute 
&amp;#x27;_get_client&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,559 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,561 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,563 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,564 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,566 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,566 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_______________ 
TestLMStudioProvider.test_provider_embed_method ________________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120363140&amp;gt;    @pytest.mark.fast    def 
test_provider_embed_method(self):        &amp;quot;&amp;quot;&amp;quot;Test the 
embed method functionality.&amp;quot;&amp;quot;&amp;quot;        provider = 
LMStudioProvider()    &amp;gt;       with patch.object(provider, 
&amp;quot;_get_client&amp;quot;) as mock_get_client:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/application/llm/test_lmstudio_provider.py:288: _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142ba46b0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142cac7a0&amp;gt; does not have the attribute 
&amp;#x27;_get_client&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,625 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,627 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,629 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,630 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,632 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,632 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507___________ 
TestLMStudioProvider.test_provider_health_check_success ____________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x1203635f0&amp;gt;    @pytest.mark.fast    def 
test_provider_health_check_success(self):        
&amp;quot;&amp;quot;&amp;quot;Test successful health 
check.&amp;quot;&amp;quot;&amp;quot;        provider = LMStudioProvider()    
&amp;gt;       with patch.object(provider, &amp;quot;_get_client&amp;quot;) as 
mock_get_client:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/application/llm/test_lmstudio_provider.py:306: _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142b37350&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142b1bb30&amp;gt; does not have the attribute 
&amp;#x27;_get_client&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,693 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,696 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,698 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,698 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,700 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,700 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507___________ 
TestLMStudioProvider.test_provider_health_check_failure ____________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120363aa0&amp;gt;    @pytest.mark.fast    def 
test_provider_health_check_failure(self):        
&amp;quot;&amp;quot;&amp;quot;Test failed health 
check.&amp;quot;&amp;quot;&amp;quot;        provider = LMStudioProvider()    
&amp;gt;       with patch.object(provider, &amp;quot;_get_client&amp;quot;) as 
mock_get_client:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/application/llm/test_lmstudio_provider.py:321: _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142b32810&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142b18680&amp;gt; does not have the attribute 
&amp;#x27;_get_client&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,763 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,766 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,769 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,769 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,771 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,771 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_____________ 
TestLMStudioProvider.test_provider_get_client_method _____________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120378440&amp;gt;    @pytest.mark.fast    def 
test_provider_get_client_method(self):        &amp;quot;&amp;quot;&amp;quot;Test
the _get_client method.&amp;quot;&amp;quot;&amp;quot;        provider = 
LMStudioProvider()            with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider.lmstudio&amp;quot;        )
as mock_lmstudio:            mock_client = MagicMock()            
mock_lmstudio.llm.return_value = mock_client    &amp;gt;           result = 
provider._get_client()                     ^^^^^^^^^^^^^^^^^^^^E           
AttributeError: &amp;#x27;LMStudioProvider&amp;#x27; object has no attribute 
&amp;#x27;_get_client&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/application/llm/test_lmstudio_provider.py:339: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,827 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,832 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,834 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,834 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,836 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,836 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507______________ 
TestLMStudioProvider.test_provider_model_property _______________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x1203788f0&amp;gt;    @pytest.mark.fast    def 
test_provider_model_property(self):        &amp;quot;&amp;quot;&amp;quot;Test 
the model property getter and setter.&amp;quot;&amp;quot;&amp;quot;        
provider = LMStudioProvider()            # Test getter&amp;gt;       assert 
provider.model == &amp;quot;default-model&amp;quot;E       AssertionError: 
assert &amp;#x27;qwen/qwen3-4b-2507&amp;#x27; == 
&amp;#x27;default-model&amp;#x27;E         E         - default-modelE         + 
qwen/qwen3-4b-2507/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/application/llm/test_lmstudio_provider.py:350: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,853 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,857 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,861 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,861 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,864 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,864 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_________ 
TestLMStudioProvider.test_provider_available_models_property _________self = 
&amp;lt;tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x120378da0&amp;gt;    @pytest.mark.fast    def 
test_provider_available_models_property(self):        
&amp;quot;&amp;quot;&amp;quot;Test the available_models 
property.&amp;quot;&amp;quot;&amp;quot;        provider = LMStudioProvider()    
&amp;gt;       with patch.object(provider, &amp;quot;_get_client&amp;quot;) as 
mock_get_client:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/application/llm/test_lmstudio_provider.py:361: _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__    original, local = 
self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;unittest.mock._patch object at 0x142c1ea50&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x142b18740&amp;gt; does not have the attribute 
&amp;#x27;_get_client&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:31,879 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:26:31,882 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:26:31,885 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:26:31,885 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:26:31,888 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:26:31,888 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507___________________ 
test_default_selection_is_deterministic ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142c2e5d0&amp;gt;    def 
test_default_selection_is_deterministic(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Factory should fall back to the next provider in 
order.&amp;quot;&amp;quot;&amp;quot;            class DummyOpenAI:            
def __init__(self, config=None):                self.config = config            
class DummyAnthropic:            def __init__(self, config=None):               
self.config = config            monkeypatch.setattr(            factory, 
&amp;quot;provider_types&amp;quot;, {&amp;quot;openai&amp;quot;: DummyOpenAI, 
&amp;quot;anthropic&amp;quot;: DummyAnthropic}        )        
monkeypatch.delitem(factory.provider_types, &amp;quot;openai&amp;quot;, 
raising=False)&amp;gt;       provider = factory.create_provider()               
^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/application/llm/test_provider_factory.py:23: _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:61: in create_provider    return 
super().create_provider(&amp;quot;offline&amp;quot;, config)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.provider_factory.ProviderFactory object at 
0x1200856a0&amp;gt;provider_type = &amp;#x27;offline&amp;#x27;, config = None   
def create_provider(        self, provider_type: str, config: Dict = None    ) 
-&amp;gt; LLMProvider:        &amp;quot;&amp;quot;&amp;quot;Create an LLM 
provider of the specified type.&amp;quot;&amp;quot;&amp;quot;        if 
provider_type not in self.provider_types:            if provider_type == 
&amp;quot;lmstudio&amp;quot;:                raise ValidationError(             
&amp;quot;LMStudio provider is unavailable. Install the 
&amp;#x27;lmstudio&amp;#x27; package to enable this provider.&amp;quot;         
)&amp;gt;           raise ValidationError(f&amp;quot;Unknown provider type: 
{provider_type}&amp;quot;)E           
devsynth.application.llm.providers.ValidationError: Unknown provider type: 
offline/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/llm/providers.py:339: ValidationError_______________________ 
test_case_insensitive_selection ________________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142c2e3c0&amp;gt;    def 
test_case_insensitive_selection(monkeypatch):        class DummyOpenAI:         
def __init__(self, config=None):                self.config = config            
monkeypatch.setattr(factory, &amp;quot;provider_types&amp;quot;, 
{&amp;quot;openai&amp;quot;: DummyOpenAI})&amp;gt;       provider = 
factory.create_provider(&amp;quot;OPENAI&amp;quot;)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/tests/unit/application/llm/test_provider_factory.py:33: _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:50: in create_provider    return 
super().create_provider(&amp;quot;offline&amp;quot;, config)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.provider_factory.ProviderFactory object at 
0x1200856a0&amp;gt;provider_type = &amp;#x27;offline&amp;#x27;, config = None   
def create_provider(        self, provider_type: str, config: Dict = None    ) 
-&amp;gt; LLMProvider:        &amp;quot;&amp;quot;&amp;quot;Create an LLM 
provider of the specified type.&amp;quot;&amp;quot;&amp;quot;        if 
provider_type not in self.provider_types:            if provider_type == 
&amp;quot;lmstudio&amp;quot;:                raise ValidationError(             
&amp;quot;LMStudio provider is unavailable. Install the 
&amp;#x27;lmstudio&amp;#x27; package to enable this provider.&amp;quot;         
)&amp;gt;           raise ValidationError(f&amp;quot;Unknown provider type: 
{provider_type}&amp;quot;)E           
devsynth.application.llm.providers.ValidationError: Unknown provider type: 
offline/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/llm/providers.py:339: ValidationError________________________ 
test_get_llm_provider_offline _________________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142b27e90&amp;gt;    
@pytest.mark.fast    def test_get_llm_provider_offline(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Selects offline provider when offline mode is 
enabled.            ReqID: FR-85&amp;quot;&amp;quot;&amp;quot;        dummy = 
DummyFactory()        monkeypatch.setattr(providers, 
&amp;quot;factory&amp;quot;, dummy)        monkeypatch.setattr(            
providers,            &amp;quot;load_config&amp;quot;,            lambda: 
types.SimpleNamespace(                as_dict=lambda: 
{&amp;quot;offline_mode&amp;quot;: True, &amp;quot;offline_provider&amp;quot;: 
&amp;quot;local&amp;quot;}            ),        )&amp;gt;       
monkeypatch.setattr(providers, &amp;quot;get_llm_settings&amp;quot;, lambda: 
{&amp;quot;provider&amp;quot;: &amp;quot;openai&amp;quot;})E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.llm.providers&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/providers.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/application/llm/test_provider_selection.py:31: 
AttributeError________________________ test_get_llm_provider_default 
_________________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x142b270e0&amp;gt;    @pytest.mark.fast    def 
test_get_llm_provider_default(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Uses configured provider when offline mode is 
disabled.            ReqID: FR-85&amp;quot;&amp;quot;&amp;quot;        dummy = 
DummyFactory()        monkeypatch.setattr(providers, 
&amp;quot;factory&amp;quot;, dummy)        monkeypatch.setattr(            
providers, &amp;quot;load_config&amp;quot;, lambda: 
types.SimpleNamespace(as_dict=lambda: {})        )&amp;gt;       
monkeypatch.setattr(providers, &amp;quot;get_llm_settings&amp;quot;, lambda: 
{&amp;quot;provider&amp;quot;: &amp;quot;local&amp;quot;})E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.application.llm.providers&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/providers.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/application/llm/test_provider_selection.py:49: 
AttributeError_______ 
TestExecutionLearningIntegration.test_learn_from_code_execution ________self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionLearningIntegration object at 0x12043c650&amp;gt;    def 
test_learn_from_code_execution(self):        &amp;quot;&amp;quot;&amp;quot;Test 
learning from code execution.&amp;quot;&amp;quot;&amp;quot;        code_snippets
= [            &amp;quot;def add(a, b): return a + b&amp;quot;,            
&amp;quot;def multiply(a, b): return a * b&amp;quot;        ]            # Mock 
the learning process        with 
patch.object(self.integration.trajectory_collector, 
&amp;#x27;collect_python_trajectories&amp;#x27;) as mock_collect:            
mock_collect.return_value = [                ExecutionTrace(                    
code=code_snippets[0],                    execution_steps=[                     
ExecutionStep(                            step_number=1,                        
line_number=1,                            code_line=&amp;quot;def add(a, 
b):&amp;quot;                        )                    ],                    
execution_outcome=&amp;quot;success&amp;quot;                )            ]     
with patch.object(self.integration.learning_algorithm, 
&amp;#x27;train_on_trajectories&amp;#x27;) as mock_train:                
mock_train.return_value = {                    
&amp;quot;trajectories_processed&amp;quot;: 1,                    
&amp;quot;patterns_extracted&amp;quot;: 2,                    
&amp;quot;validation_score&amp;quot;: 0.85,                    
&amp;quot;patterns&amp;quot;: {&amp;quot;pattern1&amp;quot;: Mock(), 
&amp;quot;pattern2&amp;quot;: Mock()},                    
&amp;quot;understandings&amp;quot;: {&amp;quot;understanding1&amp;quot;: Mock()}
}    &amp;gt;               result = 
self.integration.learn_from_code_execution(code_snippets)                       
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects
/github.com/ravenoak/devsynth/tests/unit/application/memory/test_execution_learn
ing_integration.py:66: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:57: in learn_from_code_execution    
semantic_understandings = 
self.learning_algorithm._build_semantic_understanding(trajectories, 
learning_results[&amp;quot;patterns&amp;quot;])                              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/application/memory/execution_learning_algorithm.py:248: in 
_build_semantic_understanding    relevant_patterns = 
self._find_relevant_patterns(patterns, semantic_components)                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.execution_learning_algorithm.ExecutionLearni
ngAlgorithm object at 0x142c3fcb0&amp;gt;patterns = 
{&amp;#x27;pattern1&amp;#x27;: &amp;lt;Mock 
id=&amp;#x27;5415584592&amp;#x27;&amp;gt;, &amp;#x27;pattern2&amp;#x27;: 
&amp;lt;Mock id=&amp;#x27;5415586320&amp;#x27;&amp;gt;}components = 
{&amp;#x27;avg_execution_time&amp;#x27;: 0.0, &amp;#x27;error_types&amp;#x27;: 
[], &amp;#x27;function_calls&amp;#x27;: {}, &amp;#x27;success_rate&amp;#x27;: 
1.0, ...}    def _find_relevant_patterns(self, patterns: List[ExecutionPattern],
components: Dict) -&amp;gt; List[ExecutionPattern]:        
&amp;quot;&amp;quot;&amp;quot;Find patterns relevant to the given semantic 
components.&amp;quot;&amp;quot;&amp;quot;        relevant_patterns = []         
# Match patterns based on function calls        if 
&amp;quot;function_calls&amp;quot; in components:            for pattern in 
patterns:&amp;gt;               if pattern.pattern_type == 
&amp;quot;function_behavior&amp;quot;:                   ^^^^^^^^^^^^^^^^^^^^E  
AttributeError: &amp;#x27;str&amp;#x27; object has no attribute 
&amp;#x27;pattern_type&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/application/memory/execution_learning_algorithm.py:332: 
AttributeError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,779 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:37,779 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:37,779 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:37,779 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,779 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:37,779 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,779 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:37,779 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,779 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 
1000)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)_______ 
TestExecutionLearningIntegration.test_enhance_code_understanding _______self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionLearningIntegration object at 0x12043cb90&amp;gt;    def 
test_enhance_code_understanding(self):        &amp;quot;&amp;quot;&amp;quot;Test
code understanding enhancement.&amp;quot;&amp;quot;&amp;quot;        code = 
&amp;quot;def fibonacci(n): return n if n &amp;lt;= 1 else fibonacci(n-1) + 
fibonacci(n-2)&amp;quot;            # Mock pattern library        with 
patch.object(self.integration.pattern_library, &amp;#x27;find_matches&amp;#x27;)
as mock_find:            mock_pattern = Mock()            
mock_pattern.pattern_id = &amp;quot;fib_pattern&amp;quot;            
mock_pattern.pattern_type = &amp;quot;recursive_algorithm&amp;quot;            
mock_pattern.confidence = 0.9            mock_find.return_value =               
with patch.object(self.integration.understanding_engine, 
&amp;#x27;predict_execution_behavior&amp;#x27;) as mock_predict:                
mock_predict.return_value = {                    &amp;quot;prediction&amp;quot;:
&amp;quot;recursive_execution&amp;quot;,                    
&amp;quot;confidence&amp;quot;: 0.85,                    
&amp;quot;predicted_success_rate&amp;quot;: 0.9                }                
with patch.object(self.integration.understanding_engine, 
&amp;#x27;analyze_behavioral_intent&amp;#x27;) as mock_intent:                  
mock_intent.return_value = Mock(                        
primary_purpose=&amp;quot;fibonacci_calculation&amp;quot;,                      
complexity_level=&amp;quot;moderate&amp;quot;,                        
intent_confidence=0.8                    )    &amp;gt;                   result 
= self.integration.enhance_code_understanding(code)                             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/application/memory/test_execution_learning_inte
gration.py:99: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py in enhance_code_understanding    
components = self.understanding_engine.extract_semantic_components(code)        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projec
ts/github.com/ravenoak/devsynth/src/devsynth/application/memory/semantic_underst
anding_engine.py:84: in extract_semantic_components    ast_analysis = 
self._analyze_ast_structure(code)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.semantic_understanding_engine.SemanticUnders
tandingEngine object at 0x142c688f0&amp;gt;code = &amp;#x27;def fibonacci(n): 
return n if n &amp;lt;= 1 else fibonacci(n-1) + fibonacci(n-2)&amp;#x27;    def 
_analyze_ast_structure(self, code: str) -&amp;gt; Dict:        
&amp;quot;&amp;quot;&amp;quot;Analyze AST structure for semantic 
understanding.&amp;quot;&amp;quot;&amp;quot;        try:            tree = 
ast.parse(code)                # Count different AST node types&amp;gt;         
node_counts = defaultdict(int)                          ^^^^^^^^^^^E           
NameError: name &amp;#x27;defaultdict&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/memory/semantic_understanding_engine.py:253: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,805 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:37,805 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:37,805 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:37,805 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,805 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:37,805 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,805 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:37,805 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,805 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 
1000)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)__ 
TestExecutionLearningIntegration.test_validate_against_research_benchmarks 
__self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionLearningIntegration object at 0x12043db50&amp;gt;    def 
test_validate_against_research_benchmarks(self):        
&amp;quot;&amp;quot;&amp;quot;Test validation against research 
benchmarks.&amp;quot;&amp;quot;&amp;quot;        validation_suite = {           
&amp;quot;semantic_robustness&amp;quot;: Mock(overall_score=0.91, 
benchmark_compliance={&amp;quot;mutation_resistance&amp;quot;: True}),          
&amp;quot;execution_prediction&amp;quot;: Mock(overall_score=0.83, 
benchmark_compliance={&amp;quot;prediction_accuracy&amp;quot;: True}),          
&amp;quot;multi_hop_reasoning&amp;quot;: Mock(overall_score=0.87, 
benchmark_compliance={&amp;quot;multi_hop_accuracy&amp;quot;: True})        }   
&amp;gt;       result = 
self.integration.validate_against_research_benchmarks(validation_suite)         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/ca
itlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memory/test_e
xecution_learning_integration.py:171: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.execution_learning_integration.ExecutionLear
ningIntegration object at 0x142bb5400&amp;gt;test_results = 
{&amp;#x27;execution_prediction&amp;#x27;: &amp;lt;Mock 
id=&amp;#x27;5414549360&amp;#x27;&amp;gt;, 
&amp;#x27;multi_hop_reasoning&amp;#x27;: &amp;lt;Mock 
id=&amp;#x27;5414544896&amp;#x27;&amp;gt;, 
&amp;#x27;semantic_robustness&amp;#x27;: &amp;lt;Mock 
id=&amp;#x27;5414540624&amp;#x27;&amp;gt;}    def 
validate_against_research_benchmarks(self, test_results: Dict) -&amp;gt; Dict:  
&amp;quot;&amp;quot;&amp;quot;Validate learning results against research 
benchmarks.&amp;quot;&amp;quot;&amp;quot;        benchmarks = {            
&amp;quot;semantic_understanding&amp;quot;: 0.8,  # 80% semantic understanding 
target            &amp;quot;mutation_resistance&amp;quot;: 0.9,     # 90% 
resistance to semantic mutations            
&amp;quot;pattern_accuracy&amp;quot;: 0.85,       # 85% pattern prediction 
accuracy            &amp;quot;execution_prediction&amp;quot;: 0.8     # 80% 
execution outcome prediction        }            validation_report = {          
&amp;quot;benchmark_comparison&amp;quot;: {},            
&amp;quot;research_alignment&amp;quot;: True,            
&amp;quot;improvement_areas&amp;quot;: [],            
&amp;quot;validation_method&amp;quot;: 
&amp;quot;research_benchmark_comparison&amp;quot;        }            # Compare 
against benchmarks        for metric, benchmark_value in benchmarks.items():    
if metric in test_results:                achieved_value = test_results&amp;gt; 
meets_benchmark = achieved_value &amp;gt;= benchmark_value                      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E               TypeError: 
&amp;#x27;&amp;gt;=&amp;#x27; not supported between instances of 
&amp;#x27;Mock&amp;#x27; and 
&amp;#x27;float&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/sr
c/devsynth/application/memory/execution_learning_integration.py:354: 
TypeError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,836 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:37,836 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:37,836 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:37,836 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,837 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:37,837 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,837 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:37,837 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,837 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 
1000)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)______ 
TestExecutionLearningIntegration.test_export_import_learning_state ______self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestEx
ecutionLearningIntegration object at 0x12043e090&amp;gt;    def 
test_export_import_learning_state(self):        
&amp;quot;&amp;quot;&amp;quot;Test learning state export and 
import.&amp;quot;&amp;quot;&amp;quot;        # Set up learning state        
self.integration.learning_history = [{&amp;quot;test&amp;quot;: 
&amp;quot;session&amp;quot;}]        self.integration.understanding_cache = 
{&amp;quot;test&amp;quot;: &amp;quot;cache&amp;quot;}            # Mock pattern 
library export        with patch.object(self.integration.pattern_library, 
&amp;#x27;export_patterns&amp;#x27;) as mock_export:            
mock_export.return_value = {&amp;quot;patterns&amp;quot;: {}, 
&amp;quot;total_patterns&amp;quot;: 0}                # Export state&amp;gt;    
exported_state = self.integration.export_learning_state()                       
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/raven
oak/devsynth/tests/unit/application/memory/test_execution_learning_integration.p
y:188: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:381: in export_learning_state    
&amp;quot;statistics&amp;quot;: self.get_learning_statistics(),                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/src/devsynth/application/memory/execution_learning_integration.py:312: in 
get_learning_statistics    total_patterns = 
sum(session[&amp;quot;patterns_learned&amp;quot;] for session in 
self.learning_history)                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .0 = 
&amp;lt;list_iterator object at 0x142b24a00&amp;gt;&amp;gt;   total_patterns = 
sum(session[&amp;quot;patterns_learned&amp;quot;] for session in 
self.learning_history)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^E   
KeyError: 
&amp;#x27;patterns_learned&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/src/devsynth/application/memory/execution_learning_integration.py:312: 
KeyError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,848 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:37,848 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:37,848 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:37,848 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,848 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:37,848 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:37,848 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:37,848 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,848 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 
1000)------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)_______ 
TestSemanticUnderstandingEngine.test_extract_semantic_components _______self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestSe
manticUnderstandingEngine object at 0x120448440&amp;gt;        def 
test_extract_semantic_components(self):            
&amp;quot;&amp;quot;&amp;quot;Test semantic component 
extraction.&amp;quot;&amp;quot;&amp;quot;            code = 
&amp;quot;&amp;quot;&amp;quot;    def calculate_fibonacci(n):        if n 
&amp;lt;= 1:            return n        return calculate_fibonacci(n-1) + 
calculate_fibonacci(n-2)    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;          
components = self.engine.extract_semantic_components(code)                      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/tests/unit/application/memory/test_execution_learning_integrat
ion.py:356: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:84: in extract_semantic_components    
ast_analysis = self._analyze_ast_structure(code)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.semantic_understanding_engine.SemanticUnders
tandingEngine object at 0x142c17620&amp;gt;code = &amp;#x27;\ndef 
calculate_fibonacci(n):\n    if n &amp;lt;= 1:\n        return n\n    return 
calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n&amp;#x27;    def 
_analyze_ast_structure(self, code: str) -&amp;gt; Dict:        
&amp;quot;&amp;quot;&amp;quot;Analyze AST structure for semantic 
understanding.&amp;quot;&amp;quot;&amp;quot;        try:            tree = 
ast.parse(code)                # Count different AST node types&amp;gt;         
node_counts = defaultdict(int)                          ^^^^^^^^^^^E           
NameError: name &amp;#x27;defaultdict&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/memory/semantic_understanding_engine.py:253: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,916 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,916 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized------------------------------ Captured log 
setup ------------------------------INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized_______ 
TestSemanticUnderstandingEngine.test_detect_semantic_equivalence _______self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestSe
manticUnderstandingEngine object at 0x120448ec0&amp;gt;    def 
test_detect_semantic_equivalence(self):        
&amp;quot;&amp;quot;&amp;quot;Test semantic equivalence 
detection.&amp;quot;&amp;quot;&amp;quot;        code1 = &amp;quot;def 
fibonacci(n): return n if n &amp;lt;= 1 else fibonacci(n-1) + 
fibonacci(n-2)&amp;quot;        code2 = &amp;quot;def fib_calc(num): return num 
if num &amp;lt;= 1 else fib_calc(num-1) + fib_calc(num-2)&amp;quot;    &amp;gt; 
equivalence = self.engine.detect_semantic_equivalence(code1, code2)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/git
hub.com/ravenoak/devsynth/tests/unit/application/memory/test_execution_learning_
integration.py:380: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:167: in detect_semantic_equivalence    
components1 = self.extract_semantic_components(code1)                  
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/raveno
ak/devsynth/src/devsynth/application/memory/semantic_understanding_engine.py:84:
 in extract_semantic_components    ast_analysis = 
self._analyze_ast_structure(code)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.semantic_understanding_engine.SemanticUnders
tandingEngine object at 0x142c5f830&amp;gt;code = &amp;#x27;def fibonacci(n): 
return n if n &amp;lt;= 1 else fibonacci(n-1) + fibonacci(n-2)&amp;#x27;    def 
_analyze_ast_structure(self, code: str) -&amp;gt; Dict:        
&amp;quot;&amp;quot;&amp;quot;Analyze AST structure for semantic 
understanding.&amp;quot;&amp;quot;&amp;quot;        try:            tree = 
ast.parse(code)                # Count different AST node types&amp;gt;         
node_counts = defaultdict(int)                          ^^^^^^^^^^^E           
NameError: name &amp;#x27;defaultdict&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/memory/semantic_understanding_engine.py:253: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,939 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,939 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized------------------------------ Captured log 
setup ------------------------------INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized_______ 
TestSemanticUnderstandingEngine.test_predict_execution_behavior ________self = 
&amp;lt;tests.unit.application.memory.test_execution_learning_integration.TestSe
manticUnderstandingEngine object at 0x120449400&amp;gt;    def 
test_predict_execution_behavior(self):        &amp;quot;&amp;quot;&amp;quot;Test
execution behavior prediction.&amp;quot;&amp;quot;&amp;quot;        code = 
&amp;quot;def divide(a, b): return a / b&amp;quot;            # Mock pattern 
library to return relevant patterns        mock_pattern = Mock()        
mock_pattern.pattern_id = &amp;quot;division_pattern&amp;quot;        
mock_pattern.pattern_type = &amp;quot;mathematical_operation&amp;quot;        
mock_pattern.confidence = 0.8        mock_pattern.expected_outcomes = 
{&amp;quot;success_rate&amp;quot;: 0.7}            with 
patch.object(self.engine.pattern_library, &amp;#x27;find_matches&amp;#x27;) as 
mock_find:            mock_find.return_value =     &amp;gt;           prediction
= self.engine.predict_execution_behavior(code)                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/application/memory/test_execution_learning_integrati
on.py:401: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:202: in predict_execution_behavior    
components = self.extract_semantic_components(code)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/src/devsynth/application/memory/semantic_understanding_engine.py:84: 
in extract_semantic_components    ast_analysis = 
self._analyze_ast_structure(code)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.memory.semantic_understanding_engine.SemanticUnders
tandingEngine object at 0x142cb5160&amp;gt;code = &amp;#x27;def divide(a, b): 
return a / b&amp;#x27;    def _analyze_ast_structure(self, code: str) -&amp;gt; 
Dict:        &amp;quot;&amp;quot;&amp;quot;Analyze AST structure for semantic 
understanding.&amp;quot;&amp;quot;&amp;quot;        try:            tree = 
ast.parse(code)                # Count different AST node types&amp;gt;         
node_counts = defaultdict(int)                          ^^^^^^^^^^^E           
NameError: name &amp;#x27;defaultdict&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applica
tion/memory/semantic_understanding_engine.py:253: 
NameError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:37,967 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:37,968 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized------------------------------ Captured log 
setup ------------------------------INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized_________________ 
test_query_results_from_rows_shapes_records __________________    
@pytest.mark.fast    def test_query_results_from_rows_shapes_records() -&amp;gt;
None:        &amp;quot;&amp;quot;&amp;quot;Row helpers should produce query 
results with normalized metadata.&amp;quot;&amp;quot;&amp;quot;            rows 
= [            {                &amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;,  
&amp;quot;content&amp;quot;: &amp;quot;alpha&amp;quot;,                
&amp;quot;memory_type&amp;quot;: &amp;quot;context&amp;quot;,                
&amp;quot;metadata&amp;quot;: to_serializable({&amp;quot;score&amp;quot;: 1}),  
},            {                &amp;quot;id&amp;quot;: &amp;quot;b&amp;quot;,   
&amp;quot;content&amp;quot;: &amp;quot;beta&amp;quot;,                
&amp;quot;memory_type&amp;quot;: &amp;quot;knowledge&amp;quot;,                
&amp;quot;metadata&amp;quot;: to_serializable({&amp;quot;score&amp;quot;: 2}),  
&amp;quot;source&amp;quot;: &amp;quot;secondary&amp;quot;,                
&amp;quot;similarity&amp;quot;: 0.33,            },        ]            results 
= query_results_from_rows(            &amp;quot;primary&amp;quot;,            
rows,            total=&amp;quot;2&amp;quot;,            
latency_ms=&amp;quot;3.5&amp;quot;,            metadata=to_serializable(        
{&amp;quot;batch&amp;quot;: 1, &amp;quot;started_at&amp;quot;: datetime(2024, 4,
5, 6, 7)}            ),        )            assert 
results[&amp;quot;store&amp;quot;] == &amp;quot;primary&amp;quot;        assert 
results[&amp;quot;total&amp;quot;] == 2        assert 
results[&amp;quot;latency_ms&amp;quot;] == pytest.approx(3.5)        assert 
results[&amp;quot;metadata&amp;quot;][&amp;quot;batch&amp;quot;] == 1        
assert results[&amp;quot;metadata&amp;quot;][&amp;quot;started_at&amp;quot;] == 
datetime(2024, 4, 5, 6, 7)            primary_record, secondary_record = 
results[&amp;quot;records&amp;quot;]        assert primary_record.source == 
&amp;quot;primary&amp;quot;        assert primary_record.memory_type is 
MemoryType.CONTEXT        assert secondary_record.source == 
&amp;quot;secondary&amp;quot;        assert secondary_record.similarity == 
pytest.approx(0.33)&amp;gt;       assert secondary_record.memory_type is 
MemoryType.CONTEXTE       AssertionError: assert &amp;lt;MemoryType.KNOWLEDGE: 
&amp;#x27;knowledge&amp;#x27;&amp;gt; is &amp;lt;MemoryType.CONTEXT: 
&amp;#x27;context&amp;#x27;&amp;gt;E        +  where 
&amp;lt;MemoryType.KNOWLEDGE: &amp;#x27;knowledge&amp;#x27;&amp;gt; = 
MemoryRecord(item=MemoryItem(id=&amp;#x27;b&amp;#x27;, 
content=&amp;#x27;beta&amp;#x27;, memory_type=&amp;lt;MemoryType.KNOWLEDGE: 
&amp;#x27;knowledge&amp;#x27;&amp;gt;, 
metadata={&amp;#x27;score...ted_at=datetime.datetime(2025, 10, 28, 9, 26, 38, 
216446)), similarity=0.33, source=&amp;#x27;secondary&amp;#x27;, 
metadata={&amp;#x27;score&amp;#x27;: 2}).memory_typeE        +  and   
&amp;lt;MemoryType.CONTEXT: &amp;#x27;context&amp;#x27;&amp;gt; = 
MemoryType.CONTEXT/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/application/memory/test_metadata_serialization_helpers.py:132: 
AssertionError_______ 
TestPhase3IntegrationSystem.test_process_advanced_reasoning_task _______self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestPhase3I
ntegrationSystem object at 0x120563950&amp;gt;    def 
test_process_advanced_reasoning_task(self):        
&amp;quot;&amp;quot;&amp;quot;Test processing of complex reasoning 
tasks.&amp;quot;&amp;quot;&amp;quot;        task = {            
&amp;quot;task_id&amp;quot;: &amp;quot;test_task_123&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Analyze user authentication 
system&amp;quot;,            &amp;quot;type&amp;quot;: 
&amp;quot;analysis&amp;quot;,            &amp;quot;complexity&amp;quot;: 
&amp;quot;medium&amp;quot;        }            # Mock all the component methods 
with patch.object(self.integration_system, 
&amp;#x27;_analyze_and_segment_task&amp;#x27;) as mock_segment:            
mock_segment.return_value = [                {&amp;quot;segment_id&amp;quot;: 
&amp;quot;seg1&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Analyze 
requirements&amp;quot;},                {&amp;quot;segment_id&amp;quot;: 
&amp;quot;seg2&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Review 
implementation&amp;quot;}            ]                with 
patch.object(self.integration_system, 
&amp;#x27;_execute_multi_hop_reasoning&amp;#x27;) as mock_reasoning:            
mock_reasoning.return_value = {                    &amp;quot;success&amp;quot;: 
True,                    &amp;quot;total_hops&amp;quot;: 3,                    
&amp;quot;confidence&amp;quot;: 0.85                }                    with 
patch.object(self.integration_system, 
&amp;#x27;_execute_hybrid_llm_processing&amp;#x27;) as mock_hybrid:             
mock_hybrid.return_value = {                        &amp;quot;success&amp;quot;:
True,                        &amp;quot;result&amp;quot;: 
{&amp;quot;confidence&amp;quot;: 0.9, &amp;quot;execution_time&amp;quot;: 2.5}  
}                        with patch.object(self.integration_system, 
&amp;#x27;_apply_metacognitive_enhancement&amp;#x27;) as mock_meta:             
mock_meta.return_value = {                            
&amp;quot;success&amp;quot;: True,                            
&amp;quot;insights&amp;quot;: [&amp;quot;Strategy improvement&amp;quot;, 
&amp;quot;Efficiency gain&amp;quot;]                        }                   
with patch.object(self.integration_system, 
&amp;#x27;_optimize_contextual_prompts&amp;#x27;) as mock_prompts:              
mock_prompts.return_value = {                                
&amp;quot;success&amp;quot;: True,                                
&amp;quot;engineered_prompts&amp;quot;: [&amp;quot;Prompt 1&amp;quot;, 
&amp;quot;Prompt 2&amp;quot;]                            }                      
result = self.integration_system.process_advanced_reasoning_task(task)          
assert result[&amp;quot;success&amp;quot;] is True        assert 
result[&amp;quot;task_id&amp;quot;] == &amp;quot;test_task_123&amp;quot;        
assert &amp;quot;processing_summary&amp;quot; in result        assert 
result[&amp;quot;processing_summary&amp;quot;][&amp;quot;task_segments&amp;quot;
] == 2        assert 
result[&amp;quot;processing_summary&amp;quot;][&amp;quot;reasoning_hops&amp;quot
;] == 3&amp;gt;       assert 
result[&amp;quot;processing_summary&amp;quot;][&amp;quot;confidence_score&amp;qu
ot;] &amp;gt; 0.8E       assert 0.6125 &amp;gt; 
0.8/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/m
emory/test_phase3_integration_system.py:89: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,231 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,231 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:38,231 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:38,231 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)2025-10-28 09:26:38,231
- devsynth.application.memory.enhanced_graphrag_engine - INFO - Enhanced 
GraphRAG query engine initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.automata_synthesis_engine - INFO - Automata 
synthesis engine initialized (min_samples: 10)2025-10-28 09:26:38,231 - 
devsynth.application.memory.hybrid_llm_architecture - INFO - Hybrid LLM 
architecture initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized2025-10-28 09:26:38,231 - 
devsynth.application.memory.phase3_integration_system - INFO - Phase 3 
Integration System initialized with all advanced reasoning 
components------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initializedINFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initializedINFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initializedINFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initializedINFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning 
components----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:38,232 - 
devsynth.application.memory.phase3_integration_system - INFO - Processing 
advanced reasoning task: test_task_123------------------------------ Captured 
log call -------------------------------INFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 
Processing advanced reasoning task: test_task_123_______ 
TestPhase3IntegrationSystem.test_apply_metacognitive_enhancement _______self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestPhase3I
ntegrationSystem object at 0x1205a12e0&amp;gt;    def 
test_apply_metacognitive_enhancement(self):        
&amp;quot;&amp;quot;&amp;quot;Test metacognitive enhancement 
application.&amp;quot;&amp;quot;&amp;quot;        task = 
{&amp;quot;description&amp;quot;: &amp;quot;Test metacognitive 
enhancement&amp;quot;}        hybrid_results = {&amp;quot;result&amp;quot;: 
{&amp;quot;confidence&amp;quot;: 0.85}}            # Mock metacognitive training
with patch.object(self.integration_system.metacognitive_training, 
&amp;#x27;start_think_aloud_session&amp;#x27;) as mock_start:            
mock_start.return_value = &amp;quot;session_123&amp;quot;                with 
patch.object(self.integration_system.metacognitive_training, 
&amp;#x27;record_verbalization&amp;#x27;) as mock_record:                with 
patch.object(self.integration_system.metacognitive_training, 
&amp;#x27;end_think_aloud_session&amp;#x27;) as mock_end:                    
mock_end.return_value = {                        &amp;quot;session_id&amp;quot;:
&amp;quot;session_123&amp;quot;,                        
&amp;quot;insights&amp;quot;: [&amp;quot;Strategy improvement&amp;quot;, 
&amp;quot;Error pattern&amp;quot;],                        
&amp;quot;verbalizations_count&amp;quot;: 5                    }                
result = self.integration_system._apply_metacognitive_enhancement(task, 
hybrid_results)            assert result[&amp;quot;success&amp;quot;] is 
True&amp;gt;       assert len(result[&amp;quot;insights&amp;quot;]) == 2E       
assert 0 == 2E        +  where 0 = 
len([])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/applicati
on/memory/test_phase3_integration_system.py:176: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,263 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,263 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:38,263 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:38,263 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)2025-10-28 09:26:38,263
- devsynth.application.memory.enhanced_graphrag_engine - INFO - Enhanced 
GraphRAG query engine initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.automata_synthesis_engine - INFO - Automata 
synthesis engine initialized (min_samples: 10)2025-10-28 09:26:38,263 - 
devsynth.application.memory.hybrid_llm_architecture - INFO - Hybrid LLM 
architecture initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized2025-10-28 09:26:38,263 - 
devsynth.application.memory.phase3_integration_system - INFO - Phase 3 
Integration System initialized with all advanced reasoning 
components------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initializedINFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initializedINFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initializedINFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initializedINFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components_________
TestPhase3IntegrationSystem.test_export_import_system_state __________self = 
&amp;lt;tests.unit.application.memory.test_phase3_integration_system.TestPhase3I
ntegrationSystem object at 0x1205a2d20&amp;gt;    def 
test_export_import_system_state(self):        &amp;quot;&amp;quot;&amp;quot;Test
system state export and import.&amp;quot;&amp;quot;&amp;quot;        # Set up 
some state        self.integration_system.execution_learning.learning_history = 
[{&amp;quot;test&amp;quot;: &amp;quot;session&amp;quot;}]        
self.integration_system.enhanced_graphrag.query_cache = 
{&amp;quot;query1&amp;quot;: &amp;quot;result1&amp;quot;}            # Export 
state&amp;gt;       exported_state = 
self.integration_system.export_system_state()                         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/
ravenoak/devsynth/tests/unit/application/memory/test_phase3_integration_system.p
y:267: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/phase3_integration_system.py:471: in export_system_state    
&amp;quot;execution_learning&amp;quot;: 
self.execution_learning.export_learning_state(),                          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/src/devsynth/application/memory/execution_learning_integrati
on.py:381: in export_learning_state    &amp;quot;statistics&amp;quot;: 
self.get_learning_statistics(),                  
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/src/devsynth/application/memory/execution_learning_integration.py:312: in 
get_learning_statistics    total_patterns = 
sum(session[&amp;quot;patterns_learned&amp;quot;] for session in 
self.learning_history)                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .0 = 
&amp;lt;list_iterator object at 0x142c868c0&amp;gt;&amp;gt;   total_patterns = 
sum(session[&amp;quot;patterns_learned&amp;quot;] for session in 
self.learning_history)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^E   
KeyError: 
&amp;#x27;patterns_learned&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/src/devsynth/application/memory/execution_learning_integration.py:312: 
KeyError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:38,300 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Intent discovery 
engine initialized with threshold 0.72025-10-28 09:26:38,300 - 
devsynth.application.memory.enhanced_knowledge_graph - INFO - Enhanced knowledge
graph initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)2025-10-28 
09:26:38,300 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)2025-10-28 
09:26:38,300 - devsynth.application.memory.execution_learning_algorithm - INFO -
Pattern library initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)2025-10-28 09:26:38,300
- devsynth.application.memory.enhanced_graphrag_engine - INFO - Enhanced 
GraphRAG query engine initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.automata_synthesis_engine - INFO - Automata 
synthesis engine initialized (min_samples: 10)2025-10-28 09:26:38,300 - 
devsynth.application.memory.hybrid_llm_architecture - INFO - Hybrid LLM 
architecture initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized2025-10-28 09:26:38,300 - 
devsynth.application.memory.phase3_integration_system - INFO - Phase 3 
Integration System initialized with all advanced reasoning 
components------------------------------ Captured log setup 
------------------------------INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initializedINFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)INFO  
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initializedINFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initializedINFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initializedINFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initializedINFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initializedINFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initializedINFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning 
components_________________________ test_cascading_and_federated 
_________________________router = 
&amp;lt;devsynth.application.memory.query_router.QueryRouter object at 
0x142fead20&amp;gt;    @pytest.mark.fast    def 
test_cascading_and_federated(router: QueryRouter) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Cascading and federated strategies yield 
MemoryRecord sequences.&amp;quot;&amp;quot;&amp;quot;            cascading = 
router.cascading_query(&amp;quot;topic&amp;quot;)&amp;gt;       assert 
{record.source for record in cascading} == {&amp;quot;vector&amp;quot;, 
&amp;quot;graph&amp;quot;}E       AssertionError: assert 
{&amp;#x27;graph&amp;#x27;, &amp;#x27;tinydb&amp;#x27;, 
&amp;#x27;vector&amp;#x27;} == {&amp;#x27;graph&amp;#x27;, 
&amp;#x27;vector&amp;#x27;}E         E         Extra items in the left set:E    
&amp;#x27;tinydb&amp;#x27;E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/
memory/test_query_router.py:128: AssertionError___________________ 
test_queue_update_enqueues_memory_record ___________________    
@pytest.mark.fast    def test_queue_update_enqueues_memory_record() -&amp;gt; 
None:        manager = _manager()        sync_manager: SyncManager = 
manager.sync_manager        item = MemoryItem(            
id=&amp;quot;queued-1&amp;quot;,            
content=&amp;quot;queue-test&amp;quot;,            
memory_type=MemoryType.SHORT_TERM,            
metadata={&amp;quot;origin&amp;quot;: &amp;quot;alpha&amp;quot;},            
created_at=datetime.now(),        )            
sync_manager.queue_update(&amp;quot;alpha&amp;quot;, item)            with 
sync_manager._queue_lock:  # noqa: SLF001 - internal verification for 
test&amp;gt;           queued_store, record = sync_manager._queue[-1]           
^^^^^^^^^^^^^^^^^^^^E           ValueError: too many values to unpack (expected 
2)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/me
mory/test_sync_manager_transactions.py:61: 
ValueError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:39,175 - 
devsynth.application.memory.memory_manager - INFO - Memory Manager initialized 
with adapters: alpha, beta2025-10-28 09:26:39,175 - 
devsynth.application.memory.tiered_cache - INFO - Tiered cache initialized with 
max size 502025-10-28 09:26:39,175 - devsynth.application.memory.tiered_cache - 
INFO - Cache cleared------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.memory.memory_manager:logging_setup.py:615 Memory Manager 
initialized with adapters: alpha, betaINFO     
devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered cache 
initialized with max size 50INFO     
devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache 
cleared________________ test_tinydb_adapter_serializes_bytes_and_tuple 
________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_tinydb_adapter_serializes0&amp;#x27;)    
@pytest.mark.fast    def 
test_tinydb_adapter_serializes_bytes_and_tuple(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Ensure TinyDBMemoryAdapter serializes non-JSON 
types safely.            This guards against the TypeError observed during `task
release:prep` when        metadata/content contain bytes or tuples.        
&amp;quot;&amp;quot;&amp;quot;        adapter = 
TinyDBMemoryAdapter(db_path=str(tmp_path / &amp;quot;db.json&amp;quot;))        
item = MemoryItem(            id=&amp;quot;bytes_tuple&amp;quot;,            
content={                &amp;quot;payload&amp;quot;: 
b&amp;quot;hello&amp;quot;,                &amp;quot;coords&amp;quot;: (1, 2, 
3),                &amp;quot;nested&amp;quot;: {&amp;quot;t&amp;quot;: 
(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)},            },            
memory_type=MemoryType.KNOWLEDGE,            metadata={                
&amp;quot;tags&amp;quot;: (&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;),       
&amp;quot;raw&amp;quot;: b&amp;quot;world&amp;quot;,                
&amp;quot;timestamp&amp;quot;: datetime(2024, 1, 1),            },        )     
stored_id = adapter.store(item)        assert stored_id == item.id            
retrieved = adapter.retrieve(stored_id)        assert retrieved is not None     
# bytes should become a string representation (utf-8 or base64); at least be str
assert isinstance(retrieved.content[&amp;quot;payload&amp;quot;], str)        
assert isinstance(retrieved.metadata[&amp;quot;raw&amp;quot;], str)            #
tuples should become lists        assert 
retrieved.content[&amp;quot;coords&amp;quot;] == [1, 2, 3]        assert 
retrieved.content[&amp;quot;nested&amp;quot;][&amp;quot;t&amp;quot;] == 
[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;]        assert 
retrieved.metadata[&amp;quot;tags&amp;quot;] == [&amp;quot;x&amp;quot;, 
&amp;quot;y&amp;quot;]            # datetime should round-trip as ISO string 
(already covered elsewhere but asserted here for completeness)&amp;gt;       
assert isinstance(retrieved.metadata[&amp;quot;timestamp&amp;quot;], str)E      
assert FalseE        +  where False = isinstance(datetime.datetime(2024, 1, 1, 
0, 0), 
str)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/
memory/test_tinydb_adapter_bytes_tuple.py:54: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:39,205 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB 
Memory Adapter initialized2025-10-28 09:26:39,206 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored 
memory item with ID bytes_tuple in TinyDB Memory 
Adapter------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
TinyDB Memory Adapter initializedINFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
Stored memory item with ID bytes_tuple in TinyDB Memory 
Adapter____________________ test_evaluate_change_stores_with_phase 
____________________    def test_evaluate_change_stores_with_phase():        
memory = DummyMemoryManager()        service = 
_build_service(&amp;quot;yes&amp;quot;, memory_manager=memory)        change = 
RequirementChange(requirement_id=uuid4(), created_by=&amp;quot;carol&amp;quot;) 
service.evaluate_change(change, edrr_phase=EDRRPhase.EXPAND)            assert 
memory.calls&amp;gt;       assert memory.calls[0][1] == 
MemoryType.DIALECTICAL_REASONINGE       AssertionError: assert 
&amp;lt;MemoryType.RELATIONSHIP: &amp;#x27;relationship&amp;#x27;&amp;gt; == 
&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;E        +  where 
&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt; = 
MemoryType.DIALECTICAL_REASONING/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/tests/unit/application/requirements/test_dialectical_reasoner.py:124: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:39,285 - 
devsynth.application.requirements.dialectical_reasoner - INFO - Evaluating 
change2025-10-28 09:26:39,285 - 
devsynth.application.requirements.dialectical_reasoner - INFO - Consensus 
reached for change------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615 
Evaluating changeINFO     
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615 
Consensus reached for change_______________ 
test_generate_arguments_parses_counterarguments ________________    def 
test_generate_arguments_parses_counterarguments():        response = (          
&amp;quot;Argument 1:\n&amp;quot;            &amp;quot;Position: 
Thesis\n&amp;quot;            &amp;quot;Content: Improve UX\n&amp;quot;         
&amp;quot;Counterargument: Increases complexity\n\n&amp;quot;            
&amp;quot;Argument 2:\n&amp;quot;            &amp;quot;Position: 
Antithesis\n&amp;quot;            &amp;quot;Content: Maintain 
simplicity\n&amp;quot;            &amp;quot;Counterargument: Misses UX 
gains&amp;quot;        )        service = _build_service_for_arguments(response)
change = RequirementChange(requirement_id=uuid4(), 
created_by=&amp;quot;mallory&amp;quot;)&amp;gt;       args = 
service._generate_arguments(change, &amp;quot;thesis&amp;quot;, 
&amp;quot;antithesis&amp;quot;)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projec
ts/github.com/ravenoak/devsynth/tests/unit/application/requirements/test_dialect
ical_reasoner.py:220: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:575: in _generate_arguments    prompt = 
self._create_arguments_prompt(change, thesis, antithesis)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.requirements.dialectical_reasoner.DialecticalReason
erService object at 0x142ff90d0&amp;gt;change = 
RequirementChange(id=UUID(&amp;#x27;5b541dcb-3bd3-45b3-b7fb-596cd8b07706&amp;#x2
7;), requirement_id=UUID(&amp;#x27;6bc93179-30e8-4ff0-bebd-b178d7..., 9, 26, 39,
342707), created_by=&amp;#x27;mallory&amp;#x27;, reason=&amp;#x27;&amp;#x27;, 
approved=False, approved_at=None, approved_by=None, comments=[])thesis = 
&amp;#x27;thesis&amp;#x27;, antithesis = &amp;#x27;antithesis&amp;#x27;    def 
_create_arguments_prompt(        self, change: RequirementChange, thesis: str, 
antithesis: str    ) -&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;       
Create a prompt for generating arguments.            Args:            change: 
The requirement change.            thesis: The thesis statement.            
antithesis: The antithesis statement.            Returns:            The prompt.
&amp;quot;&amp;quot;&amp;quot;        prompt = (            &amp;quot;You are a 
requirements analyst evaluating a proposed change to a requirement. &amp;quot;  
&amp;quot;Please generate a list of arguments for and against the proposed 
change. &amp;quot;            &amp;quot;For each argument, specify whether it 
supports the thesis or antithesis, provide a clear explanation, &amp;quot;      
&amp;quot;and then offer a counterargument that challenges the original point. 
&amp;quot;            &amp;quot;\n\nProposed change:\n&amp;quot;        )       
if change.change_type.value == &amp;quot;add&amp;quot;:            prompt += 
f&amp;quot;Add a new requirement: {change.new_state.title}\n&amp;quot;          
prompt += f&amp;quot;Description: {change.new_state.description}\n&amp;quot;    
elif change.change_type.value == &amp;quot;remove&amp;quot;:            prompt 
+= f&amp;quot;Remove requirement: {change.previous_state.title}\n&amp;quot;     
prompt += f&amp;quot;Description: 
{change.previous_state.description}\n&amp;quot;        elif 
change.change_type.value == &amp;quot;modify&amp;quot;:            prompt += 
f&amp;quot;Modify requirement from:\n&amp;quot;&amp;gt;           prompt += 
f&amp;quot;Title: {change.previous_state.title}\n&amp;quot;                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;title&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/sr
c/devsynth/application/requirements/dialectical_reasoner.py:1068: 
AttributeError___________ 
test_generate_arguments_handles_missing_counterargument ____________    def 
test_generate_arguments_handles_missing_counterargument():        response = (  
&amp;quot;Argument 1:\n&amp;quot;            &amp;quot;Position: 
Thesis\n&amp;quot;            &amp;quot;Content: Example argument\n\n&amp;quot; 
&amp;quot;Argument 2:\n&amp;quot;            &amp;quot;Position: 
Antithesis\n&amp;quot;            &amp;quot;Content: Another argument&amp;quot; 
)        service = _build_service_for_arguments(response)        change = 
RequirementChange(requirement_id=uuid4(), 
created_by=&amp;quot;nina&amp;quot;)&amp;gt;       args = 
service._generate_arguments(change, &amp;quot;thesis&amp;quot;, 
&amp;quot;antithesis&amp;quot;)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projec
ts/github.com/ravenoak/devsynth/tests/unit/application/requirements/test_dialect
ical_reasoner.py:247: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:575: in _generate_arguments    prompt = 
self._create_arguments_prompt(change, thesis, antithesis)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.requirements.dialectical_reasoner.DialecticalReason
erService object at 0x142b621b0&amp;gt;change = 
RequirementChange(id=UUID(&amp;#x27;aa6c4e77-77ee-44e3-9b55-68d0eeb20138&amp;#x2
7;), requirement_id=UUID(&amp;#x27;2335ad52-852f-429c-92e3-ae288c... 28, 9, 26, 
39, 373691), created_by=&amp;#x27;nina&amp;#x27;, reason=&amp;#x27;&amp;#x27;, 
approved=False, approved_at=None, approved_by=None, comments=[])thesis = 
&amp;#x27;thesis&amp;#x27;, antithesis = &amp;#x27;antithesis&amp;#x27;    def 
_create_arguments_prompt(        self, change: RequirementChange, thesis: str, 
antithesis: str    ) -&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;       
Create a prompt for generating arguments.            Args:            change: 
The requirement change.            thesis: The thesis statement.            
antithesis: The antithesis statement.            Returns:            The prompt.
&amp;quot;&amp;quot;&amp;quot;        prompt = (            &amp;quot;You are a 
requirements analyst evaluating a proposed change to a requirement. &amp;quot;  
&amp;quot;Please generate a list of arguments for and against the proposed 
change. &amp;quot;            &amp;quot;For each argument, specify whether it 
supports the thesis or antithesis, provide a clear explanation, &amp;quot;      
&amp;quot;and then offer a counterargument that challenges the original point. 
&amp;quot;            &amp;quot;\n\nProposed change:\n&amp;quot;        )       
if change.change_type.value == &amp;quot;add&amp;quot;:            prompt += 
f&amp;quot;Add a new requirement: {change.new_state.title}\n&amp;quot;          
prompt += f&amp;quot;Description: {change.new_state.description}\n&amp;quot;    
elif change.change_type.value == &amp;quot;remove&amp;quot;:            prompt 
+= f&amp;quot;Remove requirement: {change.previous_state.title}\n&amp;quot;     
prompt += f&amp;quot;Description: 
{change.previous_state.description}\n&amp;quot;        elif 
change.change_type.value == &amp;quot;modify&amp;quot;:            prompt += 
f&amp;quot;Modify requirement from:\n&amp;quot;&amp;gt;           prompt += 
f&amp;quot;Title: {change.previous_state.title}\n&amp;quot;                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;title&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/sr
c/devsynth/application/requirements/dialectical_reasoner.py:1068: 
AttributeError______________________ test_wsde_team_hook_positive_path 
_______________________    def test_wsde_team_hook_positive_path():        team 
= WSDETeam(&amp;quot;test&amp;quot;)        service = 
_build_service(&amp;quot;yes&amp;quot;)&amp;gt;       
service.register_evaluation_hook(team.requirement_evaluation_hook)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;requirement_evaluation_hook&amp;#x27;/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/application/requirements/test_dialectical_reasone
r.py:266: AttributeError______________________ test_wsde_team_hook_negative_path
_______________________    def test_wsde_team_hook_negative_path():        team 
= WSDETeam(&amp;quot;test&amp;quot;)        service = 
_build_service(&amp;quot;no&amp;quot;)&amp;gt;       
service.register_evaluation_hook(team.requirement_evaluation_hook)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;requirement_evaluation_hook&amp;#x27;/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/application/requirements/test_dialectical_reasone
r.py:279: AttributeError_____________ 
TestEnhancedTestCollector.test_nonexistent_directory _____________self = 
&amp;lt;test_enhanced_test_collector.TestEnhancedTestCollector object at 
0x1206dfbf0&amp;gt;    def test_nonexistent_directory(self):        
&amp;quot;&amp;quot;&amp;quot;Test handling of nonexistent 
directories.&amp;quot;&amp;quot;&amp;quot;        collector = 
EnhancedTestCollector()            tests = 
collector.collect_tests_by_category(&amp;quot;nonexistent&amp;quot;)        
assert len(tests) == 0    &amp;gt;       isolation_report = 
collector._isolation_analyzer.analyze_test_isolation(&amp;quot;nonexistent&amp;q
uot;)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
AttributeError: &amp;#x27;EnhancedTestCollector&amp;#x27; object has no 
attribute 
&amp;#x27;_isolation_analyzer&amp;#x27;/Users/caitlyn/Projects/github.com/raveno
ak/devsynth/tests/unit/application/testing/test_enhanced_test_collector.py:318: 
AttributeError______________ TestCacheOperations.test_cache_directory_creation 
_______________self = &amp;lt;test_enhanced_test_collector.TestCacheOperations 
object at 0x1206fd520&amp;gt;    def test_cache_directory_creation(self):       
&amp;quot;&amp;quot;&amp;quot;Test that cache directory is 
created.&amp;quot;&amp;quot;&amp;quot;        with tempfile.TemporaryDirectory()
as temp_dir:            cache_dir = Path(temp_dir) / &amp;quot;cache&amp;quot;  
collector = EnhancedTestCollector()            collector.cache_dir = cache_dir  
# Cache directory should be created&amp;gt;           assert cache_dir.exists()E
AssertionError: assert FalseE            +  where False = exists()E            +
where exists = 
PosixPath(&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpzw6dblpz
/cache&amp;#x27;).exists/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/application/testing/test_enhanced_test_collector.py:416: 
AssertionError_____________________ test_metrics_fail_patches_calculate 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1440b8440&amp;gt;    @pytest.mark.fast    def 
test_metrics_fail_patches_calculate(monkeypatch):&amp;gt;       
metrics_fail(monkeypatch)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/behavior/test_alignment_metrics_steps_unit.py:17: _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ command_context = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1440b8440&amp;gt;    
@given(&amp;quot;alignment metrics calculation fails&amp;quot;)    def 
metrics_fail(command_context: MutableMapping) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Force the simulated CLI command to raise an 
error.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
command_context[&amp;quot;force_error&amp;quot;] = True        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: &amp;#x27;MonkeyPatch&amp;#x27;
object does not support item 
assignment/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/st
eps/test_alignment_metrics_steps.py:47: TypeError_______________________ 
test_main_handles_run_cli_errors _______________________name = 
&amp;#x27;errors&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:      
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;errors&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/s
rc/devsynth/application/cli/__init__.py:101: AttributeErrorThe above exception 
was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144046990&amp;gt;    
@pytest.mark.fast    def test_main_handles_run_cli_errors(monkeypatch):        
def failing_run_cli():            raise RuntimeError(&amp;quot;boom&amp;quot;)  
handled = {}            def fake_handle_error(_bridge, err):            
handled[&amp;quot;error&amp;quot;] = err            
monkeypatch.setattr(&amp;quot;devsynth.adapters.cli.typer_adapter.run_cli&amp;qu
ot;, failing_run_cli)&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.errors.handle_error&amp;quot;, 
fake_handle_error        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_cli_er
ror_handling.py:35: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = &amp;#x27;errors&amp;#x27;, ann 
= &amp;#x27;devsynth.application.cli.errors&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.errors has no 
attribute 
&amp;#x27;errors&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.
venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_______________ test_build_app_registers_commands_from_registry 
________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x142c85a00&amp;gt;    @pytest.mark.fast    def 
test_build_app_registers_commands_from_registry(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Commands in COMMAND_REGISTRY should be registered 
with the CLI.&amp;quot;&amp;quot;&amp;quot;        called = {}            def 
sample_cmd():            called[&amp;quot;ran&amp;quot;] = True            
monkeypatch.setattr(adapter, &amp;quot;COMMAND_REGISTRY&amp;quot;, 
{&amp;quot;sample&amp;quot;: sample_cmd})&amp;gt;       
monkeypatch.setattr(adapter, &amp;quot;config_app&amp;quot;, typer.Typer())E    
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.adapters.cli.typer_adapter&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adap
ters/cli/typer_adapter.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;config_app&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/cli/test_command_registry.py:17: AttributeError_________________ 
test_global_debug_flag_sets_log_level_debug __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142c0a6f0&amp;gt;    
@pytest.mark.fast    def 
test_global_debug_flag_sets_log_level_debug(monkeypatch):        runner = 
CliRunner()        # Ensure env does not force level        
monkeypatch.delenv(&amp;quot;DEVSYNTH_LOG_LEVEL&amp;quot;, raising=False)       
monkeypatch.delenv(&amp;quot;DEVSYNTH_DEBUG&amp;quot;, raising=False)           
app = build_app()        result = runner.invoke(app, 
[&amp;quot;--debug&amp;quot;, &amp;quot;--version&amp;quot;])  # triggers 
callback and exit        assert result.exit_code == 0&amp;gt;       assert 
logging.getLogger().getEffectiveLevel() == logging.DEBUGE       assert 20 == 10E
+  where 20 = getEffectiveLevel()E        +    where getEffectiveLevel = 
&amp;lt;RootLogger root (INFO)&amp;gt;.getEffectiveLevelE        +      where 
&amp;lt;RootLogger root (INFO)&amp;gt; = &amp;lt;function getLogger at 
0x104d52700&amp;gt;()E        +        where &amp;lt;function getLogger at 
0x104d52700&amp;gt; = logging.getLoggerE        +  and   10 = 
logging.DEBUG/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli
/test_logging_flags.py:29: AssertionError__________________ 
test_env_debug_sets_log_level_when_no_flag __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1440a9910&amp;gt;    
@pytest.mark.fast    def 
test_env_debug_sets_log_level_when_no_flag(monkeypatch):        runner = 
CliRunner()        monkeypatch.setenv(&amp;quot;DEVSYNTH_DEBUG&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.delenv(&amp;quot;DEVSYNTH_LOG_LEVEL&amp;quot;, raising=False)       
app = build_app()        result = runner.invoke(app, 
[&amp;quot;--version&amp;quot;])  # triggers callback        assert 
result.exit_code == 0&amp;gt;       assert 
logging.getLogger().getEffectiveLevel() == logging.DEBUGE       assert 20 == 10E
+  where 20 = getEffectiveLevel()E        +    where getEffectiveLevel = 
&amp;lt;RootLogger root (INFO)&amp;gt;.getEffectiveLevelE        +      where 
&amp;lt;RootLogger root (INFO)&amp;gt; = &amp;lt;function getLogger at 
0x104d52700&amp;gt;()E        +        where &amp;lt;function getLogger at 
0x104d52700&amp;gt; = logging.getLoggerE        +  and   10 = 
logging.DEBUG/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli
/test_logging_flags.py:42: AssertionError__________________ 
test_log_level_option_overrides_env_debug ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1440a9490&amp;gt;    
@pytest.mark.fast    def test_log_level_option_overrides_env_debug(monkeypatch):
runner = CliRunner()        
monkeypatch.setenv(&amp;quot;DEVSYNTH_DEBUG&amp;quot;, &amp;quot;true&amp;quot;)
monkeypatch.delenv(&amp;quot;DEVSYNTH_LOG_LEVEL&amp;quot;, raising=False)       
app = build_app()        result = runner.invoke(app, 
[&amp;quot;--log-level&amp;quot;, &amp;quot;WARNING&amp;quot;, 
&amp;quot;--version&amp;quot;])  # eager        assert result.exit_code == 
0&amp;gt;       assert logging.getLogger().getEffectiveLevel() == 
logging.WARNINGE       assert 20 == 30E        +  where 20 = 
getEffectiveLevel()E        +    where getEffectiveLevel = &amp;lt;RootLogger 
root (INFO)&amp;gt;.getEffectiveLevelE        +      where &amp;lt;RootLogger 
root (INFO)&amp;gt; = &amp;lt;function getLogger at 0x104d52700&amp;gt;()E      
+        where &amp;lt;function getLogger at 0x104d52700&amp;gt; = 
logging.getLoggerE        +  and   30 = 
logging.WARNING/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/c
li/test_logging_flags.py:55: AssertionError_____________ 
test_mvuu_dashboard_module_no_run_avoids_subprocess ______________    
@pytest.mark.fast    @pytest.mark.smoke    def 
test_mvuu_dashboard_module_no_run_avoids_subprocess():        # Ensure running 
the module with --no-run exits cleanly and does not spawn subprocesses        
with mock.patch.object(subprocess, &amp;quot;run&amp;quot;) as mocked_run:      
proc = subprocess.run(                [                    sys.executable,      
&amp;quot;-m&amp;quot;,                    
&amp;quot;devsynth.application.cli.commands.mvuu_dashboard_cmd&amp;quot;,       
&amp;quot;--no-run&amp;quot;,                ],                
stdout=subprocess.PIPE,                stderr=subprocess.STDOUT,                
text=True,                timeout=15,            )        # The outer 
subprocess.run executed our Python process; it should have succeeded&amp;gt;    
assert proc.returncode == 0E       AssertionError: assert &amp;lt;MagicMock 
name=&amp;#x27;run().returncode&amp;#x27; 
id=&amp;#x27;5415488592&amp;#x27;&amp;gt; == 0E        +  where 
&amp;lt;MagicMock name=&amp;#x27;run().returncode&amp;#x27; 
id=&amp;#x27;5415488592&amp;#x27;&amp;gt; = &amp;lt;MagicMock 
name=&amp;#x27;run()&amp;#x27; 
id=&amp;#x27;5415493776&amp;#x27;&amp;gt;.returncode/Users/caitlyn/Projects/gith
ub.com/ravenoak/devsynth/tests/unit/cli/test_mvuu_dashboard_smoke.py:26: 
AssertionError__________ 
test_cli_run_tests_unit_fast_completes_with_non_zero_tests __________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x142e2ab40&amp;gt;    
@pytest.mark.fast    def 
test_cli_run_tests_unit_fast_completes_with_non_zero_tests(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;        Regression test: ensure the shared test 
runner completes in fast mode and        executes a non-zero number of tests, 
without hanging on optional providers.        &amp;quot;&amp;quot;&amp;quot;    
# Disable optional external providers to avoid network/UI stalls        
monkeypatch.setenv(&amp;quot;DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE&amp;quot;, 
&amp;quot;false&amp;quot;)            success, output = run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=(&amp;quot;fast&amp;quot;,),            verbose=False,         
report=False,            parallel=False,  # run in-process to minimize flakiness
on CI            segment=False,            segment_size=50,            
maxfail=1,        )            # Must succeed overall&amp;gt;       assert 
success, f&amp;quot;Runner did not succeed. Output:\n{output}&amp;quot;E       
AssertionError: Runner did not succeed. Output:E         ERROR: file or 
directory not found: 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
sE         E         ============================= test session starts 
==============================E         platform darwin -- Python 3.12.12, 
pytest-8.4.2, pluggy-1.6.0E         benchmark: 5.1.0 (defaults: 
timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 
max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)E   
rootdir: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_cli_run_tests_unit_fast_c0/projectE         plugins: mock-3.15.1, 
asyncio-1.2.0, anyio-4.11.0, html-4.1.1, xdist-3.8.0, langsmith-0.4.37, 
metadata-3.1.1, Faker-37.11.0, benchmark-5.1.0, hypothesis-6.142.3, bdd-8.1.0, 
rerunfailures-16.1, cov-7.0.0E         asyncio: mode=Mode.STRICT, debug=False, 
asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=functionE         collected 0 itemsE         E  
============================ no tests ran in 0.23s 
=============================E         E         Pytest exited with code 4. 
Command: /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python 
-m pytest 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_usage_hint 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_runtime_hin
t 
tests/unit/adapters/cli/test_typer_adapter.py::test_command_help_format_includes
_sections 
tests/unit/adapters/issues/test_github_adapter.py::test_fetch_github_issue 
tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_normalizes
_mapping 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_without_pa
rameters_returns_none 
tests/unit/adapters/llm/test_llm_adapter.py::test_default_factory_delegates_to_g
lobal_registry 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_uses_injected_
factory 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_emits_typed_er
ror_for_unknown_provider 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_maps_registere
d_message 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_propaga
tes_factory_rejection 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_success
tests/unit/adapters/llm/test_llm_adapter.py::test_unknown_llm_provider_error_pre
serves_cause 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_stream
_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_with_c
ontext_stream_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_chunk_response_
helper_respects_chunk_size 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_stream_chunks_y
ields_all_segments 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_mock_response_templa
te_serializes 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_round_trip_pr
eserves_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_matches_cus
tom_template 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_uses_defaul
t_when_no_template_matches 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
coerces_sequences 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
falls_back_to_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_adapter_initialises_
from_mapping 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_stream_prop
agates_generate_failure 
tests/unit/adapters/test_agent_adapter.py::test_factory_initializes_agent_with_c
onfig_payload 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_builds_consensus_p
ayload_from_solutions 
tests/unit/adapters/test_agent_adapter.py::test_process_task_without_agents_rais
es_validation_error 
tests/unit/adapters/test_agent_adapter.py::test_coerce_task_solutions_filters_in
valid_entries 
tests/unit/adapters/test_agent_adapter.py::test_import_agent_falls_back_on_error
tests/unit/adapters/test_agent_adapter.py::test_import_agent_rejects_non_class 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_caches_result
s 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_processing
_failures 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_critical_d
ecisions 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_requires_active_te
am 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_singl
e_agent_flow 
tests/unit/adapters/test_agent_adapter.py::test_agent_initialization_payload_han
dles_unknown_type 
tests/unit/adapters/test_agent_adapter.py::test_coerce_helpers_normalize_inputs 
tests/unit/adapters/test_agent_adapter.py::test_unified_agent_fallback_behaviour
tests/unit/adapters/test_agent_adapter.py::test_load_default_config_uses_yaml_lo
ader 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_uses_future_s
pecs 
tests/unit/adapters/test_agent_adapter.py::test_create_team_uses_collaborative_w
hen_memory_manager 
tests/unit/adapters/test_agent_adapter.py::test_add_agent_creates_default_team 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_multi
_agent_path 
tests/unit/adapters/test_backend_resource_gates.py::test_chromadb_adapter_import
s tests/unit/adapters/test_backend_resource_gates.py::test_kuzu_adapter_imports 
tests/unit/adapters/test_backend_resource_gates.py::test_faiss_store_imports_and
_minimal 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_transaction_commit_
and_delete 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_provider_fallback_u
ses_default_embedder 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_store_raises_after_
retries 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_search_handles_empt
y_results 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_commit_failure_mark
s_transaction 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_rollback_transactio
n_states 
tests/unit/adapters/test_fake_memory_store.py::test_fake_memory_store_store_retr
ieve_search_delete_and_txn 
tests/unit/adapters/test_fake_memory_store.py::test_fake_vector_store_similarity
_and_stats 
tests/unit/adapters/test_github_project_adapter.py::test_payload_serialization 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_request_payload
_and_helpers 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_creates_colu
mns_and_cards 
tests/unit/adapters/test_github_project_adapter.py::test_fetch_and_mutations_wit
h_stub_client 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_missing_data_ra
ises 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_skips_existi
ng_items 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_raises_on_gr
aphql_errors 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_error_formattin
g_handles_missing_messages 
tests/unit/adapters/test_jira_adapter.py::test_create_issue_payload_serializatio
n tests/unit/adapters/test_jira_adapter.py::test_transition_issue_missing_status
tests/unit/adapters/test_jira_adapter.py::test_create_issue_http_error_surfaced 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_i
nit_creates_empty_adapter 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_l
oad_model_sets_session 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_without_loaded_model_raises_error 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_with_loaded_model_calls_session_run 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_multiple_outputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_empty_inputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_propagates_onnx_exceptions 
tests/unit/adapters/test_provider_safe_defaults.py::test_default_safe_falls_back
_to_stub_without_keys_and_lmstudio 
tests/unit/adapters/test_provider_safe_defaults.py::test_openai_explicit_without
_key_raises 
tests/unit/adapters/test_provider_safe_defaults.py::test_anthropic_implicit_with
out_key_falls_back_safe_default_stub 
tests/unit/adapters/test_provider_safe_defaults.py::test_lmstudio_not_attempted_
without_availability_flag 
tests/unit/adapters/test_provider_safe_defaults.py::test_disable_providers_retur
ns_null 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_complete_and_embed
_are_deterministic 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_async_matches_sync
tests/unit/adapters/test_provider_stub.py::test_provider_system_reload_preserves
_settings_import 
tests/unit/adapters/test_provider_system.py::test_embed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_embed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_null_provider_complete_raises_
error 
tests/unit/adapters/test_provider_system.py::test_null_provider_acomplete_raises
_error 
tests/unit/adapters/test_provider_system.py::test_null_provider_embed_raises_err
or 
tests/unit/adapters/test_provider_system.py::test_null_provider_aembed_raises_er
ror 
tests/unit/adapters/test_provider_system.py::test_null_provider_initialization 
tests/unit/adapters/test_provider_system.py::test_provider_factory_create_provid
er_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_base_provider_methods_succeeds
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[OpenAIProvider-config0] 
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[LMStudioProvider-config1] 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_initializati
on_skips_health_check_when_network_guard_active 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_load_env_file_populates_config
tests/unit/adapters/test_provider_system.py::test_create_tls_config_has_expected
tests/unit/adapters/test_provider_system.py::test_get_env_or_default_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_config_has_expect
ed 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_has_e
xpected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_error
_raises_error 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_retry
_has_expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_acomplete_has_
expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_embed_has_expe
cted 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_complete_has
_expected 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_async_method
s_has_expected 
tests/unit/adapters/test_provider_system.py::test_provider_with_empty_inputs_has
_expected 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_selects_provider 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_survives_missing_settings 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_respects_ord
er 
tests/unit/adapters/test_provider_system.py::test_openai_provider_retries_after_
transient_failure 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_circuit_brea
ker_blocks_after_failure 
tests/unit/adapters/test_provider_system.py::test_complete_falls_back_to_next_pr
ovider 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_re
spects_disable_flag 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_falls_back_to_lmstudio_when_marked_available 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_req
uires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_lmstudio_provider_r
equires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_asy
nc_requires_httpx_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_defaults
_when_settings_missing 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_uses_exp
licit_settings 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_wir
ing 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_emi
ts_metrics_on_retry 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_n
o_valid_providers 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
ync_uses_circuit_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_failure_opens_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_respects_open_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_records_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
ll_failures_surface_last_error 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
hort_circuits_after_first_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
kips_providers_with_open_breakers 
tests/unit/adapters/test_provider_system_additional.py::test_complete_failure_in
crements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_embed_wraps_unexpec
ted_error 
tests/unit/adapters/test_provider_system_additional.py::test_acomplete_failure_i
ncrements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_aembed_wraps_unexpe
cted_error 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_uses_next_provider 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_raises_after_exhaustion 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_embed_wraps_une
xpected_exceptions 
tests/unit/adapters/test_provider_system_resilience.py::test_base_provider_retry
_harness_records_jitter 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_a
sync_breaker_failure_emits_metrics 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_s
ync_breaker_failure_emits_metrics 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_skips_by_def
ault 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_runs_when_en
abled 
tests/unit/adapters/test_storage_adapter_protocol.py::test_storage_adapter_proto
col_shape 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
turns_structure 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
gistered 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_returns_structure 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_registered 
tests/unit/agents/test_multi_agent_coordinator.py::test_reach_consensus_majority
_choice 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_returns_structure 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_registered 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_returns_
structure 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_register
ed 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
existing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
missing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
empty_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
with_whitespace 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_boundary_value
s_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_error_conditio
ns_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_with_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_without_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_mixed_availability 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_template_direc
tory_path_construction 
tests/unit/agents/test_tool_sandbox.py::test_file_access_restricted 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_blocked 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_allowed 
tests/unit/agents/test_tool_sandbox.py::test_sandbox_context_restores_hooks 
tests/unit/agents/test_tools.py::test_register_and_get_tool 
tests/unit/agents/test_tools.py::test_unknown_tool_returns_none 
tests/unit/agents/test_tools.py::test_export_for_openai_formats_tools 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_r
ecords_summary_and_flushes_memory 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_s
upports_primus_rotation_cycle 
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict 
tests/unit/api/test_public_api_contract.py::test_public_api_imports 
tests/unit/api/test_public_api_contract.py::test_deprecated_wrapper_emits_warnin
g 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_initializa
tion_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_process_ab
stract_method_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_create_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_update_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_get_role_p
rompt_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_error_raises_error 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_error_raises_error 
tests/unit/application/agents/test_test_agent_integration.py::test_process_scaff
olds_tests_from_context 
tests/unit/application/agents/test_validation_agent.py::test_process_affirmative
_is_valid_true 
tests/unit/application/agents/test_validation_agent.py::test_process_failure_tok
ens_set_invalid 
tests/unit/application/agents/test_validation_agent.py::test_process_neutral_tex
t_is_valid 
tests/unit/application/agents/test_validation_agent.py::test_is_valid_word_bound
ary_only 
tests/unit/application/agents/test_validation_agent.py::test_wsde_contains_agent
_and_role 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[All checks passed; no issues.-True] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[An error occurred in module A.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Exception occurred during run.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Some tests fail on CI.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Clean run; everything looks good.-True] 
tests/unit/application/agents/test_wsde_memory_integration_fast.py::test_store_a
nd_retrieve_dialectical_process 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_displays
_all_config 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_update_k
ey_value_saves_and_reports 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_list_mod
els_displays_models 
tests/unit/application/cli/commands/test_config_cmd.py::test_enable_feature_cmd_
updates_and_saves 
tests/unit/application/cli/commands/test_doctor_cmd_typed.py::test_doctor_cmd_ac
cepts_path_arguments 
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py::test_doctor_cm
d_does_not_import_streamlit_or_nicegui 
tests/unit/application/cli/commands/test_ingest_cli_command.py::test_ingest_cli_
command_uses_typed_options 
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py::test_
inspect_code_cmd_sanitizes_dynamic_output 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_preserves_alias_after_subtask_rename 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_rebinds_alias_on_multiple_description_updates 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_reports_eta_strings_when_progress_advances 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_records_failure_history_for_diagnostics 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_empty_list_returns_empty_dict 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_single_name_defaults_true 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_false_variants 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_true_variants 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
returns_mapping 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
invalid_json_returns_none 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
non_mapping_returns_none 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_allows_requests_
env_default_for_unit 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_smoke_mode_sets_
env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_feature_flag_map
ping_sets_env 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_marker_passthrou
gh 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_integration_targ
et_retains_cov_when_no_report 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_target_p
rints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_speed_pr
ints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inner_test_env_d
isables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_verbose_and_fast
_timeout_env_behavior 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_report_mode_prin
ts_report_path_message 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_target 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_speed 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_segmented_run_injects_plugins_and_emits_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate 
tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_check_requi
red_env_raises_when_missing_env 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_cmd_happy_path_with_skips 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_runs_when_not_skipped 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_run_secrets
_scan_detects_simple_pattern 
tests/unit/application/cli/commands/test_testing_cmd.py::testing_cmd 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_basic_functionality 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_shows_expected_content 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_uses_cli_bridge 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_logging_configuration 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_script_paths_checked 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_output_formatting 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_quick_actions_displayed 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_performance_achievements 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_phase_tasks_completed 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_group_cha
nges_categorizes_and_orders 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_generate_
message_includes_rationale_and_files 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_minimal_returns_text 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_simple_highlight_false_returns_str 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_standard_with_markup_returns_panel_passthrough 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_dict_and_list 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_unsupported_type_falls_back 
tests/unit/application/cli/test_command_output_formatter.py::test_format_list_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_code_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_help_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_display_does_n
ot_raise 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_defaults 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_reads_yaml 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_is_exported 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_protocol_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_aliases_listed_in_all 
tests/unit/application/cli/test_long_running_progress.py::test_update_adapts_int
erval_and_checkpoints 
tests/unit/application/cli/test_long_running_progress.py::test_status_history_tr
acks_unique_status_changes 
tests/unit/application/cli/test_long_running_progress.py::test_summary_reflects_
fake_timeline_and_sanitizes_descriptions 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_updates_r
emap_and_short_circuit 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_completio
n_rolls_up_and_freezes_summary 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_checkpoin
t_spacing_respects_minimum 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_produces_deterministic_transcript 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_tracks_history_and_alias_renames 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_remains_deterministic_after_reload 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_stays_exported 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_direct_import_succeeds 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_upd
ate_thresholds_with_deterministic_clock 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_sub
task_flow_preserves_mappings_and_progress 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_run
_with_progress_completes_after_exception 
tests/unit/application/cli/test_output.py::TestOutputType::test_output_type_valu
es 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
contains_all_types 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
values 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_in
fo 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_su
ccess 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_er
ror 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_info 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_succe
ss 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_error
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_is_concrete_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_available_at_import_time 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_protocol_exists 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_inherits_correctly 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_module_reload_preserves_base_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_import_from_module_works_after_reload 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_instantiation 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_has_expected_methods 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_deterministic_tests_can_import_base 
tests/unit/application/cli/test_requirements_commands.py::test_wizard_cmd_back_n
avigation_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_gather_requiremen
ts_cmd_yaml_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_initialize_servic
es_configures_singletons 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_handles_empty_repository 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_renders_rich_table 
tests/unit/application/cli/test_requirements_commands.py::test_create_requiremen
t_invokes_service 
tests/unit/application/cli/test_requirements_gathering.py::test_gather_cmd_loggi
ng_exc_info_succeeds 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs 
tests/unit/application/cli/test_setup_wizard_textual.py::test_textual_and_cli_pa
yloads_match 
tests/unit/application/cli/test_setup_wizard_textual.py::test_requirements_wizar
d_supports_shortcut_navigation 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_planning_cmd_re
turns_structured_plan 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_defaults_when_missing 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_handles_invalid_json 
tests/unit/application/code_analysis/test_analyzer.py::test_analyze_code_simple 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_add_docstring_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_complex_transformations_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_extract_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_remove_unused_imports_and_variables_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_identifier_no_change 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_parameter_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_variable_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_validate_syntax_is_valid 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_complexity_and_readability_metrics_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_differentiate_selects_best_option_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_expand_implementation_options_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_refine_implementation_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_retrospect_code_quality_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_initialization_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_index_files_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_detect_languages_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_categorize_file_assigns_lists 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_infer_architecture_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_identify_components_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_requirements_spec_alignment_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_generate_health_report_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_analyze_maps_dependencies_and_structure 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_initialization_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_architecture_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_unknown 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_layers_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_layer_dependencies_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_check_architecture_violations_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_code_quality_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_improvement_opportunities_succeeds 
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py::test_sel
f_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_transformer.py::TestAstTransformer::te
st_record_change_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestRedundantAssignmen
tRemover::test_remove_redundant_assignments_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestStringLiteralOptim
izer::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeStyleTransform
er::test_improve_code_style_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_code_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_file_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_directory_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_find_python_files_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestSymbolUsageCounter
::test_count_symbol_usage_succeeds 
tests/unit/application/code_analysis/test_transformer_basic.py::test_optimize_st
ring_literals_simple 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_apply_doc
string_spec_inserts_function_docstring 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_met
hod_from_function_respects_method_type 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_cla
ss_from_functions_wraps_functions 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_to_dict 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_accepts_string_payload 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_cr
eate_team_stores_in_memory 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_consensus_outcome_normalizes
_participants_and_metadata 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_peer_review_consensus_error_
embeds_serialized_outcome 
tests/unit/application/collaboration/test_memory_utils_conversion.py::test_task_
round_trip_to_memory_item 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_colla
boration_payload_protocol_support 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_messa
ge_filter_rejects_invalid_input 
tests/unit/application/collaboration/test_message_protocol.py::test_message_filt
er_invalid_timestamp_raises 
tests/unit/application/collaboration/test_peer_review_store.py::test_store_in_me
mory_persists_peer_review_record 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_returns_review_decisions 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_failure_yields_error_decision 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_wraps_consensus_error_with_serialized_outcome 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_summar
ize_voting_result_persists_summary 
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py::test_identify_conflicts_detects_opposing_opinions 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_tie 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_winner 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_consensus_result_methods 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_contradictions 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_different_approaches 
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py::tes
t_peer_review_solution_excludes_author 
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py::te
st_delegate_subtasks_assigns_best_agent 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_html_documentation_extracts_sections 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_markdown_documentation_respects_heading_levels 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_convert_docstrings_to_chunks_builds_expected_metadata 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_version_key_supports_numeric_sorting_and_literals 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_prefers_vector_results 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_falls_back_to_metadata_items 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_basic_creation 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_phase_context 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_details 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery 
tests/unit/application/edrr/test_coordinator.py::test_run_micro_cycles_stops_aft
er_threshold 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure 
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::test_enhanced_dec
ide_next_phase_respects_auto_phase 
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py::test_collect_ph
ase_metrics_uses_stubbed_helpers 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
missing_memory_manager 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flushes_
on_success 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
errors 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flush_fa
ilure_does_not_raise 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_norma
lizes_outputs 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_missi
ng_manager_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_witho
ut_support_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_stores_context 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_uses_deep_copy 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_ignores_empty 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_enforces_dependencies 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_updates_state 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_consumes_manual_override 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_requires_auto_transitions 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_returns_none_for_final_phase 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ne
xt_phase_rejects_final_phase 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error 
tests/unit/application/edrr/test_recursion_termination.py::test_micro_cycle_resp
ects_depth_bounds 
tests/unit/application/edrr/test_recursion_termination.py::test_complexity_thres
hold_triggers_termination 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_sp
rint_planning_phase_constant 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_basic 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_empty 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_partial 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_sprint_retrospective_phase_constant 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_basic 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_empty 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_none 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_partial 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_positive_in
t_handles_out_of_range 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_threshold_c
lamps_invalid_values 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_respects_config 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_returns_none_when_missing 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_micro_cycle_conf
ig_sanitizes_values 
tests/unit/application/ingestion/test_ingestion_pure.py::test_is_artifact_change
d_respects_metadata_differences 
tests/unit/application/ingestion/test_ingestion_pure.py::test_identify_improveme
nt_areas_flags_missing_manifest_information 
tests/unit/application/ingestion/test_ingestion_pure.py::test_generate_recommend
ations_reflects_project_context 
tests/unit/application/ingestion/test_phases.py::test_run_expand_phase_populates
_artifacts 
tests/unit/application/ingestion/test_phases.py::test_run_differentiate_phase_us
es_structure 
tests/unit/application/llm/test_import_without_openai.py::test_import_openai_pro
vider_without_openai_succeeds 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_in
valid_response_raises_model_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_lazy_import 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_caching 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_getattr_delegation 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_call 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_getattr 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_list_downloaded_models 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_configure_default_client 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_exists 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_has_expected_attributes 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_prefixes_with_offline 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_with_context_concatenates 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
et_embedding_is_deterministic 
tests/unit/application/llm/test_openai_env_key_mock.py::test_openai_provider_use
s_mocked_env_key_without_network 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_succ
ess_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_stre
am_yields_tokens_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_inva
lid_response_raises_model_error 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_not_selected_when_flag_false 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_selected_when_flag_true 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_offlin
e_killswitch_overrides_explicit_selection 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault 
tests/unit/application/memory/test_chromadb_store.py::test_store_and_retrieve_wi
th_fallback 
tests/unit/application/memory/test_chromadb_store_typed.py::test_search_normaliz
es_serialized_rows 
tests/unit/application/memory/test_chromadb_store_typed.py::test_fallback_retrie
ve_uses_serialization_helpers 
tests/unit/application/memory/test_circuit_breaker.py::test_circuit_breaker_open
s_after_failures 
tests/unit/application/memory/test_circuit_breaker.py::test_registry_returns_sam
e_instance 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_without_vector_extension_falls_back 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_configures_hnsw_when_enabled 
tests/unit/application/memory/test_error_logger.py::test_log_error_enforces_max_
errors 
tests/unit/application/memory/test_error_logger.py::test_log_error_accepts_neste
d_context 
tests/unit/application/memory/test_error_logger.py::test_persist_errors_respects
_toggle 
tests/unit/application/memory/test_error_logger.py::test_get_recent_errors_and_s
ummary 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_semantic_robustness_testing 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_get_learning_statistics 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_analyze_behavioral_intent 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior 
tests/unit/application/memory/test_faiss_store.py::test_store_and_retrieve_round
_trip_preserves_metadata 
tests/unit/application/memory/test_faiss_store.py::test_transaction_commit_persi
sts_changes 
tests/unit/application/memory/test_faiss_store.py::test_transaction_rollback_res
tores_snapshot 
tests/unit/application/memory/test_faiss_store.py::test_similarity_search_and_st
ats_ignore_deleted_vectors 
tests/unit/application/memory/test_fast_in_memory_components.py::test_graph_memo
ry_adapter_in_memory_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_enhanced_g
raph_memory_adapter_edrr_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_man
ager_sync_hooks_fire 
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
saction_context_commit_and_rollback 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sys
tem_adapter_in_memory_components 
tests/unit/application/memory/test_fast_in_memory_components.py::test_fallback_s
tore_falls_back_on_failure 
tests/unit/application/memory/test_fast_in_memory_components.py::test_json_file_
store_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sna
pshot_save_and_load 
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_traverse_graph_depth_and_missing_nodes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_begin_tran
saction_tracks_and_cleans_up 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_commit_tra
nsaction_persists_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_rollback_t
ransaction_discards_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_get_all_it
ems_returns_everything 
tests/unit/application/memory/test_memory_manager.py::TestRouteQuery::test_route
_query_normalizes_context_mapping 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_regist
er_and_notify_sync_hook_succeeds 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_sync_h
ook_errors_are_logged 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
disabled_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
enabled_uses_adapter_and_store 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_various_backends 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_kuzu_init
ialization_and_fallback 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_lmdb_miss
ing_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_branches_execution 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_cache_and
_transaction_workflow 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_transacti
on_wrappers_raise_without_support 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_round_trip_preserves_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_from_row_handles_stringified_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_build
_memory_record_coerces_legacy_mapping 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_analyze_and_segment_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_multi_hop_reasoning 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_hybrid_llm_processing 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_contextual_prompts 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_integrate_and_validate_results 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_get_system_status 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_benchmark_against_research 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_validate_system_integrity 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_system_performance 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_memory_graph_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execution_learning_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_automata_metacognitive_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt 
tests/unit/application/memory/test_query_router.py::test_direct_query_and_vector
_branch 
tests/unit/application/memory/test_query_router.py::test_cross_store_query_group
s_results 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
tests/unit/application/memory/test_query_router.py::test_context_aware_and_route
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_returns_existing_identifier 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_generates_uuid 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_transactio
n_methods_are_noops 
tests/unit/application/memory/test_search_memory_fallback.py::test_search_memory
_fallback_without_vector_adapter_returns_results 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record 
tests/unit/application/memory/test_sync_manager_transactions.py::test_transactio
n_rollback_uses_normalized_snapshots 
tests/unit/application/memory/test_tiered_cache_termination.py::test_eviction_lo
op_terminates 
tests/unit/application/memory/test_tiered_cache_termination.py::test_preserves_t
yped_values 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_default_
provider_registration 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_optional
_provider_guard 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_edrr_coo
rdinator_delegates_to_helper 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_returns_result 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_logs_consensus_failure 
tests/unit/application/promises/test_agent_create_promise.py::test_create_promis
e_sets_metadata_and_parent_relationship 
tests/unit/application/promises/test_interface_not_implemented.py::test_promise_
interface_id_not_implemented 
tests/unit/application/promises/test_interface_pure.py::test_basic_promise_metad
ata_round_trip 
tests/unit/application/promises/test_interface_pure.py::test_then_on_fulfilled_p
romise_invokes_callback_immediately 
tests/unit/application/promises/test_interface_pure.py::test_catch_on_rejected_p
romise_yields_handler_result 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_success_rate_and_a
verage_feedback_are_computed_from_state 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_performance_score_
combines_success_and_feedback 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_round_trip_seriali
sation_preserves_variant_fields 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_reaches_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_logs_consensus_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_failure_stores_retrospect 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_receives_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_runs_on_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_non_text_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_invalid_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_assess_im
pact_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_argument_parsing_consensus_failure_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_assess_impact_recommendations_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_requirements_collects_dependencies 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_components_merges_sources 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_asse
ss_risk_level_accounts_for_priority 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_esti
mate_effort_scales_with_affected_entities 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_writes_json 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_cancelled 
tests/unit/application/requirements/test_interactions.py::test_gather_requiremen
ts_supports_backtracking 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_updat
e_requirement_uses_typed_dto_and_dialectical_hooks 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_delet
e_requirement_emits_retrospect_phase 
tests/unit/application/requirements/test_wizard.py::test_priority_and_constraint
s_persist_after_navigation 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_each_step 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_exc_info 
tests/unit/application/sprint/test_planning.py::test_map_requirements_to_plan_ex
tracts_fields 
tests/unit/application/test_documentation_fetcher.py::test_download_success_retu
rns_manifest 
tests/unit/application/test_documentation_fetcher.py::test_download_failure_retu
rns_false_manifest 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization_with_memory_port 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_nonexistent 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_info 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_clear_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_operations 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_expiration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_file_corruption 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_as_dict 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_with_docstring 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_ttl_configuration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_unicode_decode_error 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_os_error_handling 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_memory_storage_failure 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_with_extra 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_without_extra 
tests/unit/application/utils/test_extras_helper.py::test_require_optional_packag
e_wraps_importerror 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors 
tests/unit/cli/test_cli_help.py::test_cli_help_exits_zero_and_shows_summary 
tests/unit/cli/test_command_module_loading.py::test_command_modules_register_com
mands_and_build_app 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry 
tests/unit/cli/test_command_registry.py::test_enable_feature_not_top_level 
tests/unit/cli/test_completion_progress.py::test_completion_cmd_outputs_script_a
nd_progress 
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation 
tests/unit/cli/test_entry_points_help.py::test_console_scripts_declared 
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module 
tests/unit/cli/test_help_examples.py::test_get_command_help_includes_examples 
tests/unit/cli/test_help_examples.py::test_get_command_help_unknown_command 
tests/unit/cli/test_import_gating.py::test_import_devsynth_does_not_import_heavy
_optionals 
tests/unit/cli/test_import_gating.py::test_cli_entrypoint_lazy_imports 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_list 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_json 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug 
tests/unit/cli/test_mvu_commands.py::test_mvu_help_lists_subcommands 
tests/unit/cli/test_mvu_commands.py::test_mvu_init_creates_config_and_matches_sc
hema 
tests/unit/cli/test_mvuu_command_registration.py::test_mvuu_dashboard_command_re
gistered 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_generat
es_signed_telemetry 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_uses_li
ve_connectors 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_falls_b
ack_on_connector_error 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_force_l
ocal_mode 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests 
tests/unit/cli/test_version.py::test_cli_version_option_prints_version_and_exits
_zero 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env 
tests/unit/config/test_exception_handling.py::test_is_devsynth_managed_project_i
nvalid_toml_returns_false 
tests/unit/config/test_exception_handling.py::test_unified_config_exists_returns
_false_on_invalid_toml 
tests/unit/config/test_exception_handling.py::test_load_config_malformed_toml_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_load_config_invalid_values_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_set_default_memory_dir_handle
s_configuration_error 
tests/unit/config/test_feature_flag_defaults.py::test_feature_flags_default_off 
tests/unit/config/test_feature_flag_defaults.py::test_can_enable_known_feature_f
lag 
tests/unit/config/test_provider_env.py::test_parse_bool_truthy_and_falsy_cases 
tests/unit/config/test_provider_env.py::test_from_env_defaults_and_with_test_def
aults_sets_stub_and_offline 
tests/unit/config/test_provider_env.py::test_apply_to_env_respects_existing_lmst
udio_flag 
tests/unit/config/test_provider_env.py::test_as_dict_roundtrip_and_types 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_sets_e
xpected_vars 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_does_n
ot_override_explicit_lmstudio_flag 
tests/unit/config/test_provider_env_apply_and_parse.py::test_from_env_reads_curr
ent_environment 
tests/unit/config/test_provider_env_behavior.py::test_from_env_defaults_when_uns
et 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_overrid
es_to_safe_when_unset 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_respect
s_explicit_provider 
tests/unit/config/test_provider_env_behavior.py::test_apply_to_env_and_as_dict_r
oundtrip 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_parses_
true_and_false_variants 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_unrecog
nized_values_fall_back_to_defaults 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_as_dict_reflects
_values_and_with_test_defaults_sets_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_sets_offline_stub_and_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_respects_explicit_provider 
tests/unit/config/test_unified_loader.py::test_loads_from_pyproject_succeeds 
tests/unit/core/mvu/test_api.py::test_get_by_trace_id 
tests/unit/core/mvu/test_api.py::test_get_by_affected_path 
tests/unit/core/mvu/test_atomic_rewrite.py::test_cluster_commits_by_file 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_valid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_block 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_bad_traceid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_issue 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_mvuu_false 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_mvuu 
tests/unit/core/mvu/test_mvuu_schema_validation.py::test_mvuu_example_conforms_t
o_schema tests/unit/core/mvu/test_report.py::test_generate_report_markdown 
tests/unit/core/mvu/test_report.py::test_generate_report_html 
tests/unit/core/mvu/test_storage.py::test_format_mvuu_footer_contains_json 
tests/unit/core/mvu/test_storage.py::test_append_mvuu_footer_appends_block 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_accepts_vali
d 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_rejects_bad_
header 
tests/unit/core/mvu/test_validator.py::test_validate_affected_files_reports_mism
atches 
tests/unit/core/test_config_loader.py::test_core_config_normalizes_mvuu_invalid_
entries 
tests/unit/core/test_config_loader_json_types.py::test_load_config_supports_nest
ed_json_resources 
tests/unit/core/test_config_loader_json_types.py::test_environment_override_pres
erves_resources 
tests/unit/core/test_config_loader_json_types.py::test_core_config_rejects_exces
sively_deep_resources 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
tests/unit/core/test_config_loader_optional_deps.py::test_load_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_dump_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_save_global_config_han
dles_missing_yaml 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[15-True] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[16-False] 
tests/unit/core/test_config_loader_validation.py::test_load_yaml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_load_toml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_merges_source
s_without_mutating_resources 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_deterministic_fixtures.py::test_deterministic_seed_sets_env
_and_random_sequence 
tests/unit/core/test_deterministic_fixtures.py::test_mock_datetime_fixture_freez
es_time 
tests/unit/core/test_deterministic_fixtures.py::test_mock_uuid_fixture_returns_f
ixed_uuid tests/unit/core/test_mvu.py::test_schema_has_required_fields 
tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task 
tests/unit/deployment/test_deployment_scripts.py::test_bootstrap_script_exists 
tests/unit/deployment/test_deployment_scripts.py::test_health_check_script_exist
s 
tests/unit/deployment/test_enforcement.py::test_shell_scripts_enforce_non_root_a
nd_env_validation 
tests/unit/deployment/test_enforcement.py::test_docker_compose_enforces_user_and
_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint 
tests/unit/deployment/test_scripts_dir.py::test_scripts_bootstrap_exists 
tests/unit/deployment/test_scripts_dir.py::test_scripts_health_check_exists 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_noo
p_without_flag 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_rai
ses_for_root 
tests/unit/deployment/test_security_hardening.py::test_check_required_env_vars 
tests/unit/deployment/test_security_hardening.py::test_apply_secure_umask 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_invokes_he
lpers 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_raises_whe
n_env_missing 
tests/unit/devsynth/test_consensus.py::test_build_consensus_majority 
tests/unit/devsynth/test_consensus.py::test_build_consensus_no_consensus 
tests/unit/devsynth/test_consensus.py::test_build_consensus_tracks_unique_dissen
ting_options 
tests/unit/devsynth/test_consensus.py::test_build_consensus_invalid_threshold 
tests/unit/devsynth/test_consensus.py::test_build_consensus_empty_votes 
tests/unit/devsynth/test_fallback_reliability.py::test_named_condition_callbacks
_record_metrics 
tests/unit/devsynth/test_fallback_reliability.py::test_circuit_breaker_open_hook
_and_metrics 
tests/unit/devsynth/test_logger.py::test_log_exception_object_normalized 
tests/unit/devsynth/test_logger.py::test_log_true_uses_current_exception 
tests/unit/devsynth/test_logger.py::test_log_invalid_exc_info_dropped 
tests/unit/devsynth/test_metrics.py::test_memory_metrics_increment_and_reset 
tests/unit/devsynth/test_metrics.py::test_provider_and_retry_metrics 
tests/unit/devsynth/test_metrics.py::test_dashboard_metrics 
tests/unit/devsynth/test_metrics.py::test_inc_memory_unhashable_raises_type_erro
r tests/unit/devsynth/test_simple_addition.py::test_add_returns_sum 
tests/unit/devsynth/test_simple_addition.py::test_add_raises_type_error_on_non_n
umeric 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_tests_but_
not_docs 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_docs_but_n
ot_tests 
tests/unit/domain/interfaces/test_interfaces.py::test_cli_interface_raises_not_i
mplemented 
tests/unit/domain/interfaces/test_interfaces.py::test_file_analysis_result_raise
s_not_implemented 
tests/unit/domain/interfaces/test_interfaces.py::test_onnx_runtime_raises_not_im
plemented 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_none_values 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_existing_values 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_initial
ization 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_seriali
zation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_creation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_content_has
h_generation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_serializati
on_roundtrip 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_link_manage
ment 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_salience_up
date 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_lifecycle_m
anagement 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_cognitive_t
ype_properties 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_creati
on 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_serial
ization 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_defa
ult_standard 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_mono
repo 
tests/unit/domain/models/test_project.py::test_artifact_metadata_defaults_to_sep
arate_dicts 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_init
ialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_str_
representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_repr
_representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_project_m
odel_initialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_structure_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_sta
ndard_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_mon
orepo_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
act_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
acts_by_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_relat
ed_artifacts_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_artifact_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_to_dict_s
ucceeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_add_agent_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_dialectical_hook_invok
ed_on_add_solution_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_rotate_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_empty_team_
succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_agent_by_role_succ
eeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ws
de_dataclass_initialises_timestamps 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_te
am_post_init_restores_missing_attributes 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_inserts_validation 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_noop_when_already_secure 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_error_hand
ling_wraps_body 
tests/unit/domain/models/test_wsde_decision_making.py::test_calculate_idea_simil
arity_overlap 
tests/unit/domain/models/test_wsde_decision_making.py::test_evaluate_options_ran
ks_by_weighted_score 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_similar_entries 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_handles_agent_failures 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_limits_count 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_duplicates_with_strict_threshold 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_antithe
sis_returns_typed_draft 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_categorize_criti
ques_by_domain_returns_tuples 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_synthes
is_returns_resolution_plan 
tests/unit/domain/models/test_wsde_dialectical_typing.py::test_dialectical_seque
nce_round_trip 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_dialectical_tas
k_serialization_round_trip 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_dynamic_role_reassignment_selects_expert_primus_succeeds 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_categorize_crit
iques_by_domain_groups_terms 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_identify_domain
_conflicts_finds_performance_security 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_generates_synthesis 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_requires_solution 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_combines_solutions 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_requires_solutions 
tests/unit/domain/models/test_wsde_knowledge.py::test_get_task_id_uses_existing_
id 
tests/unit/domain/models/test_wsde_knowledge.py::test_identify_relevant_knowledg
e_matches_keywords 
tests/unit/domain/models/test_wsde_knowledge.py::test_knowledge_graph_insights_p
arses_payload 
tests/unit/domain/models/test_wsde_knowledge.py::test_integrate_knowledge_builds
_summary 
tests/unit/domain/models/test_wsde_knowledge.py::test_generate_improvement_sugge
stions_deduplicates_entries 
tests/unit/domain/models/test_wsde_roles_personas.py::test_enumerate_research_pe
rsonas_includes_overlays 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Synthesizer] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Contrarian] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Fact Checker] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Planner] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Moderator] 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_s
cores_requirements 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_h
ighlights_gaps 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_identifies_best_solution 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_handles_empty 
tests/unit/domain/models/test_wsde_strategies.py::test_weighted_voting_prefers_d
omain_expertise 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores 
tests/unit/domain/models/test_wsde_strategies.py::test_multidisciplinary_analysi
s_structures_results 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_add_agent_succeed
s 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_rotate_primus_suc
ceeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_empty_
team_succeeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_assign_roles_succ
eeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds 
tests/unit/domain/models/test_wsde_utils.py::test_send_message_invokes_protocol 
tests/unit/domain/models/test_wsde_utils.py::test_broadcast_message_excludes_sen
der tests/unit/domain/models/test_wsde_utils.py::test_get_messages_uses_protocol
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_creates_cy
cle 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_collects_f
eedback 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_handles_mi
ssing_peer_review 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks 
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_logs_warni
ng_on_failure 
tests/unit/domain/models/test_wsde_voting_logic.py::test_deterministic_voting_wi
th_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_determi
nistic_with_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_tie_is_
fair 
tests/unit/domain/models/test_wsde_voting_logic.py::test_handle_tied_vote_produc
es_consensus_result 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_analyzer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_transformer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_simple_file_analysis 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches 
tests/unit/domain/test_wsde_facade.py::test_summarize_consensus_result_outputs_e
xpected_sections 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_peer_review_cross_stor
e_sync_succeeds 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_mvu_helpers_cover_modu
le 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_initial_selection_prefe
rs_unused_agent_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_assign_roles_for_phase_
rotates_after_all_primus_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_first_time_selection_prior
itizes_unused_agents_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_resets_after_all_
have_served_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_when_all_agents_u
sed_resets_flags_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_by_expertise_prefers_doc
umentation_agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotation_resets_after_all_have_served_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_prefers_doc_expertise_vi
a_config_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotate_primus_resets_usage_flags_and_r
ole_map_succeeds 
tests/unit/domain/test_wsde_team.py::test_multiple_task_cycles_reset_primus_flag
s_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds tests/unit/domain/test_wsde_team.py::test_force_wsde_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds 
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie 
tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_rounds 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_primus_t
ie 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_random 
tests/unit/fallback/test_retry_counts.py::test_retry_count_metrics 
tests/unit/fallback/test_retry_counts.py::test_retry_only_network_errors 
tests/unit/fallback/test_retry_predicates.py::test_retry_predicate_triggers_retr
y 
tests/unit/fallback/test_retry_predicates.py::test_integer_predicate_records_met
rics 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_add
_agent_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_agent_type_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_team_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_missing_parameters_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_no_agents_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_type_not_found_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_execution_error_raises_error 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_type_enum_s
ucceeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_init
ialization_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_with
_parameters_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_mvp_capabilities_
succeeds 
tests/unit/general/test_agent_system.py::test_agent_state_keys_has_expected 
tests/unit/general/test_agent_system.py::test_process_input_node_success_is_vali
d 
tests/unit/general/test_agent_system.py::test_process_input_node_empty_input_suc
ceeds 
tests/unit/general/test_agent_system.py::test_process_input_node_adds_tool_list 
tests/unit/general/test_agent_system.py::test_llm_call_node_success_succeeds 
tests/unit/general/test_agent_system.py::test_llm_call_node_llm_failure_fails 
tests/unit/general/test_agent_system.py::test_llm_call_node_skip_on_prior_error_
raises_error 
tests/unit/general/test_agent_system.py::test_llm_call_node_missing_processed_in
put_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_success_is_valid
tests/unit/general/test_agent_system.py::test_parse_output_node_missing_llm_resp
onse_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_skip_on_prior_er
ror_raises_error 
tests/unit/general/test_agent_system.py::test_base_agent_graph_compiles_raises_e
rror 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error 
tests/unit/general/test_api.py::test_verify_token_rejects_invalid_token 
tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token 
tests/unit/general/test_api_health.py::test_health_endpoint_succeeds 
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_help_shows_co
mmand 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_disabled_exit
s_with_guidance 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_enabled_dry_r
un_succeeds 
tests/unit/general/test_backend_resource_flags.py::test_backend_flag_mapping_res
pects_env_vars 
tests/unit/general/test_backend_resource_flags.py::test_rdflib_env_mapping_disab
les_rdflib 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
handles_partial_spec 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error 
tests/unit/general/test_base.py::test_dummy_adapter_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_initiali
zation_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_delete_succee
ds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_persistence_s
ucceeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_exact_
match_matches_expected 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_semant
ic_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_store_and_ret
rieve_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_token_usage_s
ucceeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_lists_comm
ands_succeeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_omits_depr
ecated_aliases_succeeds 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_provider_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_file_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_co
de_analysis_implementation_succeeds 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_fi
le_analysis_implementation_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_code_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_directo
ry_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_file_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_project_structu
re_metrics_succeeds 
tests/unit/general/test_config_loader.py::test_load_yaml_config_succeeds 
tests/unit/general/test_config_loader.py::test_load_pyproject_toml_succeeds 
tests/unit/general/test_config_loader.py::test_autocomplete_succeeds 
tests/unit/general/test_config_loader.py::test_save_persists_version_succeeds 
tests/unit/general/test_config_loader.py::test_version_mismatch_logs_warning_mat
ches_expected 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_default_values_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_from_environment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_llm_set
tings_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[True-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[TRUE-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[False-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[FALSE-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_file_not_found_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_with_dotenv_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_invalid_sec
urity_boolean_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_empty_opena
i_api_key_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_defaults_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_from_env_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_embedd
ed_attribute_lookup_succeeds 
tests/unit/general/test_core_config_loader.py::test_precedence_env_over_project_
over_global_succeeds 
tests/unit/general/test_core_config_loader.py::test_load_toml_project_succeeds 
tests/unit/general/test_core_config_loader.py::test_save_global_config_yaml_succ
eeds tests/unit/general/test_core_values.py::test_load_core_values_succeeds 
tests/unit/general/test_core_values.py::test_find_value_conflicts_succeeds 
tests/unit/general/test_core_values.py::test_check_report_for_value_conflicts_su
cceeds 
tests/unit/general/test_core_workflows.py::test_filter_args_removes_none_values_
succeeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_gather_requirements_creates_file
_succeeds 
tests/unit/general/test_core_workflows.py::test_workflow_manager_singleton_succe
eds 
tests/unit/general/test_delegate_task_disabled.py::test_delegate_task_collaborat
ion_disabled_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_c
reate_session_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_p
rocess_message_succeeds 
tests/unit/general/test_documentation_fetcher.py::test_fetcher_initialization_su
cceeds tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled 
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency 
tests/unit/general/test_dpg_flag.py::test_dpg_command_enabled 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_no_input_raises_e
rror 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_missing_
raises_error 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_success_
succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_prompt_success_su
cceeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manual_succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_custom_bridge_has
_expected 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_error_handling_ra
ises_error 
tests/unit/general/test_edrr_manifest_string.py::test_start_cycle_from_manifest_
string_succeeds 
tests/unit/general/test_exception_logging.py::test_log_exception_emits_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_message
_only_succeeds 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_error_c
ode_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_details
_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_to_dict_succeeds 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_validation_erro
r_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_configuration_e
rror_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_command_error_r
aises_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_internal_error_rai
ses_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_resource_exhausted
_error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_timeout_
error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_memory_adapter_er
ror_raises_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_agent_error_raises
_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_workflow_error_suc
ceeds 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_dialectical_reason
ing_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_error
_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_state
_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_ingestion_err
or_raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_memory_port_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_provider_port_error_
raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_agent_port_error_rai
ses_error 
tests/unit/general/test_fallback_utils.py::test_bulkhead_limits_concurrency 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_defau
lts_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_custo
m_manifest_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_dry_run_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_validate_o
nly_is_valid 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_verbose_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_forwards_a
uto_phase_flag 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_non_intera
ctive_flag_sets_env 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_env_var_en
ables_non_interactive 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_priority_u
pdates_config 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_manifest_e
rror_raises_error 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_ingestion_
error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_success_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_file_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_schema_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_validation_failed_fails 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_yaml
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_file
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_expand_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_differentiate_phase_has_
expected 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_refine_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected 
tests/unit/general/test_ingestion_edrr_integration.py::test_run_ingestion_invoke
s_edrr_phases_succeeds 
tests/unit/general/test_ingestion_type_hints.py::test_ingestion_type_hints_raise
s_error 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_update_succee
ds 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_prune_succeed
s 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_no_config_suc
ceeds 
tests/unit/general/test_inspect_config_cmd.py::test_analyze_project_structure_re
turns_directories 
tests/unit/general/test_inspect_config_cmd.py::test_compare_with_manifest_return
s_differences 
tests/unit/general/test_inspect_config_cmd.py::test_update_manifest_adds_directo
ry 
tests/unit/general/test_isolation.py::TestIsolation::test_devsynth_dir_isolation
_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_global_config_isolatio
n_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_memory_path_isolation_
succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_no_file_logging_preven
ts_directory_creation_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_path_redirection_in_te
st_environment_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_comprehensive_isolatio
n_succeeds 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_tmp_p
ath_fixture 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_netwo
rk_keyword 
tests/unit/general/test_kuzu_adapter.py::test_store_and_retrieve_vector_succeeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_succeeds 
tests/unit/general/test_kuzu_adapter.py::test_persistence_between_instances_succ
eeds 
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_without_numpy_su
cceeds 
tests/unit/general/test_kuzu_embedded_missing.py::test_ephemeral_kuzu_store_init
ialises_without_kuzu_embedded 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_creation_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_to_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_from_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_checkpoint_path_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_not_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_put_checkpoint_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
create_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
add_step_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
execute_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_save_and_get_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_list_workflows_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_registration 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_configuration_loading 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_initialization_with_defaults 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_environment_variable_handling 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_config_file_integration 
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url 
tests/unit/general/test_logger.py::test_configure_logging_creates_rotating_handl
er tests/unit/general/test_logger.py::test_dev_synth_logger_normalizes_exc_info 
tests/unit/general/test_logging_setup.py::test_log_records_include_request_conte
xt_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_passes_through_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_true_uses_current_except
ion 
tests/unit/general/test_logging_setup.py::test_extra_kwargs_and_reserved_keys_sa
fely_handled 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_idem
potent_no_duplicate_handlers 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_thre
ad_safe 
tests/unit/general/test_logging_setup_idempotent.py::test_no_file_logging_toggle
_prevents_file_handler 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_enu
m_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_ini
tialization_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_wit
h_metadata_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_ali
ases 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_typ
e_alias 
tests/unit/general/test_memory_store.py::test_memory_store_abstract_methods_succ
eeds 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_persistence_su
cceeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_token_usage_su
cceeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_add_and
_get_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_clear_c
ontext_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_get_ful
l_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_add
_and_get_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_cle
ar_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_full_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_relevant_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_per
sistence_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_tok
en_usage_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_file_bas
ed_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_in_memor
y_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_token_us
age_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_with_chromadb_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_without_vector_store_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_memory_and_vector_store_integration_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_context_manager_with_chromadb_succeeds 
tests/unit/general/test_methodology_logging.py::test_phase_timeout_logs_warning_
succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_bulk_add_agents_succeeds 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_success 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_failure 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_combines_streams 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_returns_exit_code 
tests/unit/general/test_mvu_init_cmd.py::test_mvu_init_cmd_creates_file 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure 
tests/unit/general/test_mvuu_dashboard_cli.py::test_mvuu_dashboard_help_succeeds
tests/unit/general/test_mypy_config.py::test_mypy_configuration_raises_error 
tests/unit/general/test_mypy_config.py::test_mypy_project_configuration_raises_e
rror 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_ensure_path_exists_respects_no_file_logging_succeeds 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_settings_respects_no_file_logging_succeeds 
tests/unit/general/test_onnx_port.py::test_onnx_port_load_and_run_succeeds 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds 
tests/unit/general/test_path_restrictions.py::test_configure_logging_within_proj
ect_dir_succeeds 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds 
tests/unit/general/test_primus_selection.py::test_highest_expertise_score_become
s_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_prioritizes_agents_who_have_no
t_served_as_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds 
tests/unit/general/test_primus_selection.py::test_rotation_resets_after_all_agen
ts_served_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_default_ma
nifest_returned_when_missing_returns_expected_result 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_in
itialization_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_di
rect_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_error_raises_error 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_agent_initializ
ation_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_regi
stration_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_and_fulfillment_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_unauthorized_ac
cess_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_not_
found_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_get_available_c
apabilities_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgentMixin::test_mixin_with
_custom_agent_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_initial_state_succe
eds 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_fulfilled_succ
eeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_rejected_succe
eds tests/unit/general/test_promise_system.py::TestPromise::test_catch_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_chaining_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_error_propagation_r
aises_error 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_value_stati
c_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_with_static_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_with_rejection_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_race_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_metadata_succeeds 
tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup 
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_chat_
models_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_diale
ctical_reasoning_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_impac
t_assessment_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_change_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_model_succeeds 
tests/unit/general/test_requirement_repository_interface.py::test_requirement_re
pository_interface_crud 
tests/unit/general/test_requirement_repository_port_interface.py::test_requireme
nt_repository_port_is_abstract 
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
uirement_port_methods_raise_not_implemented 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_app
rove_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_cre
ate_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_del
ete_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_rej
ect_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_upd
ate_requirement_succeeds 
tests/unit/general/test_resource_markers.py::test_is_lmstudio_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_codebase_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds 
tests/unit/general/test_resource_markers.py::test_is_resource_available_succeeds
tests/unit/general/test_resource_markers.py::test_with_resource_marker_succeeds 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds 
tests/unit/general/test_retry_failure_scenarios.py::test_named_retry_condition_a
borts_and_records_metrics 
tests/unit/general/test_retry_failure_scenarios.py::test_circuit_breaker_open_re
cords_abort_metrics 
tests/unit/general/test_speed_option.py::test_speed_option_recognized 
tests/unit/general/test_sync_manager_persistence.py::test_sync_manager_persists_
to_all_stores 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_templat
es_exist_in_temp_location_succeeds 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_can_use
_template_to_create_test_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_analyz
e_commit_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_calcul
ate_metrics_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_genera
te_metrics_report_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_get_co
mmit_history_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_main_s
ucceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_conversat
ion_tokens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_message_t
okens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_tokens_su
cceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_ensure_token_li
mit_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_fallback_tokeni
zer_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_prune_conversat
ion_succeeds 
tests/unit/general/test_unified_agent_code_prompt.py::test_process_code_task_inc
ludes_language_and_paradigm_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_yaml_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_pyproject_succe
eds 
tests/unit/general/test_unified_config_loader.py::test_save_and_exists_succeeds 
tests/unit/general/test_unified_config_loader.py::test_missing_files_succeeds 
tests/unit/general/test_unified_config_loader.py::test_version_mismatch_warning_
succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_yaml
_succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_pypr
oject_succeeds tests/unit/general/test_unit_cli_commands.py::test_cmd 
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds 
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_create_workflow_f
or_command_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_s
ucceeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_f
ailure_fails 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_h
uman_intervention_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
atus_enum_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
ep_initialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_in
itialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_wi
th_steps_succeeds 
tests/unit/general/test_wsde_dynamic_roles.py::test_assign_roles_for_phase_selec
ts_primus_by_expertise_has_expected 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_initialization_s
ucceeds 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_with_custom_valu
es_succeeds 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_wsde_team_init
ialization_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_add_agent_succ
eeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_rotate_primus_
succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_primus_suc
ceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_s
ucceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
by_expertise_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_autonomous_col
laboration_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds 
tests/unit/infrastructure/test_test_infrastructure_sanity.py::test_global_test_i
solation_sets_env_and_dirs 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_and_query_su
ccess 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_disabled_by_
flag 
tests/unit/integrations/test_autoresearch_client.py::test_query_failure_falls_ba
ck 
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_initialization 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_record_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_within_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_exceeds_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_prune_old_requests 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_multiple_clients 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_health_en
dpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_metrics_e
ndpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_init_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_gather_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_synthesiz
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_spec_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_code_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_doctor_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_edrr_cycl
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestRouter::test_router_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimitingIntegration::tes
t_rate_limiting_logic_integration 
tests/unit/interface/test_agentapi_enhanced.py::TestErrorHandling::test_error_re
sponse_structure 
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
quest_models_validation 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_answers_a
nd_defaults 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_confirm_c
hoice_coerces_booleans 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_progress_tr
acks_subtasks 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_rate_limit_
blocks_abusive_clients 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_allow
s_after_window 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_raise
s_when_exceeded 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_records_subtasks 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_normalizes_string_advances 
tests/unit/interface/test_api_endpoints.py::test_enhanced_rate_limit_state_track
s_buckets 
tests/unit/interface/test_api_endpoints.py::test_enhanced_metrics_snapshot_typed
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error 
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_sanitize_
output_succeeds 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_ini
t_with_bad_description_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_upd
ate_with_bad_inputs_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_sub
tasks_with_bad_inputs_use_fallbacks 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_ask_question_us
es_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_confirm_choice_
uses_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_prompt_fallback
_to_rich 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_noninteractive_re
turns_defaults_and_logs 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches 
tests/unit/interface/test_command_output.py::test_format_and_display_message 
tests/unit/interface/test_command_output.py::test_format_error_suggestions 
tests/unit/interface/test_command_output.py::test_list_and_structured_outputs 
tests/unit/interface/test_command_output.py::test_set_console 
tests/unit/interface/test_dpg_ui.py::test_all_buttons_trigger_callbacks_and_prog
ress tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog 
tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog_error 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_actionable_error_suggestion_str_includes_details 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_format_error_wraps_with_footer 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_defaul
t_file 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_specif
ied_file 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_invokes_strea
mlit tests/unit/interface/test_mvuu_dashboard.py::test_require_streamlit_raises 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_snaps
hot 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_witho
ut_optional_sections 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_with_overlays
_loads_telemetry 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_pointer_legacy_env 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_secret_falls_back_to
_legacy 
tests/unit/interface/test_mvuu_dashboard.py::test_resolve_telemetry_path_prefers
_legacy 
tests/unit/interface/test_nicegui_bridge.py::test_session_storage_roundtrip 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_notifies_and_re
cords 
tests/unit/interface/test_nicegui_bridge.py::test_progress_indicator_updates_and
_completes 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_falls_back_with
out_nicegui 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_update 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_complete 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_create_progress 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_main_function
_exists 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_json_yaml_with_and_without_console 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_table_fallback_and_empty_list 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_rich_renderables 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_sanitize_outp
ut_delegates_and_handles_edge_cases 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[ERROR: Disk failure-error] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Task completed successfully-success] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[INFO: FYI-info] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Routine update-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_format_messag
e_applies_status_styles 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_highl
ight_branch_emits_panel 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_witho
ut_console_raises_value_error 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_format_
message_error_styles_and_escapes_markup 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_markdow
n_branch_sanitizes_hyperlinks 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_table_b
ranch_sanitizes_script_links 
tests/unit/interface/test_output_formatter_fallbacks.py::test_table_format_falls
_back_to_text_for_nontabular_inputs 
tests/unit/interface/test_output_formatter_fallbacks.py::test_rich_format_select
s_renderables_for_data_shapes 
tests/unit/interface/test_output_formatter_fallbacks.py::test_list_of_dicts_tabl
e_renders_missing_and_complex_values 
tests/unit/interface/test_output_formatter_fallbacks.py::test_set_format_options
_and_command_output_overrides 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_mark
down_handles_nested_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_to_mark
down_handles_mixed_items 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_tabl
e_serializes_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_of_dict
s_to_table_handles_missing_keys 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_table
_and_list_preserve_sanitized_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_command_outp
ut_unknown_extension_and_highlight_panel 
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds 
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
ed_protocol 
tests/unit/interface/test_progress_helpers.py::test_subtask_snapshot_typed_struc
ture 
tests/unit/interface/test_progress_utils.py::test_progress_manager_create_get_co
mplete_and_context_manager 
tests/unit/interface/test_progress_utils.py::test_progress_manager_track_updates
_on_item_and_slice 
tests/unit/interface/test_progress_utils.py::test_progress_indicator_context_man
ager_completes 
tests/unit/interface/test_progress_utils.py::test_step_progress_sequencing_and_c
omplete 
tests/unit/interface/test_progress_utils.py::test_create_and_track_progress_help
ers_use_manager 
tests/unit/interface/test_progress_utils.py::test_progress_tracker_forced_update
_and_complete 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_prefers_di
alog_selection 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_validates_
input 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_multi_select_re
turns_checkbox_choices 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_produces_timeline_snapshot 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_merges_extended_metadata 
tests/unit/interface/test_research_telemetry.py::test_merge_extended_metadata_in
to_payload_appends_sections 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_invokes_connectors 
tests/unit/interface/test_research_telemetry.py::test_signature_roundtrip_valida
tes 
tests/unit/interface/test_research_telemetry.py::test_signature_failure_with_wro
ng_secret 
tests/unit/interface/test_textual_ux_bridge.py::test_question_and_display_intera
ctions_are_recorded 
tests/unit/interface/test_textual_ux_bridge.py::test_confirm_choice_falls_back_t
o_default 
tests/unit/interface/test_textual_ux_bridge.py::test_progress_updates_capture_ne
sted_subtasks 
tests/unit/interface/test_textual_ux_bridge.py::test_capabilities_reflect_textua
l_availability 
tests/unit/interface/test_textual_ux_bridge.py::test_require_textual_guard 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_enabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_disabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_backward_compatib
ility_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_handle_error_defa
ult_implementation 
tests/unit/interface/test_ux_bridge_coverage.py::test_progress_indicator_context
_manager 
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_fallback_i
mport tests/unit/interface/test_uxbridge_aliases.py::test_function 
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants 
tests/unit/interface/test_webui_bridge_cli_parity.py::test_webui_bridge_matches_
cli_prompt_defaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_helpers_normal
ize_mixed_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_prompt_helpers_echo_d
efaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_append
s_documentation_links 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_require_streamlit_cac
hes_and_guides 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_clamps_handle_
invalid_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_and_pr
ogress_use_formatter 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_highli
ght_falls_back_to_write 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_uses
_cached_stub 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_impo
rts_when_missing 
tests/unit/interface/test_webui_bridge_handshake.py::test_adjust_wizard_step_han
dles_invalid_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_routes_
messages_and_sanitizes 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_error_b
ranch_records_message 
tests/unit/interface/test_webui_bridge_handshake.py::test_bridge_prompt_helpers_
return_defaults 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
invalid_total_defaults_to_zero 
tests/unit/interface/test_webui_bridge_normalize.py::test_progress_indicator_rej
ects_missing_parent 
tests/unit/interface/test_webui_bridge_normalize.py::test_display_result_routes_
messages_to_streamlit 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_upda
te_paths 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations 
tests/unit/interface/test_webui_bridge_progress.py::test_require_streamlit_failu
re 
tests/unit/interface/test_webui_bridge_progress.py::test_adjust_wizard_step_edge
s 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_display_re
sult_routes_and_sanitizes 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_session_ac
cess_wrappers 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_prompt_ali
ases_and_progress 
tests/unit/interface/test_webui_bridge_progress.py::test_normalize_wizard_step_v
aried_inputs 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_require_stream
lit_raises 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_progress_indic
ator_status_transitions 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_handshake
_routes_to_streamlit 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_error_rou
te_sanitizes_output 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_respects_
sanitization_flag 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_highlight
_routes_to_info 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_success_r
outes_to_success 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_require_streamlit
_missing_dependency_surfaces_install_guidance 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_navigation
_normalization_matches_state_invariants 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_prompt_defaults_a
lign_with_uxbridge_contract 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_display_result_ch
annels_respect_output_formatter_contract 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_get_wiza
rd_manager_uses_session_state 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_session_
helpers_delegate 
tests/unit/interface/test_webui_bridge_targeted.py::test_adjust_wizard_step_inva
lid_direction_keeps_bounds 
tests/unit/interface/test_webui_bridge_targeted.py::test_normalize_wizard_step_h
andles_strings 
tests/unit/interface/test_webui_bridge_targeted.py::test_question_and_confirmati
on_defaults 
tests/unit/interface/test_webui_bridge_targeted.py::test_display_result_highligh
t_routes_to_info 
tests/unit/interface/test_webui_bridge_targeted.py::test_create_progress_cycles_
statuses 
tests/unit/interface/test_webui_bridge_targeted.py::test_session_helpers_delegat
e_to_state_access 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_requ
ires_session_state 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_nested_completion_and_sanitization 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_wizard_na
vigation_and_display_fallback 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_default_s
tatus_thresholds 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_updates_and_completion 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_nested_su
btask_lifecycle 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_display_r
esult_routes_by_type 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_get_wizar
d_manager_and_create 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute 
tests/unit/interface/test_webui_commands.py::test_cli_returns_none_when_missing 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_pass_thr
ough 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-File not found] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Permission denied] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Invalid value] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Missing key] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Type error] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_generic_
exception 
tests/unit/interface/test_webui_commands.py::test_cli_uses_cli_module_when_avail
able 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_reraises
_devsynth_error 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3] 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key &amp;#x27;api&amp;#x27;-key_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-] 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: File not found: 
config.yaml-Make sure the file exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Permission denied: 
secrets.env-necessary permissions] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Invalid value: bad 
input-Please check your input] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Missing key: 
&amp;#x27;api_key&amp;#x27;-Verify that the referenced key exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Type error: wrong type-Check
that all inputs] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_normalize_ste
p_logs_warning_on_invalid_value 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_adjust_step_w
arns_on_invalid_direction 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors 
tests/unit/interface/test_webui_progress_time.py::test_update_records_time 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/interface/test_webui_rendering_module.py::test_validate_requirements_
step_requires_fields 
tests/unit/interface/test_webui_rendering_module.py::test_handle_requirements_na
vigation_cancel_clears_state 
tests/unit/interface/test_webui_rendering_module.py::test_save_requirements_clea
rs_temporary_keys 
tests/unit/interface/test_webui_rendering_progress.py::test_gather_wizard_render
s_cli_summary 
tests/unit/interface/test_webui_rendering_progress.py::test_render_progress_summ
ary_prefers_checkpoint_eta_strings 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist 
tests/unit/interface/test_webui_routing.py::test_router_uses_session_state 
tests/unit/interface/test_webui_routing.py::test_router_resets_invalid_selection
tests/unit/interface/test_webui_routing.py::test_router_handles_sidebar_exceptio
n 
tests/unit/interface/test_webui_routing.py::test_router_surfaces_page_exception 
tests/unit/interface/test_webui_routing.py::test_router_requires_pages 
tests/unit/interface/test_webui_routing.py::test_router_honors_explicit_default 
tests/unit/interface/test_webui_routing.py::test_router_reports_missing_page_han
dler 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_main_block_succeeds 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_simulatio
n_sanitises_nested_tasks 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_require_s
treamlit_guidance 
tests/unit/interface/test_webui_state_errors.py::test_clear_reraises_after_loggi
ng 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_require_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_progre
ss_indicator_nested_lifecycle_and_statuses 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[0-0-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[10-100-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[25-100-Processing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[50-100-Halfway there...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[80-100-Almost done...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[99-100-Finalizing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[100-100-Complete] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_wizard_helpe
rs_clamp_malformed_inputs 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_valid_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_default_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_auto_model_selection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_custom_port 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_lmstudio_unavailable 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_precedence 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_counting_integration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_limit_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_c
ircuit_breaker_initialization 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_r
etry_logic_configuration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_valid_config 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_environment_variable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_without_api_key_raises_error 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_custom_base_url 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_openai_client_unavailable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_with_defaults 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_precedence 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_counting_integration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_limit_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_circu
it_breaker_initialization 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_logic_configuration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_metrics_
collection_setup 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_telemetr
y_emission 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_message_args_
and_mappings 
tests/unit/logging/test_logging_setup.py::test_redact_filter_property_loop_prese
rves_inputs 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_attaches_c
ontext 
tests/unit/logging/test_logging_setup.py::test_json_formatter_serializes_request
_context 
tests/unit/logging/test_logging_setup.py::test_redaction_in_message_and_payload 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_injects_fi
elds_and_clears 
tests/unit/logging/test_logging_setup.py::test_jsonformatter_includes_exception_
block 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_no_file_l
ogging 
tests/unit/logging/test_logging_setup.py::test_get_log_dir_and_file_use_env_over
rides 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_uses_project_dir_f
or_relative_path 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_under_te
st_project_dir 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_absolute
_outside_home 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_project_d
ir_when_file_logging_disabled 
tests/unit/logging/test_logging_setup.py::test_configure_logging_redirects_home_
and_disables_file_handler 
tests/unit/logging/test_logging_setup.py::test_short_secret_not_redacted 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_merges_and_fi
lters_kwargs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_table_normali
zation 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_does_not_muta
te_extra_inputs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_normalizes_tr
uthy_exc_info 
tests/unit/logging/test_logging_setup.py::test_configure_logging_console_only_us
es_caplog 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_secret_tokens
_via_caplog 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_handles_missing_
log_file_path 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_emits_structured
_extras_with_context 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_secrets_f
ilter_masks_values 
tests/unit/logging/test_logging_setup_additional_paths.py::test_json_formatter_i
ncludes_context_and_extras 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_respects_project_dir 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_skips_creation_when_disabled 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_warns_when_creation_fails 
tests/unit/logging/test_logging_setup_additional_paths.py::test_devsynth_logger_
filters_reserved_extra_keys 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_filter_ma
sks_args_and_payload 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_provis
ions_json_file_handler 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_consol
e_only_mode 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_idempo
tent_with_identical_configuration 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_e
xplicit_level_overrides_env 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_j
son_handler_writes_structured_output 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_r
econfigures_console_only_toggle 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_idempotent_with_identical_settings 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_preserves_filters_on_reconfigure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_falls_back_to_console_on_file_handler_failure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_create_dir_guard_preserves_console_only_mode 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_cli_context_wires_consol
e_and_json_file_handlers 
tests/unit/logging/test_logging_setup_contexts.py::test_test_context_redirects_a
nd_supports_console_only_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_create_dir_toggle_disabl
es_json_file_handler 
tests/unit/logging/test_logging_setup_contexts.py::test_console_and_json_handler
s_report_consistent_payloads 
tests/unit/logging/test_logging_setup_invariants.py::test_configure_logging_is_i
dempotent_for_handlers 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
masks_known_tokens 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
redacts_payload_and_details 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
survives_mapping_errors 
tests/unit/logging/test_logging_setup_invariants.py::test_cli_to_test_context_sw
itch_updates_log_destination 
tests/unit/logging/test_logging_setup_invariants.py::test_json_formatter_include
s_structured_extras 
tests/unit/logging/test_logging_setup_levels.py::test_configure_logging_honors_e
nv_log_level 
tests/unit/logging/test_logging_setup_levels.py::test_json_formatter_captures_re
quest_context 
tests/unit/logging/test_logging_setup_levels.py::test_dev_logger_attaches_filter
s_and_handlers 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths 
tests/unit/memory/test_issue3_regression_guard.py::test_issue3_findings_persist 
tests/unit/memory/test_layered_cache.py::test_promotes_value_to_higher_layer 
tests/unit/memory/test_layered_cache.py::test_hit_ratio_tracking 
tests/unit/memory/test_layered_cache.py::test_read_and_write_alias_methods 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_rel
oad_exposes_runtime_protocol 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_protocol_runtime_
checks_accept_custom_layers 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_pro
tocol_remains_runtime_checkable 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_missing_required_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_m
emory_store_protocol_runtime_check 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_impor
t_and_construction_succeeds 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_accep
ts_optional_backends 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_still
_requires_primary_backend 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_rejec
ts_unknown_backend_names 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_stub_store_matches
_protocol_runtime 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_memory_store_param
eters_are_runtime_typevars 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_parameterised_memo
ry_store_runtime_is_safe 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_snapshot_alias_pre
serves_runtime_origin 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_value_typevar_iden
tity_is_preserved 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_and_s
napshot_share_runtime_typevar 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_transaction_rolls_
back_typed_stores 
tests/unit/memory/test_sync_manager_transaction_failure.py::test_transaction_rol
ls_back_all_stores 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_commit_unknown_tr
ansaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_rollback_unknown_
transaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_double_commit_fai
ls_and_state_persists 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_complete
s_with_deterministic_seed 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_tr
ansitions_and_memory_integration 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_seeds_random_and_numpy_modules 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_logs_backoff_and_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_coordinator_records_each_phase 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_exits_when_total_budget_elapsed 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_accepts_dialectical_sequence_payload 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_records_unknown_phase_and_next_phase_fallbacks 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_default_path_executes 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_dia
lectical_sequence_records_with_coordinator_fallback 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_tolerates_seed_failures 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_trace_complete 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_configures_seed_providers 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_budget_precheck 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_retries_then_succeeds 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_exhaustion_sets_stop 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_copies_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_handles_dialectical_sequence 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_raises_for_non_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_halts_when_result_missing 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_matrix 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_exhausts_retry_budget_and_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_retries_clamp_sleep_to_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_stops_retry_when_total_budget_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_coordinator_records_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_records_dialectical_sequences_for_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_fallbacks_for_invalid_phase_and_next_phase 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_honors_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_seeds_random_sources 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_preserves_nonstandard_phase_without_hints 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_handles_extended_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_enforces_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retries_until_success 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_fallback_transitions_and_propagation 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_respects_max_iterations_limit 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retry_backoff_respects_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_honors_phase_and_next_phase_fields 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_clamps_retry_when_budget_consumed 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_logs_retry_exhaustion_telemetry 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_exits_when_budget_elapsed_before_iteration 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_retry_sequence_updates_phase_and_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_records_results_before_consensus_failure 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
tries_on_transient 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_emits_debug_and_clamps_sleep 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_without_budget_uses_base_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_clamps_backoff_and_respects_budget 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_remaining_budget_spent 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
cords_consensus_failure_via_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_consensus_failure_without_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_budget_already_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_invalid_next
_phase_falls_back_to_transition_map 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_missing_stat
us_relies_on_max_iterations 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_raises_for_non_mapping_results 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_handles_seed_failures_gracefully 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_logs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_import_he
lper_exposes_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_immediate_timeout_skips_apply_invocation 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_respects_total_budget_and_emits_debug 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_uses_fallback_after_invalid_phase 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_stops_after_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_seeds_random_and_numpy 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_applies_synthesis_to_task 
tests/unit/methodology/test_adhoc_adapter.py::test_should_start_cycle_true 
tests/unit/methodology/test_adhoc_adapter.py::test_should_progress_to_next_phase
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_evaluation
_terminates_with_many_hooks 
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_hooks_cont
inue_after_exception 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations 
tests/unit/methodology/test_edrr_coordinator.py::test_automate_retrospective_rev
iew_summarizes_results 
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs tests/unit/methodology/test_kanban_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_kanban_adapter.py::test_progress_respects_wip_limit 
tests/unit/methodology/test_milestone_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_milestone_adapter.py::test_progress_requires_approva
l_when_configured 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget 
tests/unit/methodology/test_sprint_adapter.py::test_calculate_phase_end_time 
tests/unit/methodology/test_sprint_adapter.py::test_is_phase_time_exceeded_false
tests/unit/methodology/test_sprint_adapter.py::test_should_progress_when_time_ex
ceeded 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase 
tests/unit/methodology/test_sprint_adapter.py::test_before_cycle_provides_contex
t 
tests/unit/methodology/test_sprint_adapter.py::test_before_expand_sets_phase_sta
rt_time 
tests/unit/methodology/test_sprint_adapter.py::test_after_retrospect_captures_sp
rint_plan 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_failure_br
anch_sets_failed 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step 
tests/unit/policies/test_verify_security_policy.py::test_passes_when_all_variabl
es_set 
tests/unit/policies/test_verify_security_policy.py::test_fails_when_variable_mis
sing 
tests/unit/providers/test_provider_contract.py::test_stub_provider_offline_defau
lts_to_stub 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_uses_
safe_provider 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_null_
provider 
tests/unit/providers/test_provider_system_additional.py::test_unknown_provider_f
alls_back 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_us
es_provider_config 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_re
spects_track_metrics_flag 
tests/unit/providers/test_provider_system_additional.py::test_stub_provider_dete
rministic_embeddings 
tests/unit/providers/test_provider_system_additional.py::test_create_tls_config_
uses_settings 
tests/unit/providers/test_provider_system_additional.py::test_provider_factory_p
refers_explicit_tls_config 
tests/unit/providers/test_provider_system_additional.py::test_fallback_async_ski
ps_open_circuit 
tests/unit/providers/test_provider_system_branches.py::test_factory_honors_disab
le_flag 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_s
tub_safe_default 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_n
ull_when_requested 
tests/unit/providers/test_provider_system_branches.py::test_explicit_openai_with
out_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_availabilit
y_guard_returns_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_fallback_fa
ilure_promotes_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_explicit_anthropic_w
ithout_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_anthropic_unsupporte
d_error 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_us
es_next_provider_on_failure 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_pr
opagates_failure_when_all_fail 
tests/unit/providers/test_provider_system_branches.py::test_fallback_disabled_tr
ies_only_first_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_initializat
ion_orders_providers_and_records_circuit_results 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_skips
_open_circuit_breaker 
tests/unit/providers/test_provider_system_branches.py::test_openai_async_retry_e
mits_telemetry 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_circu
it_breaker_recovery 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_factory_applies_tls_
and_retry_settings 
tests/unit/providers/test_provider_system_branches.py::test_emit_retry_telemetry
_logs_and_counts 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_comp
lete_builds_payload 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_reje
cts_invalid_temperature 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_embe
d_returns_embeddings 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_co
mplete_uses_custom_messages 
tests/unit/providers/test_provider_system_branches.py::test_fallback_embed_moves
_to_next_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_aembed_reco
vers_from_failure 
tests/unit/providers/test_provider_system_branches.py::test_get_provider_config_
reads_env_file 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ope
nai_success_path 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ant
hropic_requires_key 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_unk
nown_provider_uses_null 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_lms
tudio_fallback_when_openai_missing 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_asyn
c_paths 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_rea
l_module_branches 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_as
ync_paths 
tests/unit/providers/test_provider_system_branches.py::test_complete_helper_incr
ements_metrics_and_propagates_error 
tests/unit/providers/test_provider_system_branches.py::test_embed_helper_wraps_n
on_provider_errors 
tests/unit/providers/test_provider_system_branches.py::test_acomplete_helper_inc
rements_metrics 
tests/unit/providers/test_provider_system_branches.py::test_aembed_helper_promot
es_unexpected_errors 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_components_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_requirements_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_edrr_phas
e_mapping_on_persist 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_evaluatio
n_hook_invoked_on_consensus_true 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_file_operations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_network_calls 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_global_state 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_fixture_usage 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_simple_test_file 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_file_with_isolation_marker 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_handles_syntax_errors 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_generates_recommendations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages 
tests/unit/scripts/test_analyze_test_dependencies.py::TestIntegration::test_end_
to_end_analysis 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_help 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_missing
_test_dir 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_test_execution_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_coverage_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_validation_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_shell_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_syntax_errors 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_finds_
testing_scripts 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_analyz
es_overlaps 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_git_us
age_frequency 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_genera
tes_consolidation_recommendations 
tests/unit/scripts/test_audit_testing_scripts.py::TestMarkdownGeneration::test_g
enerates_markdown_report 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_help 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_missing_scr
ipts_dir 
tests/unit/scripts/test_audit_testing_scripts.py::test_integration_audit_workflo
w 
tests/unit/scripts/test_auto_issue_comment.py::test_parse_issue_numbers_extracts
_ids 
tests/unit/scripts/test_auto_issue_comment.py::test_dry_run_when_env_missing 
tests/unit/scripts/test_auto_issue_comment.py::test_posts_comment_when_env_prese
nt 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_initialization 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_timeout 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_empty 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_with_data 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_generates_recommendations 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_help 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_invalid_
workers 
tests/unit/scripts/test_benchmark_test_execution.py::test_integration_benchmark_
workflow 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
valid_anchor 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
missing_anchor 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_missing_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_unit 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity 
tests/unit/scripts/test_examples_smoke_script.py::test_main_default_examples_suc
ceeds 
tests/unit/scripts/test_examples_smoke_script.py::test_main_reports_failure_when
_analyze_raises 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors 
tests/unit/scripts/test_gen_ref_pages.py::test_gen_ref_pages_matches_examples 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_with_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_without_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_property_test_metrics 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_calculate_overall_quality_score 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_generate_quality_recommendations 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics 
tests/unit/scripts/test_generate_quality_report.py::test_html_generation 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures 
tests/unit/scripts/test_security_ops.py::test_collect_logs_missing_directory 
tests/unit/scripts/test_security_ops.py::test_run_audit_calls_security_audit 
tests/unit/scripts/test_security_ops.py::test_list_outdated_runs_poetry 
tests/unit/scripts/test_security_ops.py::test_apply_updates_runs_poetry 
tests/unit/scripts/test_security_scan_script.py::test_main_non_strict_no_tools_r
eturns_ok 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_passes_when_above 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_fails_when_below 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_valid 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_issue 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_mvuu 
tests/unit/scripts/test_verify_release_state.py::test_draft_status_missing_tag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_without_t
ag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_with_tag 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_returns
_fields 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_without
_header 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_missing 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_present 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_when_log_mi
ssing 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_unreso
lved_questions 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_only_r
esolved 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_invali
d_json 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache_i
nvalidation 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_path_fi
lter 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_f
lags_missing_docs 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_p
asses_when_documented 
tests/unit/scripts/test_verify_test_markers_cli.py::test_argparser_includes_chan
ged_flag 
tests/unit/scripts/test_verify_test_markers_cli.py::test_verify_files_with_temp_
test 
tests/unit/scripts/test_verify_test_markers_cross_check.py::test_argparser_inclu
des_cross_check_flag 
tests/unit/scripts/test_wsde_edrr_simulation.py::test_simulation_converges 
tests/unit/security/test_api_authentication.py::test_verify_token_valid_is_valid
tests/unit/security/test_api_authentication.py::test_verify_token_invalid_is_val
id 
tests/unit/security/test_api_authentication.py::test_verify_token_missing_succee
ds 
tests/unit/security/test_api_authentication.py::test_verify_token_wrong_format_s
ucceeds 
tests/unit/security/test_api_authentication.py::test_verify_token_access_control
_disabled_succeeds 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_password_hasher_parameters_safe_defaults 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_hash_and_verify_roundtrip 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_generate_key_validates_and_encrypts 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_invalid_key_rejected 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_missing_key_env_raises 
tests/unit/security/test_authentication_optional_dependency.py::test_authenticat
ion_handles_missing_argon2 
tests/unit/security/test_authorization_checks.py::test_require_authorization_all
ows_authorized_action 
tests/unit/security/test_authorization_checks.py::test_require_authorization_rai
ses_forbidden 
tests/unit/security/test_deployment_coverage.py::test_require_non_root_user_when
_not_required 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_missing_vars 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_all_present 
tests/unit/security/test_deployment_coverage.py::test_apply_secure_umask 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_with_requir
ed_env 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_without_req
uired_env 
tests/unit/security/test_encryption.py::test_generate_key_returns_expected_resul
t 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_roundtrip_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_string_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_bytes_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_env_var_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_no_key_raises_error 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_with_env_var_succee
ds 
tests/unit/security/test_encryption.py::test_decrypt_invalid_token_raises_error 
tests/unit/security/test_encryption.py::test_decrypt_with_wrong_key_raises_error
tests/unit/security/test_logging_redaction.py::test_logging_redacts_openai_api_k
ey 
tests/unit/security/test_logging_redaction.py::test_logging_redacts_in_extra_det
ails 
tests/unit/security/test_memory_encryption.py::test_json_file_store_encryption_s
ucceeds 
tests/unit/security/test_memory_encryption.py::test_lmdb_store_encryption_succee
ds 
tests/unit/security/test_memory_encryption.py::test_tinydb_store_encryption_succ
eeds tests/unit/security/test_policy_audit.py::test_audit_detects_violation 
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file 
tests/unit/security/test_review.py::test_review_due_when_interval_elapsed 
tests/unit/security/test_review.py::test_review_not_due_before_interval 
tests/unit/security/test_review.py::test_next_review_date_calculation 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_script_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_control_ch
ars_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_both_succe
eds 
tests/unit/security/test_sanitization.py::test_sanitize_input_strips_whitespace_
succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_script_tags_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_control_chars_s
ucceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_complex_script_tag
s_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_multiple_script_ta
gs_succeeds 
tests/unit/security/test_sanitization.py::test_validate_safe_input_with_safe_inp
ut_returns_expected_result 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_s
cript_raises_error 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_c
ontrol_chars_raises_error 
tests/unit/security/test_security_audit.py::test_run_executes_checks 
tests/unit/security/test_security_audit.py::test_run_raises_on_policy_failure 
tests/unit/security/test_security_audit.py::test_report_writes_results 
tests/unit/security/test_security_audit.py::test_report_records_failure 
tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_runs_che
cks 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_respects
_skip_flags 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_register
ed 
tests/unit/security/test_security_flags_env.py::test_authentication_disabled_all
ows_any_credentials 
tests/unit/security/test_security_flags_env.py::test_authentication_enabled_enfo
rces 
tests/unit/security/test_security_flags_env.py::test_authorization_disabled_allo
ws 
tests/unit/security/test_security_flags_env.py::test_authorization_enabled_enfor
ces 
tests/unit/security/test_security_flags_env.py::test_sanitization_disabled_no_er
ror 
tests/unit/security/test_security_flags_env.py::test_sanitization_enabled_raises
tests/unit/security/test_tls_config.py::test_tls_config_timeout_env_override 
tests/unit/security/test_tls_config.py::test_tls_config_timeout_explicit_overrid
e 
tests/unit/security/test_tls_config.py::test_tls_config_validation_raises_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_partial_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_key_only_succ
eeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_cert_only_suc
ceeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_missing_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_defau
lt_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_verif
y_false_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
ca_file_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_and_key_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_only_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_ca_fi
le_precedence_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_all_p
arams_succeeds 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_valid_string_
is_valid 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[   ] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[None] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_non_string_va
lue_succeeds 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[-5--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[0-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[5-1-10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[1-1-10-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[10-1-10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[-5--10-0--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[0--10-10-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[1.5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[None] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[0-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[-5-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[5-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[0--1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[100-99] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[1-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[True-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[None-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[5-choices5] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[4-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[None-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[20-choices4] 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_option 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_with_no_path 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_run_tests_command 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_standard_cli_fallback 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_missing_run_tests_m
odule 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_cli_import_errors 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_runtime_errors 
tests/unit/test_sentinel_speed_markers.py::test_sentinel_fast_bucket_present 
tests/unit/test_simple_addition.py::test_add_returns_sum_for_integers 
tests/unit/test_simple_addition.py::test_add_accepts_floats_and_mixed_numeric_ty
pes 
tests/unit/test_simple_addition.py::test_add_raises_type_error_for_non_numeric_i
nputs 
tests/unit/test_verify_test_organization_sentinel.py::test_verify_test_organizat
ion_returns_zero 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran 
tests/unit/testing/test_collect_cache_sanitize.py::test_sanitize_node_ids_strips
_line_numbers_only_when_no_function_delimiter 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_union_
reaches_threshold_with_overlap 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_thresh
old_detection_matches_cli_expectations 
tests/unit/testing/test_deterministic_seed_fixture.py::test_deterministic_seed_f
ixture_sets_env_vars 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_bad_ttl_env_falls_back_to_
default 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_sanitize_node_ids_preserve
s_function_qualifier_and_strips_line_numbers 
tests/unit/testing/test_failure_tips.py::test_failure_tips_contains_core_guidanc
e 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_can_mutate_addition 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_mutates_addition_to_subtraction 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_cannot_mutate_non_arithmetic 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_can_mutate_equality 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_mutates_equality_to_inequality 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_ca
n_mutate_and_operation 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_mu
tates_and_to_or 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_can_
mutate_not_operation 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_muta
tes_not_by_removal 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_can_mutat
e_boolean_constant 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_t
rue_to_false 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_n
umber_to_zero_and_one 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_mutations_for_simple_code 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_handles
_syntax_errors 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_different_mutation_types 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_initializa
tion 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_killed 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_survived 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
esult_dataclass 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
eport_dataclass 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow 
tests/unit/testing/test_run_tests.py::test_sanitize_node_ids_strips_line_numbers
_without_function_delimiter 
tests/unit/testing/test_run_tests.py::test_failure_tips_contains_key_guidance_li
nes 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches 
tests/unit/testing/test_run_tests.py::test_run_tests_segment_batches 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes 
tests/unit/testing/test_run_tests_additional_coverage.py::test_failure_tips_ment
ions_core_troubleshooting_flags 
tests/unit/testing/test_run_tests_additional_coverage.py::test_ensure_pytest_cov
_plugin_env_injects_and_skips 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_missing_json 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_errors 
tests/unit/testing/test_run_tests_additional_coverage.py::test_sanitize_node_ids
_removes_line_numbers 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker 
tests/unit/testing/test_run_tests_artifacts.py::test_reset_coverage_artifacts_re
moves_stale_files 
tests/unit/testing/test_run_tests_artifacts.py::test_ensure_coverage_artifacts_g
enerates_reports 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_fails_when_pytest
_cov_missing 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_successful_single
_batch 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_h
andles_missing_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_r
ejects_invalid_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_missing_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_s
uccess 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_failure_tips_con
tains_suggestions 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_i
nject_plugins_and_emit_tips 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_failure_tips_surfac
e_cli_remediations 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_batches_fo
llow_segment_size 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_han
dles_resource_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_co
verage_totals 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_skips_placeh
older_artifacts 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_env_passthrough_an
d_coverage_lifecycle 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled 
tests/unit/testing/test_run_tests_collection_cache.py::test_sanitize_node_ids_st
rips_trailing_line_without_function_delimiter 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_reset_coverage_art
ifacts_removes_files_and_directories 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_data_missing 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_no_measured_files 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_generates_reports_and_syncs_legacy 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_html_failure_still_writes_json 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_run_tests_writes_m
anifest_with_coverage_reference 
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py::test_ensure_c
overage_artifacts_combines_fragment_files 
tests/unit/testing/test_run_tests_coverage_short_circuit.py::test_ensure_coverag
e_artifacts_short_circuits_without_measured_files 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_repor
ts_missing_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_flags
_invalid_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_totals 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_html_index 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_rejec
ts_empty_html 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_succe
ss_path 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_failure_tips_formats_
return_code_and_cmd 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_reset_coverage_artifa
cts_handles_oserror 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_ensure_coverage_artif
acts_handles_unreadable_html 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags 
tests/unit/testing/test_run_tests_keyword_exec.py::test_keyword_marker_executes_
matching_node_ids 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success 
tests/unit/testing/test_run_tests_logic.py::test_sanitize_node_ids_strips_line_n
umbers_without_function_delimiter 
tests/unit/testing/test_run_tests_logic.py::test_failure_tips_contains_key_guida
nce_lines 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_success 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_failure 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_segmented_exe
cution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_marker_expres
sion_building 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_env_defaults 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exception_han
dling 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exit_code_5_s
uccess 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_dry_run_skips
_execution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_keyword_filte
r 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_maxfail_optio
n 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_smoke_mode_pl
ugin_injection 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_s
uccess 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_f
rom_existing_cache 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_c
ollection_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_maxfail 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_custom_env 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_execution_failur
e_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_no_target_p
ath_raises_error 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories 
tests/unit/testing/test_run_tests_marker_fallback.py::test_run_tests_marker_fall
back_skips_segmentation 
tests/unit/testing/test_run_tests_marker_fallback.py::test_build_segment_metadat
a_uses_typed_sequences 
tests/unit/testing/test_run_tests_marker_merge.py::test_speed_marker_merged_with
_lmstudio_keyword_filter 
tests/unit/testing/test_run_tests_marker_merge.py::test_global_marker_with_lmstu
dio_keyword_filter 
tests/unit/testing/test_run_tests_module.py::test_sanitize_node_ids_dedup_and_st
rip_line_numbers 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return 
tests/unit/testing/test_run_tests_module.py::test_failure_tips_includes_segmenta
tion_guidance 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_missing_file 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_invalid_json 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions 
tests/unit/testing/test_run_tests_option_parsing.py::test_parse_pytest_addopts_h
andles_balanced_and_unbalanced_quotes 
tests/unit/testing/test_run_tests_option_parsing.py::test_addopts_has_plugin_det
ects_split_and_concatenated_forms 
tests/unit/testing/test_run_tests_option_parsing.py::test_coverage_plugin_disabl
ed_detects_common_overrides 
tests/unit/testing/test_run_tests_orchestration.py::test_verbose_flag_adds_v_to_
pytest_command 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run 
tests/unit/testing/test_run_tests_orchestration.py::test_pytest_addopts_are_pres
erved 
tests/unit/testing/test_run_tests_orchestration.py::test_extra_marker_adds_m_fla
g_to_command 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_honors_env_timeout 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_respects_explicit_disables 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_detects_inline_plugin_token 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[--no-cov -s-False---no-cov -s] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[-k smoke-True--k smoke -p pytest_cov] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_pytest_cov_support_
status_missing_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_run_tests_aborts_wh
en_pytest_cov_missing 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_detects_existing_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_respects_explicit_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-p no:pytest_bdd -s-False--p no:pytest_bdd 
-s] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-k feature-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_strips_tra
iling_line_numbers_without_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_keeps_ids_
with_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_deduplicat
es_preserving_order 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_single_speed 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_multiple_speeds 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_no_tests_found 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_failure_with_maxfail 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_reuses_cache_and_preserves_environment 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_dry_run_batches_use_typed_requests 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_command_building 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_multiple_node_ids 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_smoke_mode_env 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_no_parallel 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_batches
_execute 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_honors_
keyword_filter 
tests/unit/testing/test_run_tests_segmented.py::test_run_segmented_tests_stop_af
ter_maxfail 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_failure_appends_tips 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_segmente
d_failure_surfaces_remediation 
tests/unit/testing/test_run_tests_segmented_failures.py::test__run_segmented_tes
ts_aggregates_outputs 
tests/unit/testing/test_run_tests_segmented_failures.py::test_segmented_runs_rei
nject_plugins_without_clobbering_addopts 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_single_b
atch_uses_request_object 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_success_invokes_publish 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_failure_skips_graph 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_reports_append_graph 
tests/unit/testing/test_run_tests_segmented_report_flag.py::test_run_segmented_t
ests_reports_only_last_segment 
tests/unit/testing/test_run_tests_speed_keyword_loop.py::test_speed_loop_uses_ke
yword_filter_and_executes_node_ids 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_merges_fast
_and_medium_collections 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_defaults_to
_fast_and_medium_when_unspecified 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_excludes_gu
i_by_default 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_allows_gui_
when_requested 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_strips_line
_numbers_without_function_selector 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_preserves_o
rder 
tests/unit/testing/test_sanitize_node_ids_minimal.py::test_sanitize_node_ids_str
ips_line_when_no_function 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_tuple_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_invalid
_exc_info 
tests/unit/utils/test_logging_coverage.py::test_get_logger_returns_correct_insta
nce 
tests/unit/utils/test_logging_coverage.py::test_setup_logging_with_different_log
_levels 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_false_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_none_ex
c_info 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
baseexception_direct 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
true_with_active_exception 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
none_and_false_explicit 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_invalid_e
xc_info_to_hit_line_48 
tests/unit/utils/test_logging_final_coverage.py::test_get_logger_function_direct
_call 
tests/unit/utils/test_logging_final_coverage.py::test_setup_logging_function_dir
ect_calls 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_with_kwar
gs 
tests/unit/utils/test_logging_utils.py::test_dev_synth_logger_normalizes_exc_inf
o_tuple_and_exception 
tests/unit/utils/test_logging_utils.py::test_setup_logging_calls_configure_loggi
ng 
tests/unit/utils/test_logging_utils.py::test_get_logger_returns_dev_synth_logger
_instance 
tests/unit/utils/test_serialization.py::test_dumps_deterministic_round_trip_simp
le tests/unit/utils/test_serialization.py::test_dump_and_load_file_round_trip 
tests/unit/utils/test_serialization.py::test_provider_env_as_dict_deterministic_
serialization 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_with_s
tring_already_having_newline 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_ensure
s_single_newline 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_no_trailing_new
line 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_trailing_newlin
e 
tests/unit/utils/test_serialization_coverage.py::test_dump_to_file_complete_cove
rage 
tests/unit/utils/test_serialization_coverage.py::test_load_from_file_complete_co
verage 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_multiple_traili
ng_newlines 
tests/unit/utils/test_serialization_coverage.py::test_serialization_with_unicode
_characters 
tests/unit/utils/test_serialization_coverage.py::test_serialization_edge_cases 
tests/unit/utils/test_serialization_coverage.py::test_file_operations_with_speci
al_paths 
tests/unit/utils/test_serialization_edges.py::test_loads_tolerates_missing_and_s
ingle_trailing_newline 
tests/unit/utils/test_serialization_edges.py::test_dump_to_file_overwrites_and_k
eeps_single_newline 
tests/unit/utils/test_serialization_extra.py::test_dumps_and_loads_deterministic
_round_trip_unicode_and_newline 
tests/unit/utils/test_serialization_extra.py::test_dump_and_load_file_round_trip
_handles_utf8 
tests/unit/utils/test_serialization_extra.py::test_loads_accepts_without_trailin
g_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
direct_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
with_string_that_might_have_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_direct_line_co
verage 
tests/unit/utils/test_serialization_final_coverage.py::test_dump_to_file_direct_
line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_load_from_file_direc
t_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_serialization_functi
ons_with_mock_to_ensure_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_with_various_n
ewline_scenarios 
tests/unit/utils/test_serialization_final_coverage.py::test_file_operations_with
_explicit_paths 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
return_path 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_return_path -m
not memory_intensive and fast and not gui --cov=src/devsynth 
--cov-report=json:test_reports/coverage.json --cov-report=html:htmlcov 
--cov-append --maxfail 1E         Troubleshooting tips:E         - Smoke mode: 
reduce third-party plugin surface to isolate issues:E           poetry run 
devsynth run-tests --smoke --speed=fast --no-parallel --maxfail=1E         - 
Marker discipline: default is &amp;#x27;-m not memory_intensive&amp;#x27;.E     
Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.E         - Plugin 
autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using --smoke; plugin 
options may fail otherwise.E         - Diagnostics: run &amp;#x27;poetry run 
devsynth doctor&amp;#x27; for a quick environment check.E         - Narrow 
scope: use &amp;#x27;-k &amp;lt;expr&amp;gt;&amp;#x27; and 
&amp;#x27;-vv&amp;#x27; to focus a failure.E         - Segment large suites to 
localize failures and flakes:E           devsynth run-tests --target unit-tests 
--speed=fast --segment --segment-size=50E         - Limit failures early to 
speed iteration:E           poetry run devsynth run-tests --target unit-tests 
--speed=fast --maxfail=1E         - Disable parallelism if xdist interaction is 
suspected:E           devsynth run-tests --target unit-tests --speed=fast 
--no-parallelE         - Generate an HTML report for context (saved under 
test_reports/):E           devsynth run-tests --target unit-tests --speed=fast 
--reportE         E       assert 
False/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_ru
n_tests_regression.py:29: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:26:44,823 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:26:52,794 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact generation 
skipped: data file missing____________________ 
test_configure_llm_settings_reads_env _____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144090140&amp;gt;    
@pytest.mark.fast    def test_configure_llm_settings_reads_env(monkeypatch):    
&amp;quot;&amp;quot;&amp;quot;Environment variables update LLM settings when 
configured.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setenv(&amp;quot;DEVSYNTH_LLM_MODEL&amp;quot;, 
&amp;quot;foo-model&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_LLM_MAX_TOKENS&amp;quot;, 
&amp;quot;123&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_LLM_TEMPERATURE&amp;quot;, 
&amp;quot;0.9&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_LLM_AUTO_SELECT_MODEL&amp;quot;, 
&amp;quot;false&amp;quot;)    &amp;gt;       
importlib.reload(config)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/config/test_config_llm_env.py:16: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ module = &amp;lt;module 
&amp;#x27;devsynth.config&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/conf
ig/__init__.py&amp;#x27;&amp;gt;    def reload(module):        
&amp;quot;&amp;quot;&amp;quot;Reload the module and return it.            The 
module must have been successfully imported before.            
&amp;quot;&amp;quot;&amp;quot;        try:            name = 
module.__spec__.name        except AttributeError:            try:              
name = module.__name__            except AttributeError:                raise 
TypeError(&amp;quot;reload() argument must be a module&amp;quot;) from None     
if sys.modules.get(name) is not module:&amp;gt;           raise 
ImportError(f&amp;quot;module {name} not in sys.modules&amp;quot;, name=name)E  
ImportError: module devsynth.config not in 
sys.modules/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/
Versions/3.12/lib/python3.12/importlib/__init__.py:111: 
ImportError____________________ test_load_config_merges_mvuu_settings 
_____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_load_config_merges_mvuu_s0&amp;#x27;)    
@pytest.mark.fast    def test_load_config_merges_mvuu_settings(tmp_path):       
mvu_cfg = {&amp;quot;schema&amp;quot;: &amp;quot;s.json&amp;quot;, 
&amp;quot;storage&amp;quot;: {&amp;quot;path&amp;quot;: 
&amp;quot;db.json&amp;quot;, &amp;quot;format&amp;quot;: 
&amp;quot;json&amp;quot;}}        dev_dir = tmp_path / 
&amp;quot;.devsynth&amp;quot;        dev_dir.mkdir()        (dev_dir / 
&amp;quot;mvu.yml&amp;quot;).write_text(yaml.safe_dump(mvu_cfg))        cfg = 
load_config(start_path=str(tmp_path))&amp;gt;       assert cfg.mvuu == mvu_cfgE 
AssertionError: assert {} == {&amp;#x27;schema&amp;#x27;: 
&amp;#x27;s...&amp;#x27;: &amp;#x27;db.json&amp;#x27;}}E         E         Right
contains 2 more items:E         {&amp;#x27;schema&amp;#x27;: 
&amp;#x27;s.json&amp;#x27;, &amp;#x27;storage&amp;#x27;: 
{&amp;#x27;format&amp;#x27;: &amp;#x27;json&amp;#x27;, &amp;#x27;path&amp;#x27;:
&amp;#x27;db.json&amp;#x27;}}E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_co
nfig_loader_mvu.py:16: AssertionError______________ 
test_bootstrap_script_rejects_invalid_environment _______________    
@pytest.mark.fast    def test_bootstrap_script_rejects_invalid_environment():   
&amp;quot;&amp;quot;&amp;quot;bootstrap.sh should reject unknown environments.  
ReqID: DEP-02&amp;quot;&amp;quot;&amp;quot;        workdir = 
Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:            cmd 
= f&amp;quot;cd &amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;bootstrap.sh&amp;#x27;} invalid&amp;quot;            result = 
subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;Invalid 
environment&amp;quot; in result.stderrE           assert &amp;#x27;Invalid 
environment&amp;#x27; in &amp;#x27;su: Sorry\n&amp;#x27;E            +  where 
&amp;#x27;su: Sorry\n&amp;#x27; = CompletedProcess(args=[&amp;#x27;su&amp;#x27;,
&amp;#x27;nobody&amp;#x27;, &amp;#x27;-s&amp;#x27;, 
&amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, &amp;quot;cd 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/.../github.com/raveno
ak/devsynth/scripts/deployment/bootstrap.sh invalid&amp;quot;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_bootstrap_script.py:29: 
AssertionError____________________ test_bootstrap_script_requires_docker 
_____________________    @pytest.mark.fast    def 
test_bootstrap_script_requires_docker():        
&amp;quot;&amp;quot;&amp;quot;bootstrap.sh should require docker to be 
installed.            ReqID: DEP-03&amp;quot;&amp;quot;&amp;quot;        workdir
= Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:            
cmd = f&amp;quot;cd &amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; 
{SCRIPTS_DIR / &amp;#x27;bootstrap.sh&amp;#x27;} development&amp;quot;          
result = subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;Docker is 
required&amp;quot; in result.stderrE           assert &amp;#x27;Docker is 
required&amp;#x27; in &amp;#x27;su: Sorry\n&amp;#x27;E            +  where 
&amp;#x27;su: Sorry\n&amp;#x27; = CompletedProcess(args=[&amp;#x27;su&amp;#x27;,
&amp;#x27;nobody&amp;#x27;, &amp;#x27;-s&amp;#x27;, 
&amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, &amp;quot;cd 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...hub.com/ravenoak/d
evsynth/scripts/deployment/bootstrap.sh development&amp;quot;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_bootstrap_script.py:49: 
AssertionError________________________ test_install_dev_installs_task 
________________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_install_dev_installs_task0&amp;#x27;)    
@pytest.mark.fast    def test_install_dev_installs_task(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;install_dev.sh should install go-task when absent.
ReqID: DEP-05&amp;quot;&amp;quot;&amp;quot;            home = tmp_path / 
&amp;quot;home&amp;quot;&amp;gt;       
home.mkdir()/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/depl
oyment/test_bootstrap_script.py:61: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_install_dev_installs_task0/home&amp;#x27;)mode =
511, parents = False, exist_ok = False    def mkdir(self, mode=0o777, 
parents=False, exist_ok=False):        &amp;quot;&amp;quot;&amp;quot;        
Create a new directory at this given path.        &amp;quot;&amp;quot;&amp;quot;
try:&amp;gt;           os.mkdir(self, mode)E           FileExistsError: [Errno 
17] File exists: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_install_dev_installs_task0/home&amp;#x27;/opt/homebrew/Cel
lar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12
/pathlib.py:1311: FileExistsError___________________ 
test_health_check_script_reports_healthy ___________________    
@pytest.mark.fast    def test_health_check_script_reports_healthy():        
&amp;quot;&amp;quot;&amp;quot;health_check.sh should succeed when endpoints 
return 200.            ReqID: DEP-02&amp;quot;&amp;quot;&amp;quot;        server
= HTTPServer((&amp;quot;127.0.0.1&amp;quot;, 0), _Handler)        port = 
server.server_address[1]        thread = 
threading.Thread(target=server.serve_forever)        thread.daemon = True       
thread.start()        workdir = Path(tempfile.mkdtemp())        
workdir.chmod(0o755)        try:            env_file = workdir / 
&amp;quot;.env&amp;quot;            
env_file.write_text(&amp;quot;DEVSYNTH_ENV=testing\n&amp;quot;)            
env_file.chmod(0o600)            url = 
f&amp;quot;http://127.0.0.1:{port}&amp;quot;            cmd = f&amp;quot;cd 
&amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;health_check.sh&amp;#x27;} {url} {url} {url}&amp;quot;            
result = subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )&amp;gt;           
assert result.returncode == 0E           assert 1 == 0E            +  where 1 = 
CompletedProcess(args=[&amp;#x27;su&amp;#x27;, &amp;#x27;nobody&amp;#x27;, 
&amp;#x27;-s&amp;#x27;, &amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, 
&amp;quot;cd &amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/... 
http://127.0.0.1:60436 http://127.0.0.1:60436 http://127.0.0.1:60436&amp;quot;],
returncode=1, stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).returncode/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/deployment/test_health_check_smoke.py:49: 
AssertionError__________________ test_health_check_script_rejects_root_user 
__________________    @pytest.mark.fast    def 
test_health_check_script_rejects_root_user():        
&amp;quot;&amp;quot;&amp;quot;health_check.sh should refuse to run as root.     
ReqID: CON-04&amp;quot;&amp;quot;&amp;quot;        workdir = 
Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:            
result = subprocess.run(                ,                cwd=workdir,           
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;Please run this script
as a non-root user.&amp;quot; in result.stderrE           AssertionError: assert
&amp;#x27;Please run this script as a non-root user.&amp;#x27; in 
&amp;#x27;Missing environment file: .env\n&amp;#x27;E            +  where 
&amp;#x27;Missing environment file: .env\n&amp;#x27; = 
CompletedProcess(args=[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/scripts/deployment/health_check.sh&amp;#x27;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;Missing environment file: 
.env\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/deployment/test_health_check_smoke.py:72: 
AssertionError__________________ test_health_check_script_requires_env_file 
__________________    @pytest.mark.fast    def 
test_health_check_script_requires_env_file():        
&amp;quot;&amp;quot;&amp;quot;health_check.sh should fail when the env file is 
missing.            ReqID: DEP-04&amp;quot;&amp;quot;&amp;quot;        workdir =
Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:            cmd 
= f&amp;quot;cd &amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;health_check.sh&amp;#x27;}&amp;quot;            result = 
subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;Missing environment 
file&amp;quot; in result.stderrE           assert &amp;#x27;Missing environment 
file&amp;#x27; in &amp;#x27;su: Sorry\n&amp;#x27;E            +  where 
&amp;#x27;su: Sorry\n&amp;#x27; = CompletedProcess(args=[&amp;#x27;su&amp;#x27;,
&amp;#x27;nobody&amp;#x27;, &amp;#x27;-s&amp;#x27;, 
&amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, &amp;quot;cd 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/r
avenoak/devsynth/scripts/deployment/health_check.sh&amp;quot;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_health_check_smoke.py:92: AssertionError_____________ 
test_health_check_script_requires_strict_permissions _____________    
@pytest.mark.fast    def test_health_check_script_requires_strict_permissions():
&amp;quot;&amp;quot;&amp;quot;health_check.sh should enforce 600 permissions on 
the env file.            ReqID: CON-04&amp;quot;&amp;quot;&amp;quot;        
workdir = Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:      
env_file = workdir / &amp;quot;.env&amp;quot;            
env_file.write_text(&amp;quot;DEVSYNTH_ENV=testing\n&amp;quot;)            
env_file.chmod(0o644)            cmd = f&amp;quot;cd 
&amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;health_check.sh&amp;#x27;}&amp;quot;            result = 
subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;must have 600 
permissions&amp;quot; in result.stderrE           assert &amp;#x27;must have 600
permissions&amp;#x27; in &amp;#x27;su: Sorry\n&amp;#x27;E            +  where 
&amp;#x27;su: Sorry\n&amp;#x27; = CompletedProcess(args=[&amp;#x27;su&amp;#x27;,
&amp;#x27;nobody&amp;#x27;, &amp;#x27;-s&amp;#x27;, 
&amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, &amp;quot;cd 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/r
avenoak/devsynth/scripts/deployment/health_check.sh&amp;quot;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_health_check_smoke.py:115: 
AssertionError_________________ test_health_check_script_rejects_invalid_url 
_________________    @pytest.mark.fast    def 
test_health_check_script_rejects_invalid_url():        
&amp;quot;&amp;quot;&amp;quot;health_check.sh should validate provided URLs.    
ReqID: DEP-03&amp;quot;&amp;quot;&amp;quot;        workdir = 
Path(tempfile.mkdtemp())        workdir.chmod(0o755)        try:            
env_file = workdir / &amp;quot;.env&amp;quot;            
env_file.write_text(&amp;quot;DEVSYNTH_ENV=testing\n&amp;quot;)            
env_file.chmod(0o600)            cmd = (                f&amp;quot;cd 
&amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;health_check.sh&amp;#x27;} not-a-url&amp;quot;                
&amp;quot; not-a-url not-a-url&amp;quot;            )            result = 
subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert &amp;quot;Invalid URL&amp;quot; 
in result.stderrE           assert &amp;#x27;Invalid URL&amp;#x27; in 
&amp;#x27;su: Sorry\n&amp;#x27;E            +  where &amp;#x27;su: 
Sorry\n&amp;#x27; = CompletedProcess(args=[&amp;#x27;su&amp;#x27;, 
&amp;#x27;nobody&amp;#x27;, &amp;#x27;-s&amp;#x27;, 
&amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, &amp;quot;cd 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...ynth/scripts/deplo
yment/health_check.sh not-a-url not-a-url not-a-url&amp;quot;], returncode=1, 
stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_health_check_smoke.py:141: AssertionError_____________ 
test_health_check_script_fails_on_unhealthy_endpoint _____________    
@pytest.mark.fast    def test_health_check_script_fails_on_unhealthy_endpoint():
&amp;quot;&amp;quot;&amp;quot;health_check.sh should report failure when an 
endpoint is unhealthy.            ReqID: DEP-01&amp;quot;&amp;quot;&amp;quot;   
server = HTTPServer((&amp;quot;127.0.0.1&amp;quot;, 0), _FailHandler)        
port = server.server_address[1]        thread = 
threading.Thread(target=server.serve_forever)        thread.daemon = True       
thread.start()        workdir = Path(tempfile.mkdtemp())        
workdir.chmod(0o755)        try:            env_file = workdir / 
&amp;quot;.env&amp;quot;            
env_file.write_text(&amp;quot;DEVSYNTH_ENV=testing\n&amp;quot;)            
env_file.chmod(0o600)            url = 
f&amp;quot;http://127.0.0.1:{port}&amp;quot;            cmd = f&amp;quot;cd 
&amp;#x27;{workdir}&amp;#x27; &amp;amp;&amp;amp; {SCRIPTS_DIR / 
&amp;#x27;health_check.sh&amp;#x27;} {url} {url} {url}&amp;quot;            
result = subprocess.run(                [&amp;quot;su&amp;quot;, 
&amp;quot;nobody&amp;quot;, &amp;quot;-s&amp;quot;, 
&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, cmd],                
capture_output=True,                text=True,            )            assert 
result.returncode != 0&amp;gt;           assert f&amp;quot;Health check failed 
for {url}&amp;quot; in result.stderrE           assert &amp;#x27;Health check 
failed for http://127.0.0.1:60441&amp;#x27; in &amp;#x27;su: Sorry\n&amp;#x27;E 
+  where &amp;#x27;su: Sorry\n&amp;#x27; = 
CompletedProcess(args=[&amp;#x27;su&amp;#x27;, &amp;#x27;nobody&amp;#x27;, 
&amp;#x27;-s&amp;#x27;, &amp;#x27;/bin/bash&amp;#x27;, &amp;#x27;-c&amp;#x27;, 
&amp;quot;cd &amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/... 
http://127.0.0.1:60441 http://127.0.0.1:60441 http://127.0.0.1:60441&amp;quot;],
returncode=1, stdout=&amp;#x27;&amp;#x27;, stderr=&amp;#x27;su: 
Sorry\n&amp;#x27;).stderr/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/deployment/test_health_check_smoke.py:170: 
AssertionError____________________ TestWSDETeam.test_get_primus_succeeds 
_____________________self = 
&amp;lt;tests.unit.domain.models.test_wsde.TestWSDETeam object at 
0x120bcd850&amp;gt;    @pytest.mark.fast    def test_get_primus_succeeds(self): 
&amp;quot;&amp;quot;&amp;quot;Test getting the current Primus agent.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        self.team.add_agent(self.agent1)
self.team.add_agent(self.agent2)        self.team.primus_index = 1        primus
= self.team.get_primus()&amp;gt;       assert primus == self.agent2E       
AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5415225872&amp;#x27;&amp;gt; == &amp;lt;MagicMock 
id=&amp;#x27;5415238256&amp;#x27;&amp;gt;E         E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/model
s/test_wsde.py:108: AssertionError----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:26:56,259 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_team2025-10-28 09:26:56,259 - devsynth.domain.models.wsde_core - INFO - 
Added agent agent2 to team test_team------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent agent2 to team test_team____________ 
TestWSDETeam.test_assign_roles_with_rotation_succeeds _____________self = 
&amp;lt;tests.unit.domain.models.test_wsde.TestWSDETeam object at 
0x120bcebd0&amp;gt;    @pytest.mark.fast    def 
test_assign_roles_with_rotation_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that role assignments change when the Primus 
rotates.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
self.team.add_agent(self.agent1)        self.team.add_agent(self.agent2)        
self.team.add_agent(self.agent3)        self.team.add_agent(self.agent4)        
self.team.primus_index = 0        self.team.assign_roles()        initial_roles 
= {            self.agent1.name: self.agent1.current_role,            
self.agent2.name: self.agent2.current_role,            self.agent3.name: 
self.agent3.current_role,            self.agent4.name: self.agent4.current_role,
}        self.team.rotate_primus()        self.team.assign_roles()&amp;gt;      
assert self.agent1.current_role != initial_rolesE       AssertionError: assert 
&amp;#x27;Primus&amp;#x27; != &amp;#x27;Primus&amp;#x27;E        +  where 
&amp;#x27;Primus&amp;#x27; = &amp;lt;MagicMock 
id=&amp;#x27;5417968912&amp;#x27;&amp;gt;.current_roleE        +    where 
&amp;lt;MagicMock id=&amp;#x27;5417968912&amp;#x27;&amp;gt; = 
&amp;lt;tests.unit.domain.models.test_wsde.TestWSDETeam object at 
0x120bcebd0&amp;gt;.agent1/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/domain/models/test_wsde.py:206: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,297 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_team2025-10-28 09:26:56,297 - devsynth.domain.models.wsde_core - INFO - 
Added agent agent2 to team test_team2025-10-28 09:26:56,297 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_team2025-10-28 09:26:56,297 - devsynth.domain.models.wsde_core - INFO - 
Added agent agent4 to team test_team2025-10-28 09:26:56,301 - 
devsynth.domain.models.wsde_roles - INFO - Role assignments for team test_team: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;agent1&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;agent2&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;agent3&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;agent4&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}2025-10-28 09:26:56,303 - 
devsynth.domain.models.wsde_roles - INFO - Role assignments for team test_team: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;agent1&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;agent2&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;agent3&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;agent4&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent agent2 to team test_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent agent4 to team test_teamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments for team
test_team: {&amp;#x27;primus&amp;#x27;: &amp;#x27;agent1&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;agent2&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;agent3&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;agent4&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments for team
test_team: {&amp;#x27;primus&amp;#x27;: &amp;#x27;agent1&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;agent2&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;agent3&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;agent4&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}_ 
TestWSDETeam.test_apply_dialectical_reasoning_with_knowledge_graph_succeeds 
__self = &amp;lt;tests.unit.domain.models.test_wsde.TestWSDETeam object at 
0x120bcf0b0&amp;gt;    @pytest.mark.fast    def 
test_apply_dialectical_reasoning_with_knowledge_graph_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test applying dialectical reasoning with knowledge
graph integration.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
self.team.add_agent(self.agent1)        self.team.add_agent(self.agent2)        
task = {            &amp;quot;id&amp;quot;: &amp;quot;task1&amp;quot;,          
&amp;quot;type&amp;quot;: &amp;quot;code_generation&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Implement a secure authentication 
system&amp;quot;,            &amp;quot;requirements&amp;quot;: [&amp;quot;user 
authentication&amp;quot;, &amp;quot;password security&amp;quot;],        }      
solution = {            &amp;quot;id&amp;quot;: &amp;quot;solution1&amp;quot;,  
&amp;quot;agent&amp;quot;: &amp;quot;agent1&amp;quot;,            
&amp;quot;content&amp;quot;: &amp;quot;&amp;quot;&amp;quot;def 
authenticate(username, password):    return username == 
&amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;&amp;quot;&amp;quot;&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Simple authentication 
function&amp;quot;,        }        self.team.solutions = 
{task[&amp;quot;id&amp;quot;]: }        mock_wsde_memory = MagicMock()        
mock_wsde_memory.query_knowledge_for_task.return_value = [            
{&amp;quot;concept&amp;quot;: &amp;quot;authentication&amp;quot;, 
&amp;quot;relevance&amp;quot;: 0.9},            {&amp;quot;concept&amp;quot;: 
&amp;quot;password&amp;quot;, &amp;quot;relevance&amp;quot;: 0.8},            
{&amp;quot;concept&amp;quot;: &amp;quot;security&amp;quot;, 
&amp;quot;relevance&amp;quot;: 0.7},        ]        
mock_wsde_memory.query_concept_relationships.return_value = [            
{&amp;quot;relationship&amp;quot;: &amp;quot;requires&amp;quot;, 
&amp;quot;direction&amp;quot;: &amp;quot;outgoing&amp;quot;}        ]&amp;gt;   
result = self.team.apply_dialectical_reasoning_with_knowledge_graph(            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            task, 
self.critic_agent, mock_wsde_memory        )E       AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;apply_dialectical_reasoning_with_knowledge_graph&amp;#x27;/Users/caitl
yn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/test_wsde.py:2
47: AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,320 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_team2025-10-28 09:26:56,320 - devsynth.domain.models.wsde_core - INFO - 
Added agent agent2 to team test_team------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent agent2 to team test_team____________________ 
TestWSDE.test_initialization_succeeds _____________________self = 
&amp;lt;tests.unit.domain.models.test_wsde.TestWSDE object at 
0x120bcf6b0&amp;gt;    @pytest.mark.fast    def 
test_initialization_succeeds(self):        &amp;quot;&amp;quot;&amp;quot;Test 
that a WSDE is initialized correctly.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;&amp;gt;       wsde = WSDE(name=&amp;quot;Test 
WSDE&amp;quot;)               ^^^^^^^^^^^^^^^^^^^^^^E       TypeError: 
WSDE.__init__() got an unexpected keyword argument 
&amp;#x27;name&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/domain/models/test_wsde.py:278: TypeError_____________ 
TestWSDE.test_initialization_with_metadata_succeeds ______________self = 
&amp;lt;tests.unit.domain.models.test_wsde.TestWSDE object at 
0x120bcfad0&amp;gt;    @pytest.mark.fast    def 
test_initialization_with_metadata_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that a WSDE is initialized correctly with 
metadata.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        metadata = 
{&amp;quot;key&amp;quot;: &amp;quot;value&amp;quot;, 
&amp;quot;another_key&amp;quot;: 123}&amp;gt;       wsde = 
WSDE(name=&amp;quot;Test WSDE&amp;quot;, metadata=metadata)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: WSDE.__init__() got 
an unexpected keyword argument 
&amp;#x27;name&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/domain/models/test_wsde.py:291: TypeError__________ 
test_apply_dialectical_reasoning_invokes_hooks_and_memory ___________    
@pytest.mark.fast    def 
test_apply_dialectical_reasoning_invokes_hooks_and_memory() -&amp;gt; None:     
&amp;quot;&amp;quot;&amp;quot;End-to-end reasoning returns typed artefacts and 
integrates hooks.&amp;quot;&amp;quot;&amp;quot;            team = 
WSDETeam(name=&amp;quot;workflow-regression&amp;quot;)        task_factory = 
DialecticalTaskFactory()        task = task_factory.build(            solution={
&amp;quot;content&amp;quot;: &amp;quot;Short solution lacking 
examples.&amp;quot;,                &amp;quot;code&amp;quot;: 
&amp;quot;print(&amp;#x27;hello world&amp;#x27;)&amp;quot;,            }        
)            critic = SimpleNamespace(name=&amp;quot;critic-agent&amp;quot;)    
memory = MemoryRecorder()        hook_calls: list[tuple[DialecticalTask, 
tuple[DialecticalSequence, ...]]] = []            def hook(task_payload: 
DialecticalTask, sequences: tuple[DialecticalSequence, ...]):            
hook_calls.append((task_payload, sequences))            
team.register_dialectical_hook(hook)            result = 
team.apply_dialectical_reasoning(task, critic, memory_integration=memory)       
assert isinstance(result, DialecticalSequence)        serialized = 
result.to_dict()        assert serialized[&amp;quot;status&amp;quot;] == 
&amp;quot;completed&amp;quot;        assert 
serialized[&amp;quot;synthesis&amp;quot;]            assert hook_calls, 
&amp;quot;Hook should be invoked with the generated sequence&amp;quot;        
hook_task, hook_sequences = hook_calls[0]        assert isinstance(hook_task, 
DialecticalTask)        assert hook_task.identifier == task.identifier&amp;gt;  
assert hook_sequences[0] is resultE       AssertionError: assert 
{&amp;#x27;antithesis&amp;#x27;: {&amp;#x27;agent&amp;#x27;: 
&amp;#x27;critic-agent&amp;#x27;, &amp;#x27;alternative_approaches&amp;#x27;: 
[&amp;#x27;Consider a more structured format with sections&amp;#x27;,... 
logging&amp;#x27;], ...}, &amp;#x27;id&amp;#x27;: 
&amp;#x27;d50144ab-8d1a-4c9d-be0b-7d413459c484&amp;#x27;, 
&amp;#x27;metadata&amp;#x27;: {}, &amp;#x27;method&amp;#x27;: 
&amp;#x27;dialectical_reasoning&amp;#x27;, ...} is 
DialecticalSequence(sequence_id=&amp;#x27;df632a6d-9c51-41d9-9bb7-3e2c2d01f67c&a
mp;#x27;, steps=(DialecticalStep(step_id=&amp;#x27;d50144ab-8d1a-... 
handling&amp;#x27;, &amp;#x27;Implement a more modular design with smaller 
functions&amp;#x27;)),), status=&amp;#x27;completed&amp;#x27;, reason=None, 
metadata={})/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/doma
in/models/test_wsde_dialectical_workflow.py:60: AssertionError__ 
TestWSDERoleReassignment.test_build_consensus_multiple_solutions_succeeds 
___self = 
&amp;lt;tests.unit.domain.models.test_wsde_dynamic_workflows.TestWSDERoleReassig
nment object at 0x120c00650&amp;gt;    def 
test_build_consensus_multiple_solutions_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that build consensus multiple solutions 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        task = 
{&amp;quot;id&amp;quot;: &amp;quot;t1&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;demo&amp;quot;}        
self.team.add_solution(task, {&amp;quot;agent&amp;quot;: 
&amp;quot;code&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;a&amp;quot;})  
self.team.add_solution(task, {&amp;quot;agent&amp;quot;: 
&amp;quot;test&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;b&amp;quot;})  
consensus = self.team.build_consensus(task)&amp;gt;       assert 
consensus[&amp;quot;consensus&amp;quot;] != &amp;quot;&amp;quot;               
^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;consensus&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/domain/models/test_wsde_dynamic_workflows.py:43: 
KeyError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:56,496 - 
devsynth.domain.models.wsde_core - INFO - Added agent code to team 
test_dynamic_workflows_team2025-10-28 09:26:56,496 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
test_dynamic_workflows_team2025-10-28 09:26:56,497 - 
devsynth.domain.models.wsde_core - INFO - Added agent test to team 
test_dynamic_workflows_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent code to team 
test_dynamic_workflows_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
test_dynamic_workflows_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent test to team 
test_dynamic_workflows_team----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,498 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task t12025-10-28 
09:26:56,498 - devsynth.domain.models.wsde_utils - INFO - Added solution for 
task t12025-10-28 09:26:56,498 - devsynth.domain.models.wsde_voting - WARNING - 
Cannot build consensus: no options provided------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
t1INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution
for task t1WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 
Cannot build consensus: no options provided_______________ 
test_check_security_best_practices_detects_issue 
_______________wsde_team_factory = &amp;lt;function 
wsde_team_factory.&amp;lt;locals&amp;gt;._factory at 0x142e8cb80&amp;gt;    
@pytest.mark.fast    def 
test_check_security_best_practices_detects_issue(wsde_team_factory):        
&amp;quot;&amp;quot;&amp;quot;ReqID: WSDE-SECURITY-01  flags insecure patterns 
for escalation.&amp;quot;&amp;quot;&amp;quot;            team = 
wsde_team_factory()        insecure_code = &amp;quot;password = 
&amp;#x27;secret&amp;#x27;\nexec(&amp;#x27;print(1)&amp;#x27;)\n&amp;quot;&amp;g
t;       assert team._check_security_best_practices(insecure_code) is FalseE    
assert {&amp;#x27;compliance_level&amp;#x27;: &amp;#x27;medium&amp;#x27;, 
&amp;#x27;issues&amp;#x27;: [&amp;#x27;Hardcoded credentials 
detected&amp;#x27;], &amp;#x27;suggestions&amp;#x27;: [&amp;#x27;Use environment
variable..., &amp;#x27;Use parameterized queries to prevent SQL 
injection&amp;#x27;, &amp;#x27;Validate all user input and implement proper 
error handling&amp;#x27;]} is FalseE        +  where 
{&amp;#x27;compliance_level&amp;#x27;: &amp;#x27;medium&amp;#x27;, 
&amp;#x27;issues&amp;#x27;: [&amp;#x27;Hardcoded credentials 
detected&amp;#x27;], &amp;#x27;suggestions&amp;#x27;: [&amp;#x27;Use environment
variable..., &amp;#x27;Use parameterized queries to prevent SQL 
injection&amp;#x27;, &amp;#x27;Validate all user input and implement proper 
error handling&amp;#x27;]} = _check_security_best_practices(&amp;quot;password =
&amp;#x27;secret&amp;#x27;\nexec(&amp;#x27;print(1)&amp;#x27;)\n&amp;quot;)E    
+    where _check_security_best_practices = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x14408dcd0&amp;gt;._check_security_best_practices/Users/caitlyn/Projects/github
.com/ravenoak/devsynth/tests/unit/domain/models/test_wsde_security_checks.py:12:
 AssertionError____________ 
test_check_security_best_practices_accepts_clean_code 
_____________wsde_team_factory = &amp;lt;function 
wsde_team_factory.&amp;lt;locals&amp;gt;._factory at 0x142e8e8e0&amp;gt;    
@pytest.mark.fast    def 
test_check_security_best_practices_accepts_clean_code(wsde_team_factory):       
&amp;quot;&amp;quot;&amp;quot;ReqID: WSDE-SECURITY-02  passes checklist when 
code avoids red flags.&amp;quot;&amp;quot;&amp;quot;            team = 
wsde_team_factory()        secure_code = (            &amp;quot;def 
process_items(items):\n&amp;quot;            &amp;quot;    processed_items = 
[]\n&amp;quot;            &amp;quot;    for element in items:\n&amp;quot;       
&amp;quot;        processed_items.append(element)\n&amp;quot;            
&amp;quot;    return processed_items\n&amp;quot;        )&amp;gt;       assert 
team._check_security_best_practices(secure_code) is TrueE       AssertionError: 
assert {&amp;#x27;compliance_level&amp;#x27;: &amp;#x27;high&amp;#x27;, 
&amp;#x27;issues&amp;#x27;: [], &amp;#x27;suggestions&amp;#x27;: []} is TrueE   
+  where {&amp;#x27;compliance_level&amp;#x27;: &amp;#x27;high&amp;#x27;, 
&amp;#x27;issues&amp;#x27;: [], &amp;#x27;suggestions&amp;#x27;: []} = 
_check_security_best_practices(&amp;#x27;def process_items(items):\n    
processed_items = []\n    for element in items:\n        
processed_items.append(element)\n    return processed_items\n&amp;#x27;)E       
+    where _check_security_best_practices = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144021400&amp;gt;._check_security_best_practices/Users/caitlyn/Projects/github
.com/ravenoak/devsynth/tests/unit/domain/models/test_wsde_security_checks.py:27:
 AssertionError_______________ test_balance_security_and_performance_idempotent 
_______________wsde_team_factory = &amp;lt;function 
wsde_team_factory.&amp;lt;locals&amp;gt;._factory at 0x142e8e700&amp;gt;    
@pytest.mark.fast    def 
test_balance_security_and_performance_idempotent(wsde_team_factory):        
&amp;quot;&amp;quot;&amp;quot;ReqID: WSDE-SECURITY-03  avoids duplicating 
checklist annotations.&amp;quot;&amp;quot;&amp;quot;            team = 
wsde_team_factory()        code = &amp;quot;def run():\n    return 
True&amp;quot;            balanced_once = 
team._balance_security_and_performance(code)        balanced_twice = 
team._balance_security_and_performance(balanced_once)            assert 
balanced_once == balanced_twice&amp;gt;       assert 
balanced_once.count(&amp;quot;Security and performance balance&amp;quot;) == 1  
^^^^^^^^^^^^^^^^^^^E       AttributeError: &amp;#x27;dict&amp;#x27; object has 
no attribute 
&amp;#x27;count&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/domain/models/test_wsde_security_checks.py:41: 
AttributeError__________________ test_role_assignment_uses_expertise_scores 
__________________    @pytest.mark.fast    def 
test_role_assignment_uses_expertise_scores():        agents = [            
DummyAgent(name=&amp;quot;lead&amp;quot;, 
expertise=[&amp;quot;leadership&amp;quot;, &amp;quot;coordination&amp;quot;]),  
DummyAgent(name=&amp;quot;builder&amp;quot;, 
expertise=[&amp;quot;development&amp;quot;, &amp;quot;testing&amp;quot;]),      
DummyAgent(name=&amp;quot;designer&amp;quot;, 
expertise=[&amp;quot;architecture&amp;quot;, &amp;quot;planning&amp;quot;]),    
DummyAgent(name=&amp;quot;reviewer&amp;quot;, 
expertise=[&amp;quot;evaluation&amp;quot;, &amp;quot;analysis&amp;quot;]),      
]        team = _bind_team(WSDETeam(name=&amp;quot;roles&amp;quot;, 
agents=agents))            assignments = team.assign_roles()        primus = 
assignments.as_name_mapping()[&amp;quot;primus&amp;quot;]        assert primus 
is not None and primus.name == &amp;quot;lead&amp;quot;&amp;gt;       role_map =
team.get_role_map()                   ^^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;get_role_map&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/tests/unit/domain/models/test_wsde_strategies.py:88: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,724 - 
devsynth.domain.models.wsde_core - INFO - Added agent lead to team 
roles2025-10-28 09:26:56,724 - devsynth.domain.models.wsde_core - INFO - Added 
agent builder to team roles2025-10-28 09:26:56,724 - 
devsynth.domain.models.wsde_core - INFO - Added agent designer to team 
roles2025-10-28 09:26:56,724 - devsynth.domain.models.wsde_core - INFO - Added 
agent reviewer to team roles2025-10-28 09:26:56,724 - 
devsynth.domain.models.wsde_roles - INFO - Role assignments for team roles: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;lead&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;builder&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;designer&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;reviewer&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent lead to team 
rolesINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
builder to team rolesINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent designer to 
team rolesINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent reviewer to team rolesINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments for team
roles: {&amp;#x27;primus&amp;#x27;: &amp;#x27;lead&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;builder&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;designer&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: &amp;#x27;reviewer&amp;#x27;, 
&amp;#x27;evaluator&amp;#x27;: None}____________________ 
TestWSDETeam.test_get_primus_succeeds _____________________self = 
&amp;lt;tests.unit.domain.models.test_wsde_team.TestWSDETeam object at 
0x120c3afc0&amp;gt;    @pytest.mark.fast    def test_get_primus_succeeds(self): 
&amp;quot;&amp;quot;&amp;quot;Test getting the current Primus agent.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        self.team.add_agent(self.agent1)
self.team.add_agent(self.agent2)        self.team.primus_index = 1        primus
= self.team.get_primus()&amp;gt;       assert primus == self.agent2E       
AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5417640800&amp;#x27;&amp;gt; == &amp;lt;MagicMock 
id=&amp;#x27;5417322064&amp;#x27;&amp;gt;E         E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/model
s/test_wsde_team.py:72: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:26:56,762 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_wsde_team2025-10-28 09:26:56,762 - devsynth.domain.models.wsde_core - INFO 
- Added agent agent2 to team test_wsde_team------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_wsde_teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent agent2 to team test_wsde_team_______ 
TestWSDETeam.test_analyze_trade_offs_detects_conflicts_succeeds ________self = 
&amp;lt;tests.unit.domain.models.test_wsde_team.TestWSDETeam object at 
0x120c50380&amp;gt;    @pytest.mark.fast    def 
test_analyze_trade_offs_detects_conflicts_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Trade-off analysis should flag options with 
similar scores as conflicts.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;
evaluated = [            {&amp;quot;id&amp;quot;: 1, &amp;quot;score&amp;quot;: 
0.8},            {&amp;quot;id&amp;quot;: 2, &amp;quot;score&amp;quot;: 0.78},  
{&amp;quot;id&amp;quot;: 3, &amp;quot;score&amp;quot;: 0.2},        ]        
trade_offs = self.team.analyze_trade_offs(            evaluated, 
conflict_detection_threshold=0.7        )&amp;gt;       opt1 = next(o for o in 
trade_offs if o[&amp;quot;id&amp;quot;] == 1)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
StopIteration/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/dom
ain/models/test_wsde_team.py:134: StopIteration----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:26:56,796 - 
devsynth.domain.models.wsde_decision_making - INFO - Analyzing trade-offs 
between options------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_decision_making:logging_setup.py:615 Analyzing 
trade-offs between options_________________ 
test_add_solution_appends_and_triggers_hooks _________________    def 
test_add_solution_appends_and_triggers_hooks():        hook = MagicMock()       
team = SimpleNamespace(solutions={}, dialectical_hooks=)        task = 
{&amp;quot;id&amp;quot;: &amp;quot;t1&amp;quot;}        solution = 
{&amp;quot;content&amp;quot;: &amp;quot;data&amp;quot;}&amp;gt;       result = 
add_solution(team, task, solution)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/de
vsynth/tests/unit/domain/models/test_wsde_utils.py:107: _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ team = 
namespace(solutions={}, dialectical_hooks=[&amp;lt;MagicMock 
id=&amp;#x27;5436393232&amp;#x27;&amp;gt;])task = {&amp;#x27;id&amp;#x27;: 
&amp;#x27;t1&amp;#x27;}, solution = {&amp;#x27;content&amp;#x27;: 
&amp;#x27;data&amp;#x27;}    def add_solution(        team: Any, task: 
TaskPayload, solution: SolutionRecord    ) -&amp;gt; SolutionRecord:        
&amp;quot;&amp;quot;&amp;quot;Add a solution to the team and trigger dialectical
hooks.&amp;quot;&amp;quot;&amp;quot;        task_id = 
task.get(&amp;quot;id&amp;quot;)        if not task_id:            task_id = 
str(uuid4())            task[&amp;quot;id&amp;quot;] = task_id        else:     
# Normalise the identifier on the task payload so future calls reuse it.        
task[&amp;quot;id&amp;quot;] = task_id    &amp;gt;       
team.solutions.add(task_id, solution)        ^^^^^^^^^^^^^^^^^^E       
AttributeError: &amp;#x27;dict&amp;#x27; object has no attribute 
&amp;#x27;add&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/domain/models/wsde_utils.py:236: AttributeError_______________ 
test_calculate_expertise_score_multiple_matches ________________    
@pytest.mark.fast    def test_calculate_expertise_score_multiple_matches():     
&amp;quot;&amp;quot;&amp;quot;Test that calculate expertise score multiple 
matches.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestWsdeExpertiseScoreTeam&amp;quot;)        agent = 
DummyAgent([&amp;quot;python&amp;quot;, &amp;quot;documentation&amp;quot;])     
team.add_agent(agent)        task = {&amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;generate 
documentation&amp;quot;}        score = team._calculate_expertise_score(agent, 
task)&amp;gt;       assert score &amp;gt; 1E       assert 1 &amp;gt; 
1/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsd
e_expertise_score.py:25: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:26:56,922 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent to team 
TestWsdeExpertiseScoreTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent to team 
TestWsdeExpertiseScoreTeam____________ 
test_summarize_voting_result_reports_winner_and_counts ____________    
@pytest.mark.fast    def 
test_summarize_voting_result_reports_winner_and_counts():        team = 
WSDETeam(name=&amp;quot;facade&amp;quot;)        
team.add_agents([SimpleAgent(&amp;quot;a&amp;quot;), 
SimpleAgent(&amp;quot;b&amp;quot;)])        voting_result = {            
&amp;quot;status&amp;quot;: &amp;quot;completed&amp;quot;,            
&amp;quot;result&amp;quot;: {                &amp;quot;method&amp;quot;: 
&amp;quot;majority&amp;quot;,                &amp;quot;winner&amp;quot;: 
&amp;quot;A&amp;quot;,                &amp;quot;tie_broken&amp;quot;: True,     
&amp;quot;tie_breaker_method&amp;quot;: &amp;quot;primus&amp;quot;,            
},            &amp;quot;vote_counts&amp;quot;: {&amp;quot;A&amp;quot;: 2, 
&amp;quot;B&amp;quot;: 1},            &amp;quot;vote_weights&amp;quot;: 
{&amp;quot;a&amp;quot;: 1.0, &amp;quot;b&amp;quot;: 1.5},        }        
summary = team.summarize_voting_result(voting_result)&amp;gt;       assert 
&amp;quot;Voting was completed using majority.&amp;quot; in summaryE       
AssertionError: assert &amp;#x27;Voting was completed using majority.&amp;#x27; 
in &amp;#x27;Vote distribution: A: 2 votes, B: 1 votes\nVote weights: a: 1.00, 
b: 
1.50&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/do
main/test_wsde_facade.py:50: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:26:56,933 - 
devsynth.domain.models.wsde_core - INFO - Added agent a to team facade2025-10-28
09:26:56,933 - devsynth.domain.models.wsde_core - INFO - Added agent b to team 
facade------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a to team 
facadeINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
b to team facade__________________ test_select_primus_updates_index_and_role 
___________________    @pytest.mark.fast    def 
test_select_primus_updates_index_and_role():        team = 
WSDETeam(name=&amp;quot;facade&amp;quot;)        doc = 
SimpleAgent(&amp;quot;doc&amp;quot;, [&amp;quot;documentation&amp;quot;])       
coder = SimpleAgent(&amp;quot;coder&amp;quot;, [&amp;quot;python&amp;quot;])    
team.add_agents()        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;coding&amp;quot;, &amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})&amp;gt;       assert team.get_primus() is coderE   
assert &amp;lt;tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x14445cfe0&amp;gt; is 
&amp;lt;tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x14445cfb0&amp;gt;E        +  where 
&amp;lt;tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x14445cfe0&amp;gt; = get_primus()E        +    where get_primus = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444814f0&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_facade_roles.py:23: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,940 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
facade2025-10-28 09:26:56,940 - devsynth.domain.models.wsde_core - INFO - Added 
agent coder to team facade2025-10-28 09:26:56,940 - 
devsynth.domain.models.wsde_roles - INFO - Selected doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
facadeINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
coder to team facadeINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as primus 
based on expertise________________ test_dynamic_role_reassignment_rotates_primus
_________________    @pytest.mark.fast    def 
test_dynamic_role_reassignment_rotates_primus():        team = 
WSDETeam(name=&amp;quot;facade&amp;quot;)        doc = 
SimpleAgent(&amp;quot;doc&amp;quot;, [&amp;quot;documentation&amp;quot;])       
coder = SimpleAgent(&amp;quot;coder&amp;quot;, [&amp;quot;python&amp;quot;])    
tester = SimpleAgent(&amp;quot;tester&amp;quot;, [&amp;quot;testing&amp;quot;]) 
team.add_agents()        
team.dynamic_role_reassignment({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})        first = team.get_primus()        
team.dynamic_role_reassignment({&amp;quot;type&amp;quot;: 
&amp;quot;coding&amp;quot;, &amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})        second = team.get_primus()&amp;gt;       
assert first is docE       assert 
&amp;lt;tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x14445a750&amp;gt; is 
&amp;lt;tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x14445a7b0&amp;gt;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/domain/test_wsde_facade_roles.py:39: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,948 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
facade2025-10-28 09:26:56,949 - devsynth.domain.models.wsde_core - INFO - Added 
agent coder to team facade2025-10-28 09:26:56,949 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
facade2025-10-28 09:26:56,949 - devsynth.domain.models.wsde_roles - INFO - 
Selected doc as primus based on expertise2025-10-28 09:26:56,949 - 
devsynth.domain.models.wsde_roles - INFO - Selected coder as primus based on 
expertise2025-10-28 09:26:56,949 - devsynth.domain.models.wsde_roles - INFO - 
Selected tester as primus based on expertise2025-10-28 09:26:56,949 - 
devsynth.domain.models.wsde_roles - INFO - Selected doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
facadeINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent
coder to team facadeINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
facadeINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
doc as primus based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester as primus
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as primus 
based on expertise_________ 
test_documentation_tasks_pick_documentation_experts_succeeds _________    
@pytest.mark.fast    def 
test_documentation_tasks_pick_documentation_experts_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that documentation tasks pick documentation 
experts succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
team = WSDETeam(name=&amp;quot;TestWsdePhaseRoleRotationTeam&amp;quot;)        
coder = _agent(&amp;quot;coder&amp;quot;, [&amp;quot;python&amp;quot;])        
doc = _agent(&amp;quot;doc&amp;quot;, [&amp;quot;documentation&amp;quot;, 
&amp;quot;markdown&amp;quot;])        team.add_agents()        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})&amp;gt;       assert team.get_primus() is 
docE       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5440648832&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5440645136&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5440648832&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144483440&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_phase_role_rotation.py:42: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:56,994 - 
devsynth.domain.models.wsde_core - INFO - Added agent coder to team 
TestWsdePhaseRoleRotationTeam2025-10-28 09:26:56,994 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdePhaseRoleRotationTeam2025-10-28 09:26:56,994 - 
devsynth.domain.models.wsde_roles - INFO - Selected coder as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder to team 
TestWsdePhaseRoleRotationTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdePhaseRoleRotationTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder as primus 
based on expertise_____________ 
test_current_primus_considered_in_selection_succeeds _____________    
@pytest.mark.fast    def test_current_primus_considered_in_selection_succeeds():
&amp;quot;&amp;quot;&amp;quot;Current primus remains candidate when reselection 
occurs.            This prevents ``select_primus_by_expertise`` from skipping 
the        existing primus simply because they&amp;#x27;ve served 
once.&amp;quot;&amp;quot;&amp;quot;            team = 
WSDETeam(name=&amp;quot;PrimusReselectionTeam&amp;quot;)        py = 
_agent(&amp;quot;py&amp;quot;, [&amp;quot;python&amp;quot;])        js = 
_agent(&amp;quot;js&amp;quot;, [&amp;quot;javascript&amp;quot;])        
team.add_agents()            team.assign_roles()        
team.select_primus_by_expertise({&amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})    &amp;gt;       assert team.get_primus() is pyE  
AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5440311744&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5440320816&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5440311744&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444a5ee0&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_primus_selection.py:69: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,027 - 
devsynth.domain.models.wsde_core - INFO - Added agent py to team 
PrimusReselectionTeam2025-10-28 09:26:57,027 - devsynth.domain.models.wsde_core 
- INFO - Added agent js to team PrimusReselectionTeam2025-10-28 09:26:57,027 - 
devsynth.domain.models.wsde_roles - INFO - Role assignments for team 
PrimusReselectionTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}2025-10-28 09:26:57,028 - 
devsynth.domain.models.wsde_roles - INFO - Selected js as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to team 
PrimusReselectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent js to team 
PrimusReselectionTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments for team
PrimusReselectionTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected js as primus 
based on expertise_____________ 
test_documentation_tasks_prefer_doc_experts_succeeds _____________    
@pytest.mark.fast    def test_documentation_tasks_prefer_doc_experts_succeeds():
&amp;quot;&amp;quot;&amp;quot;Test that documentation tasks prefer doc experts 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestWsdePrimusSelectionTeam&amp;quot;)        coder = 
_agent(&amp;quot;coder&amp;quot;, [&amp;quot;python&amp;quot;])        doc = 
_agent(&amp;quot;doc&amp;quot;, [&amp;quot;documentation&amp;quot;, 
&amp;quot;markdown&amp;quot;])        team.add_agents()        task = 
{&amp;quot;type&amp;quot;: &amp;quot;documentation&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;Write docs&amp;quot;}        
team.select_primus_by_expertise(task)&amp;gt;       assert team.get_primus() is 
docE       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5440388128&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5440383376&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5440388128&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444a5c10&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_primus_selection.py:83: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,040 - 
devsynth.domain.models.wsde_core - INFO - Added agent coder to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,040 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,040 - 
devsynth.domain.models.wsde_roles - INFO - Selected coder as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder as primus 
based on expertise_______________ 
test_nested_task_metadata_is_flattened_succeeds ________________    
@pytest.mark.fast    def test_nested_task_metadata_is_flattened_succeeds():     
&amp;quot;&amp;quot;&amp;quot;Test that nested task metadata is flattened 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestWsdePrimusSelectionTeam&amp;quot;)        py = 
_agent(&amp;quot;py&amp;quot;, [&amp;quot;python&amp;quot;])        doc = 
_agent(&amp;quot;doc&amp;quot;, [&amp;quot;documentation&amp;quot;])        
team.add_agents()        task = {&amp;quot;context&amp;quot;: 
{&amp;quot;info&amp;quot;: [{&amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;}]}}        
team.select_primus_by_expertise(task)&amp;gt;       assert team.get_primus() is 
pyE       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5440710048&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5440404176&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5440710048&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444a5820&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_primus_selection.py:98: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,051 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,052 - 
devsynth.domain.models.wsde_core - INFO - Added agent py to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,052 - 
devsynth.domain.models.wsde_roles - INFO - Selected doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as primus 
based on expertise______________ 
test_select_primus_by_expertise_coverage_succeeds _______________    
@pytest.mark.fast    def test_select_primus_by_expertise_coverage_succeeds():   
&amp;quot;&amp;quot;&amp;quot;Test that select primus by expertise coverage 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
inspect        import os        from types import SimpleNamespace            
import coverage            import devsynth.domain.models.wsde_facade as wsde    
team = wsde.WSDETeam(name=&amp;quot;TestWsdePrimusSelectionTeam&amp;quot;)      
cov = coverage.Coverage()        cov.start()        
team.select_primus_by_expertise({})        team.add_agent(            
SimpleNamespace(                name=&amp;quot;a1&amp;quot;, 
expertise=[&amp;quot;python&amp;quot;], current_role=None, has_been_primus=False
)        )        team.add_agent(            SimpleNamespace(                
name=&amp;quot;a2&amp;quot;,                config=SimpleNamespace(             
parameters={&amp;quot;expertise&amp;quot;: [&amp;quot;doc_generation&amp;quot;, 
&amp;quot;markdown&amp;quot;]}                ),                
current_role=None,                has_been_primus=False,            )        )  
team.add_agent(            SimpleNamespace(                
name=&amp;quot;a3&amp;quot;, expertise=[&amp;quot;testing&amp;quot;], 
current_role=None, has_been_primus=False            )        )        
team.add_agent(            SimpleNamespace(                
name=&amp;quot;a4&amp;quot;, expertise=[&amp;quot;security&amp;quot;], 
current_role=None, has_been_primus=False            )        )        
team.add_agent(            SimpleNamespace(                
name=&amp;quot;a5&amp;quot;,                
expertise=[&amp;quot;javascript&amp;quot;],                current_role=None,   
has_been_primus=False,            )        )        team.add_agent(            
SimpleNamespace(                name=&amp;quot;a6&amp;quot;, 
expertise=[&amp;quot;api&amp;quot;], current_role=None, has_been_primus=False   
)        )        team.add_agent(            SimpleNamespace(                
name=&amp;quot;a7&amp;quot;, expertise=[&amp;quot;design&amp;quot;], 
current_role=None, has_been_primus=False            )        )        
team.select_primus_by_expertise(            {&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;, &amp;quot;details&amp;quot;: [1, 2], 
&amp;quot;extra&amp;quot;: {&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;}}  
)        team.select_primus_by_expertise({&amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;testing&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;security&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;frontend&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;backend&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;design&amp;quot;})        cov.stop()        path = 
inspect.getsourcefile(wsde.WSDETeam.select_primus_by_expertise)        lines, 
start = inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)        
executable = []        skip = False        for i, line in enumerate(lines, 
start):            stripped = line.strip()            if 
stripped.startswith(&amp;#x27;&amp;quot;&amp;quot;&amp;quot;&amp;#x27;):        
if (                    
stripped.count(&amp;#x27;&amp;quot;&amp;quot;&amp;quot;&amp;#x27;) == 2         
and stripped.endswith(&amp;#x27;&amp;quot;&amp;quot;&amp;#x27;)                 
and stripped != &amp;#x27;&amp;quot;&amp;quot;&amp;quot;&amp;#x27;              
):                    continue                skip = not skip                
continue            if skip:                if 
stripped.endswith(&amp;#x27;&amp;quot;&amp;quot;&amp;quot;&amp;#x27;):          
skip = False                continue            if stripped:                
executable.append(i)        executed = set(cov.get_data().lines(path))        
coverage_percent = len(set(executable) &amp;amp; executed) / len(executable) * 
100&amp;gt;       assert coverage_percent &amp;gt;= 30E       assert 25.0 
&amp;gt;= 
30/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_ws
de_primus_selection.py:210: AssertionError----------------------------- Captured
stdout call -----------------------------2025-10-28 09:26:57,107 - 
devsynth.domain.models.wsde_roles - WARNING - Cannot select primus: no agents in
team2025-10-28 09:26:57,107 - devsynth.domain.models.wsde_core - INFO - Added 
agent a1 to team TestWsdePrimusSelectionTeam2025-10-28 09:26:57,107 - 
devsynth.domain.models.wsde_core - INFO - Added agent a2 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,107 - 
devsynth.domain.models.wsde_core - INFO - Added agent a3 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,107 - 
devsynth.domain.models.wsde_core - INFO - Added agent a4 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,107 - 
devsynth.domain.models.wsde_core - INFO - Added agent a5 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_core - INFO - Added agent a6 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_core - INFO - Added agent a7 to team 
TestWsdePrimusSelectionTeam2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_roles - INFO - Selected a1 as primus based on 
expertise2025-10-28 09:26:57,108 - devsynth.domain.models.wsde_roles - INFO - 
Selected a2 as primus based on expertise2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_roles - INFO - Selected a3 as primus based on 
expertise2025-10-28 09:26:57,108 - devsynth.domain.models.wsde_roles - INFO - 
Selected a4 as primus based on expertise2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_roles - INFO - Selected a5 as primus based on 
expertise2025-10-28 09:26:57,108 - devsynth.domain.models.wsde_roles - INFO - 
Selected a6 as primus based on expertise2025-10-28 09:26:57,108 - 
devsynth.domain.models.wsde_roles - INFO - Selected a7 as primus based on 
expertise2025-10-28 09:26:57,108 - devsynth.domain.models.wsde_roles - INFO - 
Selected a1 as primus based on expertise------------------------------ Captured 
log call -------------------------------WARNING  
devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select primus: no 
agents in teamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent a1 to team TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a4 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a5 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a6 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a7 to team 
TestWsdePrimusSelectionTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a2 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a3 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a4 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a5 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a6 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a7 as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as primus 
based on expertise________ 
test_vote_on_critical_decision_tie_triggers_consensus_succeeds 
________team_with_agents = (&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x14445a0c0&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440375024&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440528096&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440381216&amp;#x27;&amp;gt;)    @pytest.mark.fast    def 
test_vote_on_critical_decision_tie_triggers_consensus_succeeds(team_with_agents)
:        &amp;quot;&amp;quot;&amp;quot;Test that vote on critical decision tie 
triggers consensus succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;
team, doc, coder, _ = team_with_agents        doc.process.return_value = 
{&amp;quot;vote&amp;quot;: &amp;quot;A&amp;quot;}        
coder.process.return_value = {&amp;quot;vote&amp;quot;: &amp;quot;B&amp;quot;}  
task = {            &amp;quot;type&amp;quot;: 
&amp;quot;critical_decision&amp;quot;,            
&amp;quot;is_critical&amp;quot;: True,            &amp;quot;options&amp;quot;: 
[{&amp;quot;id&amp;quot;: &amp;quot;A&amp;quot;}, {&amp;quot;id&amp;quot;: 
&amp;quot;B&amp;quot;}],        }        with (            patch.object(team, 
&amp;quot;get_primus&amp;quot;, return_value=None),            
patch.object(team, &amp;quot;build_consensus&amp;quot;, 
return_value={&amp;quot;consensus&amp;quot;: &amp;quot;AB&amp;quot;}) as bc,    
):            result = team.vote_on_critical_decision(task)&amp;gt;           
assert result[&amp;quot;voting_initiated&amp;quot;]                   
^^^^^^^^^^^^^^^^^^^^^^^^^^E           KeyError: 
&amp;#x27;voting_initiated&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/domain/test_wsde_team.py:56: 
KeyError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:57,171 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,171 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,171 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeam___________ 
test_vote_on_critical_decision_weighted_voting_succeeds 
____________team_with_agents = 
(&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444a93d0&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440702992&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440651040&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5440649648&amp;#x27;&amp;gt;)    @pytest.mark.fast    def 
test_vote_on_critical_decision_weighted_voting_succeeds(team_with_agents):      
&amp;quot;&amp;quot;&amp;quot;Test that vote on critical decision weighted 
voting succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
team, doc, coder, tester = team_with_agents        for agent, level in [(doc, 
&amp;quot;expert&amp;quot;), (coder, &amp;quot;novice&amp;quot;), (tester, 
&amp;quot;novice&amp;quot;)]:            cfg = MagicMock()            cfg.name =
agent.name            cfg.parameters = {&amp;quot;expertise&amp;quot;: 
agent.expertise, &amp;quot;expertise_level&amp;quot;: level}            
agent.config = cfg        doc.process.return_value = {&amp;quot;vote&amp;quot;: 
&amp;quot;A&amp;quot;}        coder.process.return_value = 
{&amp;quot;vote&amp;quot;: &amp;quot;B&amp;quot;}        
tester.process.return_value = {&amp;quot;vote&amp;quot;: &amp;quot;B&amp;quot;} 
task = {            &amp;quot;type&amp;quot;: 
&amp;quot;critical_decision&amp;quot;,            &amp;quot;domain&amp;quot;: 
&amp;quot;documentation&amp;quot;,            &amp;quot;is_critical&amp;quot;: 
True,            &amp;quot;options&amp;quot;: [{&amp;quot;id&amp;quot;: 
&amp;quot;A&amp;quot;}, {&amp;quot;id&amp;quot;: &amp;quot;B&amp;quot;}],       
}        result = team.vote_on_critical_decision(task)        assert 
result[&amp;quot;result&amp;quot;][&amp;quot;winner&amp;quot;] == 
&amp;quot;A&amp;quot;&amp;gt;       assert 
result[&amp;quot;result&amp;quot;][&amp;quot;method&amp;quot;] == 
&amp;quot;weighted_vote&amp;quot;E       AssertionError: assert 
&amp;#x27;majority_vote&amp;#x27; == &amp;#x27;weighted_vote&amp;#x27;E         
E         - weighted_voteE         + 
majority_vote/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/dom
ain/test_wsde_team.py:84: AssertionError---------------------------- Captured 
stdout setup -----------------------------2025-10-28 09:26:57,190 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,190 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,191 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeam______________ test_build_consensus_multiple_and_single_succeeds
_______________team_with_agents = 
(&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1444f3ce0&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441011248&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441113408&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441117152&amp;#x27;&amp;gt;)    @pytest.mark.fast    def 
test_build_consensus_multiple_and_single_succeeds(team_with_agents):        
&amp;quot;&amp;quot;&amp;quot;Test that build consensus multiple and single 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team, doc, 
coder, _ = team_with_agents        task = {&amp;quot;id&amp;quot;: 
&amp;quot;t1&amp;quot;, &amp;quot;description&amp;quot;: 
&amp;quot;demo&amp;quot;}        team.add_solution(task, 
{&amp;quot;agent&amp;quot;: doc.name, &amp;quot;content&amp;quot;: 
&amp;quot;First&amp;quot;})        single = team.build_consensus(task)&amp;gt;  
assert single[&amp;quot;method&amp;quot;] == &amp;quot;single_solution&amp;quot;
^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;method&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/domain/test_wsde_team.py:96: KeyError---------------------------- 
Captured stdout setup -----------------------------2025-10-28 09:26:57,206 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,206 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,206 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeam----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,207 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task t12025-10-28 
09:26:57,207 - devsynth.domain.models.wsde_voting - WARNING - Cannot build 
consensus: no options provided------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
t1WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided__________ 
test_documentation_task_selects_unused_doc_agent_succeeds 
___________team_with_agents = 
(&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1445367b0&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441284352&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441288960&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441391360&amp;#x27;&amp;gt;)    @pytest.mark.fast    def 
test_documentation_task_selects_unused_doc_agent_succeeds(team_with_agents):    
&amp;quot;&amp;quot;&amp;quot;Test that documentation task selects unused doc 
agent succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team,
doc, coder, tester = team_with_agents        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;coding&amp;quot;, &amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})&amp;gt;       assert team.get_primus() is coderE   
AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5441284352&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5441288960&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5441284352&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1445367b0&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_team.py:111: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:26:57,224 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,225 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,225 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeam----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,225 - 
devsynth.domain.models.wsde_roles - INFO - Selected doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as primus 
based on expertise_______________ 
test_vote_on_critical_decision_coverage_succeeds _______________    
@pytest.mark.fast    def test_vote_on_critical_decision_coverage_succeeds():    
&amp;quot;&amp;quot;&amp;quot;Test that vote on critical decision coverage 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
inspect        from types import SimpleNamespace            import coverage     
import devsynth.domain.models.wsde_facade as wsde            team = 
wsde.WSDETeam(name=&amp;quot;TestWsdeTeamTeam&amp;quot;)        a1 = 
SimpleNamespace(            name=&amp;quot;a1&amp;quot;,            
config=SimpleNamespace(                name=&amp;quot;a1&amp;quot;, 
parameters={&amp;quot;expertise&amp;quot;: [&amp;quot;python&amp;quot;], 
&amp;quot;expertise_level&amp;quot;: &amp;quot;expert&amp;quot;}            ),  
process=lambda t: {&amp;quot;vote&amp;quot;: &amp;quot;A&amp;quot;},        )   
a2 = SimpleNamespace(            name=&amp;quot;a2&amp;quot;,            
config=SimpleNamespace(                name=&amp;quot;a2&amp;quot;, 
parameters={&amp;quot;expertise&amp;quot;: [&amp;quot;docs&amp;quot;], 
&amp;quot;expertise_level&amp;quot;: &amp;quot;novice&amp;quot;}            ),  
process=lambda t: {&amp;quot;vote&amp;quot;: &amp;quot;B&amp;quot;},        )   
a3 = SimpleNamespace(            name=&amp;quot;a3&amp;quot;,            
config=SimpleNamespace(                name=&amp;quot;a3&amp;quot;, 
parameters={&amp;quot;expertise&amp;quot;: [&amp;quot;python&amp;quot;], 
&amp;quot;expertise_level&amp;quot;: &amp;quot;novice&amp;quot;}            ),  
process=lambda t: {&amp;quot;vote&amp;quot;: &amp;quot;A&amp;quot;},        )   
team.add_agents()        cov = coverage.Coverage()        cov.start()        
team.vote_on_critical_decision({&amp;quot;type&amp;quot;: 
&amp;quot;other&amp;quot;})        team.vote_on_critical_decision(            
{&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;, 
&amp;quot;is_critical&amp;quot;: True, &amp;quot;options&amp;quot;: []}        )
team.vote_on_critical_decision(            {                
&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;,                
&amp;quot;is_critical&amp;quot;: True,                
&amp;quot;options&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;A&amp;quot;}, 
{&amp;quot;id&amp;quot;: &amp;quot;B&amp;quot;}],            }        )        
team.vote_on_critical_decision(            {                
&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;,                
&amp;quot;domain&amp;quot;: &amp;quot;python&amp;quot;,                
&amp;quot;is_critical&amp;quot;: True,                
&amp;quot;options&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;A&amp;quot;}, 
{&amp;quot;id&amp;quot;: &amp;quot;B&amp;quot;}],            }        )        
a3.process = lambda t: {&amp;quot;vote&amp;quot;: &amp;quot;B&amp;quot;}        
team.vote_on_critical_decision(            {                
&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;,                
&amp;quot;is_critical&amp;quot;: True,                
&amp;quot;options&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;A&amp;quot;}, 
{&amp;quot;id&amp;quot;: &amp;quot;B&amp;quot;}],            }        )        
path = wsde.__file__        lines, start = 
inspect.getsourcelines(wsde.WSDETeam.vote_on_critical_decision)        
executable = list(range(start, start + len(lines)))&amp;gt;       executed = 
set(cov.get_data().lines(path))                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; 
object is not 
iterable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/t
est_wsde_team.py:253: TypeError----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:26:57,279 - 
devsynth.domain.models.wsde_core - INFO - Added agent a1 to team 
TestWsdeTeamTeam2025-10-28 09:26:57,279 - devsynth.domain.models.wsde_core - 
INFO - Added agent a2 to team TestWsdeTeamTeam2025-10-28 09:26:57,280 - 
devsynth.domain.models.wsde_core - INFO - Added agent a3 to team 
TestWsdeTeamTeam2025-10-28 09:26:57,307 - devsynth.domain.models.wsde_voting - 
WARNING - Cannot conduct vote: no options provided2025-10-28 09:26:57,307 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot conduct vote: no options 
provided------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent a2 to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to team 
TestWsdeTeamTeamWARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615
Cannot conduct vote: no options providedWARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct vote: no 
options provided_____________ 
test_expertise_selection_and_flag_rotation_succeeds ______________    
@pytest.mark.fast    def test_expertise_selection_and_flag_rotation_succeeds(): 
&amp;quot;&amp;quot;&amp;quot;Test that expertise selection and flag rotation 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestWsdeTeamTeam&amp;quot;)        doc = MagicMock()    
doc.name = &amp;quot;doc&amp;quot;        doc.expertise = 
[&amp;quot;documentation&amp;quot;]        coder = MagicMock()        coder.name
= &amp;quot;coder&amp;quot;        coder.expertise = 
[&amp;quot;python&amp;quot;]        tester = MagicMock()        tester.name = 
&amp;quot;tester&amp;quot;        tester.expertise = 
[&amp;quot;testing&amp;quot;]        team.add_agents()        tasks = [         
{&amp;quot;type&amp;quot;: &amp;quot;coding&amp;quot;, 
&amp;quot;language&amp;quot;: &amp;quot;python&amp;quot;},            
{&amp;quot;type&amp;quot;: &amp;quot;documentation&amp;quot;},            
{&amp;quot;type&amp;quot;: &amp;quot;testing&amp;quot;},        ]        
expected =         for task, agent in zip(tasks, expected):            
team.select_primus_by_expertise(task)&amp;gt;           assert team.get_primus()
is agentE           AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5441506816&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5441517664&amp;#x27;&amp;gt;E            +  where &amp;lt;MagicMock
id=&amp;#x27;5441506816&amp;#x27;&amp;gt; = get_primus()E            +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x14457d4c0&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/domain/test_wsde_team.py:301: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,342 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,342 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,343 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam2025-10-28 09:26:57,344 - devsynth.domain.models.wsde_roles - 
INFO - Selected doc as primus based on expertise------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 
Selected doc as primus based on expertise_____________________ 
test_select_primus_coverage_succeeds _____________________team_with_agents = 
(&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144577140&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441548368&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441550960&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5441542560&amp;#x27;&amp;gt;)    @pytest.mark.fast    def 
test_select_primus_coverage_succeeds(team_with_agents):        
&amp;quot;&amp;quot;&amp;quot;Ensure select_primus_by_expertise maintains 
&amp;gt;80% coverage.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;       
import inspect            import coverage            import 
devsynth.domain.models.wsde_facade as wsde            team, doc, coder, tester =
team_with_agents        cov = coverage.Coverage()        cov.start()        
empty = wsde.WSDETeam(name=&amp;quot;TestWsdeTeamTeam&amp;quot;)        
empty.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;coding&amp;quot;, &amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;testing&amp;quot;})        
team.select_primus_by_expertise({&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;})        path = wsde.__file__        lines, 
start = inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)        
executable = list(range(start, start + len(lines)))&amp;gt;       executed = 
set(cov.get_data().lines(path))                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; 
object is not 
iterable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/t
est_wsde_team.py:334: TypeError---------------------------- Captured stdout 
setup -----------------------------2025-10-28 09:26:57,357 - 
devsynth.domain.models.wsde_core - INFO - Added agent doc to team 
TestWsdeTeamTeam2025-10-28 09:26:57,357 - devsynth.domain.models.wsde_core - 
INFO - Added agent coder to team TestWsdeTeamTeam2025-10-28 09:26:57,358 - 
devsynth.domain.models.wsde_core - INFO - Added agent tester to team 
TestWsdeTeamTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc to team 
TestWsdeTeamTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent coder to team TestWsdeTeamTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent tester to team
TestWsdeTeamTeam----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,392 - 
devsynth.domain.models.wsde_roles - WARNING - Cannot select primus: no agents in
team2025-10-28 09:26:57,394 - devsynth.domain.models.wsde_roles - INFO - 
Selected doc as primus based on expertise2025-10-28 09:26:57,395 - 
devsynth.domain.models.wsde_roles - INFO - Selected coder as primus based on 
expertise2025-10-28 09:26:57,396 - devsynth.domain.models.wsde_roles - INFO - 
Selected tester as primus based on expertise2025-10-28 09:26:57,396 - 
devsynth.domain.models.wsde_roles - INFO - Selected doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select primus: no 
agents in teamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 
Selected doc as primus based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder as primus 
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester as primus
based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as primus 
based on expertise_________________________ test_majority_voting_simple 
__________________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x14457cfb0&amp;gt;    @pytest.mark.fast    def 
test_majority_voting_simple(monkeypatch):        agents = 
[DummyAgent(&amp;quot;a1&amp;quot;, [&amp;quot;a&amp;quot;]), 
DummyAgent(&amp;quot;a2&amp;quot;, [&amp;quot;b&amp;quot;]), 
DummyAgent(&amp;quot;a3&amp;quot;, [&amp;quot;a&amp;quot;])]        team = 
bind(DummyTeam(agents, primus=agents[0]))        monkeypatch.setattr(random, 
&amp;quot;choice&amp;quot;, lambda x: x[0])        task = 
{&amp;quot;options&amp;quot;: [&amp;quot;option_a&amp;quot;, 
&amp;quot;option_b&amp;quot;], &amp;quot;voting_method&amp;quot;: 
&amp;quot;majority&amp;quot;}        result = 
team.vote_on_critical_decision(task)        assert 
result[&amp;quot;status&amp;quot;] == &amp;quot;completed&amp;quot;&amp;gt;     
assert result[&amp;quot;result&amp;quot;] == &amp;quot;option_a&amp;quot;E      
AssertionError: assert {&amp;#x27;method&amp;#x27;: 
&amp;#x27;majority_vote&amp;#x27;, &amp;#x27;winner&amp;#x27;: 
&amp;#x27;option_a&amp;#x27;} == 
&amp;#x27;option_a&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/tests/unit/domain/test_wsde_voting_logic.py:46: 
AssertionError_____________________ test_handle_tied_vote_primus_breaks 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x14457e7b0&amp;gt;    @pytest.mark.fast    def 
test_handle_tied_vote_primus_breaks(monkeypatch):        agents = 
[DummyAgent(&amp;quot;primus&amp;quot;, [&amp;quot;x&amp;quot;]), 
DummyAgent(&amp;quot;other&amp;quot;, [&amp;quot;y&amp;quot;])]        team = 
bind(DummyTeam(agents, primus=agents[0]))        voting_result = {            
&amp;quot;votes&amp;quot;: {&amp;quot;primus&amp;quot;: &amp;quot;A&amp;quot;, 
&amp;quot;other&amp;quot;: &amp;quot;B&amp;quot;},            
&amp;quot;options&amp;quot;: [&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;],    
&amp;quot;status&amp;quot;: &amp;quot;pending&amp;quot;,        }        
vote_counts = {&amp;quot;A&amp;quot;: 1, &amp;quot;B&amp;quot;: 1}        res = 
team._handle_tied_vote(            {&amp;quot;options&amp;quot;: 
[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;]}, voting_result, vote_counts, 
[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;]        )&amp;gt;       assert 
res[&amp;quot;result&amp;quot;] == &amp;quot;A&amp;quot;E       assert 
{&amp;#x27;consensus_result&amp;#x27;: {&amp;#x27;explanation&amp;#x27;: 
&amp;quot;Partial consensus on option &amp;#x27;A&amp;#x27; with 0.0% support 
after 3 rounds of discussion.&amp;quot;...nces&amp;#x27;: 
{&amp;#x27;other&amp;#x27;: {&amp;#x27;A&amp;#x27;: 0.0, &amp;#x27;B&amp;#x27;: 
0.0}, &amp;#x27;primus&amp;#x27;: {&amp;#x27;A&amp;#x27;: 0.0, 
&amp;#x27;B&amp;#x27;: 0.0}}, ...}, &amp;#x27;tied&amp;#x27;: True, 
&amp;#x27;tied_options&amp;#x27;: [&amp;#x27;A&amp;#x27;, 
&amp;#x27;B&amp;#x27;]} == 
&amp;#x27;A&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/domain/test_wsde_voting_logic.py:62: AssertionError__________________ 
test_weighted_voting_tie_primus_resolution __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14457da90&amp;gt;    
@pytest.mark.fast    def 
test_weighted_voting_tie_primus_resolution(monkeypatch):        a1 = 
DummyAgent(&amp;quot;p&amp;quot;, [&amp;quot;frontend&amp;quot;])        a2 = 
DummyAgent(&amp;quot;s&amp;quot;, [&amp;quot;frontend&amp;quot;])        team = 
bind(DummyTeam(, primus=a1))        monkeypatch.setattr(random, 
&amp;quot;choice&amp;quot;, lambda opts: opts[0])        task = {            
&amp;quot;options&amp;quot;: [&amp;quot;frontend&amp;quot;, 
&amp;quot;backend&amp;quot;],            &amp;quot;voting_method&amp;quot;: 
&amp;quot;weighted&amp;quot;,            &amp;quot;domain&amp;quot;: 
&amp;quot;frontend&amp;quot;,        }        # Force votes to create a tie     
voting_result = {            &amp;quot;options&amp;quot;: 
[&amp;quot;frontend&amp;quot;, &amp;quot;backend&amp;quot;],            
&amp;quot;votes&amp;quot;: {&amp;quot;p&amp;quot;: &amp;quot;frontend&amp;quot;,
&amp;quot;s&amp;quot;: &amp;quot;backend&amp;quot;},            
&amp;quot;method&amp;quot;: &amp;quot;weighted&amp;quot;,            
&amp;quot;status&amp;quot;: &amp;quot;pending&amp;quot;,        }        
vote_counts = {&amp;quot;frontend&amp;quot;: 1, &amp;quot;backend&amp;quot;: 1} 
res = team._handle_tied_vote(            task, voting_result, vote_counts, 
[&amp;quot;frontend&amp;quot;, &amp;quot;backend&amp;quot;]        )&amp;gt;    
assert res[&amp;quot;result&amp;quot;] == &amp;quot;frontend&amp;quot;E       
assert {&amp;#x27;consensus_result&amp;#x27;: {&amp;#x27;explanation&amp;#x27;: 
&amp;quot;Consensus reached on option &amp;#x27;frontend&amp;#x27; with 100.0% 
support after 1 rounds of dis...&amp;#x27;frontend&amp;#x27;: 1.0}, 
&amp;#x27;s&amp;#x27;: {&amp;#x27;backend&amp;#x27;: 0.0, 
&amp;#x27;frontend&amp;#x27;: 1.0}}, ...}, &amp;#x27;tied&amp;#x27;: True, 
&amp;#x27;tied_options&amp;#x27;: [&amp;#x27;frontend&amp;#x27;, 
&amp;#x27;backend&amp;#x27;]} == 
&amp;#x27;frontend&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/tests/unit/domain/test_wsde_voting_logic.py:88: 
AssertionError___________________ test_vote_on_critical_decision_majority 
____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object
at 0x14457d910&amp;gt;    @pytest.mark.fast    def 
test_vote_on_critical_decision_majority(monkeypatch):        agents = [         
DummyAgent(&amp;quot;a1&amp;quot;, [&amp;quot;opt1&amp;quot;]),            
DummyAgent(&amp;quot;a2&amp;quot;, [&amp;quot;opt1&amp;quot;]),            
DummyAgent(&amp;quot;a3&amp;quot;, [&amp;quot;opt2&amp;quot;]),        ]        
team = bind(DummyTeam(agents, primus=agents[0]))        
monkeypatch.setattr(random, &amp;quot;choice&amp;quot;, lambda opts: opts[0])   
task = {&amp;quot;id&amp;quot;: &amp;quot;t&amp;quot;, 
&amp;quot;options&amp;quot;: [&amp;quot;opt1&amp;quot;, 
&amp;quot;opt2&amp;quot;], &amp;quot;voting_method&amp;quot;: 
&amp;quot;majority&amp;quot;}        result = 
team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;result&amp;quot;] == &amp;quot;opt1&amp;quot;E       
AssertionError: assert {&amp;#x27;method&amp;#x27;: 
&amp;#x27;majority_vote&amp;#x27;, &amp;#x27;winner&amp;#x27;: 
&amp;#x27;opt1&amp;#x27;} == 
&amp;#x27;opt1&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/domain/test_wsde_voting_logic.py:102: AssertionError___________________ 
test_vote_on_critical_decision_weighted ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14457cda0&amp;gt;    
@pytest.mark.fast    def test_vote_on_critical_decision_weighted(monkeypatch):  
a1 = DummyAgent(&amp;quot;a1&amp;quot;, [&amp;quot;frontend&amp;quot;])        
a2 = DummyAgent(&amp;quot;a2&amp;quot;, [&amp;quot;backend&amp;quot;])        
team = bind(DummyTeam(, primus=a1))        monkeypatch.setattr(random, 
&amp;quot;choice&amp;quot;, lambda opts: opts[0])        task = {            
&amp;quot;id&amp;quot;: &amp;quot;t2&amp;quot;,            
&amp;quot;options&amp;quot;: [&amp;quot;frontend&amp;quot;, 
&amp;quot;backend&amp;quot;],            &amp;quot;voting_method&amp;quot;: 
&amp;quot;weighted&amp;quot;,            &amp;quot;domain&amp;quot;: 
&amp;quot;frontend&amp;quot;,        }        result = 
team.vote_on_critical_decision(task)        assert 
result[&amp;quot;status&amp;quot;] == &amp;quot;completed&amp;quot;&amp;gt;     
assert result[&amp;quot;result&amp;quot;] == &amp;quot;frontend&amp;quot;E      
AssertionError: assert {&amp;#x27;method&amp;#x27;: 
&amp;#x27;weighted_vote&amp;#x27;, &amp;#x27;winner&amp;#x27;: 
&amp;#x27;frontend&amp;#x27;} == 
&amp;#x27;frontend&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/tests/unit/domain/test_wsde_voting_logic.py:120: 
AssertionError______________________ test_apply_majority_voting_no_tie 
_______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1444f3f50&amp;gt;    @pytest.mark.fast    def 
test_apply_majority_voting_no_tie(monkeypatch):        agents = 
[DummyAgent(&amp;quot;a1&amp;quot;), DummyAgent(&amp;quot;a2&amp;quot;)]        
team = bind(DummyTeam(agents, primus=agents[0]))        voting_result = {       
&amp;quot;options&amp;quot;: [&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;],    
&amp;quot;votes&amp;quot;: {&amp;quot;a1&amp;quot;: &amp;quot;X&amp;quot;, 
&amp;quot;a2&amp;quot;: &amp;quot;X&amp;quot;},            
&amp;quot;vote_counts&amp;quot;: {&amp;quot;X&amp;quot;: 2, 
&amp;quot;Y&amp;quot;: 0},            &amp;quot;status&amp;quot;: 
&amp;quot;pending&amp;quot;,        }        res = 
team._apply_majority_voting({}, voting_result)&amp;gt;       assert 
res[&amp;quot;result&amp;quot;] == &amp;quot;X&amp;quot;               
^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;result&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/domain/test_wsde_voting_logic.py:134: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:26:57,504 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot conduct vote: no options 
provided------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct vote: no 
options provided_____________________________ test_consensus_vote 
______________________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444f1220&amp;gt;    
@pytest.mark.fast    def test_consensus_vote(monkeypatch):        agents = 
[DummyAgent(&amp;quot;a1&amp;quot;, [&amp;quot;front&amp;quot;]), 
DummyAgent(&amp;quot;a2&amp;quot;, [&amp;quot;front&amp;quot;])]        team = 
bind(DummyTeam(agents, primus=agents[0]))        monkeypatch.setattr(random, 
&amp;quot;choice&amp;quot;, lambda opts: opts[0])        task = 
{&amp;quot;options&amp;quot;: [&amp;quot;front&amp;quot;, 
&amp;quot;back&amp;quot;]}        res = wsde_voting.consensus_vote(team, 
task)&amp;gt;       assert res[&amp;quot;decision&amp;quot;] in 
{&amp;quot;front&amp;quot;, &amp;quot;back&amp;quot;}               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: unhashable type: 
&amp;#x27;dict&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/domain/test_wsde_voting_logic.py:145: TypeError________________________ 
test_health_endpoint_succeeds _________________________api_token_env = None    
@pytest.mark.fast    def test_health_endpoint_succeeds(api_token_env):        
&amp;quot;&amp;quot;&amp;quot;Test that health endpoint succeeds.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;&amp;gt;       resp = 
_get_client().get(&amp;quot;/health&amp;quot;, 
headers={&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer 
test-token&amp;quot;})               
^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_api_health.py:47: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _     def _get_client():        
&amp;quot;&amp;quot;&amp;quot;Get the test client, initializing 
lazily.&amp;quot;&amp;quot;&amp;quot;        global client&amp;gt;       if 
client is None:           ^^^^^^E       NameError: name 
&amp;#x27;client&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/t
est_api_health.py:28: NameError________________________ 
test_metrics_endpoint_succeeds ________________________api_token_env = None    
@pytest.mark.fast    def test_metrics_endpoint_succeeds(api_token_env):        
&amp;quot;&amp;quot;&amp;quot;Test that metrics endpoint succeeds.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;&amp;gt;       resp = 
_get_client().get(&amp;quot;/metrics&amp;quot;, 
headers={&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer 
test-token&amp;quot;})               
^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_api_health.py:57: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _     def _get_client():        
&amp;quot;&amp;quot;&amp;quot;Get the test client, initializing 
lazily.&amp;quot;&amp;quot;&amp;quot;        global client&amp;gt;       if 
client is None:           ^^^^^^E       NameError: name 
&amp;#x27;client&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/t
est_api_health.py:28: NameError_________ 
test_skip_if_missing_backend_converts_find_spec_value_error 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144645250&amp;gt;    @pytest.mark.fast    def 
test_skip_if_missing_backend_converts_find_spec_value_error(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;ValueError from ``find_spec`` should result in a 
clean skip marker.&amp;quot;&amp;quot;&amp;quot;            from tests.fixtures 
import resources            calls: list[tuple] = []            def 
raising_find_spec(_name: str):            raise ValueError(&amp;quot;bad 
spec&amp;quot;)            def fake_importorskip(name: str, *, reason: str) 
-&amp;gt; None:            calls.append((name, reason))            raise 
pytest.skip.Exception(reason)            
monkeypatch.setattr(resources.importlib.util, &amp;quot;find_spec&amp;quot;, 
raising_find_spec)        monkeypatch.setattr(resources, 
&amp;quot;is_resource_available&amp;quot;, lambda _: True)        
monkeypatch.setattr(resources.pytest, &amp;quot;importorskip&amp;quot;, 
fake_importorskip)            marks = 
resources.skip_if_missing_backend(&amp;quot;faiss&amp;quot;, 
include_requires_resource=False)            assert any(mark.name == 
&amp;quot;skip&amp;quot; for mark in marks)&amp;gt;       assert calls and 
calls[0][0] == &amp;quot;faiss&amp;quot;E       assert 
([])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_backend_resource_flags.py:98: AssertionError_________ 
TestChromaDBAdapter.test_store_and_retrieve_vector_succeeds __________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f03260&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x1445148c0&amp;gt;    @pytest.mark.fast    def 
test_store_and_retrieve_vector_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test storing and retrieving a vector.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        vector = MemoryVector(          
id=&amp;quot;test-vector-1&amp;quot;,            content=&amp;quot;This is a 
test vector&amp;quot;,            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],          
metadata={&amp;quot;test&amp;quot;: &amp;quot;metadata&amp;quot;, 
&amp;quot;category&amp;quot;: &amp;quot;test&amp;quot;},        )        
vector_id = adapter.store_vector(vector)        assert vector_id == 
&amp;quot;test-vector-1&amp;quot;        retrieved_vector = 
adapter.retrieve_vector(vector_id)        assert retrieved_vector is not None   
assert retrieved_vector.id == vector.id&amp;gt;       assert 
retrieved_vector.content == vector.contentE       AssertionError: assert None ==
&amp;#x27;This is a test vector&amp;#x27;E        +  where None = 
MemoryVector(id=&amp;#x27;test-vector-1&amp;#x27;, content=None, 
embedding=&amp;lt;MagicMock 
name=&amp;#x27;mock.get_collection().get().__getitem__().__getitem__()&amp;#x27;
id=&amp;#x27;5441515168&amp;#x27;&amp;gt;, metadata={}, 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 140258)).contentE        +
and   &amp;#x27;This is a test vector&amp;#x27; = 
MemoryVector(id=&amp;#x27;test-vector-1&amp;#x27;, content=&amp;#x27;This is a 
test vector&amp;#x27;, embedding=[0.1, 0.2, 0.3, 0.4, 0.5], 
metadata={&amp;#x27;test&amp;#x27;: &amp;#x27;metadata&amp;#x27;, 
&amp;#x27;category&amp;#x27;: &amp;#x27;test&amp;#x27;}, 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 
137911)).content/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
general/test_chroma_db_adapter.py:70: AssertionError----------------------------
Captured stdout setup -----------------------------2025-10-28 09:27:10,137 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphpvoimeu2025-10-28 
09:27:10,137 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphpvoimeuINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,138 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-1 in ChromaDB2025-10-28 09:27:10,140 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Retrieved vector with ID 
test-vector-1 from ChromaDB------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-1 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Retrieved vector
with ID test-vector-1 from ChromaDB__________ 
TestChromaDBAdapter.test_store_vector_without_id_succeeds ___________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f03740&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x1444594f0&amp;gt;    @pytest.mark.fast    def 
test_store_vector_without_id_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test storing a vector without an ID.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        vector = MemoryVector(          
id=&amp;quot;&amp;quot;,            content=&amp;quot;Vector without 
ID&amp;quot;,            embedding=[0.5, 0.4, 0.3, 0.2, 0.1],            
metadata={&amp;quot;test&amp;quot;: &amp;quot;no-id&amp;quot;},        )        
vector_id = adapter.store_vector(vector)        assert vector_id is not None    
assert vector_id != &amp;quot;&amp;quot;        retrieved_vector = 
adapter.retrieve_vector(vector_id)        assert retrieved_vector is not None   
assert retrieved_vector.id == vector_id&amp;gt;       assert 
retrieved_vector.content == vector.contentE       AssertionError: assert None ==
&amp;#x27;Vector without ID&amp;#x27;E        +  where None = 
MemoryVector(id=&amp;#x27;6302b749-e69c-4b12-9c60-1c485e619619&amp;#x27;, 
content=None, embedding=&amp;lt;MagicMock 
name=&amp;#x27;mock.get_collection()...etitem__().__getitem__()&amp;#x27; 
id=&amp;#x27;5440581568&amp;#x27;&amp;gt;, metadata={}, 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 156453)).contentE        +
and   &amp;#x27;Vector without ID&amp;#x27; = 
MemoryVector(id=&amp;#x27;6302b749-e69c-4b12-9c60-1c485e619619&amp;#x27;, 
content=&amp;#x27;Vector without ID&amp;#x27;, embedding=[0.5, 0.4, 0.3, 0.2, 
0.1], metadata={&amp;#x27;test&amp;#x27;: &amp;#x27;no-id&amp;#x27;}, 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 
155136)).content/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
general/test_chroma_db_adapter.py:92: AssertionError----------------------------
Captured stdout setup -----------------------------2025-10-28 09:27:10,153 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpinuancfe2025-10-28 
09:27:10,154 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpinuancfeINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,155 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
6302b749-e69c-4b12-9c60-1c485e619619 in ChromaDB2025-10-28 09:27:10,156 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Retrieved vector with ID 
6302b749-e69c-4b12-9c60-1c485e619619 from ChromaDB------------------------------
Captured log call -------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID 6302b749-e69c-4b12-9c60-1c485e619619 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Retrieved vector
with ID 6302b749-e69c-4b12-9c60-1c485e619619 from ChromaDB_____________ 
TestChromaDBAdapter.test_similarity_search_succeeds ______________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f03c20&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x14464fa40&amp;gt;    @pytest.mark.fast    def 
test_similarity_search_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test similarity search functionality.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        vectors = [            
MemoryVector(                id=f&amp;quot;test-vector-{i}&amp;quot;,           
content=f&amp;quot;Test vector {i}&amp;quot;,                
embedding=[(float(j) / 10) for j in range(i, i + 5)],                
metadata={&amp;quot;index&amp;quot;: i},            )            for i in 
range(1, 6)        ]        for vector in vectors:            
adapter.store_vector(vector)        query_embedding = [0.15, 0.25, 0.35, 0.45, 
0.55]        results = adapter.similarity_search(query_embedding, 
top_k=3)&amp;gt;       assert len(results) == 3E       assert 0 == 3E        +  
where 0 = 
len([])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/t
est_chroma_db_adapter.py:113: AssertionError---------------------------- 
Captured stdout setup -----------------------------2025-10-28 09:27:10,164 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpw3q30ksk2025-10-28 
09:27:10,164 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpw3q30kskINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-1 in ChromaDB2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-2 in ChromaDB2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-3 in ChromaDB2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-4 in ChromaDB2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-5 in ChromaDB2025-10-28 09:27:10,165 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Found 0 similar vectors in 
ChromaDB------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-1 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-2 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-3 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-4 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-5 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Found 0 similar 
vectors in ChromaDB_______________ 
TestChromaDBAdapter.test_delete_vector_succeeds ________________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f241a0&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x144683290&amp;gt;    @pytest.mark.fast    def 
test_delete_vector_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test deleting a vector.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        vector = MemoryVector(            
id=&amp;quot;test-vector-delete&amp;quot;,            content=&amp;quot;Vector 
to delete&amp;quot;,            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],            
metadata={&amp;quot;test&amp;quot;: &amp;quot;delete&amp;quot;},        )       
adapter.store_vector(vector)        result = adapter.delete_vector(vector.id)   
assert result is True        retrieved_vector = 
adapter.retrieve_vector(vector.id)&amp;gt;       assert retrieved_vector is 
NoneE       AssertionError: assert 
MemoryVector(id=&amp;#x27;test-vector-delete&amp;#x27;, content=None, 
embedding=&amp;lt;MagicMock 
name=&amp;#x27;mock.get_collection().get().__getitem__().__getitem__()&amp;#x27;
id=&amp;#x27;5442581424&amp;#x27;&amp;gt;, metadata={}, 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 176524)) is 
None/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_chroma_db_adapter.py:132: AssertionError---------------------------- Captured 
stdout setup -----------------------------2025-10-28 09:27:10,174 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwt4lu8442025-10-28 
09:27:10,174 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpwt4lu844INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,175 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
test-vector-delete in ChromaDB2025-10-28 09:27:10,176 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Deleted vector with ID 
test-vector-delete from ChromaDB2025-10-28 09:27:10,176 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Retrieved vector with ID 
test-vector-delete from ChromaDB------------------------------ Captured log call
-------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID test-vector-delete in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted vector 
with ID test-vector-delete from ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Retrieved vector
with ID test-vector-delete from ChromaDB_________ 
TestChromaDBAdapter.test_delete_nonexistent_vector_succeeds __________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f24680&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x14464e510&amp;gt;    @pytest.mark.fast    def 
test_delete_nonexistent_vector_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test deleting a vector that doesn&amp;#x27;t 
exist.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        result = 
adapter.delete_vector(&amp;quot;nonexistent-vector&amp;quot;)&amp;gt;       
assert result is FalseE       assert True is 
False/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/tes
t_chroma_db_adapter.py:140: AssertionError---------------------------- Captured 
stdout setup -----------------------------2025-10-28 09:27:10,184 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpd48n7ljc2025-10-28 
09:27:10,184 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpd48n7ljcINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,185 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Deleted vector with ID 
nonexistent-vector from ChromaDB------------------------------ Captured log call
-------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted vector 
with ID nonexistent-vector from ChromaDB____________ 
TestChromaDBAdapter.test_get_collection_stats_succeeds ____________self = 
&amp;lt;tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x120f24b60&amp;gt;adapter = 
&amp;lt;devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x144498fe0&amp;gt;    @pytest.mark.fast    def 
test_get_collection_stats_succeeds(self, adapter):        
&amp;quot;&amp;quot;&amp;quot;Test getting collection statistics.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        vectors = [            
MemoryVector(                id=f&amp;quot;stats-vector-{i}&amp;quot;,          
content=f&amp;quot;Stats vector {i}&amp;quot;,                
embedding=[(float(j) / 10) for j in range(i, i + 5)],                
metadata={&amp;quot;index&amp;quot;: i},            )            for i in 
range(1, 4)        ]        for vector in vectors:            
adapter.store_vector(vector)        stats = adapter.get_collection_stats()      
assert stats[&amp;quot;collection_name&amp;quot;] == 
&amp;quot;test_vectors&amp;quot;&amp;gt;       assert 
stats[&amp;quot;vector_count&amp;quot;] &amp;gt;= 3E       assert 0 &amp;gt;= 
3/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ch
roma_db_adapter.py:160: AssertionError---------------------------- Captured 
stdout setup -----------------------------2025-10-28 09:27:10,193 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Initialized ChromaDB client 
with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnfe6g09l2025-10-28 
09:27:10,194 - devsynth.adapters.memory.chroma_db_adapter - INFO - Using 
existing ChromaDB collection: test_vectors------------------------------ 
Captured log setup ------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Initialized 
ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnfe6g09lINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using existing 
ChromaDB collection: test_vectors----------------------------- Captured stdout 
call -----------------------------2025-10-28 09:27:10,194 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
stats-vector-1 in ChromaDB2025-10-28 09:27:10,194 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
stats-vector-2 in ChromaDB2025-10-28 09:27:10,194 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Stored vector with ID 
stats-vector-3 in ChromaDB2025-10-28 09:27:10,195 - 
devsynth.adapters.memory.chroma_db_adapter - INFO - Retrieved collection 
statistics: {&amp;#x27;collection_name&amp;#x27;: 
&amp;#x27;test_vectors&amp;#x27;, &amp;#x27;vector_count&amp;#x27;: 0, 
&amp;#x27;embedding_dimensions&amp;#x27;: 0, 
&amp;#x27;persist_directory&amp;#x27;: 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnfe6g09l&amp;#x27;
}------------------------------ Captured log call 
-------------------------------INFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID stats-vector-1 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID stats-vector-2 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored vector 
with ID stats-vector-3 in ChromaDBINFO     
devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Retrieved 
collection statistics: {&amp;#x27;collection_name&amp;#x27;: 
&amp;#x27;test_vectors&amp;#x27;, &amp;#x27;vector_count&amp;#x27;: 0, 
&amp;#x27;embedding_dimensions&amp;#x27;: 0, 
&amp;#x27;persist_directory&amp;#x27;: 
&amp;#x27;/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnfe6g09l&amp;#x27;
}_____________ TestDialecticalReasoner.test_assess_impact_succeeds 
______________self = &amp;lt;MagicMock 
name=&amp;#x27;mock.notify_impact_assessment_completed&amp;#x27; 
id=&amp;#x27;5440637600&amp;#x27;&amp;gt;args = 
(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x2
7;), change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8db...ation 
2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;),)kwargs = {}expected = 
call(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp
;#x27;), change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316...dation 
2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;))actual = 
call(ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID(&amp;#x27;460
e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x27;), change_id=...test_user&amp;#x27;),
edrr_phase=&amp;lt;EDRRPhase.REFINE: &amp;#x27;REFINE&amp;#x27;&amp;gt;, 
triggered_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851416)))_error_message 
= &amp;lt;function 
NonCallableMock.assert_called_with.&amp;lt;locals&amp;gt;._error_message at 
0x1446089a0&amp;gt;cause = None    def assert_called_with(self, /, *args, 
**kwargs):        &amp;quot;&amp;quot;&amp;quot;assert that the last call was 
made with the specified arguments.            Raises an AssertionError if the 
args and keyword args passed in are        different to the last call to the 
mock.&amp;quot;&amp;quot;&amp;quot;        if self.call_args is None:           
expected = self._format_mock_call_signature(args, kwargs)            actual = 
&amp;#x27;not called.&amp;#x27;            error_message = (&amp;#x27;expected 
call not found.\nExpected: %s\n  Actual: %s&amp;#x27;                    % 
(expected, actual))            raise AssertionError(error_message)            
def _error_message():            msg = self._format_mock_failure_message(args, 
kwargs)            return msg        expected = self._call_matcher(_Call((args, 
kwargs), two=True))        actual = self._call_matcher(self.call_args)        if
actual != expected:            cause = expected if isinstance(expected, 
Exception) else None&amp;gt;           raise AssertionError(_error_message()) 
from causeE           AssertionError: expected call not found.E           
Expected: 
notify_impact_assessment_completed(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8
311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;))E             Actual: 
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;), edrr_phase=&amp;lt;EDRRPhase.REFINE: 
&amp;#x27;REFINE&amp;#x27;&amp;gt;, triggered_at=datetime.datetime(2025, 10, 28,
9, 27, 10, 
851416)))/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Ve
rsions/3.12/lib/python3.12/unittest/mock.py:949: AssertionErrorDuring handling 
of the above exception, another exception occurred:self = &amp;lt;MagicMock 
name=&amp;#x27;mock.notify_impact_assessment_completed&amp;#x27; 
id=&amp;#x27;5440637600&amp;#x27;&amp;gt;args = 
(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x2
7;), change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8db...ation 
2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;),)kwargs = {}    def 
assert_called_once_with(self, /, *args, **kwargs):        
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called exactly once and 
that that call was        with the specified 
arguments.&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:    
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to be called once. Called %s 
times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))            raise AssertionError(msg)&amp;gt;       return 
self.assert_called_with(*args, **kwargs)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: expected call 
not found.E       Expected: 
notify_impact_assessment_completed(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8
311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;))E         Actual: 
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;), edrr_phase=&amp;lt;EDRRPhase.REFINE: 
&amp;#x27;REFINE&amp;#x27;&amp;gt;, triggered_at=datetime.datetime(2025, 10, 28,
9, 27, 10, 851416)))E       E       pytest introspection follows:E       E      
Args:E       assert (ImpactNotifi...10, 851416)),) == 
(ImpactAssess...&amp;#x27;test_user&amp;#x27;),)E         E         At index 0 
diff: 
ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID(&amp;#x27;460e3e51
-8311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;), edrr_phase=&amp;lt;E...E         E   
...Full output truncated (2 lines hidden), use &amp;#x27;-vv&amp;#x27; to 
show/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Version
s/3.12/lib/python3.12/unittest/mock.py:961: AssertionErrorDuring handling of the
above exception, another exception occurred:self = 
&amp;lt;tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_assess_impact_succeeds&amp;gt;        def 
test_assess_impact_succeeds(self):            &amp;quot;&amp;quot;&amp;quot;Test
assessing the impact of a change.                ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;            self.llm_service.query.side_effect 
= [                &amp;quot;This is an impact analysis&amp;quot;,              
&amp;quot;&amp;quot;&amp;quot;- Recommendation 1    - Recommendation 2    - 
Recommendation 3&amp;quot;&amp;quot;&amp;quot;,            ]            
dependent_requirement = Requirement(                id=uuid4(),                
title=&amp;quot;Dependent Requirement&amp;quot;,                
description=&amp;quot;This requirement depends on the test 
requirement&amp;quot;,                status=RequirementStatus.DRAFT,           
priority=RequirementPriority.MEDIUM,                
type=RequirementType.FUNCTIONAL,                
created_by=&amp;quot;test_user&amp;quot;,                dependencies=,         
)            self.requirement_repository.save_requirement(dependent_requirement)
impact = self.reasoner.assess_impact(self.change)            
self.assertEqual(impact.change_id, self.change.id)            
self.assertEqual(impact.analysis, &amp;quot;This is an impact 
analysis&amp;quot;)            self.assertEqual(len(impact.recommendations), 3) 
self.assertEqual(impact.recommendations[0], &amp;quot;Recommendation 
1&amp;quot;)            self.assertEqual(impact.recommendations[1], 
&amp;quot;Recommendation 2&amp;quot;)            
self.assertEqual(impact.recommendations[2], &amp;quot;Recommendation 
3&amp;quot;)            self.assertEqual(len(impact.affected_requirements), 2)  
self.assertIn(self.requirement.id, impact.affected_requirements)            
self.assertIn(dependent_requirement.id, impact.affected_requirements)           
saved_impact = self.impact_repository.get_impact_assessment(impact.id)          
self.assertIsNotNone(saved_impact)            self.assertEqual(saved_impact.id, 
impact.id)            impact_by_change = 
self.impact_repository.get_impact_assessment_for_change(                
self.change.id            )            self.assertIsNotNone(impact_by_change)   
self.assertEqual(impact_by_change.id, impact.id)&amp;gt;           
self.notification_service.notify_impact_assessment_completed.assert_called_once_
with(                impact            )E           AssertionError: expected 
call not found.E           Expected: 
notify_impact_assessment_completed(ImpactAssessment(id=UUID(&amp;#x27;460e3e51-8
311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;))E             Actual: 
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID(&amp;#x27;460e3e51-8311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;), edrr_phase=&amp;lt;EDRRPhase.REFINE: 
&amp;#x27;REFINE&amp;#x27;&amp;gt;, triggered_at=datetime.datetime(2025, 10, 28,
9, 27, 10, 851416)))E           E           pytest introspection follows:E      
E           Args:E           assert (ImpactNotifi...10, 851416)),) == 
(ImpactAssess...&amp;#x27;test_user&amp;#x27;),)E             E             At 
index 0 diff: 
ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID(&amp;#x27;460e3e51
-8311-4a69-9e71-a8ba4418634e&amp;#x27;), 
change_id=UUID(&amp;#x27;5091fcdd-c88f-42d2-b730-968b316c8dbd&amp;#x27;), 
affected_requirements=[UUID(&amp;#x27;6a6d4683-5c97-4346-bb60-ac2f913491c9&amp;#
x27;), UUID(&amp;#x27;bc7681f1-fcef-405f-9099-677ccbcf4daf&amp;#x27;)], 
affected_components=[], risk_level=&amp;#x27;low&amp;#x27;, 
estimated_effort=&amp;#x27;low&amp;#x27;, analysis=&amp;#x27;This is an impact 
analysis&amp;#x27;, recommendations=[&amp;#x27;Recommendation 1&amp;#x27;, 
&amp;#x27;Recommendation 2&amp;#x27;, &amp;#x27;Recommendation 3&amp;#x27;], 
created_at=datetime.datetime(2025, 10, 28, 9, 27, 10, 851317), 
created_by=&amp;#x27;test_user&amp;#x27;), edrr_phase=&amp;lt;E...E             
E             ...Full output truncated (2 lines hidden), use 
&amp;#x27;-vv&amp;#x27; to 
show/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_dialectical_reasoner.py:213: AssertionError________ 
TestDialecticalReasoner.test_evaluate_change_consensus_failure ________self = 
&amp;lt;MagicMock name=&amp;#x27;mock.store_with_edrr_phase&amp;#x27; 
id=&amp;#x27;5440711968&amp;#x27;&amp;gt;    def assert_called_once(self):      
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called only once.        
&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:            
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to have been called once. 
Called %s times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;store_with_edrr_phase&amp;#x27; to have been 
called once. Called 2 times.E           Calls: [call({&amp;#x27;type&amp;#x27;: 
&amp;#x27;requirement_to_reasoning&amp;#x27;, &amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;, 
&amp;#x27;reasoning_id&amp;#x27;: 
&amp;#x27;7d7d1aa8-0eb6-4371-a84c-239ae8e14b2c&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.RELATIONSHIP: 
&amp;#x27;relationship&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;RETROSPECT&amp;#x27;, 
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;, 
&amp;#x27;link&amp;#x27;: &amp;#x27;requirement-&amp;gt;reasoning&amp;#x27;}),E 
call({&amp;#x27;id&amp;#x27;: 
UUID(&amp;#x27;7d7d1aa8-0eb6-4371-a84c-239ae8e14b2c&amp;#x27;), 
&amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;), 
&amp;#x27;thesis&amp;#x27;: &amp;#x27;This is a thesis&amp;#x27;, 
&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is an antithesis&amp;#x27;, 
&amp;#x27;synthesis&amp;#x27;: &amp;#x27;This is a synthesis&amp;#x27;, 
&amp;#x27;arguments&amp;#x27;: [{&amp;#x27;position&amp;#x27;: 
&amp;#x27;Thesis&amp;#x27;, &amp;#x27;content&amp;#x27;: &amp;#x27;Argument for 
thesis&amp;#x27;, &amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;position&amp;#x27;: &amp;#x27;Antithesis&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for antithesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, 
&amp;#x27;recommendation&amp;#x27;: &amp;#x27;This is a 
recommendation&amp;#x27;, &amp;#x27;created_at&amp;#x27;: 
datetime.datetime(2025, 10, 28, 9, 27, 10, 929679), 
&amp;#x27;updated_at&amp;#x27;: datetime.datetime(2025, 10, 28, 9, 27, 10, 
929680), &amp;#x27;created_by&amp;#x27;: &amp;#x27;test_user&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;RETROSPECT&amp;#x27;, 
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;})]./opt/homebrew/Cellar
/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/un
ittest/mock.py:928: AssertionErrorDuring handling of the above exception, 
another exception occurred:self = 
&amp;lt;tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_evaluate_change_consensus_failure&amp;gt;        def 
test_evaluate_change_consensus_failure(self):            
&amp;quot;&amp;quot;&amp;quot;Ensure consensus failure triggers logging and 
memory storage.&amp;quot;&amp;quot;&amp;quot;            
self.llm_service.query.side_effect = [                &amp;quot;This is a 
thesis&amp;quot;,                &amp;quot;This is an antithesis&amp;quot;,     
&amp;quot;&amp;quot;&amp;quot;Argument 1:    Position: Thesis    Content: 
Argument for thesis        Argument 2:    Position: Antithesis    Content: 
Argument for antithesis&amp;quot;&amp;quot;&amp;quot;,                
&amp;quot;This is a synthesis&amp;quot;,                
&amp;quot;&amp;quot;&amp;quot;Conclusion: This is a conclusion        
Recommendation: This is a recommendation&amp;quot;&amp;quot;&amp;quot;,         
&amp;quot;no&amp;quot;,            ]            with patch(                
&amp;quot;devsynth.application.requirements.dialectical_reasoner.logger&amp;quot
;            ) as mock_logger:                with 
self.assertRaises(ConsensusError):                    
self.reasoner.evaluate_change(self.change)                
mock_logger.error.assert_any_call(                    &amp;quot;Consensus not 
reached for change&amp;quot;,                    
extra={&amp;quot;change_id&amp;quot;: str(self.change.id), 
&amp;quot;event&amp;quot;: &amp;quot;consensus_failed&amp;quot;},               
)&amp;gt;           
self.memory_manager.store_with_edrr_phase.assert_called_once()E           
AssertionError: Expected &amp;#x27;store_with_edrr_phase&amp;#x27; to have been 
called once. Called 2 times.E           Calls: [call({&amp;#x27;type&amp;#x27;: 
&amp;#x27;requirement_to_reasoning&amp;#x27;, &amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;, 
&amp;#x27;reasoning_id&amp;#x27;: 
&amp;#x27;7d7d1aa8-0eb6-4371-a84c-239ae8e14b2c&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.RELATIONSHIP: 
&amp;#x27;relationship&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;RETROSPECT&amp;#x27;, 
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;, 
&amp;#x27;link&amp;#x27;: &amp;#x27;requirement-&amp;gt;reasoning&amp;#x27;}),E 
call({&amp;#x27;id&amp;#x27;: 
UUID(&amp;#x27;7d7d1aa8-0eb6-4371-a84c-239ae8e14b2c&amp;#x27;), 
&amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;), 
&amp;#x27;thesis&amp;#x27;: &amp;#x27;This is a thesis&amp;#x27;, 
&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is an antithesis&amp;#x27;, 
&amp;#x27;synthesis&amp;#x27;: &amp;#x27;This is a synthesis&amp;#x27;, 
&amp;#x27;arguments&amp;#x27;: [{&amp;#x27;position&amp;#x27;: 
&amp;#x27;Thesis&amp;#x27;, &amp;#x27;content&amp;#x27;: &amp;#x27;Argument for 
thesis&amp;#x27;, &amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;position&amp;#x27;: &amp;#x27;Antithesis&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for antithesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, 
&amp;#x27;recommendation&amp;#x27;: &amp;#x27;This is a 
recommendation&amp;#x27;, &amp;#x27;created_at&amp;#x27;: 
datetime.datetime(2025, 10, 28, 9, 27, 10, 929679), 
&amp;#x27;updated_at&amp;#x27;: datetime.datetime(2025, 10, 28, 9, 27, 10, 
929680), &amp;#x27;created_by&amp;#x27;: &amp;#x27;test_user&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;RETROSPECT&amp;#x27;, 
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;})].E           E       
pytest introspection follows:E           E           Args:E           assert 
({&amp;#x27;antithesis...usion&amp;#x27;, ...},) == ()E             E           
Left contains one more item: {&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is 
an antithesis&amp;#x27;, &amp;#x27;arguments&amp;#x27;: 
[{&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for thesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;, &amp;#x27;posit...: 
&amp;#x27;Antithesis&amp;#x27;}], &amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;), 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, ...}E 
Use -v to get more diffE           Kwargs:E           assert 
{&amp;#x27;edrr_phase&amp;#x27;...c8305d2f234&amp;#x27;}} == {}E             E  
Left contains 3 more items:E             {&amp;#x27;edrr_phase&amp;#x27;: 
&amp;#x27;RETROSPECT&amp;#x27;,E              &amp;#x27;memory_type&amp;#x27;: 
&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;,E              
&amp;#x27;metadata&amp;#x27;: {&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;8fe20973-ccbb-43c5-b228-8c8305d2f234&amp;#x27;}}E             Use -v 
to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_dialectical_reasoner.py:170: AssertionError____________ 
TestDialecticalReasoner.test_evaluate_change_succeeds _____________self = 
&amp;lt;MagicMock name=&amp;#x27;mock.store_with_edrr_phase&amp;#x27; 
id=&amp;#x27;5442321056&amp;#x27;&amp;gt;    def assert_called_once(self):      
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called only once.        
&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:            
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to have been called once. 
Called %s times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;store_with_edrr_phase&amp;#x27; to have been 
called once. Called 2 times.E           Calls: [call({&amp;#x27;type&amp;#x27;: 
&amp;#x27;requirement_to_reasoning&amp;#x27;, &amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;, 
&amp;#x27;reasoning_id&amp;#x27;: 
&amp;#x27;f81115db-b5c8-4ddc-9d36-26e7d4a07584&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.RELATIONSHIP: 
&amp;#x27;relationship&amp;#x27;&amp;gt;, edrr_phase=&amp;#x27;REFINE&amp;#x27;,
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;, 
&amp;#x27;link&amp;#x27;: &amp;#x27;requirement-&amp;gt;reasoning&amp;#x27;}),E 
call({&amp;#x27;id&amp;#x27;: 
UUID(&amp;#x27;f81115db-b5c8-4ddc-9d36-26e7d4a07584&amp;#x27;), 
&amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;), 
&amp;#x27;thesis&amp;#x27;: &amp;#x27;This is a thesis&amp;#x27;, 
&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is an antithesis&amp;#x27;, 
&amp;#x27;synthesis&amp;#x27;: &amp;#x27;This is a synthesis&amp;#x27;, 
&amp;#x27;arguments&amp;#x27;: [{&amp;#x27;position&amp;#x27;: 
&amp;#x27;Thesis&amp;#x27;, &amp;#x27;content&amp;#x27;: &amp;#x27;Argument for 
thesis&amp;#x27;, &amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;position&amp;#x27;: &amp;#x27;Antithesis&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for antithesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, 
&amp;#x27;recommendation&amp;#x27;: &amp;#x27;This is a 
recommendation&amp;#x27;, &amp;#x27;created_at&amp;#x27;: 
datetime.datetime(2025, 10, 28, 9, 27, 10, 975635), 
&amp;#x27;updated_at&amp;#x27;: datetime.datetime(2025, 10, 28, 9, 27, 10, 
975640), &amp;#x27;created_by&amp;#x27;: &amp;#x27;test_user&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;REFINE&amp;#x27;, metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;})]./opt/homebrew/Cellar
/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/un
ittest/mock.py:928: AssertionErrorDuring handling of the above exception, 
another exception occurred:self = 
&amp;lt;tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_evaluate_change_succeeds&amp;gt;        def 
test_evaluate_change_succeeds(self):            
&amp;quot;&amp;quot;&amp;quot;Test evaluating a change using dialectical 
reasoning.                ReqID: N/A&amp;quot;&amp;quot;&amp;quot;            
self.llm_service.query.side_effect = [                &amp;quot;This is a 
thesis&amp;quot;,                &amp;quot;This is an antithesis&amp;quot;,     
&amp;quot;&amp;quot;&amp;quot;Argument 1:    Position: Thesis    Content: 
Argument for thesis        Argument 2:    Position: Antithesis    Content: 
Argument for antithesis&amp;quot;&amp;quot;&amp;quot;,                
&amp;quot;This is a synthesis&amp;quot;,                
&amp;quot;&amp;quot;&amp;quot;Conclusion: This is a conclusion        
Recommendation: This is a recommendation&amp;quot;&amp;quot;&amp;quot;,         
&amp;quot;yes&amp;quot;,            ]            with patch(                
&amp;quot;devsynth.application.requirements.dialectical_reasoner.logger&amp;quot
;            ) as mock_logger:                reasoning = 
self.reasoner.evaluate_change(self.change)                
mock_logger.info.assert_any_call(                    &amp;quot;Consensus reached
for change&amp;quot;,                    extra={&amp;quot;change_id&amp;quot;: 
str(self.change.id), &amp;quot;event&amp;quot;: 
&amp;quot;consensus_reached&amp;quot;},                )&amp;gt;           
self.memory_manager.store_with_edrr_phase.assert_called_once()E           
AssertionError: Expected &amp;#x27;store_with_edrr_phase&amp;#x27; to have been 
called once. Called 2 times.E           Calls: [call({&amp;#x27;type&amp;#x27;: 
&amp;#x27;requirement_to_reasoning&amp;#x27;, &amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;, 
&amp;#x27;reasoning_id&amp;#x27;: 
&amp;#x27;f81115db-b5c8-4ddc-9d36-26e7d4a07584&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.RELATIONSHIP: 
&amp;#x27;relationship&amp;#x27;&amp;gt;, edrr_phase=&amp;#x27;REFINE&amp;#x27;,
metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;, 
&amp;#x27;link&amp;#x27;: &amp;#x27;requirement-&amp;gt;reasoning&amp;#x27;}),E 
call({&amp;#x27;id&amp;#x27;: 
UUID(&amp;#x27;f81115db-b5c8-4ddc-9d36-26e7d4a07584&amp;#x27;), 
&amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;), 
&amp;#x27;thesis&amp;#x27;: &amp;#x27;This is a thesis&amp;#x27;, 
&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is an antithesis&amp;#x27;, 
&amp;#x27;synthesis&amp;#x27;: &amp;#x27;This is a synthesis&amp;#x27;, 
&amp;#x27;arguments&amp;#x27;: [{&amp;#x27;position&amp;#x27;: 
&amp;#x27;Thesis&amp;#x27;, &amp;#x27;content&amp;#x27;: &amp;#x27;Argument for 
thesis&amp;#x27;, &amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;position&amp;#x27;: &amp;#x27;Antithesis&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for antithesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, 
&amp;#x27;recommendation&amp;#x27;: &amp;#x27;This is a 
recommendation&amp;#x27;, &amp;#x27;created_at&amp;#x27;: 
datetime.datetime(2025, 10, 28, 9, 27, 10, 975635), 
&amp;#x27;updated_at&amp;#x27;: datetime.datetime(2025, 10, 28, 9, 27, 10, 
975640), &amp;#x27;created_by&amp;#x27;: &amp;#x27;test_user&amp;#x27;}, 
memory_type=&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;, 
edrr_phase=&amp;#x27;REFINE&amp;#x27;, metadata={&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;})].E           E       
pytest introspection follows:E           E           Args:E           assert 
({&amp;#x27;antithesis...usion&amp;#x27;, ...},) == ()E             E           
Left contains one more item: {&amp;#x27;antithesis&amp;#x27;: &amp;#x27;This is 
an antithesis&amp;#x27;, &amp;#x27;arguments&amp;#x27;: 
[{&amp;#x27;content&amp;#x27;: &amp;#x27;Argument for thesis&amp;#x27;, 
&amp;#x27;counterargument&amp;#x27;: &amp;#x27;&amp;#x27;, &amp;#x27;posit...: 
&amp;#x27;Antithesis&amp;#x27;}], &amp;#x27;change_id&amp;#x27;: 
UUID(&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;), 
&amp;#x27;conclusion&amp;#x27;: &amp;#x27;This is a conclusion&amp;#x27;, ...}E 
Use -v to get more diffE           Kwargs:E           assert 
{&amp;#x27;edrr_phase&amp;#x27;...a873ca075bf&amp;#x27;}} == {}E             E  
Left contains 3 more items:E             {&amp;#x27;edrr_phase&amp;#x27;: 
&amp;#x27;REFINE&amp;#x27;,E              &amp;#x27;memory_type&amp;#x27;: 
&amp;lt;MemoryType.DIALECTICAL_REASONING: 
&amp;#x27;dialectical_reasoning&amp;#x27;&amp;gt;,E              
&amp;#x27;metadata&amp;#x27;: {&amp;#x27;change_id&amp;#x27;: 
&amp;#x27;879293a9-04f8-4ef5-81e8-ca873ca075bf&amp;#x27;}}E             Use -v 
to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_dialectical_reasoner.py:120: AssertionError__________________________ 
test_dpg_command_disabled ___________________________name = 
&amp;#x27;commands&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:    
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above exception
was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444f0500&amp;gt;    def 
test_dpg_command_disabled(monkeypatch):&amp;gt;       monkeypatch.setattr(      
&amp;quot;devsynth.application.cli.commands.dpg_cmd.get_settings&amp;quot;,     
lambda reload=True: types.SimpleNamespace(gui_enabled=False),        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dp
g_flag.py:88: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = &amp;#x27;commands&amp;#x27;, 
ann = &amp;#x27;devsynth.application.cli.commands&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.commands has no 
attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_____________________ test_dpg_command_missing_dependency 
______________________name = &amp;#x27;commands&amp;#x27;    def 
__getattr__(name: str) -&amp;gt; object:        
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above exception
was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444f1520&amp;gt;    def 
test_dpg_command_missing_dependency(monkeypatch):&amp;gt;       
monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.dpg_cmd.get_settings&amp;quot;,     
lambda reload=True: types.SimpleNamespace(gui_enabled=True),        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dp
g_flag.py:99: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = &amp;#x27;commands&amp;#x27;, 
ann = &amp;#x27;devsynth.application.cli.commands&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.commands has no 
attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError________________ TestPhases.test_retrospect_phase_has_expected 
_________________self = &amp;lt;tests.unit.general.test_ingest_cmd.TestPhases 
object at 0x120ffe690&amp;gt;mock_bridge = &amp;lt;MagicMock 
id=&amp;#x27;5440311360&amp;#x27;&amp;gt;mock_memory_manager = &amp;lt;MagicMock
name=&amp;#x27;MemoryManager()&amp;#x27; 
id=&amp;#x27;5447078720&amp;#x27;&amp;gt;    @pytest.mark.fast    def 
test_retrospect_phase_has_expected(self, mock_bridge, mock_memory_manager):     
&amp;quot;&amp;quot;&amp;quot;Test retrospect_phase function.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        
mock_memory_manager.reset_mock()&amp;gt;       result = retrospect_phase(       
{&amp;quot;projectName&amp;quot;: &amp;quot;TestProject&amp;quot;},            
{&amp;quot;relationships_created&amp;quot;: 75},            verbose=True,       
bridge=mock_bridge,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_in
gest_cmd.py:504: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ manifest = {&amp;#x27;projectName&amp;#x27;: 
&amp;#x27;TestProject&amp;#x27;}refine_results = 
{&amp;#x27;relationships_created&amp;#x27;: 75}, verbose = True    def 
retrospect_phase(        manifest: ManifestModel,        refine_results: 
RefinePhaseResult,        verbose: bool = False,        *,        bridge: 
Optional[UXBridge] = None,        memory_manager: Union[MemoryManager, None] = 
None,        code_analyzer: Union[CodeAnalyzer, None] = None,        wsde_team: 
Union[WSDETeam, None] = None,    ) -&amp;gt; RetrospectPhaseResult:        
&amp;quot;&amp;quot;&amp;quot;Summarize results and suggest 
improvements.&amp;quot;&amp;quot;&amp;quot;            start = 
time.perf_counter()            bridge = bridge or DEFAULT_BRIDGE        
memory_manager = memory_manager or MemoryManager(            
adapters={&amp;quot;tinydb&amp;quot;: TinyDBMemoryAdapter()}        )        
analyzer = code_analyzer or CodeAnalyzer()        wsde_team = wsde_team or 
WSDETeam(name=&amp;quot;IngestCmdTeam&amp;quot;)            if verbose:         
bridge.print(&amp;quot;  Generating retrospective report...&amp;quot;)          
improvements = refine_results.get(&amp;quot;relationships_created&amp;quot;, 0) 
gaps = refine_results.get(&amp;quot;outdated_items_archived&amp;quot;, 0)       
learnings = cast(JSONValue, wsde_team.extract_learnings(refine_results, True))  
patterns = cast(            JSONValue,            wsde_team.recognize_patterns( 
learnings,                
historical_context=memory_manager.retrieve_historical_patterns(),               
code_analyzer=analyzer,            ),        )        integrated = cast(        
JSONValue,&amp;gt;           wsde_team.integrate_knowledge(learnings, patterns, 
memory_manager),            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^        )E    
TypeError: _integrate_knowledge() takes 3 positional arguments but 4 were 
given/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applicati
on/cli/ingest_cmd.py:654: TypeError----------------------------- Captured stdout
call -----------------------------2025-10-28 09:27:11,822 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB 
Memory Adapter initialized------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
TinyDB Memory Adapter initialized_____________ 
test_offline_mode_selects_offline_provider_succeeds ______________obj = 
&amp;lt;module &amp;#x27;devsynth.application.llm&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;get_llm_settings&amp;#x27;, ann = 
&amp;#x27;devsynth.application.llm&amp;#x27;    def annotated_getattr(obj: 
object, name: str, ann: str) -&amp;gt; object:        try:&amp;gt;           obj
= getattr(obj, name)                  ^^^^^^^^^^^^^^^^^^E           
AttributeError: module &amp;#x27;devsynth.application.llm&amp;#x27; has no 
attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144abf050&amp;gt;    @pytest.mark.fast    def 
test_offline_mode_selects_offline_provider_succeeds(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Test that offline mode selects offline provider 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setattr(            
&amp;quot;devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE&amp;quot;,
False        )        monkeypatch.setattr(            
&amp;quot;devsynth.application.llm.load_config&amp;quot;, lambda: 
_mock_config(True)        )&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.llm.get_llm_settings&amp;quot;,            
lambda: {                &amp;quot;provider&amp;quot;: 
&amp;quot;openai&amp;quot;,                &amp;quot;openai_api_key&amp;quot;: 
&amp;quot;key&amp;quot;,                &amp;quot;openai_model&amp;quot;: 
&amp;quot;gpt-3.5-turbo&amp;quot;,            },        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ll
m_provider_selection.py:30: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.llm&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;get_llm_settings&amp;#x27;, ann = 
&amp;#x27;devsynth.application.llm&amp;#x27;    def annotated_getattr(obj: 
object, name: str, ann: str) -&amp;gt; object:        try:            obj = 
getattr(obj, name)        except AttributeError as e:&amp;gt;           raise 
AttributeError(                f&amp;quot;{type(obj).__name__!r} object at {ann}
has no attribute {name!r}&amp;quot;            ) from eE           
AttributeError: &amp;#x27;module&amp;#x27; object at devsynth.application.llm 
has no attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError______________ test_online_mode_uses_configured_provider_succeeds 
______________obj = &amp;lt;module &amp;#x27;devsynth.application.llm&amp;#x27; 
from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;get_llm_settings&amp;#x27;, ann = 
&amp;#x27;devsynth.application.llm&amp;#x27;    def annotated_getattr(obj: 
object, name: str, ann: str) -&amp;gt; object:        try:&amp;gt;           obj
= getattr(obj, name)                  ^^^^^^^^^^^^^^^^^^E           
AttributeError: module &amp;#x27;devsynth.application.llm&amp;#x27; has no 
attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:90: 
AttributeErrorThe above exception was the direct cause of the following 
exception:monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144abe990&amp;gt;    @pytest.mark.fast    def 
test_online_mode_uses_configured_provider_succeeds(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Test that online mode uses configured provider 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setattr(            
&amp;quot;devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE&amp;quot;,
False        )        monkeypatch.setattr(            
&amp;quot;devsynth.application.llm.load_config&amp;quot;, lambda: 
_mock_config(False)        )&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.llm.get_llm_settings&amp;quot;,            
lambda: {                &amp;quot;provider&amp;quot;: 
&amp;quot;openai&amp;quot;,                &amp;quot;openai_api_key&amp;quot;: 
&amp;quot;key&amp;quot;,                &amp;quot;openai_model&amp;quot;: 
&amp;quot;gpt-3.5-turbo&amp;quot;,            },        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ll
m_provider_selection.py:53: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath    
annotated_getattr(target, attr, ann=module)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.llm&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/llm/__init__.py&amp;#x27;&amp;gt;name = 
&amp;#x27;get_llm_settings&amp;#x27;, ann = 
&amp;#x27;devsynth.application.llm&amp;#x27;    def annotated_getattr(obj: 
object, name: str, ann: str) -&amp;gt; object:        try:            obj = 
getattr(obj, name)        except AttributeError as e:&amp;gt;           raise 
AttributeError(                f&amp;quot;{type(obj).__name__!r} object at {ann}
has no attribute {name!r}&amp;quot;            ) from eE           
AttributeError: &amp;#x27;module&amp;#x27; object at devsynth.application.llm 
has no attribute 
&amp;#x27;get_llm_settings&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_____ 
TestLMStudioIntegrationRegression.test_lmstudio_settings_extraction ______self =
&amp;lt;tests.unit.general.test_lmstudio_integration_regression.TestLMStudioInte
grationRegression object at 0x1210584a0&amp;gt;    def 
test_lmstudio_settings_extraction(self):        
&amp;quot;&amp;quot;&amp;quot;Test that LLM settings can be extracted for LM 
Studio.            ReqID: LMSTUDIO-REG-3        
&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from devsynth.config import 
get_llm_settingsE       ImportError: cannot import name 
&amp;#x27;get_llm_settings&amp;#x27; from &amp;#x27;devsynth.config&amp;#x27; 
(unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general
/test_lmstudio_integration_regression.py:91: ImportError_ 
TestLMStudioIntegrationRegression.test_lmstudio_provider_mock_initialization 
_self = 
&amp;lt;tests.unit.general.test_lmstudio_integration_regression.TestLMStudioInte
grationRegression object at 0x121058dd0&amp;gt;mock_lmstudio = &amp;lt;MagicMock
name=&amp;#x27;lmstudio&amp;#x27; id=&amp;#x27;5446684736&amp;#x27;&amp;gt;    
@patch(&amp;quot;devsynth.application.llm.lmstudio_provider.lmstudio&amp;quot;) 
def test_lmstudio_provider_mock_initialization(self, mock_lmstudio):        
&amp;quot;&amp;quot;&amp;quot;Test LM Studio provider with mocked LM Studio 
service.            ReqID: LMSTUDIO-REG-5        
&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
....application.llm.lmstudio_provider import LMStudioProviderE       
ImportError: attempted relative import beyond top-level 
package/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/t
est_lmstudio_integration_regression.py:149: ImportError_ 
TestMultiAgentAdapterWorkflow.test_multi_agent_consensus_and_primus_selection_su
cceeds _self = &amp;lt;MagicMock name=&amp;#x27;build_consensus&amp;#x27; 
id=&amp;#x27;5442832816&amp;#x27;&amp;gt;args = ({&amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;confiden...s&amp;#x27;, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;content&amp;#x27;: 
&amp;#x27;doc&amp;#x27;, &amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;},)kwargs = {}expected = 
call({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1....nt&amp;#x27;: 
&amp;#x27;DocAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;doc&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;})actual = 
call({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1....&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]})_error_message = &amp;lt;function 
NonCallableMock.assert_called_with.&amp;lt;locals&amp;gt;._error_message at 
0x1445fd300&amp;gt;cause = None    def assert_called_with(self, /, *args, 
**kwargs):        &amp;quot;&amp;quot;&amp;quot;assert that the last call was 
made with the specified arguments.            Raises an AssertionError if the 
args and keyword args passed in are        different to the last call to the 
mock.&amp;quot;&amp;quot;&amp;quot;        if self.call_args is None:           
expected = self._format_mock_call_signature(args, kwargs)            actual = 
&amp;#x27;not called.&amp;#x27;            error_message = (&amp;#x27;expected 
call not found.\nExpected: %s\n  Actual: %s&amp;#x27;                    % 
(expected, actual))            raise AssertionError(error_message)            
def _error_message():            msg = self._format_mock_failure_message(args, 
kwargs)            return msg        expected = self._call_matcher(_Call((args, 
kwargs), two=True))        actual = self._call_matcher(self.call_args)        if
actual != expected:            cause = expected if isinstance(expected, 
Exception) else None&amp;gt;           raise AssertionError(_error_message()) 
from causeE           AssertionError: expected call not found.E           
Expected: build_consensus({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;,
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;})E             Actual: 
build_consensus({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]})/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Py
thon.framework/Versions/3.12/lib/python3.12/unittest/mock.py:949: 
AssertionErrorDuring handling of the above exception, another exception 
occurred:self = &amp;lt;MagicMock name=&amp;#x27;build_consensus&amp;#x27; 
id=&amp;#x27;5442832816&amp;#x27;&amp;gt;args = ({&amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;confiden...s&amp;#x27;, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;content&amp;#x27;: 
&amp;#x27;doc&amp;#x27;, &amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}], 
&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;},)kwargs = {}    def 
assert_called_once_with(self, /, *args, **kwargs):        
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called exactly once and 
that that call was        with the specified 
arguments.&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:    
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to be called once. Called %s 
times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))            raise AssertionError(msg)&amp;gt;       return 
self.assert_called_with(*args, **kwargs)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: expected call 
not found.E       Expected: build_consensus({&amp;#x27;type&amp;#x27;: 
&amp;#x27;coding&amp;#x27;, &amp;#x27;language&amp;#x27;: 
&amp;#x27;python&amp;#x27;, &amp;#x27;solutions&amp;#x27;: 
[{&amp;#x27;agent&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;})E         Actual: 
build_consensus({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]})E       E       pytest introspection follows:E       E
Args:E       assert ({&amp;#x27;id&amp;#x27;: &amp;#x27;f213...: 
&amp;#x27;&amp;#x27;}], ...},) == ({&amp;#x27;id&amp;#x27;: 
&amp;#x27;f213...&amp;#x27;: &amp;#x27;coding&amp;#x27;},)E         E         At
index 0 diff: {&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]} != {&amp;#x27;type&amp;#x27;: 
&amp;#x27;coding&amp;#x27;, &amp;#x27;language&amp;#x27;: 
&amp;#x27;python&amp;#x27;, &amp;#x27;solutions&amp;#x27;: 
[{&amp;#x27;agent&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, ...E         E         ...Full output 
truncated (2 lines hidden), use &amp;#x27;-vv&amp;#x27; to 
show/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Version
s/3.12/lib/python3.12/unittest/mock.py:961: AssertionErrorDuring handling of the
above exception, another exception occurred:self = 
&amp;lt;tests.unit.general.test_multi_agent_adapter_workflow.TestMultiAgentAdapt
erWorkflow object at 0x121093950&amp;gt;    @pytest.mark.fast    def 
test_multi_agent_consensus_and_primus_selection_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that multi agent consensus and primus 
selection succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
task = {&amp;quot;type&amp;quot;: &amp;quot;coding&amp;quot;, 
&amp;quot;language&amp;quot;: &amp;quot;python&amp;quot;}        with (         
patch.object(                self.team,                
&amp;quot;build_consensus&amp;quot;,                return_value={              
&amp;quot;consensus&amp;quot;: &amp;quot;done&amp;quot;,                    
&amp;quot;contributors&amp;quot;: [&amp;quot;PythonAgent&amp;quot;, 
&amp;quot;JSAgent&amp;quot;, &amp;quot;DocAgent&amp;quot;],                    
&amp;quot;method&amp;quot;: &amp;quot;consensus_synthesis&amp;quot;,            
&amp;quot;reasoning&amp;quot;: &amp;quot;&amp;quot;,                },          
) as mock_consensus,            patch.object(                self.team,         
&amp;quot;select_primus_by_expertise&amp;quot;,                
wraps=self.team.select_primus_by_expertise,            ) as mock_select,        
):            result = self.adapter.process_task(task)&amp;gt;           
mock_consensus.assert_called_once_with(task)E           AssertionError: expected
call not found.E           Expected: build_consensus({&amp;#x27;type&amp;#x27;: 
&amp;#x27;coding&amp;#x27;, &amp;#x27;language&amp;#x27;: 
&amp;#x27;python&amp;#x27;, &amp;#x27;solutions&amp;#x27;: 
[{&amp;#x27;agent&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;})E             Actual: 
build_consensus({&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]})E           E           pytest introspection follows:E
E           Args:E           assert ({&amp;#x27;id&amp;#x27;: &amp;#x27;f213...:
&amp;#x27;&amp;#x27;}], ...},) == ({&amp;#x27;id&amp;#x27;: 
&amp;#x27;f213...&amp;#x27;: &amp;#x27;coding&amp;#x27;},)E             E       
At index 0 diff: {&amp;#x27;type&amp;#x27;: &amp;#x27;coding&amp;#x27;, 
&amp;#x27;language&amp;#x27;: &amp;#x27;python&amp;#x27;, 
&amp;#x27;solutions&amp;#x27;: [{&amp;#x27;agent&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;content&amp;#x27;: 
&amp;#x27;py&amp;#x27;, &amp;#x27;confidence&amp;#x27;: 1.0, 
&amp;#x27;reasoning&amp;#x27;: &amp;#x27;&amp;#x27;}, 
{&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;DocAgent&amp;#x27;,
&amp;#x27;content&amp;#x27;: &amp;#x27;doc&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}], &amp;#x27;id&amp;#x27;: 
&amp;#x27;f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;py&amp;#x27;, &amp;#x27;js&amp;#x27;, 
&amp;#x27;doc&amp;#x27;]} != {&amp;#x27;type&amp;#x27;: 
&amp;#x27;coding&amp;#x27;, &amp;#x27;language&amp;#x27;: 
&amp;#x27;python&amp;#x27;, &amp;#x27;solutions&amp;#x27;: 
[{&amp;#x27;agent&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;py&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, &amp;#x27;reasoning&amp;#x27;: 
&amp;#x27;&amp;#x27;}, {&amp;#x27;agent&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;content&amp;#x27;: &amp;#x27;js&amp;#x27;, 
&amp;#x27;confidence&amp;#x27;: 1.0, ...E             E             ...Full 
output truncated (2 lines hidden), use &amp;#x27;-vv&amp;#x27; to 
show/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_multi_agent_adapter_workflow.py:63: AssertionError---------------------------- 
Captured stdout setup -----------------------------2025-10-28 09:27:12,585 - 
devsynth.domain.models.wsde_core - INFO - Added agent PythonAgent to team 
team12025-10-28 09:27:12,586 - devsynth.domain.models.wsde_core - INFO - Added 
agent JSAgent to team team12025-10-28 09:27:12,586 - 
devsynth.domain.models.wsde_roles - INFO - Role assignments for team team1: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}2025-10-28 09:27:12,586 - 
devsynth.domain.models.wsde_core - INFO - Added agent DocAgent to team 
team12025-10-28 09:27:12,586 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team team1: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;JSAgent&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: 
&amp;#x27;DocAgent&amp;#x27;, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}------------------------------ Captured log 
setup ------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent PythonAgent to
team team1INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent JSAgent to team team1INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments for team
team1: {&amp;#x27;primus&amp;#x27;: &amp;#x27;PythonAgent&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;JSAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent DocAgent to 
team team1INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role 
assignments for team team1: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;PythonAgent&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;JSAgent&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: 
&amp;#x27;DocAgent&amp;#x27;, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_roles - INFO - Selected JSAgent as primus based on 
expertise2025-10-28 09:27:12,587 - devsynth.domain.models.wsde_utils - INFO - 
Added solution for task f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 
09:27:12,587 - devsynth.domain.models.wsde_utils - INFO - Added solution for 
task f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_enhanced_dialectical - INFO - Applying enhanced 
multi-solution dialectical reasoning to task: 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_solution_analysis - INFO - Analyzing solution 1 for 
task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_solution_analysis - INFO - Analyzing solution 2 for 
task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_solution_analysis - INFO - Analyzing solution 3 for 
task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf32025-10-28 09:27:12,587 - 
devsynth.domain.models.wsde_solution_analysis - INFO - Generating comparative 
analysis for task: 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected JSAgent as 
primus based on expertiseINFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_enhanced_dialectical:logging_setup.py:615 Applying 
enhanced multi-solution dialectical reasoning to task: 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 Analyzing 
solution 1 for task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 Analyzing 
solution 2 for task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 Analyzing 
solution 3 for task: f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3INFO     
devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 Generating 
comparative analysis for task: 
f21340da-9d3d-4c3c-a4fc-fc92ea0edcf3__________________________ 
test_mvu_lint_cli_success ___________________________name = 
&amp;#x27;commands&amp;#x27;    def __getattr__(name: str) -&amp;gt; object:    
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above exception
was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144a129c0&amp;gt;    
@pytest.mark.fast    def test_mvu_lint_cli_success(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;CLI should report success when no errors are 
returned.&amp;quot;&amp;quot;&amp;quot;        runner = CliRunner()        app =
build_app()&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.mvu_lint_cmd.lint_range&amp;quot;,  
lambda _rev: [],        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mv
u_lint_cli.py:36: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = &amp;#x27;commands&amp;#x27;, 
ann = &amp;#x27;devsynth.application.cli.commands&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.commands has no 
attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError__________________________ test_mvu_lint_cli_failure 
___________________________name = &amp;#x27;commands&amp;#x27;    def 
__getattr__(name: str) -&amp;gt; object:        
&amp;quot;&amp;quot;&amp;quot;Lazily expose CLI command callables when 
requested.&amp;quot;&amp;quot;&amp;quot;            if (            name        
in {                &amp;quot;config_app&amp;quot;,                
&amp;quot;inspect_code_cmd&amp;quot;,                
&amp;quot;ingest_cmd&amp;quot;,            }            or name in 
COMMAND_ATTRIBUTE_NAMES        ):            _register_commands()            if 
name in globals() and globals() is not None:                return globals()    
raise AttributeError(f&amp;quot;CLI command &amp;#x27;{name}&amp;#x27; is 
unavailable&amp;quot;)&amp;gt;       raise AttributeError(f&amp;quot;module 
&amp;#x27;{__name__}&amp;#x27; has no attribute 
&amp;#x27;{name}&amp;#x27;&amp;quot;)E       AttributeError: module 
&amp;#x27;devsynth.application.cli&amp;#x27; has no attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/cli/__init__.py:101: AttributeErrorThe above exception
was the direct cause of the following exception:monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144a12540&amp;gt;    
@pytest.mark.fast    def test_mvu_lint_cli_failure(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;CLI should exit with error when linter reports 
problems.&amp;quot;&amp;quot;&amp;quot;        runner = CliRunner()        app =
build_app()&amp;gt;       monkeypatch.setattr(            
&amp;quot;devsynth.application.cli.commands.mvu_lint_cmd.lint_range&amp;quot;,  
lambda _rev: [&amp;quot;abc123: error&amp;quot;],        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mv
u_lint_cli.py:50: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath    target = 
resolve(module)             
^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/py
thon3.12/site-packages/_pytest/monkeypatch.py:84: in resolve    found = 
annotated_getattr(found, part, used)            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ obj = &amp;lt;module 
&amp;#x27;devsynth.application.cli&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/appl
ication/cli/__init__.py&amp;#x27;&amp;gt;name = &amp;#x27;commands&amp;#x27;, 
ann = &amp;#x27;devsynth.application.cli.commands&amp;#x27;    def 
annotated_getattr(obj: object, name: str, ann: str) -&amp;gt; object:        
try:            obj = getattr(obj, name)        except AttributeError as 
e:&amp;gt;           raise AttributeError(                
f&amp;quot;{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}&amp;quot;            ) from eE           AttributeError: 
&amp;#x27;module&amp;#x27; object at devsynth.application.cli.commands has no 
attribute 
&amp;#x27;commands&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py:92: 
AttributeError_____________ test_ensure_path_exists_within_project_dir_succeeds 
______________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_ensure_path_exists_within0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144abfb30&amp;gt;    
@pytest.mark.fast    def 
test_ensure_path_exists_within_project_dir_succeeds(tmp_path, monkeypatch):     
&amp;quot;&amp;quot;&amp;quot;Test that ensure path exists within project dir 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        project_dir
= tmp_path / &amp;quot;project&amp;quot;        outside_dir = tmp_path / 
&amp;quot;outside&amp;quot;        project_dir.mkdir(exist_ok=True)        
outside_dir.mkdir(exist_ok=True)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(project_dir))  
monkeypatch.setenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, 
&amp;quot;1&amp;quot;)&amp;gt;       settings = 
importlib.reload(settings_module)                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/tests/unit/general/test_path_restrictions.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ module = &amp;lt;module 
&amp;#x27;devsynth.config.settings&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/conf
ig/settings.py&amp;#x27;&amp;gt;    def reload(module):        
&amp;quot;&amp;quot;&amp;quot;Reload the module and return it.            The 
module must have been successfully imported before.            
&amp;quot;&amp;quot;&amp;quot;        try:            name = 
module.__spec__.name        except AttributeError:            try:              
name = module.__name__            except AttributeError:                raise 
TypeError(&amp;quot;reload() argument must be a module&amp;quot;) from None     
if sys.modules.get(name) is not module:&amp;gt;           raise 
ImportError(f&amp;quot;module {name} not in sys.modules&amp;quot;, name=name)E  
ImportError: module devsynth.config.settings not in 
sys.modules/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/
Versions/3.12/lib/python3.12/importlib/__init__.py:111: ImportError________ 
test_documentation_tasks_prefer_documentation_experts_succeeds ________    def 
test_documentation_tasks_prefer_documentation_experts_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that documentation tasks prefer documentation
experts succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
team = WSDETeam(name=&amp;quot;TestPrimusSelectionTeam&amp;quot;)        coder =
create_agent(&amp;quot;Coder&amp;quot;, [&amp;quot;python&amp;quot;])        
doc_agent = create_agent(&amp;quot;Doc&amp;quot;, 
[&amp;quot;documentation&amp;quot;, &amp;quot;markdown&amp;quot;])        
team.add_agents()        task = {&amp;quot;type&amp;quot;: 
&amp;quot;documentation&amp;quot;, &amp;quot;description&amp;quot;: 
&amp;quot;Update docs&amp;quot;}        
team.select_primus_by_expertise(task)&amp;gt;       assert team.get_primus() is 
doc_agentE       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5444689104&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5444692896&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5444689104&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144a12690&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/general/test_primus_selection.py:91: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:15,636 - 
devsynth.domain.models.wsde_core - INFO - Added agent Coder to team 
TestPrimusSelectionTeam2025-10-28 09:27:15,636 - 
devsynth.domain.models.wsde_core - INFO - Added agent Doc to team 
TestPrimusSelectionTeam2025-10-28 09:27:15,636 - 
devsynth.domain.models.wsde_roles - INFO - Selected Coder as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder to team 
TestPrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc to team 
TestPrimusSelectionTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder as primus 
based on expertise_____________ 
test_weighted_expertise_prefers_specialist_succeeds ______________weighted_team 
= (&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144825460&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5444354272&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5444358544&amp;#x27;&amp;gt;)    def 
test_weighted_expertise_prefers_specialist_succeeds(weighted_team):        
&amp;quot;&amp;quot;&amp;quot;Test that weighted expertise prefers specialist 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team, 
generalist, specialist = weighted_team        task = {&amp;quot;type&amp;quot;: 
&amp;quot;coding&amp;quot;, &amp;quot;language&amp;quot;: 
&amp;quot;python&amp;quot;, &amp;quot;domain&amp;quot;: 
&amp;quot;backend&amp;quot;}        
team.select_primus_by_expertise(task)&amp;gt;       assert team.get_primus() is 
specialistE       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5444354272&amp;#x27;&amp;gt; is &amp;lt;MagicMock 
id=&amp;#x27;5444358544&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
id=&amp;#x27;5444354272&amp;#x27;&amp;gt; = get_primus()E        +    where 
get_primus = &amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144825460&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/general/test_primus_selection.py:102: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:15,650 - 
devsynth.domain.models.wsde_core - INFO - Added agent Generalist to team 
TestPrimusSelectionTeam2025-10-28 09:27:15,650 - 
devsynth.domain.models.wsde_core - INFO - Added agent Specialist to team 
TestPrimusSelectionTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Generalist to 
team TestPrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Specialist to 
team TestPrimusSelectionTeam----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:15,651 - 
devsynth.domain.models.wsde_roles - INFO - Selected Generalist as primus based 
on expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Generalist as 
primus based on expertise_________ 
test_documentation_tasks_prioritize_best_doc_expert_succeeds 
_________documentation_team = 
(&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x1448640e0&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5444622944&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5446872496&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5446868608&amp;#x27;&amp;gt;)    def 
test_documentation_tasks_prioritize_best_doc_expert_succeeds(documentation_team)
:        &amp;quot;&amp;quot;&amp;quot;Test that documentation tasks prioritize 
best doc expert succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;   
team, coder, writer, doc = documentation_team        task = 
{&amp;quot;type&amp;quot;: &amp;quot;documentation&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;Write docs&amp;quot;}        
team.select_primus_by_expertise(task)        primus = team.get_primus()&amp;gt; 
assert primus in (writer, doc)E       AssertionError: assert &amp;lt;MagicMock 
id=&amp;#x27;5444622944&amp;#x27;&amp;gt; in (&amp;lt;MagicMock 
id=&amp;#x27;5446872496&amp;#x27;&amp;gt;, &amp;lt;MagicMock 
id=&amp;#x27;5446868608&amp;#x27;&amp;gt;)/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/general/test_primus_selection.py:128: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:15,675 - 
devsynth.domain.models.wsde_core - INFO - Added agent Coder to team 
TestPrimusSelectionTeam2025-10-28 09:27:15,676 - 
devsynth.domain.models.wsde_core - INFO - Added agent Writer to team 
TestPrimusSelectionTeam2025-10-28 09:27:15,676 - 
devsynth.domain.models.wsde_core - INFO - Added agent Doc to team 
TestPrimusSelectionTeam------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder to team 
TestPrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Writer to team
TestPrimusSelectionTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc to team 
TestPrimusSelectionTeam----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:15,677 - 
devsynth.domain.models.wsde_roles - INFO - Selected Coder as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder as primus 
based on expertise________________________ test_is_cli_available_succeeds 
________________________    @pytest.mark.fast    def 
test_is_cli_available_succeeds():        &amp;quot;&amp;quot;&amp;quot;Test that
is_cli_available checks environment variables and CLI availability.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        with patch.dict(os.environ, 
{&amp;quot;DEVSYNTH_RESOURCE_CLI_AVAILABLE&amp;quot;: 
&amp;quot;false&amp;quot;}):            from tests.conftest import 
is_cli_available                assert not is_cli_available()        with 
patch(&amp;quot;subprocess.run&amp;quot;) as mock_run:            
mock_run.return_value.returncode = 0            from tests.conftest import 
is_cli_available                assert is_cli_available()        with 
patch(&amp;quot;subprocess.run&amp;quot;) as mock_run:            
mock_run.return_value.returncode = 1            from tests.conftest import 
is_cli_available                assert not is_cli_available()        with 
patch(&amp;quot;subprocess.run&amp;quot;, 
side_effect=Exception(&amp;quot;Command not found&amp;quot;)):            from 
tests.conftest import is_cli_available    &amp;gt;           assert not 
is_cli_available()E           assert not TrueE            +  where True = 
&amp;lt;function is_cli_available at 
0x113a007c0&amp;gt;()/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/general/test_resource_markers.py:89: AssertionError_________________ 
test_pytest_collection_modifyitems_succeeds __________________    
@pytest.mark.fast    def test_pytest_collection_modifyitems_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that pytest_collection_modifyitems skips 
tests with unavailable resources.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        from tests.conftest import 
pytest_collection_modifyitems            class MockMarker:                def 
__init__(self, name, args):                self.name = name                
self.args = args            class MockItem:                def __init__(self, 
name, markers=None):                self.name = name                
self._markers = markers or []                self.user_properties = []          
def iter_markers(self, name=None):                if name:                    
return                 return self._markers                def add_marker(self, 
marker):                self._markers.append(marker)            class 
MockConfig:                def __init__(self):                pass            
item1 = MockItem(            &amp;quot;test_unavailable_resource&amp;quot;,     
[MockMarker(&amp;quot;requires_resource&amp;quot;, 
[&amp;quot;unavailable_resource&amp;quot;])],        )        item2 = MockItem( 
&amp;quot;test_available_resource&amp;quot;,            
[MockMarker(&amp;quot;requires_resource&amp;quot;, 
[&amp;quot;available_resource&amp;quot;])],        )        item3 = 
MockItem(&amp;quot;test_no_resource_marker&amp;quot;)        items =         
with patch(            &amp;quot;tests.conftest.is_resource_available&amp;quot;,
lambda r: r == &amp;quot;available_resource&amp;quot;        ):&amp;gt;         
pytest_collection_modifyitems(MockConfig(), 
items)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/te
st_resource_markers.py:161: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ config = 
&amp;lt;tests.unit.general.test_resource_markers.test_pytest_collection_modifyit
ems_succeeds.&amp;lt;locals&amp;gt;.MockConfig object at 
0x14482e150&amp;gt;items = 
[&amp;lt;tests.unit.general.test_resource_markers.test_pytest_collection_modifyi
tems_succeeds.&amp;lt;locals&amp;gt;.MockItem object at 
0x1...nit.general.test_resource_markers.test_pytest_collection_modifyitems_succe
eds.&amp;lt;locals&amp;gt;.MockItem object at 0x14482e1b0&amp;gt;]    def 
pytest_collection_modifyitems(config, items):        
&amp;quot;&amp;quot;&amp;quot;        Normalize and enforce resource gating, 
smoke-mode behavior, property-based testing collection,        and conservative 
xdist-parallel safety.            - Resource gating: validate 
@pytest.mark.requires_resource(&amp;quot;&amp;lt;name&amp;gt;&amp;quot;) and 
skip when unavailable or malformed/unknown.        - Smoke mode: when 
PYTEST_DISABLE_PLUGIN_AUTOLOAD=1, skip tests under tests/behavior/ that rely on 
third-party plugins.        - Property-based tests: skip unless 
DEVSYNTH_PROPERTY_TESTING is enabled.        - Xdist safety: conservatively mark
integration and performance tests as @pytest.mark.isolation to avoid 
parallelization issues.        &amp;quot;&amp;quot;&amp;quot;        # Validate 
and apply resource gating        for item in items:            for marker in 
item.iter_markers(name=&amp;quot;requires_resource&amp;quot;):                # 
Validate marker arguments                if (                    not marker.args
or not isinstance(marker.args[0], str)                    or not 
marker.args[0].strip()                ):                    item.add_marker(    
pytest.mark.skip(                            reason=&amp;quot;Malformed 
requires_resource marker: expected a non-empty resource name&amp;quot;          
)                    )                    continue                resource = 
marker.args[0].strip()                    # Validate known resource             
known_resources = {                    &amp;quot;anthropic&amp;quot;,           
&amp;quot;llm_provider&amp;quot;,                    
&amp;quot;lmstudio&amp;quot;,                    &amp;quot;openai&amp;quot;,    
&amp;quot;codebase&amp;quot;,                    &amp;quot;cli&amp;quot;,       
&amp;quot;chromadb&amp;quot;,                    &amp;quot;tinydb&amp;quot;,    
&amp;quot;duckdb&amp;quot;,                    &amp;quot;faiss&amp;quot;,       
&amp;quot;kuzu&amp;quot;,                    &amp;quot;lmdb&amp;quot;,          
&amp;quot;rdflib&amp;quot;,                    &amp;quot;memory&amp;quot;,      
&amp;quot;test_resource&amp;quot;,                    &amp;quot;webui&amp;quot;,
}                if resource not in known_resources:                    
item.add_marker(                        pytest.mark.skip(                       
reason=f&amp;quot;Unknown resource &amp;#x27;{resource}&amp;#x27; not recognized
by test harness&amp;quot;                        )                    )         
continue                    # Add a derived static marker to enable &amp;#x27;-m
resource_&amp;lt;name&amp;gt;&amp;#x27; selection                try:           
item.add_marker(getattr(pytest.mark, f&amp;quot;resource_{resource}&amp;quot;)) 
except Exception:                    # Defensive: do not fail collection if 
dynamic marker attachment has issues                    pass                    
# Skip if resource is not available                if not 
is_resource_available(resource):                    item.add_marker(            
pytest.mark.skip(reason=f&amp;quot;Resource &amp;#x27;{resource}&amp;#x27; not 
available&amp;quot;)                    )            # Smoke-mode behavior: skip
behavior tests when plugins are disabled        smoke = 
os.environ.get(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;0&amp;quot;).lower() in {            &amp;quot;1&amp;quot;,           
&amp;quot;true&amp;quot;,            &amp;quot;yes&amp;quot;,        }        if
smoke:            skip_behavior = pytest.mark.skip(                reason=(     
&amp;quot;Smoke mode (plugins disabled): skipping behavior tests that require 
third-party plugins&amp;quot;                )            )            for item 
in items:                try:                    fspath = getattr(item, 
&amp;quot;fspath&amp;quot;, None)                    path_str = str(fspath) if 
fspath is not None else &amp;quot;&amp;quot;                except Exception:   
path_str = &amp;quot;&amp;quot;                norm = 
path_str.replace(&amp;quot;\\&amp;quot;, &amp;quot;/&amp;quot;)                
if &amp;quot;/tests/behavior/&amp;quot; in norm or 
norm.endswith(&amp;quot;/tests/behavior&amp;quot;):                    
item.add_marker(skip_behavior)            # Conservatively mark integration and 
performance tests as isolation for xdist safety        for item in items:       
try:                fspath = getattr(item, &amp;quot;fspath&amp;quot;, None)    
path_str = str(fspath) if fspath is not None else &amp;quot;&amp;quot;          
except Exception:                path_str = &amp;quot;&amp;quot;            norm
= path_str.replace(&amp;quot;\\&amp;quot;, &amp;quot;/&amp;quot;)            if 
&amp;quot;/tests/integration/&amp;quot; in norm or 
&amp;quot;/tests/performance/&amp;quot; in norm:                if not 
item.get_closest_marker(&amp;quot;isolation&amp;quot;):                    
item.add_marker(pytest.mark.isolation)            # Broaden isolation 
auto-marking heuristics for fragile tests that touch filesystem/network        #
Apply only when a test lacks an explicit @pytest.mark.isolation        
network_keywords = (&amp;quot;network&amp;quot;, &amp;quot;http&amp;quot;, 
&amp;quot;https&amp;quot;, &amp;quot;socket&amp;quot;, 
&amp;quot;requests&amp;quot;, &amp;quot;ftp&amp;quot;)        fs_fixtures = {   
&amp;quot;tmp_path&amp;quot;,            &amp;quot;tmpdir&amp;quot;,            
&amp;quot;tmpdir_factory&amp;quot;,            &amp;quot;temp_log_dir&amp;quot;,
&amp;quot;tmp_project_dir&amp;quot;,        }        for item in items:         
try:                name = getattr(item, &amp;quot;name&amp;quot;, 
&amp;quot;&amp;quot;) or &amp;quot;&amp;quot;                nodeid = 
getattr(item, &amp;quot;nodeid&amp;quot;, &amp;quot;&amp;quot;) or 
&amp;quot;&amp;quot;                fixturenames = set(getattr(item, 
&amp;quot;fixturenames&amp;quot;, []) or [])            except Exception:       
name, nodeid, fixturenames = &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, 
set()&amp;gt;           if 
item.get_closest_marker(&amp;quot;isolation&amp;quot;):               
^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;MockItem&amp;#x27; 
object has no attribute 
&amp;#x27;get_closest_marker&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/conftest.py:1158: AttributeError_______________________ 
test_cli_bridge_methods_succeeds _______________________self = &amp;lt;MagicMock
name=&amp;#x27;ask&amp;#x27; id=&amp;#x27;5447670848&amp;#x27;&amp;gt;    def 
assert_called_once(self):        &amp;quot;&amp;quot;&amp;quot;assert that the 
mock was called only once.        &amp;quot;&amp;quot;&amp;quot;        if not 
self.call_count == 1:            msg = (&amp;quot;Expected 
&amp;#x27;%s&amp;#x27; to have been called once. Called %s times.%s&amp;quot;   
% (self._mock_name or &amp;#x27;mock&amp;#x27;,                      
self.call_count,                      self._calls_repr()))&amp;gt;           
raise AssertionError(msg)E           AssertionError: Expected 
&amp;#x27;ask&amp;#x27; to have been called once. Called 0 
times./opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versi
ons/3.12/lib/python3.12/unittest/mock.py:928: AssertionErrorDuring handling of 
the above exception, another exception occurred:    @pytest.mark.fast    def 
test_cli_bridge_methods_succeeds():        &amp;quot;&amp;quot;&amp;quot;Test 
that cli bridge methods succeeds.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        bridge = CLIUXBridge()        with 
patch(&amp;quot;rich.prompt.Prompt.ask&amp;quot;, 
return_value=&amp;quot;ans&amp;quot;) as ask:            resp = 
bridge.ask_question(&amp;quot;Q?&amp;quot;)&amp;gt;           
ask.assert_called_once()E           AssertionError: Expected 
&amp;#x27;ask&amp;#x27; to have been called once. Called 0 
times./Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/te
st_ux_bridge.py:19: AssertionError----------------------------- Captured stdout 
call -----------------------------Q?  Q?______________________ 
test_webui_bridge_methods_succeeds ______________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144a2ab10&amp;gt;    
@pytest.mark.fast    def test_webui_bridge_methods_succeeds(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;Test that webui bridge methods succeeds.          
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        st = 
ModuleType(&amp;quot;streamlit&amp;quot;)        st.text_input = 
MagicMock(return_value=&amp;quot;text&amp;quot;)        st.selectbox = 
MagicMock(return_value=&amp;quot;choice&amp;quot;)        st.checkbox = 
MagicMock(return_value=True)        st.write = MagicMock()        st.markdown = 
MagicMock()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, st)        import importlib    &amp;gt;       
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/tes
t_ux_bridge.py:47: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module for 
DevSynth.        This module provides web interface components for DevSynth.    
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________ 
TestWorkflowManager.test_handle_human_intervention_succeeds __________self = 
&amp;lt;tests.unit.general.test_workflow.TestWorkflowManager object at 
0x1211b4080&amp;gt;workflow_manager = 
&amp;lt;devsynth.application.orchestration.workflow.WorkflowManager object at 
0x144bf64e0&amp;gt;    @pytest.mark.fast    def 
test_handle_human_intervention_succeeds(self, workflow_manager):        
&amp;quot;&amp;quot;&amp;quot;Test handling human intervention.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        with (            patch(        
&amp;quot;devsynth.application.orchestration.workflow.console&amp;quot;         
) as mock_console,            patch(                
&amp;quot;devsynth.application.orchestration.workflow.Prompt.ask&amp;quot;,     
return_value=&amp;quot;User input&amp;quot;,            ),        ):            
response = workflow_manager._handle_human_intervention(                
&amp;quot;workflow-id&amp;quot;, &amp;quot;step-id&amp;quot;, &amp;quot;Need 
your input&amp;quot;            )&amp;gt;           assert response == 
&amp;quot;User input&amp;quot;E           AssertionError: assert 
&amp;#x27;&amp;#x27; == &amp;#x27;User input&amp;#x27;E             E           
- User 
input/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/tes
t_workflow.py:44: AssertionError----------------------------- Captured stdout 
call -----------------------------Human intervention required:Need your 
inputYour input          Your input__________ 
TestWorkflowManager.test_add_init_workflow_steps_succeeds ___________self = 
&amp;lt;tests.unit.general.test_workflow.TestWorkflowManager object at 
0x1211b4950&amp;gt;workflow_manager = 
&amp;lt;devsynth.application.orchestration.workflow.WorkflowManager object at 
0x144e3e9f0&amp;gt;    @pytest.mark.fast    def 
test_add_init_workflow_steps_succeeds(self, workflow_manager):        
&amp;quot;&amp;quot;&amp;quot;Test adding steps for init workflow.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        mock_workflow = MagicMock()     
workflow_manager._add_init_workflow_steps(            mock_workflow, 
{&amp;quot;path&amp;quot;: &amp;quot;./test-project&amp;quot;}        )&amp;gt; 
assert workflow_manager.orchestration_port.add_step.call_count == 3E       
AssertionError: assert 6 == 3E        +  where 6 = &amp;lt;MagicMock 
name=&amp;#x27;OrchestrationPort.add_step&amp;#x27; 
id=&amp;#x27;5450516432&amp;#x27;&amp;gt;.call_countE        +    where 
&amp;lt;MagicMock name=&amp;#x27;OrchestrationPort.add_step&amp;#x27; 
id=&amp;#x27;5450516432&amp;#x27;&amp;gt; = &amp;lt;MagicMock 
name=&amp;#x27;OrchestrationPort&amp;#x27; 
id=&amp;#x27;5450750800&amp;#x27;&amp;gt;.add_stepE        +      where 
&amp;lt;MagicMock name=&amp;#x27;OrchestrationPort&amp;#x27; 
id=&amp;#x27;5450750800&amp;#x27;&amp;gt; = 
&amp;lt;devsynth.application.orchestration.workflow.WorkflowManager object at 
0x144e3e9f0&amp;gt;.orchestration_port/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/unit/general/test_workflow.py:83: AssertionError_______________
test_assign_roles_with_explicit_mapping_succeeds _______________    def 
test_assign_roles_with_explicit_mapping_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that assign roles with explicit mapping 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestWsdeRoleMappingTeam&amp;quot;)        a1 = 
MagicMock()        a2 = MagicMock()        a3 = MagicMock()        
team.add_agents()        # The primus agent should not also be listed as a 
worker or the        # assignment logic will overwrite the Primus role.  Provide
a distinct        # worker mapping to verify role assignment order.        
mapping = {            &amp;quot;primus&amp;quot;: a1,            
&amp;quot;worker&amp;quot;: ,            &amp;quot;supervisor&amp;quot;: a2,    
&amp;quot;designer&amp;quot;: a3,            &amp;quot;evaluator&amp;quot;: 
None,        }&amp;gt;       
team.assign_roles(mapping)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/general/test_wsde_role_mapping.py:33: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:663: in assign_roles    return _manager(self).assign(role_mapping)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/d
evsynth/src/devsynth/domain/models/wsde_roles.py:372: in assign    
self._validate_role_mapping(normalised)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
RoleAssignmentManager(team=&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x144e075c0&amp;gt;)mapping = {&amp;lt;RoleName.DESIGNER: 
&amp;#x27;designer&amp;#x27;&amp;gt;: &amp;lt;MagicMock 
id=&amp;#x27;5450803408&amp;#x27;&amp;gt;, &amp;lt;RoleName.EVALUATOR: 
&amp;#x27;evaluator&amp;#x27;&amp;gt;: None, &amp;lt;RoleName.PRIMUS: 
&amp;#x27;primus&amp;#x27;&amp;gt;: &amp;lt;MagicMock 
id=&amp;#x27;5450759824&amp;#x27;&amp;gt;, &amp;lt;RoleName.SUPERVISOR: 
&amp;#x27;supervisor&amp;#x27;&amp;gt;: &amp;lt;MagicMock 
id=&amp;#x27;5450756320&amp;#x27;&amp;gt;, ...}    def _validate_role_mapping(  
self, mapping: Mapping[RoleName, SupportsTeamAgent | None]    ) -&amp;gt; None: 
valid_roles = set(RoleName)        for role, agent in mapping.items():          
if role not in valid_roles:                raise ValueError(                    
f&amp;quot;Invalid role: {role}. Valid roles are: {&amp;#x27;, 
&amp;#x27;.join(r.value for r in valid_roles)}&amp;quot;                )       
if agent is not None and agent not in self.team.agents:&amp;gt;               
raise ValueError(f&amp;quot;Agent {agent.name} is not a member of this 
team&amp;quot;)                                          ^^^^^^^^^^E            
AttributeError: &amp;#x27;list&amp;#x27; object has no attribute 
&amp;#x27;name&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src
/devsynth/domain/models/wsde_roles.py:520: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,240 - 
devsynth.domain.models.wsde_core - INFO - Added agent &amp;lt;MagicMock 
name=&amp;#x27;mock.name&amp;#x27; id=&amp;#x27;5450744512&amp;#x27;&amp;gt; to 
team TestWsdeRoleMappingTeam2025-10-28 09:27:20,240 - 
devsynth.domain.models.wsde_core - INFO - Added agent &amp;lt;MagicMock 
name=&amp;#x27;mock.name&amp;#x27; id=&amp;#x27;5448292384&amp;#x27;&amp;gt; to 
team TestWsdeRoleMappingTeam2025-10-28 09:27:20,241 - 
devsynth.domain.models.wsde_core - INFO - Added agent &amp;lt;MagicMock 
name=&amp;#x27;mock.name&amp;#x27; id=&amp;#x27;5448108944&amp;#x27;&amp;gt; to 
team TestWsdeRoleMappingTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450744512&amp;#x27;&amp;gt; to team TestWsdeRoleMappingTeamINFO   
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5448292384&amp;#x27;&amp;gt; to team TestWsdeRoleMappingTeamINFO   
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5448108944&amp;#x27;&amp;gt; to team 
TestWsdeRoleMappingTeam_____________ 
TestWSDETeam.test_get_role_specific_agents_succeeds ______________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211cb080&amp;gt;mock_agent = &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5451311840&amp;#x27;&amp;gt;    
def test_get_role_specific_agents_succeeds(self, mock_agent):        
&amp;quot;&amp;quot;&amp;quot;Test getting agents by their specific roles.      
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        agent1 = 
MagicMock(spec=BaseAgent)        agent2 = MagicMock(spec=BaseAgent)        
agent3 = MagicMock(spec=BaseAgent)        agent4 = MagicMock(spec=BaseAgent)    
agent5 = MagicMock(spec=BaseAgent)        team.add_agent(agent1)        
team.add_agent(agent2)        team.add_agent(agent3)        
team.add_agent(agent4)        team.add_agent(agent5)        agent1.current_role 
= &amp;quot;Primus&amp;quot;        agent2.current_role = 
&amp;quot;Worker&amp;quot;        agent3.current_role = 
&amp;quot;Supervisor&amp;quot;        agent4.current_role = 
&amp;quot;Designer&amp;quot;        agent5.current_role = 
&amp;quot;Evaluator&amp;quot;&amp;gt;       assert team.get_worker() == agent2E 
AssertionError: assert None == &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5451316160&amp;#x27;&amp;gt;E   
+  where None = _get_worker()E        +    where _get_worker = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144ec6ab0&amp;gt;.get_worker/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/general/test_wsde_team_extended.py:153: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,357 - 
devsynth.domain.models.wsde_core - INFO - Added agent &amp;lt;MagicMock 
name=&amp;#x27;mock.name&amp;#x27; id=&amp;#x27;5451317120&amp;#x27;&amp;gt; to 
team TestTeam2025-10-28 09:27:20,360 - devsynth.domain.models.wsde_core - INFO -
Added agent &amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5451303824&amp;#x27;&amp;gt; to team TestTeam2025-10-28 
09:27:20,360 - devsynth.domain.models.wsde_core - INFO - Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450600608&amp;#x27;&amp;gt; to team TestTeam2025-10-28 
09:27:20,360 - devsynth.domain.models.wsde_core - INFO - Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450608864&amp;#x27;&amp;gt; to team TestTeam2025-10-28 
09:27:20,361 - devsynth.domain.models.wsde_core - INFO - Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450601520&amp;#x27;&amp;gt; to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5451317120&amp;#x27;&amp;gt; to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5451303824&amp;#x27;&amp;gt; to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450600608&amp;#x27;&amp;gt; to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450608864&amp;#x27;&amp;gt; to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
&amp;lt;MagicMock name=&amp;#x27;mock.name&amp;#x27; 
id=&amp;#x27;5450601520&amp;#x27;&amp;gt; to team TestTeam_______________ 
TestWSDETeam.test_peer_based_structure_succeeds ________________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211cba40&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b4af20&amp;gt;    def test_peer_based_structure_succeeds(self, 
mock_agent_with_expertise):        &amp;quot;&amp;quot;&amp;quot;Test that all 
agents are treated as peers with no permanent hierarchy.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        agent1 = 
mock_agent_with_expertise(&amp;quot;Agent1&amp;quot;, 
[&amp;quot;skill1&amp;quot;])        agent2 = 
mock_agent_with_expertise(&amp;quot;Agent2&amp;quot;, 
[&amp;quot;skill2&amp;quot;])        agent3 = 
mock_agent_with_expertise(&amp;quot;Agent3&amp;quot;, 
[&amp;quot;skill3&amp;quot;])        team.add_agent(agent1)        
team.add_agent(agent2)        team.add_agent(agent3)        assert 
team.get_primus() == agent1        task2 = {&amp;quot;type&amp;quot;: 
&amp;quot;task&amp;quot;, &amp;quot;requires&amp;quot;: 
&amp;quot;skill2&amp;quot;}        
team.select_primus_by_expertise(task2)&amp;gt;       assert team.get_primus() ==
agent2E       AssertionError: assert &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5448109232&amp;#x27;&amp;gt; == 
&amp;lt;MagicMock spec=&amp;#x27;BaseAgent&amp;#x27; 
id=&amp;#x27;5448121904&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5448109232&amp;#x27;&amp;gt; = 
get_primus()E        +    where get_primus = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144bb8b60&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/general/test_wsde_team_extended.py:207: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,405 - 
devsynth.domain.models.wsde_core - INFO - Added agent Agent1 to team 
TestTeam2025-10-28 09:27:20,405 - devsynth.domain.models.wsde_core - INFO - 
Added agent Agent2 to team TestTeam2025-10-28 09:27:20,405 - 
devsynth.domain.models.wsde_core - INFO - Added agent Agent3 to team 
TestTeam2025-10-28 09:27:20,405 - devsynth.domain.models.wsde_roles - INFO - 
Selected Agent1 as primus based on expertise------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Agent1 to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent Agent2 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Agent3 to team
TestTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
Agent1 as primus based on expertise__________ 
TestWSDETeam.test_consensus_based_decision_making_succeeds __________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e0440&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b2ba60&amp;gt;    def test_consensus_based_decision_making_succeeds(self, 
mock_agent_with_expertise):        &amp;quot;&amp;quot;&amp;quot;Test 
facilitating consensus building among agents.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        agent1 = 
mock_agent_with_expertise(&amp;quot;Agent1&amp;quot;, 
[&amp;quot;skill1&amp;quot;])        agent2 = 
mock_agent_with_expertise(&amp;quot;Agent2&amp;quot;, 
[&amp;quot;skill2&amp;quot;])        agent3 = 
mock_agent_with_expertise(&amp;quot;Agent3&amp;quot;, 
[&amp;quot;skill3&amp;quot;])        team.add_agent(agent1)        
team.add_agent(agent2)        team.add_agent(agent3)        task = 
{&amp;quot;type&amp;quot;: &amp;quot;decision_task&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;A task requiring consensus&amp;quot;}
solution1 = {&amp;quot;agent&amp;quot;: &amp;quot;Agent1&amp;quot;, 
&amp;quot;content&amp;quot;: &amp;quot;Solution from Agent1&amp;quot;}        
solution2 = {&amp;quot;agent&amp;quot;: &amp;quot;Agent2&amp;quot;, 
&amp;quot;content&amp;quot;: &amp;quot;Solution from Agent2&amp;quot;}        
solution3 = {&amp;quot;agent&amp;quot;: &amp;quot;Agent3&amp;quot;, 
&amp;quot;content&amp;quot;: &amp;quot;Solution from Agent3&amp;quot;}        
team.add_solution(task, solution1)        team.add_solution(task, solution2)    
team.add_solution(task, solution3)        consensus = 
team.build_consensus(task)&amp;gt;       assert &amp;quot;Agent1&amp;quot; in 
consensus[&amp;quot;contributors&amp;quot;]                           
^^^^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;contributors&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/tests/unit/general/test_wsde_team_extended.py:262: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,443 - 
devsynth.domain.models.wsde_core - INFO - Added agent Agent1 to team 
TestTeam2025-10-28 09:27:20,443 - devsynth.domain.models.wsde_core - INFO - 
Added agent Agent2 to team TestTeam2025-10-28 09:27:20,443 - 
devsynth.domain.models.wsde_core - INFO - Added agent Agent3 to team 
TestTeam2025-10-28 09:27:20,443 - devsynth.domain.models.wsde_utils - INFO - 
Added solution for task 83773538-fa6b-4b17-8daf-af4ceb66d71e2025-10-28 
09:27:20,443 - devsynth.domain.models.wsde_utils - INFO - Added solution for 
task 83773538-fa6b-4b17-8daf-af4ceb66d71e2025-10-28 09:27:20,443 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
83773538-fa6b-4b17-8daf-af4ceb66d71e2025-10-28 09:27:20,443 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot build consensus: no 
options provided------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Agent1 to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent Agent2 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Agent3 to team
TestTeamINFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added 
solution for task 83773538-fa6b-4b17-8daf-af4ceb66d71eINFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
83773538-fa6b-4b17-8daf-af4ceb66d71eINFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
83773538-fa6b-4b17-8daf-af4ceb66d71eWARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build consensus: 
no options provided____________ 
TestWSDETeam.test_dialectical_review_process_succeeds _____________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e0920&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b2a700&amp;gt;    def test_dialectical_review_process_succeeds(self, 
mock_agent_with_expertise):        &amp;quot;&amp;quot;&amp;quot;Test the 
dialectical review process with thesis, antithesis, and synthesis.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        code_agent = 
mock_agent_with_expertise(&amp;quot;CodeAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])        test_agent = 
mock_agent_with_expertise(&amp;quot;TestAgent&amp;quot;, 
[&amp;quot;testing&amp;quot;, &amp;quot;quality&amp;quot;])        critic_agent 
= mock_agent_with_expertise(            &amp;quot;CriticAgent&amp;quot;, 
[&amp;quot;dialectical_reasoning&amp;quot;, &amp;quot;critique&amp;quot;]       
)        team.add_agent(code_agent)        team.add_agent(test_agent)        
team.add_agent(critic_agent)        task = {            
&amp;quot;type&amp;quot;: &amp;quot;implementation_task&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Implement a user authentication 
system&amp;quot;,        }        thesis = {            
&amp;quot;agent&amp;quot;: &amp;quot;CodeAgent&amp;quot;,            
&amp;quot;content&amp;quot;: &amp;quot;Implement authentication using a simple 
username/password check&amp;quot;,            &amp;quot;code&amp;quot;: 
&amp;quot;&amp;quot;&amp;quot;def authenticate(username, password):    return 
username == &amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;&amp;quot;&amp;quot;&amp;quot;,        }        
team.add_solution(task, thesis)        dialectical_result = 
team.apply_dialectical_reasoning(task, critic_agent)        assert 
&amp;quot;thesis&amp;quot; in dialectical_result        assert 
&amp;quot;antithesis&amp;quot; in dialectical_result        assert 
&amp;quot;synthesis&amp;quot; in dialectical_result&amp;gt;       assert 
dialectical_result[&amp;quot;thesis&amp;quot;][&amp;quot;agent&amp;quot;] == 
&amp;quot;CodeAgent&amp;quot;               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;NoneType&amp;#x27; object is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_wsde_team_extended.py:295: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:20,458 - 
devsynth.domain.models.wsde_core - INFO - Added agent CodeAgent to team 
TestTeam2025-10-28 09:27:20,458 - devsynth.domain.models.wsde_core - INFO - 
Added agent TestAgent to team TestTeam2025-10-28 09:27:20,458 - 
devsynth.domain.models.wsde_core - INFO - Added agent CriticAgent to team 
TestTeam2025-10-28 09:27:20,458 - devsynth.domain.models.wsde_utils - INFO - 
Added solution for task 980c2b76-76a2-41e1-bad7-2d324a9c2ec02025-10-28 
09:27:20,458 - devsynth.domain.models.wsde_core - WARNING - Cannot apply 
dialectical reasoning: no solution provided------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent CodeAgent to 
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent TestAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent CriticAgent to
team TestTeamINFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 
Added solution for task 980c2b76-76a2-41e1-bad7-2d324a9c2ec0WARNING  
devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply dialectical 
reasoning: no solution provided_______ 
TestWSDETeam.test_peer_review_with_acceptance_criteria_succeeds ________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e0e00&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b29ee0&amp;gt;    def test_peer_review_with_acceptance_criteria_succeeds(  
self, mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test 
the peer review process with specific acceptance criteria.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        author_agent = 
mock_agent_with_expertise(&amp;quot;AuthorAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])        reviewer1 = 
mock_agent_with_expertise(&amp;quot;ReviewerAgent1&amp;quot;, 
[&amp;quot;testing&amp;quot;, &amp;quot;quality&amp;quot;])        reviewer2 = 
mock_agent_with_expertise(            &amp;quot;ReviewerAgent2&amp;quot;, 
[&amp;quot;security&amp;quot;, &amp;quot;best_practices&amp;quot;]        )     
team.add_agent(author_agent)        team.add_agent(reviewer1)        
team.add_agent(reviewer2)        work_product = {            
&amp;quot;code&amp;quot;: &amp;quot;&amp;quot;&amp;quot;def 
authenticate(username, password):    return username == 
&amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;&amp;quot;&amp;quot;&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Simple authentication 
function&amp;quot;,        }        acceptance_criteria = [            
&amp;quot;Code follows security best practices&amp;quot;,            
&amp;quot;Function handles edge cases&amp;quot;,            &amp;quot;Code is 
well-documented&amp;quot;,        ]        review = team.request_peer_review(   
work_product=work_product,            author=author_agent,            
reviewer_agents=,        )&amp;gt;       review.acceptance_criteria = 
acceptance_criteria        ^^^^^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: 
property &amp;#x27;acceptance_criteria&amp;#x27; of 
&amp;#x27;PeerReview&amp;#x27; object has no 
setter/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/te
st_wsde_team_extended.py:330: AttributeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:20,478 - 
devsynth.domain.models.wsde_core - INFO - Added agent AuthorAgent to team 
TestTeam2025-10-28 09:27:20,478 - devsynth.domain.models.wsde_core - INFO - 
Added agent ReviewerAgent1 to team TestTeam2025-10-28 09:27:20,478 - 
devsynth.domain.models.wsde_core - INFO - Added agent ReviewerAgent2 to team 
TestTeam2025-10-28 09:27:20,478 - devsynth.domain.models.wsde_roles - INFO - 
Selected AuthorAgent as primus based on expertise2025-10-28 09:27:20,479 - 
devsynth.domain.models.wsde_roles - INFO - Rotated roles for team TestTeam: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: 
None}------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent AuthorAgent to
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent ReviewerAgent1 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent ReviewerAgent2
to team TestTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 
Selected AuthorAgent as primus based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: 
None}__________ TestWSDETeam.test_peer_review_with_revision_cycle_succeeds 
__________self = &amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam
object at 0x1211cad20&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b49c60&amp;gt;    def test_peer_review_with_revision_cycle_succeeds(self, 
mock_agent_with_expertise):        &amp;quot;&amp;quot;&amp;quot;Test the peer 
review process with a revision cycle.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        author_agent = 
mock_agent_with_expertise(&amp;quot;AuthorAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])        reviewer1 = 
mock_agent_with_expertise(&amp;quot;ReviewerAgent1&amp;quot;, 
[&amp;quot;testing&amp;quot;, &amp;quot;quality&amp;quot;])        reviewer2 = 
mock_agent_with_expertise(            &amp;quot;ReviewerAgent2&amp;quot;, 
[&amp;quot;security&amp;quot;, &amp;quot;best_practices&amp;quot;]        )     
team.add_agent(author_agent)        team.add_agent(reviewer1)        
team.add_agent(reviewer2)        work_product = {            
&amp;quot;code&amp;quot;: &amp;quot;&amp;quot;&amp;quot;def 
authenticate(username, password):    return username == 
&amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;&amp;quot;&amp;quot;&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Simple authentication 
function&amp;quot;,        }        review = team.request_peer_review(          
work_product=work_product,            author=author_agent,            
reviewer_agents=,        )        for reviewer in review.reviewers:            
review.reviews = {                &amp;quot;overall_feedback&amp;quot;: 
&amp;quot;The code needs improvement&amp;quot;,                
&amp;quot;suggestions&amp;quot;: [                    &amp;quot;Use a secure 
password hashing algorithm&amp;quot;,                    &amp;quot;Add input 
validation&amp;quot;,                ],                
&amp;quot;approved&amp;quot;: False,            }        
review.collect_reviews()        review.request_revision()        assert 
review.status == &amp;quot;revision_requested&amp;quot;        revised_work = { 
&amp;quot;code&amp;quot;: &amp;quot;&amp;quot;&amp;quot;def 
authenticate(username, password):    # Validate inputs    if not username or not
password:        return False        # In a real system, this would use a secure
password hashing algorithm    # and compare against stored hashed passwords    
import hashlib    hashed_password = 
hashlib.sha256(password.encode()).hexdigest()    return username == 
&amp;#x27;admin&amp;#x27; and hashed_password == 
&amp;#x27;5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8&amp;#
x27;    &amp;quot;&amp;quot;&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Improved authentication function with
input validation and hashing&amp;quot;,        }        new_review = 
review.submit_revision(revised_work)        assert new_review.previous_review ==
review        assert new_review.work_product == revised_work        for reviewer
in new_review.reviewers:            new_review.reviews = {                
&amp;quot;overall_feedback&amp;quot;: &amp;quot;The code is now 
acceptable&amp;quot;,                &amp;quot;suggestions&amp;quot;: [],       
&amp;quot;approved&amp;quot;: True,            }        
new_review.collect_reviews()        new_review.quality_score = 0.9        
final_result = new_review.finalize(approved=True)        assert 
final_result[&amp;quot;status&amp;quot;] == &amp;quot;approved&amp;quot;&amp;gt;
assert final_result[&amp;quot;previous_review_id&amp;quot;] == review.review_id 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;previous_review_id&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/unit/general/test_wsde_team_extended.py:424: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,499 - 
devsynth.domain.models.wsde_core - INFO - Added agent AuthorAgent to team 
TestTeam2025-10-28 09:27:20,499 - devsynth.domain.models.wsde_core - INFO - 
Added agent ReviewerAgent1 to team TestTeam2025-10-28 09:27:20,500 - 
devsynth.domain.models.wsde_core - INFO - Added agent ReviewerAgent2 to team 
TestTeam2025-10-28 09:27:20,500 - devsynth.domain.models.wsde_roles - INFO - 
Selected AuthorAgent as primus based on expertise2025-10-28 09:27:20,501 - 
devsynth.domain.models.wsde_roles - INFO - Rotated roles for team TestTeam: 
{&amp;#x27;primus&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: 
None}2025-10-28 09:27:20,503 - devsynth.domain.models.wsde_roles - INFO - 
Rotated roles for team TestTeam: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;ReviewerAgent1&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;ReviewerAgent2&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: 
&amp;#x27;AuthorAgent&amp;#x27;, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}2025-10-28 09:27:20,503 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
4ed35e0c-5d58-4e2e-8138-e620d7c4da452025-10-28 09:27:20,503 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
4ed35e0c-5d58-4e2e-8138-e620d7c4da452025-10-28 09:27:20,503 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot build consensus: no 
options provided2025-10-28 09:27:20,504 - devsynth.domain.models.wsde_roles - 
INFO - Rotated roles for team TestTeam: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;AuthorAgent&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;ReviewerAgent1&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: 
&amp;#x27;ReviewerAgent2&amp;#x27;, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}2025-10-28 09:27:20,505 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
bf81d0a8-aad0-4233-9ce8-4f259d5fa8202025-10-28 09:27:20,505 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
bf81d0a8-aad0-4233-9ce8-4f259d5fa8202025-10-28 09:27:20,506 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot build consensus: no 
options provided2025-10-28 09:27:20,506 - 
devsynth.application.collaboration.peer_review - ERROR - Error in finalize: 
&amp;#x27;dict&amp;#x27; object has no attribute 
&amp;#x27;reviewer&amp;#x27;------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent AuthorAgent to
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent ReviewerAgent1 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent ReviewerAgent2
to team TestTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 
Selected AuthorAgent as primus based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: None}INFO    
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: None}INFO    
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
4ed35e0c-5d58-4e2e-8138-e620d7c4da45INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
4ed35e0c-5d58-4e2e-8138-e620d7c4da45WARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build consensus: 
no options providedINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;ReviewerAgent1&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: &amp;#x27;ReviewerAgent2&amp;#x27;, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: None}INFO    
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
bf81d0a8-aad0-4233-9ce8-4f259d5fa820INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
bf81d0a8-aad0-4233-9ce8-4f259d5fa820WARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build consensus: 
no options providedERROR    
devsynth.application.collaboration.peer_review:logging_setup.py:615 Error in 
finalize: &amp;#x27;dict&amp;#x27; object has no attribute 
&amp;#x27;reviewer&amp;#x27;_______ 
TestWSDETeam.test_peer_review_with_dialectical_analysis_succeeds _______self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e0410&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b2aac0&amp;gt;        def 
test_peer_review_with_dialectical_analysis_succeeds(            self, 
mock_agent_with_expertise        ):            
&amp;quot;&amp;quot;&amp;quot;Test the peer review process with dialectical 
analysis.                ReqID: N/A&amp;quot;&amp;quot;&amp;quot;            
team = WSDETeam(name=&amp;quot;TestTeam&amp;quot;)            author_agent = 
mock_agent_with_expertise(&amp;quot;AuthorAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])            
critic_agent = mock_agent_with_expertise(                
&amp;quot;CriticAgent&amp;quot;, [&amp;quot;dialectical_reasoning&amp;quot;, 
&amp;quot;critique&amp;quot;]            )            
team.add_agent(author_agent)            team.add_agent(critic_agent)            
work_product = {                &amp;quot;code&amp;quot;: 
&amp;quot;&amp;quot;&amp;quot;def authenticate(username, password):        
return username == &amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;&amp;quot;&amp;quot;&amp;quot;,                
&amp;quot;description&amp;quot;: &amp;quot;Simple authentication 
function&amp;quot;,            }            review = team.request_peer_review(  
work_product=work_product,                author=author_agent,                
reviewer_agents=,            )            dialectical_analysis = {              
&amp;quot;thesis&amp;quot;: {                    &amp;quot;strengths&amp;quot;: 
[                        &amp;quot;Simple and easy to understand&amp;quot;,     
&amp;quot;Functional for basic use cases&amp;quot;,                    ],       
&amp;quot;key_points&amp;quot;: [&amp;quot;Direct string comparison for 
authentication&amp;quot;],                },                
&amp;quot;antithesis&amp;quot;: {                    
&amp;quot;weaknesses&amp;quot;: [                        &amp;quot;Security 
vulnerability: Hardcoded credentials&amp;quot;,                        
&amp;quot;No input validation&amp;quot;,                        &amp;quot;No 
error handling&amp;quot;,                        &amp;quot;No password 
hashing&amp;quot;,                    ],                    
&amp;quot;challenges&amp;quot;: [                        &amp;quot;Insecure for 
production use&amp;quot;,                        &amp;quot;Vulnerable to timing 
attacks&amp;quot;,                    ],                },                
&amp;quot;synthesis&amp;quot;: {                    
&amp;quot;improvements&amp;quot;: [                        &amp;quot;Use secure 
password hashing&amp;quot;,                        &amp;quot;Add input 
validation&amp;quot;,                        &amp;quot;Implement proper error 
handling&amp;quot;,                        &amp;quot;Use environment variables 
or a secure configuration for credentials&amp;quot;,                    ],      
&amp;quot;improved_solution&amp;quot;: &amp;quot;&amp;quot;&amp;quot;def 
authenticate(username, password):        # Validate inputs        if not 
username or not password:            return False            try:            # 
In a real system, this would use a secure password hashing algorithm            
# and compare against stored hashed passwords            import hashlib         
import hmac            import os                # Use constant-time comparison 
to prevent timing attacks            stored_hash = 
hashlib.sha256(os.environ.get(&amp;#x27;ADMIN_PASSWORD&amp;#x27;, 
&amp;#x27;&amp;#x27;).encode()).hexdigest()            user_hash = 
hashlib.sha256(password.encode()).hexdigest()                return username == 
os.environ.get(&amp;#x27;ADMIN_USERNAME&amp;#x27;, &amp;#x27;&amp;#x27;) and 
hmac.compare_digest(stored_hash, user_hash)        except Exception as e:       
logger.error(f&amp;quot;Authentication error: {e}&amp;quot;)            return 
False    &amp;quot;&amp;quot;&amp;quot;,                },            }         
review.reviews = {                &amp;quot;overall_feedback&amp;quot;: 
&amp;quot;The code needs significant improvement for security&amp;quot;,        
&amp;quot;dialectical_analysis&amp;quot;: dialectical_analysis,                
&amp;quot;approved&amp;quot;: False,            }            
review.collect_reviews()&amp;gt;           feedback = 
review.aggregate_feedback()                       
^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/general/test_wsde_team_extended.py:505: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:945: in aggregate_feedback    record = 
self._build_peer_review_record(decisions)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/rave
noak/devsynth/src/devsynth/application/collaboration/peer_review.py:537: in 
_build_peer_review_record    reviewer_names = tuple(_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .0 = &amp;lt;tuple_iterator 
object at 0x144e4add0&amp;gt;    reviewer_names = tuple(&amp;gt;       
decision.reviewer or &amp;quot;unknown&amp;quot; for decision in decision_list  
^^^^^^^^^^^^^^^^^    )E   AttributeError: &amp;#x27;dict&amp;#x27; object has no
attribute 
&amp;#x27;reviewer&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/src/devsynth/application/collaboration/peer_review.py:538: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,531 - 
devsynth.domain.models.wsde_core - INFO - Added agent AuthorAgent to team 
TestTeam2025-10-28 09:27:20,532 - devsynth.domain.models.wsde_core - INFO - 
Added agent CriticAgent to team TestTeam2025-10-28 09:27:20,532 - 
devsynth.domain.models.wsde_roles - INFO - Selected AuthorAgent as primus based 
on expertise2025-10-28 09:27:20,532 - devsynth.domain.models.wsde_roles - INFO -
Rotated roles for team TestTeam: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;CriticAgent&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;AuthorAgent&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: None, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: 
None}2025-10-28 09:27:20,532 - devsynth.domain.models.wsde_roles - INFO - 
Rotated roles for team TestTeam: {&amp;#x27;primus&amp;#x27;: 
&amp;#x27;AuthorAgent&amp;#x27;, &amp;#x27;worker&amp;#x27;: 
&amp;#x27;CriticAgent&amp;#x27;, &amp;#x27;supervisor&amp;#x27;: None, 
&amp;#x27;designer&amp;#x27;: None, &amp;#x27;evaluator&amp;#x27;: 
None}2025-10-28 09:27:20,532 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 5f6c2980-4245-4357-b4f6-1a4cbc117a4f2025-10-28 09:27:20,532 - 
devsynth.domain.models.wsde_core - WARNING - Cannot apply dialectical reasoning:
no solution provided2025-10-28 09:27:20,533 - 
devsynth.application.collaboration.peer_review - WARNING - Error applying 
dialectical reasoning: &amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;get&amp;#x27;2025-10-28 09:27:20,533 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
d202c166-7684-4a62-9106-88bf98f6276b2025-10-28 09:27:20,533 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot build consensus: no 
options provided------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent AuthorAgent to
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent CriticAgent to team TestTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected AuthorAgent as 
primus based on expertiseINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;CriticAgent&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}INFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles for team 
TestTeam: {&amp;#x27;primus&amp;#x27;: &amp;#x27;AuthorAgent&amp;#x27;, 
&amp;#x27;worker&amp;#x27;: &amp;#x27;CriticAgent&amp;#x27;, 
&amp;#x27;supervisor&amp;#x27;: None, &amp;#x27;designer&amp;#x27;: None, 
&amp;#x27;evaluator&amp;#x27;: None}INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
5f6c2980-4245-4357-b4f6-1a4cbc117a4fWARNING  
devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply dialectical 
reasoning: no solution providedWARNING  
devsynth.application.collaboration.peer_review:logging_setup.py:615 Error 
applying dialectical reasoning: &amp;#x27;NoneType&amp;#x27; object has no 
attribute &amp;#x27;get&amp;#x27;INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
d202c166-7684-4a62-9106-88bf98f6276bWARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build consensus: 
no options provided_____________ 
TestWSDETeam.test_contextdriven_leadership_succeeds ______________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e15e0&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b2bba0&amp;gt;    def test_contextdriven_leadership_succeeds(self, 
mock_agent_with_expertise):        &amp;quot;&amp;quot;&amp;quot;Test 
context-driven leadership in the WSDE team.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        python_agent = 
mock_agent_with_expertise(&amp;quot;PythonAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;backend&amp;quot;])        js_agent = 
mock_agent_with_expertise(&amp;quot;JSAgent&amp;quot;, 
[&amp;quot;javascript&amp;quot;, &amp;quot;frontend&amp;quot;])        
security_agent = mock_agent_with_expertise(            
&amp;quot;SecurityAgent&amp;quot;, [&amp;quot;security&amp;quot;, 
&amp;quot;authentication&amp;quot;]        )        design_agent = 
mock_agent_with_expertise(&amp;quot;DesignAgent&amp;quot;, 
[&amp;quot;design&amp;quot;, &amp;quot;ui&amp;quot;, &amp;quot;ux&amp;quot;])   
doc_agent = mock_agent_with_expertise(            &amp;quot;DocAgent&amp;quot;, 
[&amp;quot;documentation&amp;quot;, &amp;quot;technical_writing&amp;quot;]      
)        team.add_agent(python_agent)        team.add_agent(js_agent)        
team.add_agent(security_agent)        team.add_agent(design_agent)        
team.add_agent(doc_agent)        doc_task = {            
&amp;quot;type&amp;quot;: &amp;quot;documentation_task&amp;quot;,            
&amp;quot;description&amp;quot;: &amp;quot;Write API documentation&amp;quot;,   
&amp;quot;domain&amp;quot;: &amp;quot;documentation&amp;quot;,            
&amp;quot;requirements&amp;quot;: [&amp;quot;Clear examples&amp;quot;, 
&amp;quot;Complete coverage&amp;quot;],        }        
team.select_primus_by_expertise(doc_task)        assert team.get_primus() == 
doc_agent        assert doc_agent.current_role == &amp;quot;Primus&amp;quot;    
roles = &amp;gt;       assert &amp;quot;Worker&amp;quot; in rolesE       
AssertionError: assert &amp;#x27;Worker&amp;#x27; in [None, None, None, None, 
&amp;#x27;Primus&amp;#x27;]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/general/test_wsde_team_extended.py:547: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,598 - 
devsynth.domain.models.wsde_core - INFO - Added agent PythonAgent to team 
TestTeam2025-10-28 09:27:20,598 - devsynth.domain.models.wsde_core - INFO - 
Added agent JSAgent to team TestTeam2025-10-28 09:27:20,598 - 
devsynth.domain.models.wsde_core - INFO - Added agent SecurityAgent to team 
TestTeam2025-10-28 09:27:20,598 - devsynth.domain.models.wsde_core - INFO - 
Added agent DesignAgent to team TestTeam2025-10-28 09:27:20,598 - 
devsynth.domain.models.wsde_core - INFO - Added agent DocAgent to team 
TestTeam2025-10-28 09:27:20,598 - devsynth.domain.models.wsde_roles - INFO - 
Selected DocAgent as primus based on expertise------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent PythonAgent to
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent JSAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent SecurityAgent 
to team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent DesignAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent DocAgent to 
team TestTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 
Selected DocAgent as primus based on expertise___ 
TestWSDETeam.test_dialectical_reasoning_with_external_knowledge_succeeds ___self
= &amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e1ac0&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b7cc20&amp;gt;        def 
test_dialectical_reasoning_with_external_knowledge_succeeds(            self, 
mock_agent_with_expertise        ):            
&amp;quot;&amp;quot;&amp;quot;Test the dialectical reasoning process with 
external knowledge integration.                ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;            team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)            code_agent = 
mock_agent_with_expertise(&amp;quot;CodeAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])            
security_agent = mock_agent_with_expertise(                
&amp;quot;SecurityAgent&amp;quot;, [&amp;quot;security&amp;quot;, 
&amp;quot;authentication&amp;quot;]            )            critic_agent = 
mock_agent_with_expertise(                &amp;quot;CriticAgent&amp;quot;, 
[&amp;quot;dialectical_reasoning&amp;quot;, &amp;quot;critique&amp;quot;]       
)            task = {                &amp;quot;type&amp;quot;: 
&amp;quot;implementation_task&amp;quot;,                
&amp;quot;description&amp;quot;: &amp;quot;Implement a secure user 
authentication system with multi-factor authentication&amp;quot;,            }  
thesis = {                &amp;quot;agent&amp;quot;: 
&amp;quot;CodeAgent&amp;quot;,                &amp;quot;content&amp;quot;: 
&amp;quot;Implement authentication using username/password with JWT 
tokens&amp;quot;,                &amp;quot;code&amp;quot;: 
&amp;quot;&amp;quot;&amp;quot;    def authenticate(username, password):        
if username == &amp;#x27;admin&amp;#x27; and password == 
&amp;#x27;password&amp;#x27;:            token = generate_jwt_token(username)   
return token        return None        def generate_jwt_token(username):        
# Generate a JWT token        return &amp;quot;jwt_token_placeholder&amp;quot;  
&amp;quot;&amp;quot;&amp;quot;,            }            team.add_solution(task, 
thesis)            external_knowledge = {                
&amp;quot;security_best_practices&amp;quot;: {                    
&amp;quot;authentication&amp;quot;: [                        &amp;quot;Use 
multi-factor authentication for sensitive operations&amp;quot;,                 
&amp;quot;Store passwords using strong, adaptive hashing algorithms (e.g., 
bcrypt, Argon2)&amp;quot;,                        &amp;quot;Implement rate 
limiting to prevent brute force attacks&amp;quot;,                        
&amp;quot;Use HTTPS for all authentication requests&amp;quot;,                  
&amp;quot;Set secure and HttpOnly flags on authentication cookies&amp;quot;,    
],                    &amp;quot;data_protection&amp;quot;: [                    
&amp;quot;Encrypt sensitive data at rest and in transit&amp;quot;,              
&amp;quot;Implement proper access controls&amp;quot;,                        
&amp;quot;Follow the principle of least privilege&amp;quot;,                    
&amp;quot;Regularly audit access to sensitive data&amp;quot;,                   
&amp;quot;Have a data breach response plan&amp;quot;,                    ],     
},                &amp;quot;industry_standards&amp;quot;: {                    
&amp;quot;OWASP&amp;quot;: [                        &amp;quot;OWASP Top 10 Web 
Application Security Risks&amp;quot;,                        &amp;quot;OWASP 
Application Security Verification Standard (ASVS)&amp;quot;,                    
&amp;quot;OWASP Secure Coding Practices&amp;quot;,                    ],        
&amp;quot;ISO&amp;quot;: [                        &amp;quot;ISO/IEC 27001 - 
Information security management&amp;quot;,                        
&amp;quot;ISO/IEC 27002 - Code of practice for information security 
controls&amp;quot;,                    ],                    
&amp;quot;NIST&amp;quot;: [                        &amp;quot;NIST Special 
Publication 800-53 - Security and Privacy Controls&amp;quot;,                   
&amp;quot;NIST Cybersecurity Framework&amp;quot;,                    ],         
},                &amp;quot;compliance_requirements&amp;quot;: {                
&amp;quot;GDPR&amp;quot;: [                        &amp;quot;Obtain explicit 
consent for data collection&amp;quot;,                        &amp;quot;Provide 
mechanisms for users to access, modify, and delete their data&amp;quot;,        
&amp;quot;Report data breaches within 72 hours&amp;quot;,                       
&amp;quot;Conduct Data Protection Impact Assessments (DPIA)&amp;quot;,          
],                    &amp;quot;HIPAA&amp;quot;: [                        
&amp;quot;Implement technical safeguards for PHI&amp;quot;,                     
&amp;quot;Conduct regular risk assessments&amp;quot;,                        
&amp;quot;Maintain audit trails of PHI access&amp;quot;,                        
&amp;quot;Have Business Associate Agreements (BAA) in place&amp;quot;,          
],                    &amp;quot;PCI-DSS&amp;quot;: [                        
&amp;quot;Maintain a secure network and systems&amp;quot;,                      
&amp;quot;Protect cardholder data&amp;quot;,                        
&amp;quot;Implement strong access control measures&amp;quot;,                   
&amp;quot;Regularly test security systems and processes&amp;quot;,              
],                },            }&amp;gt;           dialectical_result = 
team.apply_enhanced_dialectical_reasoning_with_knowledge(                       
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                task, 
critic_agent, external_knowledge            )E           AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;apply_enhanced_dialectical_reasoning_with_knowledge&amp;#x27;/Users/ca
itlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsde_team_ex
tended.py:639: AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,619 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
8d5d87ae-0201-4bd7-9dc8-91f37b861f5c------------------------------ Captured log 
call -------------------------------INFO     
devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution for task 
8d5d87ae-0201-4bd7-9dc8-91f37b861f5c_____ 
TestWSDETeam.test_multi_disciplinary_dialectical_reasoning_succeeds ______self =
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e1fa0&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b4a480&amp;gt;        def 
test_multi_disciplinary_dialectical_reasoning_succeeds(            self, 
mock_agent_with_expertise        ):            
&amp;quot;&amp;quot;&amp;quot;Test the dialectical reasoning process with 
multiple disciplinary perspectives.                ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;            team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)            code_agent = 
mock_agent_with_expertise(&amp;quot;CodeAgent&amp;quot;, 
[&amp;quot;python&amp;quot;, &amp;quot;coding&amp;quot;])            
security_agent = mock_agent_with_expertise(                
&amp;quot;SecurityAgent&amp;quot;, [&amp;quot;security&amp;quot;, 
&amp;quot;authentication&amp;quot;]            )            ux_agent = 
mock_agent_with_expertise(                &amp;quot;UXAgent&amp;quot;, 
[&amp;quot;user_experience&amp;quot;, &amp;quot;interface_design&amp;quot;]     
)            performance_agent = mock_agent_with_expertise(                
&amp;quot;PerformanceAgent&amp;quot;, [&amp;quot;performance&amp;quot;, 
&amp;quot;optimization&amp;quot;]            )            accessibility_agent = 
mock_agent_with_expertise(                
&amp;quot;AccessibilityAgent&amp;quot;, [&amp;quot;accessibility&amp;quot;, 
&amp;quot;inclusive_design&amp;quot;]            )            critic_agent = 
mock_agent_with_expertise(                &amp;quot;CriticAgent&amp;quot;, 
[&amp;quot;dialectical_reasoning&amp;quot;, &amp;quot;critique&amp;quot;, 
&amp;quot;synthesis&amp;quot;]            )            
team.add_agent(code_agent)            team.add_agent(security_agent)            
team.add_agent(ux_agent)            team.add_agent(performance_agent)           
team.add_agent(accessibility_agent)            team.add_agent(critic_agent)     
task = {                &amp;quot;type&amp;quot;: 
&amp;quot;implementation_task&amp;quot;,                
&amp;quot;description&amp;quot;: &amp;quot;Implement a user authentication 
system with a focus on security, usability, performance, and 
accessibility&amp;quot;,            }            thesis = {                
&amp;quot;agent&amp;quot;: &amp;quot;CodeAgent&amp;quot;,                
&amp;quot;content&amp;quot;: &amp;quot;Implement authentication using 
username/password with JWT tokens&amp;quot;,                
&amp;quot;code&amp;quot;: &amp;quot;&amp;quot;&amp;quot;    def 
authenticate(username, password):        if username == 
&amp;#x27;admin&amp;#x27; and password == &amp;#x27;password&amp;#x27;:         
token = generate_jwt_token(username)            return token        return None 
def generate_jwt_token(username):        # Generate a JWT token        return 
&amp;quot;jwt_token_placeholder&amp;quot;                
&amp;quot;&amp;quot;&amp;quot;,            }            team.add_solution(task, 
thesis)            disciplinary_knowledge = {                
&amp;quot;security&amp;quot;: {                    
&amp;quot;authentication_best_practices&amp;quot;: [                        
&amp;quot;Use multi-factor authentication for sensitive operations&amp;quot;,   
&amp;quot;Store passwords using strong, adaptive hashing algorithms (e.g., 
bcrypt, Argon2)&amp;quot;,                        &amp;quot;Implement rate 
limiting to prevent brute force attacks&amp;quot;,                        
&amp;quot;Use HTTPS for all authentication requests&amp;quot;,                  
&amp;quot;Set secure and HttpOnly flags on authentication cookies&amp;quot;,    
]                },                &amp;quot;user_experience&amp;quot;: {       
&amp;quot;authentication_ux_principles&amp;quot;: [                        
&amp;quot;Minimize friction in the authentication process&amp;quot;,            
&amp;quot;Provide clear error messages for failed authentication 
attempts&amp;quot;,                        &amp;quot;Offer password recovery 
options&amp;quot;,                        &amp;quot;Remember user preferences 
where appropriate&amp;quot;,                        &amp;quot;Support single 
sign-on where possible&amp;quot;,                    ]                },        
&amp;quot;performance&amp;quot;: {                    
&amp;quot;authentication_performance_considerations&amp;quot;: [                
&amp;quot;Optimize token validation for minimal latency&amp;quot;,              
&amp;quot;Cache frequently used authentication data&amp;quot;,                  
&amp;quot;Use asynchronous processing for non-critical authentication 
tasks&amp;quot;,                        &amp;quot;Implement efficient database 
queries for user lookup&amp;quot;,                        &amp;quot;Monitor and 
optimize authentication service response times&amp;quot;,                    ]  
},                &amp;quot;accessibility&amp;quot;: {                    
&amp;quot;authentication_accessibility_guidelines&amp;quot;: [                  
&amp;quot;Ensure all authentication forms are keyboard navigable&amp;quot;,     
&amp;quot;Provide appropriate ARIA labels for authentication form 
elements&amp;quot;,                        &amp;quot;Support screen readers for 
error messages and instructions&amp;quot;,                        
&amp;quot;Maintain sufficient color contrast for text and interactive 
elements&amp;quot;,                        &amp;quot;Allow authentication 
timeout extensions for users who need more time&amp;quot;,                    ] 
},            }            dialectical_result = 
team.apply_multi_disciplinary_dialectical_reasoning(                task,       
critic_agent,                disciplinary_knowledge,                ,           
)&amp;gt;           assert &amp;quot;thesis&amp;quot; in dialectical_resultE    
AssertionError: assert &amp;#x27;thesis&amp;#x27; in 
{&amp;#x27;reason&amp;#x27;: &amp;#x27;no_solution&amp;#x27;, 
&amp;#x27;status&amp;#x27;: 
&amp;#x27;failed&amp;#x27;}/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/general/test_wsde_team_extended.py:751: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,641 - 
devsynth.domain.models.wsde_core - INFO - Added agent CodeAgent to team 
TestTeam2025-10-28 09:27:20,641 - devsynth.domain.models.wsde_core - INFO - 
Added agent SecurityAgent to team TestTeam2025-10-28 09:27:20,641 - 
devsynth.domain.models.wsde_core - INFO - Added agent UXAgent to team 
TestTeam2025-10-28 09:27:20,641 - devsynth.domain.models.wsde_core - INFO - 
Added agent PerformanceAgent to team TestTeam2025-10-28 09:27:20,641 - 
devsynth.domain.models.wsde_core - INFO - Added agent AccessibilityAgent to team
TestTeam2025-10-28 09:27:20,641 - devsynth.domain.models.wsde_core - INFO - 
Added agent CriticAgent to team TestTeam2025-10-28 09:27:20,641 - 
devsynth.domain.models.wsde_utils - INFO - Added solution for task 
0d43d2ed-3902-4ad6-ad6a-e8d7be4a9ae12025-10-28 09:27:20,642 - 
devsynth.domain.models.wsde_multidisciplinary - WARNING - Cannot apply 
multi-disciplinary dialectical reasoning: no solution 
provided------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent CodeAgent to 
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent SecurityAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent UXAgent to 
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent PerformanceAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
AccessibilityAgent to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent CriticAgent to
team TestTeamINFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 
Added solution for task 0d43d2ed-3902-4ad6-ad6a-e8d7be4a9ae1WARNING  
devsynth.domain.models.wsde_multidisciplinary:logging_setup.py:615 Cannot apply 
multi-disciplinary dialectical reasoning: no solution provided____ 
TestWSDETeam.test_assign_roles_for_phase_varied_contexts_has_expected _____self 
= &amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e2480&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b4bc40&amp;gt;    def 
test_assign_roles_for_phase_varied_contexts_has_expected(        self, 
mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test that 
different phases can have different primus agents.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        expand_agent = 
mock_agent_with_expertise(            &amp;quot;Expand&amp;quot;, 
[&amp;quot;brainstorming&amp;quot;, &amp;quot;exploration&amp;quot;, 
&amp;quot;creativity&amp;quot;]        )        diff_agent = 
mock_agent_with_expertise(            &amp;quot;Diff&amp;quot;, 
[&amp;quot;comparison&amp;quot;, &amp;quot;analysis&amp;quot;, 
&amp;quot;evaluation&amp;quot;]        )        refine_agent = 
mock_agent_with_expertise(            &amp;quot;Refine&amp;quot;, 
[&amp;quot;implementation&amp;quot;, &amp;quot;coding&amp;quot;, 
&amp;quot;development&amp;quot;]        )        doc_agent = 
mock_agent_with_expertise(            &amp;quot;Doc&amp;quot;, 
[&amp;quot;documentation&amp;quot;, &amp;quot;reflection&amp;quot;, 
&amp;quot;learning&amp;quot;]        )        team.add_agents()        
team.assign_roles_for_phase(Phase.EXPAND, {&amp;quot;description&amp;quot;: 
&amp;quot;demo&amp;quot;})        team.primus_index = 
team.agents.index(expand_agent)&amp;gt;       
team.role_assignments[&amp;quot;primus&amp;quot;] = expand_agent        
^^^^^^^^^^^^^^^^^^^^^E       AttributeError: &amp;#x27;WSDETeam&amp;#x27; object
has no attribute &amp;#x27;role_assignments&amp;#x27;. Did you mean: 
&amp;#x27;get_role_assignments&amp;#x27;?/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/general/test_wsde_team_extended.py:800: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,673 - 
devsynth.domain.models.wsde_core - INFO - Added agent Expand to team 
TestTeam2025-10-28 09:27:20,673 - devsynth.domain.models.wsde_core - INFO - 
Added agent Diff to team TestTeam2025-10-28 09:27:20,673 - 
devsynth.domain.models.wsde_core - INFO - Added agent Refine to team 
TestTeam2025-10-28 09:27:20,673 - devsynth.domain.models.wsde_core - INFO - 
Added agent Doc to team TestTeam2025-10-28 09:27:20,673 - 
devsynth.domain.models.wsde_roles - INFO - Assigning roles for phase EXPAND in 
team TestTeam2025-10-28 09:27:20,674 - devsynth.domain.models.wsde_roles - INFO 
- Selected Expand as primus based on expertise------------------------------ 
Captured log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Expand to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent Diff to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Refine to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent Doc to team TestTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles for phase
EXPAND in team TestTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Expand as primus
based on expertise______ 
TestWSDETeam.test_vote_on_critical_decision_majority_path_succeeds ______self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e2960&amp;gt;mock_agent = &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5446478992&amp;#x27;&amp;gt;    
def test_vote_on_critical_decision_majority_path_succeeds(self, mock_agent):    
&amp;quot;&amp;quot;&amp;quot;Test that vote on critical decision majority path 
succeeds.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        a1 = mock_agent        
a1.name = &amp;quot;a1&amp;quot;        a2 = MagicMock(spec=BaseAgent)        
a2.name = &amp;quot;a2&amp;quot;        a3 = MagicMock(spec=BaseAgent)        
a3.name = &amp;quot;a3&amp;quot;        for a in :            a.process = 
MagicMock()            team.add_agent(a)        a1.process.return_value = 
{&amp;quot;vote&amp;quot;: &amp;quot;o1&amp;quot;}        
a2.process.return_value = {&amp;quot;vote&amp;quot;: &amp;quot;o1&amp;quot;}    
a3.process.return_value = {&amp;quot;vote&amp;quot;: &amp;quot;o2&amp;quot;}    
task = {            &amp;quot;type&amp;quot;: 
&amp;quot;critical_decision&amp;quot;,            
&amp;quot;is_critical&amp;quot;: True,            &amp;quot;options&amp;quot;: 
[{&amp;quot;id&amp;quot;: &amp;quot;o1&amp;quot;}, {&amp;quot;id&amp;quot;: 
&amp;quot;o2&amp;quot;}],        }        result = 
team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;result&amp;quot;][&amp;quot;winner&amp;quot;] == 
&amp;quot;o1&amp;quot;E       AssertionError: assert &amp;#x27;o2&amp;#x27; == 
&amp;#x27;o1&amp;#x27;E         E         - o1E         + 
o2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_w
sde_team_extended.py:838: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:27:20,703 - 
devsynth.domain.models.wsde_core - INFO - Added agent a1 to team 
TestTeam2025-10-28 09:27:20,703 - devsynth.domain.models.wsde_core - INFO - 
Added agent a2 to team TestTeam2025-10-28 09:27:20,703 - 
devsynth.domain.models.wsde_core - INFO - Added agent a3 to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to team 
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent a2 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to team 
TestTeam______ 
TestWSDETeam.test_vote_on_critical_decision_weighted_path_succeeds ______self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e2e40&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b7c900&amp;gt;    def 
test_vote_on_critical_decision_weighted_path_succeeds(        self, 
mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test that 
vote on critical decision weighted path succeeds.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        expert = 
mock_agent_with_expertise(&amp;quot;expert&amp;quot;, 
[&amp;quot;security&amp;quot;])        expert.config = MagicMock()        
expert.config.name = &amp;quot;expert&amp;quot;        expert.config.parameters 
= {            &amp;quot;expertise&amp;quot;: [&amp;quot;security&amp;quot;],   
&amp;quot;expertise_level&amp;quot;: &amp;quot;expert&amp;quot;,        }       
intermediate = mock_agent_with_expertise(&amp;quot;inter&amp;quot;, 
[&amp;quot;security&amp;quot;])        intermediate.config = MagicMock()        
intermediate.config.name = &amp;quot;inter&amp;quot;        
intermediate.config.parameters = {            &amp;quot;expertise&amp;quot;: 
[&amp;quot;security&amp;quot;],            &amp;quot;expertise_level&amp;quot;: 
&amp;quot;intermediate&amp;quot;,        }        novice = 
mock_agent_with_expertise(&amp;quot;novice&amp;quot;, 
[&amp;quot;python&amp;quot;])        novice.config = MagicMock()        
novice.config.name = &amp;quot;novice&amp;quot;        novice.config.parameters 
= {            &amp;quot;expertise&amp;quot;: [&amp;quot;python&amp;quot;],     
&amp;quot;expertise_level&amp;quot;: &amp;quot;novice&amp;quot;,        }       
for a in :            a.process = MagicMock()            team.add_agent(a)      
expert.process.return_value = {&amp;quot;vote&amp;quot;: &amp;quot;b&amp;quot;} 
intermediate.process.return_value = {&amp;quot;vote&amp;quot;: 
&amp;quot;a&amp;quot;}        novice.process.return_value = 
{&amp;quot;vote&amp;quot;: &amp;quot;a&amp;quot;}        task = {            
&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;,            
&amp;quot;domain&amp;quot;: &amp;quot;security&amp;quot;,            
&amp;quot;is_critical&amp;quot;: True,            &amp;quot;options&amp;quot;: 
[{&amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;}, {&amp;quot;id&amp;quot;: 
&amp;quot;b&amp;quot;}],        }        result = 
team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;result&amp;quot;][&amp;quot;method&amp;quot;] == 
&amp;quot;weighted_vote&amp;quot;E       AssertionError: assert 
&amp;#x27;majority_vote&amp;#x27; == &amp;#x27;weighted_vote&amp;#x27;E         
E         - weighted_voteE         + 
majority_vote/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_wsde_team_extended.py:882: AssertionError-----------------------------
Captured stdout call -----------------------------2025-10-28 09:27:20,731 - 
devsynth.domain.models.wsde_core - INFO - Added agent expert to team 
TestTeam2025-10-28 09:27:20,731 - devsynth.domain.models.wsde_core - INFO - 
Added agent inter to team TestTeam2025-10-28 09:27:20,732 - 
devsynth.domain.models.wsde_core - INFO - Added agent novice to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent expert to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent inter to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent novice to team
TestTeam_ 
TestWSDETeam.test_documentation_task_selects_doc_agent_and_updates_role_assignme
nts_succeeds _self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e3320&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b7c400&amp;gt;    def 
test_documentation_task_selects_doc_agent_and_updates_role_assignments_succeeds(
self, mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test 
that documentation task selects doc agent and updates role assignments succeeds.
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        coder = 
mock_agent_with_expertise(&amp;quot;Coder&amp;quot;, 
[&amp;quot;python&amp;quot;])        coder.has_been_primus = True        
doc_agent = mock_agent_with_expertise(&amp;quot;Doc&amp;quot;, 
[&amp;quot;documentation&amp;quot;, &amp;quot;markdown&amp;quot;])        
doc_agent.has_been_primus = False        team.add_agents()        task = 
{&amp;quot;type&amp;quot;: &amp;quot;documentation&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;Write docs&amp;quot;}        
team.select_primus_by_expertise(task)        assert team.get_primus() is 
doc_agent&amp;gt;       assert team.role_assignments[&amp;quot;primus&amp;quot;]
is doc_agent               ^^^^^^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;WSDETeam&amp;#x27; object has no attribute 
&amp;#x27;role_assignments&amp;#x27;. Did you mean: 
&amp;#x27;get_role_assignments&amp;#x27;?/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/general/test_wsde_team_extended.py:901: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,753 - 
devsynth.domain.models.wsde_core - INFO - Added agent Coder to team 
TestTeam2025-10-28 09:27:20,753 - devsynth.domain.models.wsde_core - INFO - 
Added agent Doc to team TestTeam2025-10-28 09:27:20,754 - 
devsynth.domain.models.wsde_roles - INFO - Selected Doc as primus based on 
expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder to team 
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent Doc to team TestTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Doc as primus 
based on expertise______ 
TestWSDETeam.test_select_primus_fallback_when_no_expertise_matches ______self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e3800&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b7fa60&amp;gt;    def 
test_select_primus_fallback_when_no_expertise_matches(        self, 
mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test that 
select primus fallback when no expertise matches.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        a1 = 
mock_agent_with_expertise(&amp;quot;A1&amp;quot;, [&amp;quot;python&amp;quot;]) 
a2 = mock_agent_with_expertise(&amp;quot;A2&amp;quot;, 
[&amp;quot;javascript&amp;quot;])        a3 = 
mock_agent_with_expertise(&amp;quot;A3&amp;quot;, [&amp;quot;design&amp;quot;]) 
for a in :            a.has_been_primus = True            team.add_agent(a)     
task = {&amp;quot;type&amp;quot;: &amp;quot;unknown&amp;quot;, 
&amp;quot;topic&amp;quot;: &amp;quot;nothing&amp;quot;}        
team.select_primus_by_expertise(task)        assert team.get_primus() is 
a1&amp;gt;       assert team.role_assignments[&amp;quot;primus&amp;quot;] is a1 
^^^^^^^^^^^^^^^^^^^^^E       AttributeError: &amp;#x27;WSDETeam&amp;#x27; object
has no attribute &amp;#x27;role_assignments&amp;#x27;. Did you mean: 
&amp;#x27;get_role_assignments&amp;#x27;?/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/general/test_wsde_team_extended.py:920: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,789 - 
devsynth.domain.models.wsde_core - INFO - Added agent A1 to team 
TestTeam2025-10-28 09:27:20,789 - devsynth.domain.models.wsde_core - INFO - 
Added agent A2 to team TestTeam2025-10-28 09:27:20,789 - 
devsynth.domain.models.wsde_core - INFO - Added agent A3 to team 
TestTeam2025-10-28 09:27:20,789 - devsynth.domain.models.wsde_roles - INFO - 
Selected A1 as primus based on expertise------------------------------ Captured 
log call -------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A1 to team 
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent A2 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A3 to team 
TestTeamINFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected
A1 as primus based on expertise________ 
TestWSDETeam.test_documentation_expert_becomes_primus_succeeds ________self = 
&amp;lt;tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x1211e3ce0&amp;gt;mock_agent_with_expertise = &amp;lt;function 
TestWSDETeam.mock_agent_with_expertise.&amp;lt;locals&amp;gt;._create_agent at 
0x144b7d440&amp;gt;    def test_documentation_expert_becomes_primus_succeeds(   
self, mock_agent_with_expertise    ):        &amp;quot;&amp;quot;&amp;quot;Test 
that documentation expert becomes primus succeeds.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        generalist = 
mock_agent_with_expertise(&amp;quot;Generalist&amp;quot;, 
[&amp;quot;python&amp;quot;])        doc_agent = 
mock_agent_with_expertise(&amp;quot;Doc&amp;quot;, 
[&amp;quot;documentation&amp;quot;])        team.add_agents()        task = 
{&amp;quot;type&amp;quot;: &amp;quot;documentation&amp;quot;, 
&amp;quot;description&amp;quot;: &amp;quot;Write docs&amp;quot;}        
team.select_primus_by_expertise(task)&amp;gt;       assert team.get_primus() is 
doc_agentE       AssertionError: assert &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5448293920&amp;#x27;&amp;gt; is 
&amp;lt;MagicMock spec=&amp;#x27;BaseAgent&amp;#x27; 
id=&amp;#x27;5448292192&amp;#x27;&amp;gt;E        +  where &amp;lt;MagicMock 
spec=&amp;#x27;BaseAgent&amp;#x27; id=&amp;#x27;5448293920&amp;#x27;&amp;gt; = 
get_primus()E        +    where get_primus = 
&amp;lt;devsynth.domain.models.wsde_facade.WSDETeam object at 
0x144be6c00&amp;gt;.get_primus/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/general/test_wsde_team_extended.py:934: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,830 - 
devsynth.domain.models.wsde_core - INFO - Added agent Generalist to team 
TestTeam2025-10-28 09:27:20,830 - devsynth.domain.models.wsde_core - INFO - 
Added agent Doc to team TestTeam2025-10-28 09:27:20,830 - 
devsynth.domain.models.wsde_roles - INFO - Selected Generalist as primus based 
on expertise------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Generalist to 
team TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 
Added agent Doc to team TestTeamINFO     
devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Generalist as 
primus based on expertise___________ 
test_vote_on_critical_decision_not_critical_raises_error ___________    def 
test_vote_on_critical_decision_not_critical_raises_error():        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision returns an 
error when task is not critical.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        result = 
team.vote_on_critical_decision({&amp;quot;type&amp;quot;: 
&amp;quot;other&amp;quot;})&amp;gt;       assert 
result[&amp;quot;voting_initiated&amp;quot;] is False               
^^^^^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;voting_initiated&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/general/test_wsde_team_voting_invalid.py:14: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,862 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot conduct vote: no agents in
team------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct vote: no 
agents in team____________ 
test_vote_on_critical_decision_no_options_raises_error ____________    def 
test_vote_on_critical_decision_no_options_raises_error():        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision returns an 
error when no options are provided.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        task = 
{&amp;quot;type&amp;quot;: &amp;quot;critical_decision&amp;quot;, 
&amp;quot;is_critical&amp;quot;: True, &amp;quot;options&amp;quot;: []}        
result = team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;voting_initiated&amp;quot;] is False               
^^^^^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;voting_initiated&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/general/test_wsde_team_voting_invalid.py:25: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,874 - 
devsynth.domain.models.wsde_voting - WARNING - Cannot conduct vote: no agents in
team------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct vote: no 
agents in team____________ test_majority_vote_with_three_unique_choices_succeeds
_____________    def test_majority_vote_with_three_unique_choices_succeeds():   
&amp;quot;&amp;quot;&amp;quot;Test that when three agents vote for three 
different options, it results in a tie.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        a1 = 
_make_agent(&amp;quot;a1&amp;quot;, &amp;quot;option1&amp;quot;)        a2 = 
_make_agent(&amp;quot;a2&amp;quot;, &amp;quot;option2&amp;quot;)        a3 = 
_make_agent(&amp;quot;a3&amp;quot;, &amp;quot;option3&amp;quot;)        
team.add_agents()        task = _basic_task()        with patch.object(team, 
&amp;quot;build_consensus&amp;quot;, 
return_value={&amp;quot;consensus&amp;quot;: &amp;quot;&amp;quot;}):            
result = team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;result&amp;quot;][&amp;quot;method&amp;quot;] == 
&amp;quot;tied_vote&amp;quot;E       AssertionError: assert 
&amp;#x27;majority_vote&amp;#x27; == &amp;#x27;tied_vote&amp;#x27;E         E   
- tied_voteE         + 
majority_vote/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_wsde_voting.py:47: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:20,883 - 
devsynth.domain.models.wsde_core - INFO - Added agent a1 to team 
TestTeam2025-10-28 09:27:20,884 - devsynth.domain.models.wsde_core - INFO - 
Added agent a2 to team TestTeam2025-10-28 09:27:20,884 - 
devsynth.domain.models.wsde_core - INFO - Added agent a3 to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to team 
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent a2 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to team 
TestTeam_________________ test_tie_triggers_handle_tied_vote_succeeds 
__________________self = &amp;lt;MagicMock 
name=&amp;#x27;_handle_tied_vote&amp;#x27; 
id=&amp;#x27;5450264176&amp;#x27;&amp;gt;    def assert_called_once(self):      
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called only once.        
&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:            
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to have been called once. 
Called %s times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;_handle_tied_vote&amp;#x27; to have been 
called once. Called 0 
times./opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versi
ons/3.12/lib/python3.12/unittest/mock.py:928: AssertionErrorDuring handling of 
the above exception, another exception occurred:    def 
test_tie_triggers_handle_tied_vote_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that a tie between two options triggers the 
_handle_tied_vote method.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;   
team = WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        a1 = 
_make_agent(&amp;quot;a1&amp;quot;, &amp;quot;option1&amp;quot;)        a2 = 
_make_agent(&amp;quot;a2&amp;quot;, &amp;quot;option2&amp;quot;)        
team.add_agents()        task = _basic_task()        with (            
patch.object(team, &amp;quot;build_consensus&amp;quot;, 
return_value={&amp;quot;consensus&amp;quot;: &amp;quot;&amp;quot;}),            
patch.object(team, &amp;quot;_handle_tied_vote&amp;quot;, 
wraps=team._handle_tied_vote) as mocked,        ):            
team.vote_on_critical_decision(task)&amp;gt;           
mocked.assert_called_once()E           AssertionError: Expected 
&amp;#x27;_handle_tied_vote&amp;#x27; to have been called once. Called 0 
times./Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/te
st_wsde_voting.py:65: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:27:20,899 - 
devsynth.domain.models.wsde_core - INFO - Added agent a1 to team 
TestTeam2025-10-28 09:27:20,899 - devsynth.domain.models.wsde_core - INFO - 
Added agent a2 to team TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to team 
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent a2 to team TestTeam______________ 
test_weighted_voting_prefers_expert_vote_succeeds _______________    def 
test_weighted_voting_prefers_expert_vote_succeeds():        
&amp;quot;&amp;quot;&amp;quot;Test that weighted voting gives preference to 
expert votes over novice votes.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        team = 
WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        expert = 
_make_agent(&amp;quot;expert&amp;quot;, &amp;quot;option2&amp;quot;, 
[&amp;quot;security&amp;quot;], level=&amp;quot;expert&amp;quot;)        novice1
= _make_agent(&amp;quot;novice1&amp;quot;, &amp;quot;option1&amp;quot;, 
[&amp;quot;security&amp;quot;], level=&amp;quot;novice&amp;quot;)        novice2
= _make_agent(&amp;quot;novice2&amp;quot;, &amp;quot;option1&amp;quot;, 
[&amp;quot;security&amp;quot;], level=&amp;quot;novice&amp;quot;)        
team.add_agents()        task = _basic_task()        
task[&amp;quot;domain&amp;quot;] = &amp;quot;security&amp;quot;        result = 
team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;result&amp;quot;][&amp;quot;method&amp;quot;] == 
&amp;quot;weighted_vote&amp;quot;E       AssertionError: assert 
&amp;#x27;majority_vote&amp;#x27; == &amp;#x27;weighted_vote&amp;#x27;E         
E         - weighted_voteE         + 
majority_vote/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/gen
eral/test_wsde_voting.py:80: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:20,956 - 
devsynth.domain.models.wsde_core - INFO - Added agent expert to team 
TestTeam2025-10-28 09:27:20,956 - devsynth.domain.models.wsde_core - INFO - 
Added agent novice1 to team TestTeam2025-10-28 09:27:20,957 - 
devsynth.domain.models.wsde_core - INFO - Added agent novice2 to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent expert to team
TestTeamINFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added 
agent novice1 to team TestTeamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent novice2 to 
team TestTeam_______________ test_vote_on_critical_decision_no_votes_succeeds 
_______________    def test_vote_on_critical_decision_no_votes_succeeds():      
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision handles the 
case when no votes are cast.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;
team = WSDETeam(name=&amp;quot;TestTeam&amp;quot;)        a1 = 
_make_agent(&amp;quot;a1&amp;quot;, vote=None)        a1.process.return_value = 
{}        team.add_agent(a1)        task = _basic_task()        result = 
team.vote_on_critical_decision(task)&amp;gt;       assert 
result[&amp;quot;voting_initiated&amp;quot;] is True               
^^^^^^^^^^^^^^^^^^^^^^^^^^E       KeyError: 
&amp;#x27;voting_initiated&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/general/test_wsde_voting.py:94: 
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:20,975 - 
devsynth.domain.models.wsde_core - INFO - Added agent a1 to team 
TestTeam------------------------------ Captured log call 
-------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to team 
TestTeam_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_initiates_voting_succeed
s _self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211f9f10&amp;gt;    def 
test_vote_on_critical_decision_initiates_voting_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision initiates a 
voting process.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
self.agent1.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent2.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.agent3.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option3&amp;quot;})        self.agent4.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option1&amp;quot;}) 
result = self.team.vote_on_critical_decision(self.critical_task)&amp;gt;       
assert &amp;quot;voting_initiated&amp;quot; in resultE       AssertionError: 
assert &amp;#x27;voting_initiated&amp;#x27; in {&amp;#x27;explanation&amp;#x27;:
&amp;#x27;Vote tied; initiated consensus process.&amp;#x27;, 
&amp;#x27;id&amp;#x27;: 
&amp;#x27;04d2f101-cea6-41c2-b256-6969433fdaca&amp;#x27;, 
&amp;#x27;method&amp;#x27;: &amp;#x27;majority&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;option1&amp;#x27;, 
&amp;#x27;option2&amp;#x27;, &amp;#x27;option3&amp;#x27;], 
...}/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_wsde_voting_mechanisms.py:104: AssertionError---------------------------- 
Captured stdout setup -----------------------------2025-10-28 09:27:20,991 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:20,991 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:20,992 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:20,992 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_majority_vote_succeeds 
_self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211fa360&amp;gt;    def 
test_vote_on_critical_decision_majority_vote_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision uses majority 
vote to make decisions.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;     
self.agent1.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent2.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.agent3.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent4.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option3&amp;quot;}) 
result = self.team.vote_on_critical_decision(self.critical_task)        assert 
&amp;quot;result&amp;quot; in result        assert 
result[&amp;quot;result&amp;quot;] is not None        assert 
&amp;quot;winner&amp;quot; in result[&amp;quot;result&amp;quot;]&amp;gt;       
assert result[&amp;quot;result&amp;quot;][&amp;quot;winner&amp;quot;] == 
&amp;quot;option1&amp;quot;E       AssertionError: assert 
&amp;#x27;option3&amp;#x27; == &amp;#x27;option1&amp;#x27;E         E         - 
option1E         ?       ^E         + option3E         ?       
^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ws
de_voting_mechanisms.py:129: AssertionError---------------------------- Captured
stdout setup -----------------------------2025-10-28 09:27:21,009 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,010 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,010 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,010 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team__ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_tied_vote_succeeds 
__self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211fa840&amp;gt;    def 
test_vote_on_critical_decision_tied_vote_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision handles tied 
votes correctly.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
self.agent1.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent2.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.agent3.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent4.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.team.build_consensus = MagicMock(            return_value={                
&amp;quot;consensus&amp;quot;: &amp;quot;Use a hybrid architecture combining 
microservices and monolith&amp;quot;,                
&amp;quot;contributors&amp;quot;: [&amp;quot;agent1&amp;quot;, 
&amp;quot;agent2&amp;quot;, &amp;quot;agent3&amp;quot;, 
&amp;quot;agent4&amp;quot;],                &amp;quot;method&amp;quot;: 
&amp;quot;consensus_synthesis&amp;quot;,                
&amp;quot;reasoning&amp;quot;: &amp;quot;Combined the best elements from both 
options&amp;quot;,            }        )        result = 
self.team.vote_on_critical_decision(self.critical_task)        assert 
&amp;quot;result&amp;quot; in result&amp;gt;       assert 
&amp;quot;tied&amp;quot; in result[&amp;quot;result&amp;quot;]E       
AssertionError: assert &amp;#x27;tied&amp;#x27; in {&amp;#x27;method&amp;#x27;: 
&amp;#x27;majority_vote&amp;#x27;, &amp;#x27;winner&amp;#x27;: 
&amp;#x27;option2&amp;#x27;}/Users/caitlyn/Projects/github.com/ravenoak/devsynth
/tests/unit/general/test_wsde_voting_mechanisms.py:155: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:21,033 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,034 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,034 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,035 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_weighted_vote_succeeds 
_self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211fad20&amp;gt;    def 
test_vote_on_critical_decision_weighted_vote_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision uses weighted 
voting for domain-specific decisions.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        self.agent1.config.parameters = {      
&amp;quot;expertise&amp;quot;: [&amp;quot;security&amp;quot;, 
&amp;quot;encryption&amp;quot;, &amp;quot;authentication&amp;quot;],            
&amp;quot;expertise_level&amp;quot;: &amp;quot;expert&amp;quot;,        }       
self.agent2.config.parameters = {            &amp;quot;expertise&amp;quot;: 
[&amp;quot;security&amp;quot;, &amp;quot;firewalls&amp;quot;],            
&amp;quot;expertise_level&amp;quot;: &amp;quot;intermediate&amp;quot;,        } 
self.agent3.config.parameters = {            &amp;quot;expertise&amp;quot;: 
[&amp;quot;security&amp;quot;],            &amp;quot;expertise_level&amp;quot;: 
&amp;quot;novice&amp;quot;,        }        self.agent4.config.parameters = {   
&amp;quot;expertise&amp;quot;: [&amp;quot;python&amp;quot;, 
&amp;quot;javascript&amp;quot;],            &amp;quot;expertise_level&amp;quot;:
&amp;quot;intermediate&amp;quot;,        }        self.agent1.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.agent2.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent3.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option1&amp;quot;}) 
self.agent4.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option3&amp;quot;})        result = 
self.team.vote_on_critical_decision(self.domain_task)        assert 
&amp;quot;result&amp;quot; in result        assert 
result[&amp;quot;result&amp;quot;] is not None        if 
&amp;quot;winner&amp;quot; in result[&amp;quot;result&amp;quot;]:            
assert result[&amp;quot;result&amp;quot;][&amp;quot;winner&amp;quot;] == 
&amp;quot;option2&amp;quot;        elif &amp;quot;tied&amp;quot; in 
result[&amp;quot;result&amp;quot;] and 
result[&amp;quot;result&amp;quot;][&amp;quot;tied&amp;quot;]:            assert 
&amp;quot;tie_breaking_attempts&amp;quot; in result[&amp;quot;result&amp;quot;] 
assert 
len(result[&amp;quot;result&amp;quot;][&amp;quot;tie_breaking_attempts&amp;quot;
]) &amp;gt; 0            assert 
result[&amp;quot;result&amp;quot;][&amp;quot;tie_breaking_attempts&amp;quot;][0]
[&amp;quot;winner&amp;quot;] == &amp;quot;option2&amp;quot;        assert 
&amp;quot;votes&amp;quot; in result&amp;gt;       assert 
result[&amp;quot;votes&amp;quot;][&amp;quot;agent1&amp;quot;] == 
&amp;quot;option2&amp;quot;E       AssertionError: assert 
&amp;#x27;option3&amp;#x27; == &amp;#x27;option2&amp;#x27;E         E         - 
option2E         ?       ^E         + option3E         ?       
^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ws
de_voting_mechanisms.py:208: AssertionError---------------------------- Captured
stdout setup -----------------------------2025-10-28 09:27:21,056 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,056 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,057 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,057 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_records_results_succeeds
_self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211fb200&amp;gt;    def 
test_vote_on_critical_decision_records_results_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Test that vote_on_critical_decision records the 
voting results.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
self.agent1.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent2.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option2&amp;quot;}) 
self.agent3.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        self.agent4.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option1&amp;quot;}) 
result = self.team.vote_on_critical_decision(self.critical_task)&amp;gt;       
assert &amp;quot;voting_initiated&amp;quot; in resultE       assert 
&amp;#x27;voting_initiated&amp;#x27; in {&amp;#x27;explanation&amp;#x27;: 
&amp;quot;Option &amp;#x27;option1&amp;#x27; received 3 votes out of 4 
(75.0%).&amp;quot;, &amp;#x27;id&amp;#x27;: 
&amp;#x27;187c7854-c1d2-4be3-a73b-c40f03cf4043&amp;#x27;, 
&amp;#x27;method&amp;#x27;: &amp;#x27;majority&amp;#x27;, 
&amp;#x27;options&amp;#x27;: [&amp;#x27;option1&amp;#x27;, 
&amp;#x27;option2&amp;#x27;, &amp;#x27;option3&amp;#x27;], 
...}/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test
_wsde_voting_mechanisms.py:222: AssertionError---------------------------- 
Captured stdout setup -----------------------------2025-10-28 09:27:21,079 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,079 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,080 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,080 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_updates_history_succeeds
_self = 
&amp;lt;tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x1211fb6e0&amp;gt;    def 
test_vote_on_critical_decision_updates_history_succeeds(self):        
&amp;quot;&amp;quot;&amp;quot;Ensure voting history is recorded after a vote.   
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        self.agent1.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option1&amp;quot;}) 
self.agent2.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option2&amp;quot;})        self.agent3.process = 
MagicMock(return_value={&amp;quot;vote&amp;quot;: &amp;quot;option1&amp;quot;}) 
self.agent4.process = MagicMock(return_value={&amp;quot;vote&amp;quot;: 
&amp;quot;option1&amp;quot;})        result = 
self.team.vote_on_critical_decision(self.critical_task)        assert 
len(self.team.voting_history) == 1        entry = 
self.team.voting_history[0]&amp;gt;       assert 
entry[&amp;quot;task_id&amp;quot;] == self.team._get_task_id(self.critical_task)
^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: &amp;#x27;WSDETeam&amp;#x27; 
object has no attribute 
&amp;#x27;_get_task_id&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/tests/unit/general/test_wsde_voting_mechanisms.py:245: 
AttributeError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:21,101 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent1 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,101 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent2 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,102 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent3 to team 
test_voting_mechanisms_team2025-10-28 09:27:21,103 - 
devsynth.domain.models.wsde_core - INFO - Added agent agent4 to team 
test_voting_mechanisms_team------------------------------ Captured log setup 
------------------------------INFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent1 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent2 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent3 to team
test_voting_mechanisms_teamINFO     
devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent4 to team
test_voting_mechanisms_team_______________ 
test_enhanced_init_endpoint_returns_typed_error ________________enhanced_api = 
&amp;lt;module &amp;#x27;devsynth.interface.agentapi_enhanced&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/agentapi_enhanced.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_enhanced_init_endpoint_re0&amp;#x27;)    
@pytest.mark.fast    def 
test_enhanced_init_endpoint_returns_typed_error(enhanced_api, tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Initialization failures surface typed error 
payloads.&amp;quot;,            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
request = 
SimpleNamespace(client=SimpleNamespace(host=&amp;quot;9.9.9.9&amp;quot;))       
init_request = InitRequest(path=str(tmp_path / 
&amp;quot;does-not-exist&amp;quot;))    &amp;gt;       with 
pytest.raises(enhanced_api.HTTPException) as exc:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       Failed: DID NOT RAISE 
&amp;lt;class 
&amp;#x27;fastapi.exceptions.HTTPException&amp;#x27;&amp;gt;/Users/caitlyn/Proje
cts/github.com/ravenoak/devsynth/tests/unit/interface/test_api_endpoints.py:451:
 Failed__________________________ test_print_alias_delegates 
__________________________    def test_print_alias_delegates():        bridge = 
DummyBridge()&amp;gt;       bridge.print(&amp;quot;msg&amp;quot;, 
highlight=True)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/i
nterface/test_uxbridge_aliases.py:39: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;tests.unit.interface.test_uxbridge_aliases.DummyBridge object at 
0x14464c470&amp;gt;message = &amp;#x27;msg&amp;#x27;    def print(        self, 
message: str,        *,        highlight: bool = False,        message_type: str
| None = None,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Backward compatible alias for 
:meth:`display_result`.            Args:            message: Message to display 
to the user.            highlight: Whether to emphasise the message.            
message_type: Optional semantic type forwarded to                
:meth:`display_result`.        &amp;quot;&amp;quot;&amp;quot;&amp;gt;       
self.display_result(message, highlight=highlight, message_type=message_type)E   
TypeError: DummyBridge.display_result() got an unexpected keyword argument 
&amp;#x27;message_type&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devs
ynth/src/devsynth/interface/ux_bridge.py:420: TypeError___________________ 
test_lazy_streamlit_forwards_attributes ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14464c440&amp;gt;    def 
test_lazy_streamlit_forwards_attributes(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;`_LazyStreamlit` proxies 
attribute lookups through `_require_streamlit`.&amp;quot;&amp;quot;&amp;quot;   
calls: list[tuple] = []            class SentinelStreamlit:            def 
header(self, text: str) -&amp;gt; str:                
calls.append((&amp;quot;header&amp;quot;, text))                return 
f&amp;quot;header::{text}&amp;quot;            sentinel = SentinelStreamlit()   
def fake_require_streamlit() -&amp;gt; SentinelStreamlit:            
calls.append((&amp;quot;require&amp;quot;, None))            return sentinel    
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None, 
raising=False)&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_require_streamlit&amp;quot;, fake_require_streamlit)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_require_streamlit&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:317: 
AttributeError__________________ test_require_streamlit_guidance_and_cache 
___________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x14464e9c0&amp;gt;    def 
test_require_streamlit_guidance_and_cache(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Lazy loader emits install 
guidance once and caches the module.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None, raising=False) 
sentinel = SimpleNamespace(name=&amp;quot;streamlit-sentinel&amp;quot;)        
import_attempts: list = []            def import_once(name: str) -&amp;gt; 
SimpleNamespace:            import_attempts.append(name)            return 
sentinel    &amp;gt;       monkeypatch.setattr(webui.importlib, 
&amp;quot;import_module&amp;quot;, import_once)                            
^^^^^^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;importlib&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/interface/test_webui_behavior_checklist_fast.py:344: 
AttributeError____________ 
test_ask_question_and_confirm_choice_respects_defaults ____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144688500&amp;gt;    def 
test_ask_question_and_confirm_choice_respects_defaults(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Select boxes and checkboxes mirror CLI defaults 
without Streamlit.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:383: TypeError_____________ 
test_display_result_routes_error_and_highlight_paths _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14459de80&amp;gt;    def 
test_display_result_routes_error_and_highlight_paths(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Error prompts, highlights, headers, and markdown 
obey spec routing.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:459: TypeError______________ 
test_display_result_handles_multiple_message_types ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14459c860&amp;gt;    def 
test_display_result_handles_multiple_message_types(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Warnings, successes, info, and unknown types route
to the right channels.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:545: TypeError____________ 
test_display_result_info_and_error_fallbacks_sanitize _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14459f710&amp;gt;    def 
test_display_result_info_and_error_fallbacks_sanitize(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Missing Streamlit channels fall back to ``write`` 
with sanitized payloads.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.delattr(BehaviorStreamlitStub, &amp;quot;info&amp;quot;)        stub
= install_streamlit_stub(monkeypatch)            sanitized_inputs: list = []    
def fake_sanitize(text: str) -&amp;gt; str:            
sanitized_inputs.append(text)            return 
text.replace(&amp;quot;&amp;lt;&amp;quot;, 
&amp;quot;&amp;amp;lt;&amp;quot;).replace(&amp;quot;&amp;gt;&amp;quot;, 
&amp;quot;&amp;amp;gt;&amp;quot;)    &amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;sanitize_output&amp;quot;, fake_sanitize)E       AttributeError: 
&amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;sanitize_output&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/d
evsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:590: 
AttributeError________________ test_display_result_markup_fallback_uses_write 
________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14459f890&amp;gt;    def test_display_result_markup_fallback_uses_write(      
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Markdown rendering falls back to ``write`` when 
``markdown`` is absent.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.delattr(BehaviorStreamlitStub, &amp;quot;markdown&amp;quot;)        
stub = install_streamlit_stub(monkeypatch)            sanitized_inputs: list = 
[]            def fake_sanitize(text: str) -&amp;gt; str:            
sanitized_inputs.append(text)            return 
text.replace(&amp;quot;&amp;lt;&amp;quot;, &amp;quot;&amp;amp;lt;&amp;quot;)    
&amp;gt;       monkeypatch.setattr(webui, &amp;quot;sanitize_output&amp;quot;, 
fake_sanitize)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;sanitize_output&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/d
evsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:636: 
AttributeError______________ test_display_result_error_prefix_triggers_guidance 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14459de50&amp;gt;    def test_display_result_error_prefix_triggers_guidance(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Error prefixes emit suggestions, docs, warnings, 
and success markers.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:660: TypeError_______________ 
test_display_result_covers_all_message_channels ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14459dca0&amp;gt;    def 
test_display_result_covers_all_message_channels(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Sanitized conversion and every message channel 
execute as specified.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:707: TypeError____________________ 
test_render_traceback_captures_output _____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14459f350&amp;gt;    def 
test_render_traceback_captures_output(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
None:        &amp;quot;&amp;quot;&amp;quot;Traceback rendering opens an expander
and streams the code block.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()            
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:859: TypeError____________________ 
test_error_mapping_helpers_cover_cases ____________________    def 
test_error_mapping_helpers_cover_cases() -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Error type and helper tables provide consistent 
guidance across keywords.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       ui = 
webui.WebUI()             ^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:881: TypeError___________________ 
test_ui_progress_estimates_and_subtasks ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144546ba0&amp;gt;    def 
test_ui_progress_estimates_and_subtasks(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Progress lifecycle mirrors 
CLI telemetry with sanitized subtasks.&amp;quot;&amp;quot;&amp;quot;            
stub = install_streamlit_stub(monkeypatch)        times = count(start=0, 
step=10)&amp;gt;       monkeypatch.setattr(webui.time, &amp;quot;time&amp;quot;,
lambda: next(times))                            ^^^^^^^^^^E       
AttributeError: module &amp;#x27;devsynth.interface.webui&amp;#x27; has no 
attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_behavior_checklist_fast.py:931: 
AttributeError__________ 
test_ui_progress_complete_cascades_and_falls_back_to_write __________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144689790&amp;gt;    def 
test_ui_progress_complete_cascades_and_falls_back_to_write(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Completing the root task finalizes subtasks and 
falls back when ``success`` is absent.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.delattr(BehaviorStreamlitStub, &amp;quot;success&amp;quot;)        
stub = install_streamlit_stub(monkeypatch)        times = iter(range(0, 
20))&amp;gt;       monkeypatch.setattr(webui.time, &amp;quot;time&amp;quot;, 
lambda: next(times))                            ^^^^^^^^^^E       
AttributeError: module &amp;#x27;devsynth.interface.webui&amp;#x27; has no 
attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_behavior_checklist_fast.py:1037: 
AttributeError______________________ test_ui_progress_eta_formats_hours 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x144a88dd0&amp;gt;    def 
test_ui_progress_eta_formats_hours(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Long running tasks display hour-level
ETAs.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)        times = iter([0, 1000, 2000, 3000, 
4000, 5000])&amp;gt;       monkeypatch.setattr(webui.time, 
&amp;quot;time&amp;quot;, lambda: next(times))                            
^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_behavior_checklist_fast.py:1070: 
AttributeError___________ 
test_ui_progress_status_transitions_cover_all_thresholds ___________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144a34bf0&amp;gt;    def 
test_ui_progress_status_transitions_cover_all_thresholds(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Status messages progress from starting through 
completion with sanitized text.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)        times = iter(range(0, 40))&amp;gt;   
monkeypatch.setattr(webui.time, &amp;quot;time&amp;quot;, lambda: next(times))  
^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_behavior_checklist_fast.py:1092: 
AttributeError_____________________ test_ui_progress_eta_minutes_branch 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x144a34770&amp;gt;    def 
test_ui_progress_eta_minutes_branch(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Slow progress projections report 
minute-level ETAs.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)        times = iter(range(0, 10))&amp;gt;   
monkeypatch.setattr(webui.time, &amp;quot;time&amp;quot;, lambda: next(times))  
^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_behavior_checklist_fast.py:1158: 
AttributeError________________ test_get_layout_config_breakpoints[500-1-True] 
________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a35340&amp;gt;screen_width = 500, expected_columns = 1, expected_mobile = 
True    @pytest.mark.parametrize(        (&amp;quot;screen_width&amp;quot;, 
&amp;quot;expected_columns&amp;quot;, &amp;quot;expected_mobile&amp;quot;),     
[            (500, 1, True),            (800, 2, False),            (1300, 3, 
False),            (&amp;quot;absent&amp;quot;, 3, False),        ],    )    def
test_get_layout_config_breakpoints(        monkeypatch: pytest.MonkeyPatch,     
screen_width: int | str,        expected_columns: int,        expected_mobile: 
bool,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Layout config 
responds to breakpoints and missing measurements.&amp;quot;&amp;quot;&amp;quot; 
stub = install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()     
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:1190: TypeError_______________ 
test_get_layout_config_breakpoints[800-2-False] ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a34aa0&amp;gt;screen_width = 800, expected_columns = 2, expected_mobile = 
False    @pytest.mark.parametrize(        (&amp;quot;screen_width&amp;quot;, 
&amp;quot;expected_columns&amp;quot;, &amp;quot;expected_mobile&amp;quot;),     
[            (500, 1, True),            (800, 2, False),            (1300, 3, 
False),            (&amp;quot;absent&amp;quot;, 3, False),        ],    )    def
test_get_layout_config_breakpoints(        monkeypatch: pytest.MonkeyPatch,     
screen_width: int | str,        expected_columns: int,        expected_mobile: 
bool,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Layout config 
responds to breakpoints and missing measurements.&amp;quot;&amp;quot;&amp;quot; 
stub = install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()     
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:1190: TypeError_______________ 
test_get_layout_config_breakpoints[1300-3-False] _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a347d0&amp;gt;screen_width = 1300, expected_columns = 3, expected_mobile = 
False    @pytest.mark.parametrize(        (&amp;quot;screen_width&amp;quot;, 
&amp;quot;expected_columns&amp;quot;, &amp;quot;expected_mobile&amp;quot;),     
[            (500, 1, True),            (800, 2, False),            (1300, 3, 
False),            (&amp;quot;absent&amp;quot;, 3, False),        ],    )    def
test_get_layout_config_breakpoints(        monkeypatch: pytest.MonkeyPatch,     
screen_width: int | str,        expected_columns: int,        expected_mobile: 
bool,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Layout config 
responds to breakpoints and missing measurements.&amp;quot;&amp;quot;&amp;quot; 
stub = install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()     
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:1190: TypeError______________ 
test_get_layout_config_breakpoints ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144a37830&amp;gt;screen_width = &amp;#x27;absent&amp;#x27;, expected_columns =
3, expected_mobile = False    @pytest.mark.parametrize(        
(&amp;quot;screen_width&amp;quot;, &amp;quot;expected_columns&amp;quot;, 
&amp;quot;expected_mobile&amp;quot;),        [            (500, 1, True),       
(800, 2, False),            (1300, 3, False),            
(&amp;quot;absent&amp;quot;, 3, False),        ],    )    def 
test_get_layout_config_breakpoints(        monkeypatch: pytest.MonkeyPatch,     
screen_width: int | str,        expected_columns: int,        expected_mobile: 
bool,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Layout config 
responds to breakpoints and missing measurements.&amp;quot;&amp;quot;&amp;quot; 
stub = install_streamlit_stub(monkeypatch)&amp;gt;       ui = webui.WebUI()     
^^^^^^^^^^^^^E       TypeError: &amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:1190: TypeError_______________ 
test_run_responsive_layout_and_router_invocation _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144a350a0&amp;gt;    def 
test_run_responsive_layout_and_router_invocation(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`run()` applies defaults, injects resize JS, and 
invokes the router.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)            navigation = 
{&amp;quot;Home&amp;quot;: lambda: None}&amp;gt;       
monkeypatch.setattr(webui.WebUI, &amp;quot;navigation_items&amp;quot;, lambda 
self: navigation)E       AttributeError: None has no attribute 
&amp;#x27;navigation_items&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:1219: 
AttributeError________________________ test_run_handles_html_failure 
_________________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x14450da00&amp;gt;    def test_run_handles_html_failure(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;HTML 
injection failures surface as display_result 
messages.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)        navigation = 
{&amp;quot;Home&amp;quot;: lambda: None}&amp;gt;       
monkeypatch.setattr(webui.WebUI, &amp;quot;navigation_items&amp;quot;, lambda 
self: navigation)E       AttributeError: None has no attribute 
&amp;#x27;navigation_items&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:1282: 
AttributeError______________________ test_run_handles_page_config_error 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x144646240&amp;gt;    def 
test_run_handles_page_config_error(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Configuration errors surface as WebUI
messages without router execution.&amp;quot;&amp;quot;&amp;quot;            stub
= install_streamlit_stub(monkeypatch)        stub.page_config_error = 
RuntimeError(&amp;quot;No display&amp;quot;)        navigation = 
{&amp;quot;Home&amp;quot;: lambda: None}&amp;gt;       
monkeypatch.setattr(webui.WebUI, &amp;quot;navigation_items&amp;quot;, lambda 
self: navigation)E       AttributeError: None has no attribute 
&amp;#x27;navigation_items&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:1332: 
AttributeError__________________ test_run_without_components_invokes_router 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x1444de1e0&amp;gt;    def 
test_run_without_components_invokes_router(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Router still runs when 
components module is absent.&amp;quot;&amp;quot;&amp;quot;            stub = 
install_streamlit_stub(monkeypatch)        stub.components = None        
navigation = {&amp;quot;Docs&amp;quot;: lambda: None}&amp;gt;       
monkeypatch.setattr(webui.WebUI, &amp;quot;navigation_items&amp;quot;, lambda 
self: navigation)E       AttributeError: None has no attribute 
&amp;#x27;navigation_items&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/interface/test_webui_behavior_checklist_fast.py:1382: 
AttributeError__________________ test_ensure_router_caches_router_instance 
___________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x1444f3800&amp;gt;    def test_ensure_router_caches_router_instance(        
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`_ensure_router` only instantiates the router 
once.&amp;quot;&amp;quot;&amp;quot;            
install_streamlit_stub(monkeypatch)        init_calls: list = []            
class RouterSpy:            def __init__(                self, owner: 
webui.WebUI, pages: dict[str, Callable[[], None]]            ) -&amp;gt; None:  
init_calls.append(&amp;quot;init&amp;quot;)                def run(self) 
-&amp;gt; None:  # pragma: no cover - not exercised in this test                
raise AssertionError(&amp;quot;run should not be called&amp;quot;)            
monkeypatch.setattr(webui, &amp;quot;Router&amp;quot;, RouterSpy)&amp;gt;       
ui = webui.WebUI()             ^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_behavior_checklist_fast.py:1425: TypeError_________________ 
test_run_module_entrypoint_invokes_webui_run _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444f0a70&amp;gt;    def 
test_run_module_entrypoint_invokes_webui_run(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;The module-level run() helper instantiates WebUI 
and executes run().&amp;quot;&amp;quot;&amp;quot;            
install_streamlit_stub(monkeypatch)        call_sequence: list = []    &amp;gt; 
class Runner(webui.WebUI):E       TypeError: NoneType takes no 
arguments/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_behavior_checklist_fast.py:1449: TypeError_____________ 
test_webui_run_registers_router_and_hydrates_session _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1444df950&amp;gt;    
@pytest.mark.fast    def test_webui_run_registers_router_and_hydrates_session(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        dummy_streamlit = 
DummyStreamlit()        recorded: dict = {}            class DummyRouter:       
def __init__(                self, ui, pages            ) -&amp;gt; None:  # 
noqa: ANN001 - interface dictated by Router                
recorded[&amp;quot;ui&amp;quot;] = ui                
recorded[&amp;quot;pages&amp;quot;] = dict(pages)                def run(self) 
-&amp;gt; None:                recorded[&amp;quot;ran&amp;quot;] = True    
&amp;gt;       monkeypatch.setattr(webui, &amp;quot;st&amp;quot;, 
dummy_streamlit)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_bootstrap_fast.py:26: AttributeError_______________ 
test_webui_command_dispatch_invokes_cli_targets ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144688ce0&amp;gt;    
@pytest.mark.fast    def test_webui_command_dispatch_invokes_cli_targets(       
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        dummy_streamlit = 
DummyStreamlit()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, dummy_streamlit)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_bootstrap_fast.py:45: AttributeError_______________ 
test_webui_command_dispatch_reports_value_errors _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14464fe60&amp;gt;    
@pytest.mark.fast    def test_webui_command_dispatch_reports_value_errors(      
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        dummy_streamlit = 
DummyStreamlit()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, dummy_streamlit)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;st&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests
/unit/interface/test_webui_bootstrap_fast.py:70: AttributeError__________ 
test_z_progress_indicator_extensive_paths_cover_hierarchy ___________bridge_live
= namespace(module=&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;, streamlit=&amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;)monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446890a0&amp;gt;    def 
test_z_progress_indicator_extensive_paths_cover_hierarchy(        bridge_live: 
SimpleNamespace, monkeypatch: pytest.MonkeyPatch    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;SpecRef: docs/specifications/webui-integration.md 
Progress indicators;        docs/developer_guides/progress_indicators.md 
Testing with Progress Indicators.            Exercises fallback descriptions, 
nested subtasks, and default status        thresholds so the hierarchy mirrors 
the documented UX affordances.        &amp;quot;&amp;quot;&amp;quot;            
bridge = bridge_live.module            class Explodes:            def 
__str__(self) -&amp;gt; str:                raise ValueError(&amp;quot;cannot 
stringify&amp;quot;)            sanitize_inputs: list = []            def 
fake_sanitize(value: str) -&amp;gt; str:            
sanitize_inputs.append(value)            if value == &amp;quot;raise&amp;quot;: 
raise ValueError(&amp;quot;boom&amp;quot;)            return 
f&amp;quot;san::{value}&amp;quot;            monkeypatch.setattr(bridge, 
&amp;quot;sanitize_output&amp;quot;, fake_sanitize)        tick = 
iter(range(1000))        monkeypatch.setattr(bridge.time, 
&amp;quot;time&amp;quot;, lambda: next(tick))            indicator = 
bridge.WebUIProgressIndicator(&amp;quot;start&amp;quot;, 12)            
indicator.update(description=&amp;quot;step-1&amp;quot;, 
status=&amp;quot;ok&amp;quot;, advance=2)        assert indicator._description 
== &amp;quot;san::step-1&amp;quot;        assert indicator._status == 
&amp;quot;san::ok&amp;quot;            indicator.update(description=Explodes(), 
advance=0)        assert indicator._description == 
&amp;quot;san::step-1&amp;quot;            
indicator.update(status=&amp;quot;raise&amp;quot;, advance=0)&amp;gt;       
assert indicator._status == &amp;quot;In progress...&amp;quot;E       
AssertionError: assert &amp;#x27;Starting...&amp;#x27; == &amp;#x27;In 
progress...&amp;#x27;E         E         - In progress...E         + 
Starting.../Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/inter
face/test_webui_bridge_aa_coverage.py:66: AssertionError__________ 
test_z_bridge_accessors_and_wizard_paths_cover_invariants ___________bridge_live
= namespace(module=&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;, streamlit=&amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;)monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144593cb0&amp;gt;caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x142c6b4d0&amp;gt;    def 
test_z_bridge_accessors_and_wizard_paths_cover_invariants(        bridge_live: 
SimpleNamespace,        monkeypatch: pytest.MonkeyPatch,        caplog: 
pytest.LogCaptureFixture,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;SpecRef: 
docs/implementation/requirements_wizard_wizardstate_integration.md Summary;    
docs/implementation/webui_invariants.md Bounded Step Navigation;        
docs/implementation/output_formatter_invariants.md Formatting Rules.           
Wizard navigation, session helpers, and Streamlit display channels follow       
the published invariants and reuse the shared OutputFormatter pipeline.        
&amp;quot;&amp;quot;&amp;quot;            bridge = bridge_live.module        
bridge.st = None        bridge._require_streamlit()        assert bridge.st is 
bridge_live.streamlit            ui = bridge.WebUIBridge()            with 
caplog.at_level(&amp;quot;WARNING&amp;quot;):            assert (               
bridge.WebUIBridge.adjust_wizard_step(&amp;quot;1&amp;quot;, 
direction=&amp;quot;next&amp;quot;, total=2) == 1            )        
caplog.clear()        with caplog.at_level(&amp;quot;WARNING&amp;quot;):        
assert (                bridge.WebUIBridge.adjust_wizard_step(0, 
direction=&amp;quot;back&amp;quot;, total=&amp;quot;bad&amp;quot;) == 0         
)        caplog.clear()        with 
caplog.at_level(&amp;quot;WARNING&amp;quot;):            assert (               
bridge.WebUIBridge.adjust_wizard_step(0, direction=&amp;quot;sideways&amp;quot;,
total=1) == 0            )            assert 
bridge.WebUIBridge.normalize_wizard_step(1.2, total=3) == 1        assert 
bridge.WebUIBridge.normalize_wizard_step(&amp;quot; 2 &amp;quot;, total=3) == 2 
caplog.clear()        with caplog.at_level(&amp;quot;WARNING&amp;quot;):        
assert bridge.WebUIBridge.normalize_wizard_step(&amp;quot;bad&amp;quot;, 
total=3) == 0            assert ui.ask_question(&amp;quot;Q?&amp;quot;, 
default=&amp;quot;answer&amp;quot;) == &amp;quot;answer&amp;quot;        assert 
ui.confirm_choice(&amp;quot;Continue?&amp;quot;, default=False) is False        
formatter_calls: list[tuple] = []            def fake_format(            
message: str, message_type: str | None = None, highlight: bool = False        ) 
-&amp;gt; str:            formatter_calls.append((message, message_type, 
highlight))            return f&amp;quot;{message_type or 
&amp;#x27;normal&amp;#x27;}::{highlight}&amp;quot;            
monkeypatch.setattr(            ui,            &amp;quot;formatter&amp;quot;,   
SimpleNamespace(format_message=fake_format),            raising=False,        ) 
st = bridge_live.streamlit        for name in (&amp;quot;error&amp;quot;, 
&amp;quot;warning&amp;quot;, &amp;quot;success&amp;quot;, 
&amp;quot;info&amp;quot;, &amp;quot;write&amp;quot;):            getattr(st, 
name).reset_mock()            ui.display_result(&amp;quot;serious&amp;quot;, 
message_type=&amp;quot;error&amp;quot;, highlight=True)        
ui.display_result(&amp;quot;heads-up&amp;quot;, 
message_type=&amp;quot;warning&amp;quot;)        
ui.display_result(&amp;quot;victory&amp;quot;, 
message_type=&amp;quot;success&amp;quot;)        
ui.display_result(&amp;quot;note&amp;quot;, 
message_type=&amp;quot;info&amp;quot;)        
ui.display_result(&amp;quot;progress&amp;quot;, highlight=True)        
ui.display_result(&amp;quot;plain&amp;quot;)            assert formatter_calls 
== [            (&amp;quot;serious&amp;quot;, &amp;quot;error&amp;quot;, True), 
(&amp;quot;heads-up&amp;quot;, &amp;quot;warning&amp;quot;, False),            
(&amp;quot;victory&amp;quot;, &amp;quot;success&amp;quot;, False),            
(&amp;quot;note&amp;quot;, &amp;quot;info&amp;quot;, False),            
(&amp;quot;progress&amp;quot;, None, True),            
(&amp;quot;plain&amp;quot;, None, False),        ]        assert 
st.error.call_args[0][0] == &amp;quot;error::True&amp;quot;        assert 
st.warning.call_args[0][0] == &amp;quot;warning::False&amp;quot;        assert 
st.success.call_args[0][0] == &amp;quot;success::False&amp;quot;        assert 
st.info.call_count == 2        assert st.info.call_args_list[0][0][0] == 
&amp;quot;info::False&amp;quot;        assert st.info.call_args_list[1][0][0] ==
&amp;quot;normal::True&amp;quot;        assert st.write.call_args[0][0] == 
&amp;quot;normal::False&amp;quot;        assert ui.messages == [            
&amp;quot;error::True&amp;quot;,            &amp;quot;warning::False&amp;quot;, 
&amp;quot;success::False&amp;quot;,            &amp;quot;info::False&amp;quot;, 
&amp;quot;normal::True&amp;quot;,            &amp;quot;normal::False&amp;quot;, 
]            progress = ui.create_progress(&amp;quot;Task&amp;quot;, total=3)   
assert isinstance(progress, bridge.WebUIProgressIndicator)            
st.session_state.clear()&amp;gt;       manager = 
ui.get_wizard_manager(&amp;quot;wiz&amp;quot;, steps=2, 
initial_state={&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;})               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitly
n/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_webui_bridge_a
a_coverage.py:258: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager    return 
self.create_wizard_manager(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/interface/webui_bridge.py:529: in create_wizard_manager    from 
devsynth.interface.wizard_state_manager import WizardStateManager_ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
&amp;quot;&amp;quot;&amp;quot;    Wizard state management coordination.        
This module provides a centralized way to manage wizard state,    ensuring 
consistency between WebUIBridge and WizardState.    
&amp;quot;&amp;quot;&amp;quot;        import logging    from pathlib import Path
from typing import Any, Dict, Optional, Sequence    &amp;gt;   from 
devsynth.config import get_project_config, save_configE   ImportError: cannot 
import name &amp;#x27;get_project_config&amp;#x27; from 
&amp;#x27;devsynth.config&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/wizard_state_manager.py:12: ImportError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:23,727 - 
devsynth.interface.webui_bridge - WARNING - Invalid total steps: bad, defaulting
to 12025-10-28 09:27:23,727 - devsynth.interface.webui_bridge - WARNING - 
Invalid direction: sideways, keeping current step2025-10-28 09:27:23,727 - 
devsynth.interface.webui_bridge - WARNING - Failed to normalize step value 
&amp;#x27;bad&amp;#x27;: could not convert string to float: 
&amp;#x27;bad&amp;#x27;, defaulting to 02025-10-28 09:27:23,727 - 
devsynth.interface.webui_bridge - ERROR - WebUI displaying error: 
serious2025-10-28 09:27:23,727 - devsynth.interface.webui_bridge - WARNING - 
WebUI displaying warning: heads-up2025-10-28 09:27:23,728 - 
devsynth.interface.webui_bridge - INFO - WebUI displaying success: 
victory------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.interface.webui_bridge:logging_setup.py:615 Invalid total steps: bad, 
defaulting to 1WARNING  devsynth.interface.webui_bridge:logging_setup.py:615 
Invalid direction: sideways, keeping current stepWARNING  
devsynth.interface.webui_bridge:logging_setup.py:615 Failed to normalize step 
value &amp;#x27;bad&amp;#x27;: could not convert string to float: 
&amp;#x27;bad&amp;#x27;, defaulting to 0ERROR    
devsynth.interface.webui_bridge:logging_setup.py:615 WebUI displaying error: 
seriousWARNING  devsynth.interface.webui_bridge:logging_setup.py:615 WebUI 
displaying warning: heads-upINFO     
devsynth.interface.webui_bridge:logging_setup.py:615 WebUI displaying success: 
victory__________ test_nested_subtask_handles_fallbacks_and_missing_parents 
___________webui_bridge_module = (&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int.
..ce/webui_bridge.py&amp;#x27;&amp;gt;, &amp;lt;module 
&amp;#x27;streamlit&amp;#x27; from &amp;lt;function 
StreamlitRecorder.__getattr__.&amp;lt;locals&amp;gt;._noop at 
0x144817b00&amp;gt;&amp;gt;)    def 
test_nested_subtask_handles_fallbacks_and_missing_parents(        
webui_bridge_module: tuple[ModuleType, StreamlitRecorder],    ) -&amp;gt; None: 
&amp;quot;&amp;quot;&amp;quot;Nested subtasks fall back to safe labels and 
ignore missing parents.&amp;quot;&amp;quot;&amp;quot;            bridge, _ = 
webui_bridge_module        indicator = 
bridge.WebUIProgressIndicator(&amp;quot;Main task&amp;quot;, 10)            
parent_id = indicator.add_subtask(BadString(), total=5)        parent = 
indicator._subtasks&amp;gt;       assert parent[&amp;quot;description&amp;quot;]
== &amp;quot;&amp;lt;subtask&amp;gt;&amp;quot;               
^^^^^^^^^^^^^^^^^^^^^E       TypeError: &amp;#x27;SubtaskState&amp;#x27; object 
is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/int
erface/test_webui_bridge_fast_suite.py:82: TypeError________ 
test_nested_subtask_status_progression_without_explicit_status 
________webui_bridge_module = (&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int.
..ce/webui_bridge.py&amp;#x27;&amp;gt;, &amp;lt;module 
&amp;#x27;streamlit&amp;#x27; from &amp;lt;function 
StreamlitRecorder.__getattr__.&amp;lt;locals&amp;gt;._noop at 
0x144816de0&amp;gt;&amp;gt;)    def 
test_nested_subtask_status_progression_without_explicit_status(        
webui_bridge_module: tuple[ModuleType, StreamlitRecorder],    ) -&amp;gt; None: 
&amp;quot;&amp;quot;&amp;quot;Omitting ``status`` triggers the automatic status 
lifecycle.&amp;quot;&amp;quot;&amp;quot;            bridge, _ = 
webui_bridge_module        indicator = 
bridge.WebUIProgressIndicator(&amp;quot;Main task&amp;quot;, 100)        
parent_id = indicator.add_subtask(&amp;quot;Parent&amp;quot;, total=100)        
nested_id = indicator.add_nested_subtask(parent_id, &amp;quot;Child&amp;quot;, 
total=100)&amp;gt;       nested = 
indicator._subtasks[&amp;quot;nested_subtasks&amp;quot;]                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;SubtaskState&amp;#x27; object is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/int
erface/test_webui_bridge_fast_suite.py:121: TypeError_____________ 
test_progress_indicator_nested_tasks_cover_fallbacks _____________bridge_module 
= &amp;lt;module &amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;    def 
test_progress_indicator_nested_tasks_cover_fallbacks(bridge_module):        
&amp;quot;&amp;quot;&amp;quot;Nested subtasks use safe placeholders and default 
statuses.&amp;quot;&amp;quot;&amp;quot;            indicator = 
bridge_module.WebUIProgressIndicator(&amp;quot;Task&amp;quot;, 4)            
parent_id = indicator.add_subtask(RaisingStr(), total=4)        parent = 
indicator._subtasks&amp;gt;       assert parent[&amp;quot;description&amp;quot;]
== &amp;quot;&amp;lt;subtask&amp;gt;&amp;quot;               
^^^^^^^^^^^^^^^^^^^^^E       TypeError: &amp;#x27;SubtaskState&amp;#x27; object 
is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/int
erface/test_webui_bridge_handshake.py:108: TypeError____________ 
test_progress_indicator_status_defaults_and_fallbacks _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1449b7170&amp;gt;bridge_module = &amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;    def 
test_progress_indicator_status_defaults_and_fallbacks(        monkeypatch: 
pytest.MonkeyPatch, bridge_module    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Status strings fall back to defaults and sanitize 
valid updates.&amp;quot;&amp;quot;&amp;quot;            monkeypatch.setattr(    
bridge_module, &amp;quot;sanitize_output&amp;quot;, lambda value: 
f&amp;quot;S:{value}&amp;quot;, raising=False        )        indicator = 
bridge_module.WebUIProgressIndicator(&amp;quot;Task&amp;quot;, 100)            
indicator.update(description=&amp;quot;Start&amp;quot;)        assert 
indicator._description == &amp;quot;S:Start&amp;quot;            
indicator.update(description=RaisingStr(), status=RaisingStr())        assert 
indicator._description == &amp;quot;S:Start&amp;quot;&amp;gt;       assert 
indicator._status == &amp;quot;In progress...&amp;quot;E       AssertionError: 
assert &amp;#x27;Starting...&amp;#x27; == &amp;#x27;In progress...&amp;#x27;E   
E         - In progress...E         + 
Starting.../Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/inter
face/test_webui_bridge_handshake.py:147: AssertionError____________ 
test_progress_indicator_subtasks_and_nested_operations ____________sanitize_spy 
= &amp;lt;tests.unit.interface.test_webui_bridge_progress._SanitizeSpy object at
0x144fea330&amp;gt;    @pytest.mark.fast    def 
test_progress_indicator_subtasks_and_nested_operations(        sanitize_spy: 
_SanitizeSpy,    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Subtask 
lifecycle sanitizes text, handles fallbacks, and ignores invalid IDs.           
ReqID: N/A        &amp;quot;&amp;quot;&amp;quot;            indicator = 
webui_bridge.WebUIProgressIndicator(&amp;quot;main&amp;quot;, 100)            
task_id = indicator.add_subtask(&amp;quot;alpha&amp;quot;, total=10)&amp;gt;    
assert sanitize_spy.calls[-1] == &amp;quot;alpha&amp;quot;E       
AssertionError: assert &amp;#x27;Starting...&amp;#x27; == 
&amp;#x27;alpha&amp;#x27;E         E         - alphaE         + 
Starting.../Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/inter
face/test_webui_bridge_progress.py:146: AssertionError___________________ 
test_nested_subtask_default_status_cycle ___________________sanitize_spy = 
&amp;lt;tests.unit.interface.test_webui_bridge_progress._SanitizeSpy object at 
0x144febf20&amp;gt;    @pytest.mark.fast    def 
test_nested_subtask_default_status_cycle(sanitize_spy: _SanitizeSpy) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Nested subtasks update status text 
according to default progress thresholds.            ReqID: N/A        
&amp;quot;&amp;quot;&amp;quot;            indicator = 
webui_bridge.WebUIProgressIndicator(&amp;quot;root&amp;quot;, 100)            
subtask_id = indicator.add_subtask(&amp;quot;outer&amp;quot;, total=100)        
nested_id = indicator.add_nested_subtask(subtask_id, &amp;quot;inner&amp;quot;, 
total=100)            # Descriptions are sanitized through the shared spy 
helper.&amp;gt;       assert sanitize_spy.calls[-2:] == 
[&amp;quot;outer&amp;quot;, &amp;quot;inner&amp;quot;]E       AssertionError: 
assert [&amp;#x27;inner&amp;#x27;, &amp;#x27;Starting...&amp;#x27;] == 
[&amp;#x27;outer&amp;#x27;, &amp;#x27;inner&amp;#x27;]E         E         At 
index 0 diff: &amp;#x27;inner&amp;#x27; != &amp;#x27;outer&amp;#x27;E         
Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/te
st_webui_bridge_progress.py:359: AssertionError_______________ 
test_nested_progress_status_defaults_follow_spec _______________bridge_env = 
namespace(module=&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;, streamlit=&amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;)monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144feb290&amp;gt;    def 
test_nested_progress_status_defaults_follow_spec(        bridge_env: 
SimpleNamespace, monkeypatch: pytest.MonkeyPatch    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;SpecRef: docs/specifications/webui-integration.md 
Progress indicators;        docs/developer_guides/progress_indicators.md 
UXBridge Integration.            Nested subtasks surface the documented status 
thresholds and cascade        completion through parent subtasks.        
&amp;quot;&amp;quot;&amp;quot;            bridge = bridge_env.module        tick
= iter(range(1000))        monkeypatch.setattr(bridge.time, 
&amp;quot;time&amp;quot;, lambda: next(tick))            indicator = 
bridge.WebUIProgressIndicator(&amp;quot;Collect&amp;quot;, 100)        parent_id
= indicator.add_subtask(&amp;quot;Gather&amp;quot;, total=40)        nested_id =
indicator.add_nested_subtask(parent_id, &amp;quot;Validate&amp;quot;, total=8)  
expectations = [            (0, &amp;quot;Starting...&amp;quot;),            (2,
&amp;quot;Processing...&amp;quot;),            (4, &amp;quot;Halfway 
there...&amp;quot;),            (6, &amp;quot;Almost done...&amp;quot;),        
(8 * 0.99, &amp;quot;Finalizing...&amp;quot;),            (8, 
&amp;quot;Complete&amp;quot;),        ]            for progress, status in 
expectations:&amp;gt;           
indicator._subtasks[&amp;quot;nested_subtasks&amp;quot;][            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                
&amp;quot;current&amp;quot;            ] = progressE           TypeError: 
&amp;#x27;SubtaskState&amp;#x27; object is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/int
erface/test_webui_bridge_spec_alignment.py:90: TypeError____________ 
test_wizard_manager_accessors_follow_integration_guide ____________bridge_env = 
namespace(module=&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;, streamlit=&amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;)    def 
test_wizard_manager_accessors_follow_integration_guide(        bridge_env: 
SimpleNamespace,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;SpecRef: 
docs/implementation/requirements_wizard_wizardstate_integration.md Summary;    
docs/specifications/webui-integration.md Wizard state wiring.            
Session-backed managers persist WizardState across calls and guard missing      
session_state inputs with DevSynthError.        &amp;quot;&amp;quot;&amp;quot;  
bridge = bridge_env.module        bridge_env.streamlit.session_state.clear()    
ui = bridge.WebUIBridge()    &amp;gt;       manager = ui.get_wizard_manager(    
&amp;quot;requirements&amp;quot;, steps=2, 
initial_state={&amp;quot;title&amp;quot;: &amp;quot;Draft&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_bridge_spec_alignment.py:158: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager    return 
self.create_wizard_manager(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/interface/webui_bridge.py:529: in create_wizard_manager    from 
devsynth.interface.wizard_state_manager import WizardStateManager_ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
&amp;quot;&amp;quot;&amp;quot;    Wizard state management coordination.        
This module provides a centralized way to manage wizard state,    ensuring 
consistency between WebUIBridge and WizardState.    
&amp;quot;&amp;quot;&amp;quot;        import logging    from pathlib import Path
from typing import Any, Dict, Optional, Sequence    &amp;gt;   from 
devsynth.config import get_project_config, save_configE   ImportError: cannot 
import name &amp;#x27;get_project_config&amp;#x27; from 
&amp;#x27;devsynth.config&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/wizard_state_manager.py:12: ImportError__________ 
test_webui_bridge_create_wizard_manager_instantiates_stub ___________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1450891f0&amp;gt;    
@pytest.mark.fast    def 
test_webui_bridge_create_wizard_manager_instantiates_stub(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:&amp;gt;       import 
devsynth.interface.wizard_state_manager as 
wizard_state_module/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_bridge_state_fast.py:51: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
&amp;quot;&amp;quot;&amp;quot;    Wizard state management coordination.        
This module provides a centralized way to manage wizard state,    ensuring 
consistency between WebUIBridge and WizardState.    
&amp;quot;&amp;quot;&amp;quot;        import logging    from pathlib import Path
from typing import Any, Dict, Optional, Sequence    &amp;gt;   from 
devsynth.config import get_project_config, save_configE   ImportError: cannot 
import name &amp;#x27;get_project_config&amp;#x27; from 
&amp;#x27;devsynth.config&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/wizard_state_manager.py:12: ImportError____________________ 
test_get_wizard_manager_persists_state ____________________bridge_under_test = 
namespace(module=&amp;lt;module 
&amp;#x27;devsynth.interface.webui_bridge&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui_bridge.py&amp;#x27;&amp;gt;, streamlit=&amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;)    def 
test_get_wizard_manager_persists_state(        bridge_under_test: 
SimpleNamespace,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Wizard managers reuse the same Streamlit-backed 
session state.            ReqID: coverage-webui-bridge        
&amp;quot;&amp;quot;&amp;quot;            bridge = bridge_under_test.module     
bridge_under_test.streamlit.session_state = {}        ui = bridge.WebUIBridge() 
&amp;gt;       manager = ui.get_wizard_manager(            
&amp;quot;requirements&amp;quot;, steps=3, 
initial_state={&amp;quot;title&amp;quot;: &amp;quot;Draft&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_bridge_targeted.py:164: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager    return 
self.create_wizard_manager(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/interface/webui_bridge.py:529: in create_wizard_manager    from 
devsynth.interface.wizard_state_manager import WizardStateManager_ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
&amp;quot;&amp;quot;&amp;quot;    Wizard state management coordination.        
This module provides a centralized way to manage wizard state,    ensuring 
consistency between WebUIBridge and WizardState.    
&amp;quot;&amp;quot;&amp;quot;        import logging    from pathlib import Path
from typing import Any, Dict, Optional, Sequence    &amp;gt;   from 
devsynth.config import get_project_config, save_configE   ImportError: cannot 
import name &amp;#x27;get_project_config&amp;#x27; from 
&amp;#x27;devsynth.config&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/wizard_state_manager.py:12: ImportError______________________ 
test_cli_returns_module_attribute _______________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451dad80&amp;gt;    def 
test_cli_returns_module_attribute(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;ReqID: FR-203 prefer WebUI overrides when 
resolving commands.&amp;quot;&amp;quot;&amp;quot;        dummy = DummyCommands()
def sentinel():            return &amp;quot;ok&amp;quot;            
monkeypatch.setattr(webui, &amp;quot;dummy_cmd&amp;quot;, sentinel, 
raising=False)&amp;gt;       assert dummy._cli(&amp;quot;dummy_cmd&amp;quot;) is
sentinelE       AssertionError: assert None is &amp;lt;function 
test_cli_returns_module_attribute.&amp;lt;locals&amp;gt;.sentinel at 
0x145198360&amp;gt;E        +  where None = _cli(&amp;#x27;dummy_cmd&amp;#x27;)E
+    where _cli = &amp;lt;tests.unit.interface.test_webui_commands.DummyCommands
object at 
0x1451eb020&amp;gt;._cli/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_commands.py:40: 
AssertionError______________________ test_require_streamlit_lazy_loader 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x14531e2a0&amp;gt;    def 
test_require_streamlit_lazy_loader(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Lazy ``st`` proxy loads Streamlit 
only when accessed.&amp;quot;&amp;quot;&amp;quot;            fake_streamlit = 
FakeStreamlit()        monkeypatch.setitem(sys.modules, 
&amp;quot;streamlit&amp;quot;, fake_streamlit)    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_display_and_layout.py:40: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________________ 
test_webui_display_result_highlight_succeeds _________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;, clean_state = None    def 
test_webui_display_result_highlight_succeeds(mock_streamlit, clean_state):      
&amp;quot;&amp;quot;&amp;quot;Test that highlighted messages use st.info.       
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib            from
devsynth.interface import webui            # Reload the module to ensure clean 
state&amp;gt;       
importlib.reload(webui)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/interface/test_webui_enhanced.py:101: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ module = &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt;    def reload(module):        
&amp;quot;&amp;quot;&amp;quot;Reload the module and return it.            The 
module must have been successfully imported before.            
&amp;quot;&amp;quot;&amp;quot;        try:            name = 
module.__spec__.name        except AttributeError:            try:              
name = module.__name__            except AttributeError:                raise 
TypeError(&amp;quot;reload() argument must be a module&amp;quot;) from None     
if sys.modules.get(name) is not module:&amp;gt;           raise 
ImportError(f&amp;quot;module {name} not in sys.modules&amp;quot;, name=name)E  
ImportError: module devsynth.interface.webui not in 
sys.modules/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/
Versions/3.12/lib/python3.12/importlib/__init__.py:111: 
ImportError_________________ test_webui_display_result_error_raises_error 
_________________mock_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_error_raises_error(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that error messages use st.error.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:116: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________________ 
test_webui_display_result_warning_succeeds __________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_warning_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that warning messages use st.warning.        
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:132: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________________ 
test_webui_display_result_success_succeeds __________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_success_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that success messages use st.success.        
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:148: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________________ 
test_webui_display_result_heading_succeeds __________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_heading_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that heading messages use st.header.         
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:164: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________________ 
test_webui_display_result_subheading_succeeds _________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_subheading_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that subheading messages use st.subheader.   
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:180: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________________ 
test_webui_display_result_rich_markup_succeeds ________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_rich_markup_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that Rich markup is converted to 
Markdown/HTML.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import
importlib    &amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:196: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________________ 
test_webui_display_result_normal_succeeds ___________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_display_result_normal_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test that normal messages use st.write.           
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:214: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________________ 
test_webui_progress_indicator_succeeds ____________________mock_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_webui_progress_indicator_succeeds(mock_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test the enhanced progress indicator.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_enhanced.py:230: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI module 
for DevSynth.        This module provides web interface components for DevSynth.
&amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import (        
LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______________ 
test_get_layout_config_breakpoints[640-expected0] _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144acd7f0&amp;gt;screen_width = 640expected = {&amp;#x27;columns&amp;#x27;: 1,
&amp;#x27;content_width&amp;#x27;: &amp;#x27;100%&amp;#x27;, 
&amp;#x27;font_size&amp;#x27;: &amp;#x27;small&amp;#x27;, 
&amp;#x27;is_mobile&amp;#x27;: True, ...}    @pytest.mark.parametrize(        
(&amp;quot;screen_width&amp;quot;, &amp;quot;expected&amp;quot;),        [      
(                640,                {                    
&amp;quot;columns&amp;quot;: 1,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;small&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;0.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: True,                },            ),            
(                820,                {                    
&amp;quot;columns&amp;quot;: 2,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;30%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;70%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),           
(                1200,                {                    
&amp;quot;columns&amp;quot;: 3,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;20%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;80%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),        ], 
)    def test_get_layout_config_breakpoints(        monkeypatch: 
pytest.MonkeyPatch, screen_width, expected    ):        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``WebUI.get_layout_config`` adapts 
layout by screen width.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub(session_width=screen_width)&amp;gt;       
_install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144acd7f0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=640), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record...cals&amp;gt;.method at 
0x1449f80e0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449faca0&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError______________ test_get_layout_config_breakpoints[820-expected1] 
_______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144acdfd0&amp;gt;screen_width = 820expected = {&amp;#x27;columns&amp;#x27;: 2,
&amp;#x27;content_width&amp;#x27;: &amp;#x27;70%&amp;#x27;, 
&amp;#x27;font_size&amp;#x27;: &amp;#x27;medium&amp;#x27;, 
&amp;#x27;is_mobile&amp;#x27;: False, ...}    @pytest.mark.parametrize(        
(&amp;quot;screen_width&amp;quot;, &amp;quot;expected&amp;quot;),        [      
(                640,                {                    
&amp;quot;columns&amp;quot;: 1,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;small&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;0.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: True,                },            ),            
(                820,                {                    
&amp;quot;columns&amp;quot;: 2,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;30%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;70%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),           
(                1200,                {                    
&amp;quot;columns&amp;quot;: 3,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;20%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;80%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),        ], 
)    def test_get_layout_config_breakpoints(        monkeypatch: 
pytest.MonkeyPatch, screen_width, expected    ):        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``WebUI.get_layout_config`` adapts 
layout by screen width.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub(session_width=screen_width)&amp;gt;       
_install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144acdfd0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=820), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record...cals&amp;gt;.method at 
0x1449f8180&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449f8220&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError______________ test_get_layout_config_breakpoints[1200-expected2] 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144ace270&amp;gt;screen_width = 1200expected = {&amp;#x27;columns&amp;#x27;: 
3, &amp;#x27;content_width&amp;#x27;: &amp;#x27;80%&amp;#x27;, 
&amp;#x27;font_size&amp;#x27;: &amp;#x27;medium&amp;#x27;, 
&amp;#x27;is_mobile&amp;#x27;: False, ...}    @pytest.mark.parametrize(        
(&amp;quot;screen_width&amp;quot;, &amp;quot;expected&amp;quot;),        [      
(                640,                {                    
&amp;quot;columns&amp;quot;: 1,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;100%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;small&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;0.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: True,                },            ),            
(                820,                {                    
&amp;quot;columns&amp;quot;: 2,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;30%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;70%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),           
(                1200,                {                    
&amp;quot;columns&amp;quot;: 3,                    
&amp;quot;sidebar_width&amp;quot;: &amp;quot;20%&amp;quot;,                    
&amp;quot;content_width&amp;quot;: &amp;quot;80%&amp;quot;,                    
&amp;quot;font_size&amp;quot;: &amp;quot;medium&amp;quot;,                    
&amp;quot;padding&amp;quot;: &amp;quot;1.5rem&amp;quot;,                    
&amp;quot;is_mobile&amp;quot;: False,                },            ),        ], 
)    def test_get_layout_config_breakpoints(        monkeypatch: 
pytest.MonkeyPatch, screen_width, expected    ):        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``WebUI.get_layout_config`` adapts 
layout by screen width.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub(session_width=screen_width)&amp;gt;       
_install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ace270&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1449f91c0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449f9120&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError________________ test_display_result_rich_markup_uses_markdown 
_________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at
0x144ace390&amp;gt;    def test_display_result_rich_markup_uses_markdown(       
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Rich markup renders via ``markdown`` 
with HTML spans.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:113: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ace390&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1449fa0c0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449fa160&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError________________ test_display_result_error_type_renders_context 
________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144acd310&amp;gt;    def test_display_result_error_type_renders_context(      
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - 
``message_type=&amp;#x27;error&amp;#x27;`` surfaces suggestions and 
docs.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:133: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144acd310&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x144097240&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x144096d40&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError______________ test_display_result_message_types 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144acdf10&amp;gt;message_type = &amp;#x27;warning&amp;#x27;, expected_method =
&amp;#x27;warning&amp;#x27;    @pytest.mark.parametrize(        
(&amp;quot;message_type&amp;quot;, &amp;quot;expected_method&amp;quot;),        
[            (&amp;quot;warning&amp;quot;, &amp;quot;warning&amp;quot;),        
(&amp;quot;success&amp;quot;, &amp;quot;success&amp;quot;),            
(&amp;quot;info&amp;quot;, &amp;quot;info&amp;quot;),            
(&amp;quot;unexpected&amp;quot;, &amp;quot;write&amp;quot;),        ],    )    
def test_display_result_message_types(        monkeypatch: pytest.MonkeyPatch, 
message_type, expected_method    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``display_result`` delegates to 
Streamlit per type.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:184: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144acdf10&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x144b7c860&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x144b7c360&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError______________ test_display_result_message_types 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1449a2ea0&amp;gt;message_type = &amp;#x27;success&amp;#x27;, expected_method =
&amp;#x27;success&amp;#x27;    @pytest.mark.parametrize(        
(&amp;quot;message_type&amp;quot;, &amp;quot;expected_method&amp;quot;),        
[            (&amp;quot;warning&amp;quot;, &amp;quot;warning&amp;quot;),        
(&amp;quot;success&amp;quot;, &amp;quot;success&amp;quot;),            
(&amp;quot;info&amp;quot;, &amp;quot;info&amp;quot;),            
(&amp;quot;unexpected&amp;quot;, &amp;quot;write&amp;quot;),        ],    )    
def test_display_result_message_types(        monkeypatch: pytest.MonkeyPatch, 
message_type, expected_method    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``display_result`` delegates to 
Streamlit per type.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:184: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a2ea0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1449f8720&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449f9300&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError_________________ test_display_result_message_types 
_________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at
0x1449a1970&amp;gt;message_type = &amp;#x27;info&amp;#x27;, expected_method = 
&amp;#x27;info&amp;#x27;    @pytest.mark.parametrize(        
(&amp;quot;message_type&amp;quot;, &amp;quot;expected_method&amp;quot;),        
[            (&amp;quot;warning&amp;quot;, &amp;quot;warning&amp;quot;),        
(&amp;quot;success&amp;quot;, &amp;quot;success&amp;quot;),            
(&amp;quot;info&amp;quot;, &amp;quot;info&amp;quot;),            
(&amp;quot;unexpected&amp;quot;, &amp;quot;write&amp;quot;),        ],    )    
def test_display_result_message_types(        monkeypatch: pytest.MonkeyPatch, 
message_type, expected_method    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``display_result`` delegates to 
Streamlit per type.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:184: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a1970&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1449fade0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1449f87c0&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError_____________ test_display_result_message_types 
______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1449a0410&amp;gt;message_type = &amp;#x27;unexpected&amp;#x27;, 
expected_method = &amp;#x27;write&amp;#x27;    @pytest.mark.parametrize(        
(&amp;quot;message_type&amp;quot;, &amp;quot;expected_method&amp;quot;),        
[            (&amp;quot;warning&amp;quot;, &amp;quot;warning&amp;quot;),        
(&amp;quot;success&amp;quot;, &amp;quot;success&amp;quot;),            
(&amp;quot;info&amp;quot;, &amp;quot;info&amp;quot;),            
(&amp;quot;unexpected&amp;quot;, &amp;quot;write&amp;quot;),        ],    )    
def test_display_result_message_types(        monkeypatch: pytest.MonkeyPatch, 
message_type, expected_method    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - ``display_result`` delegates to 
Streamlit per type.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:184: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a0410&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1450aad40&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1450a8fe0&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError___________________ test_display_result_highlight_uses_info 
____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object
at 0x1449a1700&amp;gt;    def 
test_display_result_highlight_uses_info(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Highlighting 
without type uses ``info`` output.&amp;quot;&amp;quot;&amp;quot;            stub
= _make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:196: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a1700&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x1450a8cc0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x1450a8860&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError____________________ test_display_result_defaults_to_write 
_____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1449a27b0&amp;gt;    def 
test_display_result_defaults_to_write(monkeypatch: pytest.MonkeyPatch) -&amp;gt;
None:        &amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Plain messages fall back
to ``write`` output.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:208: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a27b0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x144b7e480&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x144b7f740&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError_______ test_display_result_renders_headings _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a0d70&amp;gt;message = 
&amp;#x27;# Overview&amp;#x27;, expected_calls = [(&amp;#x27;header&amp;#x27;, 
(&amp;#x27;Overview&amp;#x27;,), {})]    @pytest.mark.parametrize(        
(&amp;quot;message&amp;quot;, &amp;quot;expected_calls&amp;quot;),        [     
(&amp;quot;# Overview&amp;quot;, [(&amp;quot;header&amp;quot;, 
(&amp;quot;Overview&amp;quot;,), {})]),            (&amp;quot;## 
Section&amp;quot;, [(&amp;quot;subheader&amp;quot;, 
(&amp;quot;Section&amp;quot;,), {})]),            (&amp;quot;### Deep 
Dive&amp;quot;, [(&amp;quot;markdown&amp;quot;, (&amp;quot;**Deep 
Dive**&amp;quot;,), {})]),        ],    )    def 
test_display_result_renders_headings(        monkeypatch: pytest.MonkeyPatch, 
message: str, expected_calls    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Markdown headings map onto Streamlit 
helpers.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:230: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a0d70&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x14460b1a0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x144b7d3a0&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError_______ test_display_result_renders_headings _______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1454ddfd0&amp;gt;message = 
&amp;#x27;## Section&amp;#x27;, expected_calls = 
[(&amp;#x27;subheader&amp;#x27;, (&amp;#x27;Section&amp;#x27;,), {})]    
@pytest.mark.parametrize(        (&amp;quot;message&amp;quot;, 
&amp;quot;expected_calls&amp;quot;),        [            (&amp;quot;# 
Overview&amp;quot;, [(&amp;quot;header&amp;quot;, 
(&amp;quot;Overview&amp;quot;,), {})]),            (&amp;quot;## 
Section&amp;quot;, [(&amp;quot;subheader&amp;quot;, 
(&amp;quot;Section&amp;quot;,), {})]),            (&amp;quot;### Deep 
Dive&amp;quot;, [(&amp;quot;markdown&amp;quot;, (&amp;quot;**Deep 
Dive**&amp;quot;,), {})]),        ],    )    def 
test_display_result_renders_headings(        monkeypatch: pytest.MonkeyPatch, 
message: str, expected_calls    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Markdown headings map onto Streamlit 
helpers.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:230: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1454ddfd0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x144b7d4e0&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x144b7e0c0&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError_____ test_display_result_renders_headings ______monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a19d0&amp;gt;message = 
&amp;#x27;### Deep Dive&amp;#x27;expected_calls = 
[(&amp;#x27;markdown&amp;#x27;, (&amp;#x27;**Deep Dive**&amp;#x27;,), {})]    
@pytest.mark.parametrize(        (&amp;quot;message&amp;quot;, 
&amp;quot;expected_calls&amp;quot;),        [            (&amp;quot;# 
Overview&amp;quot;, [(&amp;quot;header&amp;quot;, 
(&amp;quot;Overview&amp;quot;,), {})]),            (&amp;quot;## 
Section&amp;quot;, [(&amp;quot;subheader&amp;quot;, 
(&amp;quot;Section&amp;quot;,), {})]),            (&amp;quot;### Deep 
Dive&amp;quot;, [(&amp;quot;markdown&amp;quot;, (&amp;quot;**Deep 
Dive**&amp;quot;,), {})]),        ],    )    def 
test_display_result_renders_headings(        monkeypatch: pytest.MonkeyPatch, 
message: str, expected_calls    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: N/A - Markdown headings map onto Streamlit 
helpers.&amp;quot;&amp;quot;&amp;quot;            stub = 
_make_streamlit_stub()&amp;gt;       _install_streamlit_stub(monkeypatch, 
stub)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_layout_and_display_branching.py:230: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1449a19d0&amp;gt;stub = 
namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.recor...cals&amp;gt;.method at 
0x142e04360&amp;gt;, subheader=&amp;lt;function 
_make_streamlit_stub.&amp;lt;locals&amp;gt;.record.&amp;lt;locals&amp;gt;.method
at 0x142e06200&amp;gt;)    def _install_streamlit_stub(        monkeypatch: 
pytest.MonkeyPatch, stub: SimpleNamespace    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Patch :mod:`devsynth.interface.webui` to use the 
provided Streamlit stub.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui_module, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_layout_and_display_branching.py:52: 
AttributeError____________________ test_lazy_streamlit_proxy_imports_once 
____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object
at 0x1457b9430&amp;gt;    def 
test_lazy_streamlit_proxy_imports_once(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;``st`` forwards attribute 
access while caching the loaded module.&amp;quot;&amp;quot;&amp;quot;           
sentinel = SimpleNamespace(marker=&amp;quot;streamlit-sentinel&amp;quot;)       
imports: list = []            monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, None, raising=False)            def 
fake_import(name: str) -&amp;gt; SimpleNamespace:            
imports.append(name)            return sentinel    &amp;gt;       
monkeypatch.setattr(webui.importlib, &amp;quot;import_module&amp;quot;, 
fake_import)                            ^^^^^^^^^^^^^^^E       AttributeError: 
module &amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;importlib&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/tests/unit/interface/test_webui_lazy_loader_fast.py:107: 
AttributeError____________________ test_ui_progress_tracks_status_and_eta 
____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object
at 0x14584f080&amp;gt;    def 
test_ui_progress_tracks_status_and_eta(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Progress lifecycle updates 
sanitize text, surface ETA, and mark completion.&amp;quot;&amp;quot;&amp;quot;  
stub = DummyStreamlit()        monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub, raising=False)            time_ticks = 
count(start=100, step=10)&amp;gt;       monkeypatch.setattr(webui.time, 
&amp;quot;time&amp;quot;, lambda: next(time_ticks))                            
^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_lazy_loader_fast.py:126: 
AttributeError__________________ test_ensure_router_creates_single_instance 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x14584eb40&amp;gt;    def 
test_ensure_router_creates_single_instance(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Router initialization 
defers until first access and memoizes the 
instance.&amp;quot;&amp;quot;&amp;quot;            created: 
list[tuple[webui.WebUI, dict]] = []            class RecordingRouter:           
def __init__(self, owner: webui.WebUI, navigation: dict) -&amp;gt; None:        
created.append((owner, navigation))                self.owner = owner           
self.navigation = navigation            monkeypatch.setattr(webui, 
&amp;quot;Router&amp;quot;, RecordingRouter)            navigation = 
{&amp;quot;Home&amp;quot;: &amp;quot;render_home&amp;quot;, 
&amp;quot;Docs&amp;quot;: &amp;quot;render_docs&amp;quot;}&amp;gt;       
monkeypatch.setattr(webui.WebUI, &amp;quot;navigation_items&amp;quot;, lambda 
self: navigation)E       AttributeError: None has no attribute 
&amp;#x27;navigation_items&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/
devsynth/tests/unit/interface/test_webui_lazy_loader_fast.py:189: 
AttributeError_______________ test_missing_streamlit_surfaces_install_guidance 
_______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14584fef0&amp;gt;    def test_missing_streamlit_surfaces_install_guidance(    
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Import failures raise :class:`DevSynthError` with 
actionable guidance.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py:140: 
AttributeError_____________________ test_lazy_streamlit_import_is_cached 
_____________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x14584f650&amp;gt;    def 
test_lazy_streamlit_import_is_cached(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Feature: webui_core.feature Scenario:
Lazy Streamlit loader caches module.&amp;quot;&amp;quot;&amp;quot;            
from devsynth.interface import webui            call_log: List = []        
streamlit_stub = make_streamlit_mock()            original_import = 
importlib.import_module            def fake_import(name: str, package: str | 
None = None):            call_log.append(name)            if name == 
&amp;quot;streamlit&amp;quot;:                return streamlit_stub            
return original_import(name, package)            monkeypatch.setattr(importlib, 
&amp;quot;import_module&amp;quot;, fake_import)    &amp;gt;       module = 
importlib.reload(webui)                 
^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/interface/test_webui_lazy_streamlit_and_wizard.py:60: _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ module = 
&amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt;    def reload(module):        
&amp;quot;&amp;quot;&amp;quot;Reload the module and return it.            The 
module must have been successfully imported before.            
&amp;quot;&amp;quot;&amp;quot;        try:            name = 
module.__spec__.name        except AttributeError:            try:              
name = module.__name__            except AttributeError:                raise 
TypeError(&amp;quot;reload() argument must be a module&amp;quot;) from None     
if sys.modules.get(name) is not module:&amp;gt;           raise 
ImportError(f&amp;quot;module {name} not in sys.modules&amp;quot;, name=name)E  
ImportError: module devsynth.interface.webui not in 
sys.modules/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/
Versions/3.12/lib/python3.12/importlib/__init__.py:111: ImportError___________ 
test_ui_progress_eta_displays_seconds_when_under_minute 
____________mock_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458a8da0&amp;gt;clean_state
= None    @pytest.mark.fast    def 
test_ui_progress_eta_displays_seconds_when_under_minute(        mock_streamlit, 
monkeypatch, clean_state    ):        &amp;quot;&amp;quot;&amp;quot;Render ETA 
in seconds when less than a minute remains.&amp;quot;&amp;quot;&amp;quot;    
&amp;gt;       progress, _, time_container = _init_progress_with_time(          
mock_streamlit,            monkeypatch,            times=[0.0, 5.0, 10.0],      
description=&amp;quot;ETA Seconds&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_progress.py:244: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time    module = _get_webui_module()
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_progress.py:16: in _get_webui_module    import 
devsynth.interface.webui as webui_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ 
test_ui_progress_eta_displays_minutes_when_under_hour 
_____________mock_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14583a660&amp;gt;clean_state
= None    @pytest.mark.fast    def 
test_ui_progress_eta_displays_minutes_when_under_hour(        mock_streamlit, 
monkeypatch, clean_state    ):        &amp;quot;&amp;quot;&amp;quot;Render ETA 
rounded to whole minutes when under an hour 
remains.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       progress, _, 
time_container = _init_progress_with_time(            mock_streamlit,           
monkeypatch,            times=[0.0, 100.0, 200.0],            
description=&amp;quot;ETA Minutes&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_progress.py:263: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time    module = _get_webui_module()
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_progress.py:16: in _get_webui_module    import 
devsynth.interface.webui as webui_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_______________ 
test_ui_progress_eta_displays_hours_and_minutes ________________mock_streamlit =
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145839850&amp;gt;clean_state
= None    @pytest.mark.fast    def 
test_ui_progress_eta_displays_hours_and_minutes(        mock_streamlit, 
monkeypatch, clean_state    ):        &amp;quot;&amp;quot;&amp;quot;Render ETA 
with hours and minutes when exceeding an hour.&amp;quot;&amp;quot;&amp;quot;    
&amp;gt;       progress, _, time_container = _init_progress_with_time(          
mock_streamlit,            monkeypatch,            times=[0.0, 100.0, 200.0],   
description=&amp;quot;ETA Hours&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_progress.py:282: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time    module = _get_webui_module()
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_progress.py:16: in _get_webui_module    import 
devsynth.interface.webui as webui_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_________ 
test_ui_progress_status_transitions_without_explicit_status 
__________mock_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458380e0&amp;gt;clean_state
= None    @pytest.mark.fast    def 
test_ui_progress_status_transitions_without_explicit_status(        
mock_streamlit, monkeypatch, clean_state    ):        
&amp;quot;&amp;quot;&amp;quot;Ensure automatic status text transitions at 
documented thresholds.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       progress,
_, time_container = _init_progress_with_time(            mock_streamlit,        
monkeypatch,            times=[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],         
description=&amp;quot;Status Thresholds&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_progress.py:301: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time    module = _get_webui_module()
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_progress.py:16: in _get_webui_module    import 
devsynth.interface.webui as webui_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______________ 
test_ui_progress_subtasks_update_with_frozen_time _______________mock_streamlit 
= &amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14583bc50&amp;gt;clean_state
= None    @pytest.mark.fast    def 
test_ui_progress_subtasks_update_with_frozen_time(        mock_streamlit, 
monkeypatch, clean_state    ):        &amp;quot;&amp;quot;&amp;quot;Subtask 
updates still render when the clock is frozen.&amp;quot;&amp;quot;&amp;quot;    
&amp;gt;       progress, _, time_container = _init_progress_with_time(          
mock_streamlit,            monkeypatch,            times=[100.0],            
description=&amp;quot;Frozen Subtasks&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_
webui_progress.py:335: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time    module = _get_webui_module()
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/un
it/interface/test_webui_progress.py:16: in _get_webui_module    import 
devsynth.interface.webui as webui_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________ 
TestProjectSetupPages.test_project_setup_pages_inheritance __________self = 
&amp;lt;tests.unit.interface.test_webui_rendering.TestProjectSetupPages object 
at 0x123736900&amp;gt;    def test_project_setup_pages_inheritance(self):       
&amp;quot;&amp;quot;&amp;quot;Test that ProjectSetupPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_rendering.py:98: ImportError_____________ 
TestLifecyclePages.test_lifecycle_pages_inheritance ______________self = 
&amp;lt;tests.unit.interface.test_webui_rendering.TestLifecyclePages object at 
0x123737710&amp;gt;    def test_lifecycle_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that LifecyclePages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_rendering.py:129: ImportError____________ 
TestOperationsPages.test_operations_pages_inheritance _____________self = 
&amp;lt;tests.unit.interface.test_webui_rendering.TestOperationsPages object at 
0x12374c590&amp;gt;    def test_operations_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that OperationsPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_rendering.py:161: ImportError_______________ 
TestSupportPages.test_support_pages_inheritance ________________self = 
&amp;lt;tests.unit.interface.test_webui_rendering.TestSupportPages object at 
0x12374d3d0&amp;gt;    def test_support_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that SupportPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_rendering.py:192: ImportError________ 
TestWebUIRenderingUtilities.test_rendering_import_dependencies ________self = 
&amp;lt;tests.unit.interface.test_webui_rendering.TestWebUIRenderingUtilities 
object at 0x12374f320&amp;gt;    def test_rendering_import_dependencies(self):  
&amp;quot;&amp;quot;&amp;quot;Test that rendering imports work 
correctly.&amp;quot;&amp;quot;&amp;quot;        # Test that key imports are 
available&amp;gt;       from devsynth.interface.webui.rendering import (        
LifecyclePages,            OperationsPages,            PageRenderer,            
ProjectSetupPages,            SupportPages,        )E       ImportError: cannot 
import name &amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/test_webui_rendering.py:295: ImportError____________________ 
test_require_streamlit_returns_module _____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458c35c0&amp;gt;    
@pytest.mark.fast    def test_require_streamlit_returns_module(monkeypatch):    
&amp;quot;&amp;quot;&amp;quot;Successful import returns module.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        module = 
types.SimpleNamespace()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, None)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_require_streamlit.py:16: 
AttributeError________________________ test_require_streamlit_raises 
_________________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1458c2810&amp;gt;    @pytest.mark.fast    def 
test_require_streamlit_raises(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Import failure raises DevSynthError.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;&amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_require_streamlit.py:26: 
AttributeError___________________ test_requirements_wizard_initialization 
____________________stub_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;, clean_state = None    def 
test_requirements_wizard_initialization(stub_streamlit, clean_state):        
import importlib    &amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______________ 
test_requirements_wizard_step_navigation_succeeds _______________stub_streamlit 
= &amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;, clean_state = None    
def test_requirements_wizard_step_navigation_succeeds(stub_streamlit, 
clean_state):        import importlib    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:60: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ 
test_requirements_wizard_save_requirements_succeeds ______________stub_streamlit
= &amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;, clean_state = None    
def test_requirements_wizard_save_requirements_succeeds(stub_streamlit, 
clean_state):        import importlib    &amp;gt;       import 
devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:78: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_______________________ 
test_validate_requirements_step ________________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_validate_requirements_step(stub_streamlit):&amp;gt;       from 
devsynth.interface.webui import 
WebUI/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:102: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError___________________ 
test_handle_requirements_navigation_next ___________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_handle_requirements_navigation_next(stub_streamlit):&amp;gt;       from 
devsynth.interface.webui import 
WebUI/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:113: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______________________ 
test_save_requirements_writes_file ______________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_save_requirements_writes_file(stub_streamlit):&amp;gt;       from 
devsynth.interface.webui import 
WebUI/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:130: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError__________________ 
test_priority_persists_through_navigation ___________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_priority_persists_through_navigation(stub_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Priority selection should persist when navigating 
steps.&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;       
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:156: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError______________________ 
test_title_and_description_persist ______________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_title_and_description_persist(stub_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Title and description should persist across 
navigation.&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;   
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_requirements_wizard.py:181: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web 
UI module for DevSynth.        This module provides web interface components for
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________________ 
test_run_method_with_invalid_navigation_option ________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;, clean_state = None    def 
test_run_method_with_invalid_navigation_option(stub_streamlit, clean_state):    
&amp;quot;&amp;quot;&amp;quot;Test the run method with an invalid navigation 
option.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
importlib            from devsynth.interface import webui            # Reload 
the module to ensure clean state&amp;gt;       
importlib.reload(webui)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/interface/test_webui_run_edge_cases.py:59: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ module = &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt;    def reload(module):        
&amp;quot;&amp;quot;&amp;quot;Reload the module and return it.            The 
module must have been successfully imported before.            
&amp;quot;&amp;quot;&amp;quot;        try:            name = 
module.__spec__.name        except AttributeError:            try:              
name = module.__name__            except AttributeError:                raise 
TypeError(&amp;quot;reload() argument must be a module&amp;quot;) from None     
if sys.modules.get(name) is not module:&amp;gt;           raise 
ImportError(f&amp;quot;module {name} not in sys.modules&amp;quot;, name=name)E  
ImportError: module devsynth.interface.webui not in 
sys.modules/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/
Versions/3.12/lib/python3.12/importlib/__init__.py:111: 
ImportError_______________ test_run_method_with_page_exception_raises_error 
_______________stub_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_run_method_with_page_exception_raises_error(stub_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test the run method when a page method raises an 
exception.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
importlib    &amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:82: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ 
test_run_method_with_streamlit_exception_raises_error 
_____________stub_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446deea0&amp;gt;    def 
test_run_method_with_streamlit_exception_raises_error(stub_streamlit, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;Test the run method when 
streamlit raises an exception.            ReqID: 
N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;       
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:106: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ 
test_run_method_with_sidebar_exception_raises_error ______________stub_streamlit
= &amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_run_method_with_sidebar_exception_raises_error(stub_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test the run method when sidebar raises an 
exception.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
importlib    &amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:126: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________ 
test_run_method_with_multiple_exceptions_raises_error 
_____________stub_streamlit = &amp;lt;module 
&amp;#x27;streamlit&amp;#x27;&amp;gt;    def 
test_run_method_with_multiple_exceptions_raises_error(stub_streamlit):        
&amp;quot;&amp;quot;&amp;quot;Test the run method when multiple exceptions 
occur.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import 
importlib    &amp;gt;       import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:146: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError____________________ 
test_standalone_run_function_succeeds _____________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446dcf80&amp;gt;    def 
test_standalone_run_function_succeeds(stub_streamlit, monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Test the standalone run function.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:169: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError________________________ 
test_run_webui_alias_succeeds _________________________stub_streamlit = 
&amp;lt;module &amp;#x27;streamlit&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1446dffe0&amp;gt;    def 
test_run_webui_alias_succeeds(stub_streamlit, monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;Test the run_webui alias function.            
ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        import importlib    &amp;gt;    
import devsynth.interface.webui as 
webui/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/t
est_webui_run_edge_cases.py:187: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     &amp;quot;&amp;quot;&amp;quot;    Web UI 
module for DevSynth.        This module provides web interface components for 
DevSynth.    &amp;quot;&amp;quot;&amp;quot;    &amp;gt;   from .rendering import
(        LifecyclePages,        OperationsPages,        PageRenderer,        
ProjectSetupPages,        SupportPages,    )E   ImportError: cannot import name 
&amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inter
face/webui/__init__.py:7: ImportError_____________ 
test_rendering_simulation_records_summary_and_errors _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144afcda0&amp;gt;    
@pytest.mark.fast    def test_rendering_simulation_records_summary_and_errors(  
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;simulate_progress_rendering mirrors CLI telemetry 
and sanitises errors.&amp;quot;&amp;quot;&amp;quot;            stub = 
BehaviorStreamlitStub()        harness = RenderHarness(stub)            summary 
= {            &amp;quot;description&amp;quot;: &amp;quot;Gather 
&amp;lt;docs&amp;gt;&amp;quot;,            &amp;quot;progress&amp;quot;: 0.75,  
&amp;quot;remaining&amp;quot;: 45.0,            &amp;quot;elapsed&amp;quot;: 
90.0,            &amp;quot;eta&amp;quot;: 200.0,            
&amp;quot;history&amp;quot;: [                {&amp;quot;status&amp;quot;: 
&amp;quot;Queued &amp;lt;init&amp;gt;&amp;quot;, &amp;quot;progress&amp;quot;: 
0.25, &amp;quot;time&amp;quot;: 100.0},                
{&amp;quot;status&amp;quot;: &amp;quot;Collecting&amp;quot;, 
&amp;quot;progress&amp;quot;: 0.5, &amp;quot;time&amp;quot;: 150.0},            
],            &amp;quot;checkpoints&amp;quot;: [                
{&amp;quot;progress&amp;quot;: 0.25, &amp;quot;time&amp;quot;: 110.0, 
&amp;quot;eta&amp;quot;: 200.0},                {&amp;quot;progress&amp;quot;: 
0.5, &amp;quot;time&amp;quot;: 160.0, &amp;quot;eta&amp;quot;: 200.0},          
],            &amp;quot;subtasks_detail&amp;quot;: [                {           
&amp;quot;description&amp;quot;: &amp;quot;Docs 
&amp;lt;survey&amp;gt;&amp;quot;,                    
&amp;quot;progress&amp;quot;: 1.0,                    
&amp;quot;status&amp;quot;: &amp;quot;Complete&amp;quot;,                    
&amp;quot;history&amp;quot;: [                        
{&amp;quot;status&amp;quot;: &amp;quot;Scanning&amp;quot;, 
&amp;quot;progress&amp;quot;: 0.5, &amp;quot;time&amp;quot;: 140.0},            
],                    &amp;quot;checkpoints&amp;quot;: [                        
{&amp;quot;progress&amp;quot;: 0.5, &amp;quot;time&amp;quot;: 145.0, 
&amp;quot;eta&amp;quot;: 150.0},                    ],                }         
],        }            clock = _LinearClock(start=100.0, step=5.0)&amp;gt;      
result = rendering.simulate_progress_rendering(                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            harness,            summary,   
errors=[&amp;quot;&amp;lt;boom &amp;amp; fail&amp;gt;&amp;quot;],            
clock=clock,        )E       AttributeError: module 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; has no attribute 
&amp;#x27;simulate_progress_rendering&amp;#x27;/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/interface/test_webui_simulations_fast.py:84: 
AttributeError__________ 
test_rendering_simulation_handles_nested_summary_and_clock __________    
@pytest.mark.fast    def 
test_rendering_simulation_handles_nested_summary_and_clock() -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;simulate_progress_rendering formats nested history
with a scripted clock.&amp;quot;&amp;quot;&amp;quot;            stub = 
BehaviorStreamlitStub()        harness = RenderHarness(stub)        primary = 
stub.container()            summary = {            
&amp;quot;description&amp;quot;: &amp;quot;Main 
&amp;lt;summary&amp;gt;&amp;quot;,            &amp;quot;progress&amp;quot;: 0.5,
&amp;quot;eta&amp;quot;: 250.0,            &amp;quot;remaining&amp;quot;: 120.0,
&amp;quot;elapsed&amp;quot;: 80.0,            &amp;quot;history&amp;quot;: [    
{&amp;quot;status&amp;quot;: &amp;quot;Queued &amp;lt;1&amp;gt;&amp;quot;, 
&amp;quot;progress&amp;quot;: 0.1, &amp;quot;time&amp;quot;: 50.0},             
{                    &amp;quot;status&amp;quot;: &amp;quot;Processing&amp;quot;,
&amp;quot;completed&amp;quot;: 30,                    &amp;quot;total&amp;quot;:
100,                    &amp;quot;time&amp;quot;: 70.0,                },       
],            &amp;quot;checkpoints&amp;quot;: [                
{&amp;quot;progress&amp;quot;: 0.25, &amp;quot;time&amp;quot;: 60.0, 
&amp;quot;eta&amp;quot;: 250.0},                {&amp;quot;completed&amp;quot;: 
80, &amp;quot;total&amp;quot;: 100, &amp;quot;time&amp;quot;: 80.0, 
&amp;quot;eta&amp;quot;: 250.0},            ],            
&amp;quot;subtasks_detail&amp;quot;: [                {                    
&amp;quot;description&amp;quot;: &amp;quot;stage 
&amp;lt;alpha&amp;gt;&amp;quot;,                    
&amp;quot;progress&amp;quot;: 0.75,                    
&amp;quot;status&amp;quot;: &amp;quot;Working &amp;lt;soon&amp;gt;&amp;quot;,   
&amp;quot;history&amp;quot;: [                        
{&amp;quot;status&amp;quot;: &amp;quot;Primed &amp;lt;1&amp;gt;&amp;quot;, 
&amp;quot;progress&amp;quot;: 0.25, &amp;quot;time&amp;quot;: 40.0},            
{                            &amp;quot;status&amp;quot;: 
&amp;quot;Working&amp;quot;,                            
&amp;quot;completed&amp;quot;: 30,                            
&amp;quot;total&amp;quot;: 40,                            
&amp;quot;time&amp;quot;: 70.0,                        },                    ], 
&amp;quot;checkpoints&amp;quot;: [                        
{&amp;quot;progress&amp;quot;: 0.5, &amp;quot;time&amp;quot;: 45.0, 
&amp;quot;eta&amp;quot;: 250.0},                    ],                },        
{                    &amp;quot;description&amp;quot;: &amp;quot;stage 
beta&amp;quot;,                    &amp;quot;completed&amp;quot;: 20,           
&amp;quot;total&amp;quot;: 40,                    &amp;quot;status&amp;quot;: 
&amp;quot;Queued&amp;quot;,                    &amp;quot;history&amp;quot;: [   
{                            &amp;quot;status&amp;quot;: 
&amp;quot;Queued&amp;quot;,                            
&amp;quot;completed&amp;quot;: 10,                            
&amp;quot;total&amp;quot;: 40,                            
&amp;quot;time&amp;quot;: 65.0,                        }                    ],  
&amp;quot;checkpoints&amp;quot;: [                        
{&amp;quot;completed&amp;quot;: 20, &amp;quot;total&amp;quot;: 40, 
&amp;quot;time&amp;quot;: 75.0, &amp;quot;eta&amp;quot;: 250.0},                
],                },            ],        }            clock = 
_LinearClock(start=200.0, step=0.0)&amp;gt;       result = 
rendering.simulate_progress_rendering(                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^            harness,            summary,   
container=primary,            
errors=[&amp;quot;&amp;lt;primary&amp;gt;&amp;quot;, 
&amp;quot;&amp;lt;secondary&amp;gt;&amp;quot;],            clock=clock,        
)E       AttributeError: module 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; has no attribute 
&amp;#x27;simulate_progress_rendering&amp;#x27;/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/interface/test_webui_simulations_fast.py:170: 
AttributeError____________ test_ui_progress_simulation_drives_eta_and_completion
_____________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144ef7230&amp;gt;    @pytest.mark.fast    def 
test_ui_progress_simulation_drives_eta_and_completion(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;WebUI._UIProgress under a stubbed Streamlit emits 
ETA and success messages.&amp;quot;&amp;quot;&amp;quot;            stub = 
BehaviorStreamlitStub()        monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub, raising=False)        monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub, raising=False)            clock_values = 
iter([0.0, 2.0, 4.0, 6.5, 9.0, 12.0, 15.5, 18.0, 20.0])            def 
fake_time() -&amp;gt; float:            try:                return 
float(next(clock_values))            except StopIteration:                return
20.0    &amp;gt;       monkeypatch.setattr(webui.time, &amp;quot;time&amp;quot;,
fake_time, raising=False)                            ^^^^^^^^^^E       
AttributeError: module &amp;#x27;devsynth.interface.webui&amp;#x27; has no 
attribute 
&amp;#x27;time&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/interface/test_webui_simulations_fast.py:244: 
AttributeError__________________ test_webui_display_result_sanitises_error 
___________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x144ef6d80&amp;gt;    @pytest.mark.fast    def 
test_webui_display_result_sanitises_error(monkeypatch: pytest.MonkeyPatch) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;display_result escapes 
markup before routing to Streamlit error channel.&amp;quot;&amp;quot;&amp;quot; 
stub = BehaviorStreamlitStub()        monkeypatch.setattr(webui, 
&amp;quot;st&amp;quot;, stub, raising=False)        monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub, raising=False)    &amp;gt;       ui = 
webui.WebUI()             ^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;NoneType&amp;#x27; object is not 
callable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfac
e/test_webui_simulations_fast.py:279: TypeError______________________ 
test_webui_require_streamlit_cache ______________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x144ef69c0&amp;gt;    
@pytest.mark.fast    def test_webui_require_streamlit_cache(monkeypatch: 
pytest.MonkeyPatch) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;The 
WebUI lazy loader imports Streamlit once and caches the 
module.&amp;quot;&amp;quot;&amp;quot;            sentinel = object()        
calls: list = []            def fake_import(name: str) -&amp;gt; object:        
calls.append(name)            return sentinel            
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None, raising=False) 
monkeypatch.setattr(importlib, &amp;quot;import_module&amp;quot;, fake_import)  
&amp;gt;       loaded = webui._require_streamlit()                 
^^^^^^^^^^^^^^^^^^^^^^^^E       AttributeError: module 
&amp;#x27;devsynth.interface.webui&amp;#x27; has no attribute 
&amp;#x27;_require_streamlit&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/unit/interface/test_webui_simulations_fast.py:336: 
AttributeError____________ test_webui_require_streamlit_reports_install_guidance
_____________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144ad5e20&amp;gt;    @pytest.mark.fast    def 
test_webui_require_streamlit_reports_install_guidance(        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Missing Streamlit surfaces actionable 
guidance.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:356: 
AttributeError__ test_webui_display_result_sanitizes_without_streamlit 
__monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145345e80&amp;gt;branch = &amp;#x27;error&amp;#x27;, kwargs = 
{&amp;#x27;message_type&amp;#x27;: &amp;#x27;error&amp;#x27;}, expected_method =
&amp;#x27;error&amp;#x27;    @pytest.mark.fast    @pytest.mark.parametrize(     
(&amp;quot;branch&amp;quot;, &amp;quot;kwargs&amp;quot;, 
&amp;quot;expected_method&amp;quot;),        [            
(&amp;quot;error&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;error&amp;quot;}, &amp;quot;error&amp;quot;),            
(&amp;quot;warning&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;warning&amp;quot;}, &amp;quot;warning&amp;quot;),            
(&amp;quot;success&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;success&amp;quot;}, &amp;quot;success&amp;quot;),            
(&amp;quot;highlight&amp;quot;, {&amp;quot;highlight&amp;quot;: True}, 
&amp;quot;info&amp;quot;),        ],    )    def 
test_webui_display_result_sanitizes_without_streamlit(        monkeypatch: 
pytest.MonkeyPatch,        branch: str,        kwargs: dict,        
expected_method: str,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`WebUI.display_result` routes sanitized text 
through the stubbed Streamlit API.&amp;quot;&amp;quot;&amp;quot;            stub
= _WebUIStreamlitStub()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:411: 
AttributeError_ test_webui_display_result_sanitizes_without_streamlit 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144591040&amp;gt;branch = &amp;#x27;warning&amp;#x27;, kwargs = 
{&amp;#x27;message_type&amp;#x27;: &amp;#x27;warning&amp;#x27;}expected_method =
&amp;#x27;warning&amp;#x27;    @pytest.mark.fast    @pytest.mark.parametrize(   
(&amp;quot;branch&amp;quot;, &amp;quot;kwargs&amp;quot;, 
&amp;quot;expected_method&amp;quot;),        [            
(&amp;quot;error&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;error&amp;quot;}, &amp;quot;error&amp;quot;),            
(&amp;quot;warning&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;warning&amp;quot;}, &amp;quot;warning&amp;quot;),            
(&amp;quot;success&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;success&amp;quot;}, &amp;quot;success&amp;quot;),            
(&amp;quot;highlight&amp;quot;, {&amp;quot;highlight&amp;quot;: True}, 
&amp;quot;info&amp;quot;),        ],    )    def 
test_webui_display_result_sanitizes_without_streamlit(        monkeypatch: 
pytest.MonkeyPatch,        branch: str,        kwargs: dict,        
expected_method: str,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`WebUI.display_result` routes sanitized text 
through the stubbed Streamlit API.&amp;quot;&amp;quot;&amp;quot;            stub
= _WebUIStreamlitStub()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:411: 
AttributeError_ test_webui_display_result_sanitizes_without_streamlit 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144593d10&amp;gt;branch = &amp;#x27;success&amp;#x27;, kwargs = 
{&amp;#x27;message_type&amp;#x27;: &amp;#x27;success&amp;#x27;}expected_method =
&amp;#x27;success&amp;#x27;    @pytest.mark.fast    @pytest.mark.parametrize(   
(&amp;quot;branch&amp;quot;, &amp;quot;kwargs&amp;quot;, 
&amp;quot;expected_method&amp;quot;),        [            
(&amp;quot;error&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;error&amp;quot;}, &amp;quot;error&amp;quot;),            
(&amp;quot;warning&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;warning&amp;quot;}, &amp;quot;warning&amp;quot;),            
(&amp;quot;success&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;success&amp;quot;}, &amp;quot;success&amp;quot;),            
(&amp;quot;highlight&amp;quot;, {&amp;quot;highlight&amp;quot;: True}, 
&amp;quot;info&amp;quot;),        ],    )    def 
test_webui_display_result_sanitizes_without_streamlit(        monkeypatch: 
pytest.MonkeyPatch,        branch: str,        kwargs: dict,        
expected_method: str,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`WebUI.display_result` routes sanitized text 
through the stubbed Streamlit API.&amp;quot;&amp;quot;&amp;quot;            stub
= _WebUIStreamlitStub()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:411: 
AttributeError_ test_webui_display_result_sanitizes_without_streamlit 
_monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x144592e70&amp;gt;branch = &amp;#x27;highlight&amp;#x27;, kwargs = 
{&amp;#x27;highlight&amp;#x27;: True}, expected_method = 
&amp;#x27;info&amp;#x27;    @pytest.mark.fast    @pytest.mark.parametrize(      
(&amp;quot;branch&amp;quot;, &amp;quot;kwargs&amp;quot;, 
&amp;quot;expected_method&amp;quot;),        [            
(&amp;quot;error&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;error&amp;quot;}, &amp;quot;error&amp;quot;),            
(&amp;quot;warning&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;warning&amp;quot;}, &amp;quot;warning&amp;quot;),            
(&amp;quot;success&amp;quot;, {&amp;quot;message_type&amp;quot;: 
&amp;quot;success&amp;quot;}, &amp;quot;success&amp;quot;),            
(&amp;quot;highlight&amp;quot;, {&amp;quot;highlight&amp;quot;: True}, 
&amp;quot;info&amp;quot;),        ],    )    def 
test_webui_display_result_sanitizes_without_streamlit(        monkeypatch: 
pytest.MonkeyPatch,        branch: str,        kwargs: dict,        
expected_method: str,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;`WebUI.display_result` routes sanitized text 
through the stubbed Streamlit API.&amp;quot;&amp;quot;&amp;quot;            stub
= _WebUIStreamlitStub()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:411: 
AttributeError______________________ test_webui_ui_progress_eta_formats 
______________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1444dc2c0&amp;gt;    @pytest.mark.fast    def 
test_webui_ui_progress_eta_formats(monkeypatch: pytest.MonkeyPatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;The Streamlit UI progress indicator 
formats ETA strings across ranges.&amp;quot;&amp;quot;&amp;quot;            stub
= _WebUIStreamlitStub()&amp;gt;       monkeypatch.setattr(webui, 
&amp;quot;_STREAMLIT&amp;quot;, stub)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.interface.webui&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_free_regressions.py:521: 
AttributeError_______________ test_missing_streamlit_surfaces_install_guidance 
_______________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14445d550&amp;gt;    def test_missing_streamlit_surfaces_install_guidance(    
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Import failures raise DevSynthError with 
installation instructions.&amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(webui, &amp;quot;_STREAMLIT&amp;quot;, None)E       
AttributeError: &amp;lt;module &amp;#x27;devsynth.interface.webui&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/inte
rface/webui/__init__.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_STREAMLIT&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/interface/test_webui_streamlit_stub.py:248: 
AttributeError__________ 
TestProjectSetupPages.test_project_setup_pages_inheritance __________self = 
&amp;lt;test_rendering.TestProjectSetupPages object at 0x1238199a0&amp;gt;    
def test_project_setup_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that ProjectSetupPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/webui/test_rendering.py:98: ImportError_____________ 
TestLifecyclePages.test_lifecycle_pages_inheritance ______________self = 
&amp;lt;test_rendering.TestLifecyclePages object at 0x12381a3f0&amp;gt;    def 
test_lifecycle_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that LifecyclePages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/webui/test_rendering.py:129: ImportError____________ 
TestOperationsPages.test_operations_pages_inheritance _____________self = 
&amp;lt;test_rendering.TestOperationsPages object at 0x12381b200&amp;gt;    def 
test_operations_pages_inheritance(self):        
&amp;quot;&amp;quot;&amp;quot;Test that OperationsPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/webui/test_rendering.py:161: ImportError_______________ 
TestSupportPages.test_support_pages_inheritance ________________self = 
&amp;lt;test_rendering.TestSupportPages object at 0x12382c560&amp;gt;    def 
test_support_pages_inheritance(self):        &amp;quot;&amp;quot;&amp;quot;Test 
that SupportPages inherits from 
CommandHandlingMixin.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       from 
devsynth.interface.webui.commands import CommandHandlingMixinE       
ImportError: cannot import name &amp;#x27;CommandHandlingMixin&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.commands&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/webui/test_rendering.py:192: ImportError________ 
TestWebUIRenderingUtilities.test_rendering_import_dependencies ________self = 
&amp;lt;test_rendering.TestWebUIRenderingUtilities object at 0x12382dfa0&amp;gt;
def test_rendering_import_dependencies(self):        
&amp;quot;&amp;quot;&amp;quot;Test that rendering imports work 
correctly.&amp;quot;&amp;quot;&amp;quot;        # Test that key imports are 
available&amp;gt;       from devsynth.interface.webui.rendering import (        
LifecyclePages,            OperationsPages,            PageRenderer,            
ProjectSetupPages,            SupportPages,        )E       ImportError: cannot 
import name &amp;#x27;LifecyclePages&amp;#x27; from 
&amp;#x27;devsynth.interface.webui.rendering&amp;#x27; (unknown 
location)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interfa
ce/webui/test_rendering.py:295: ImportError__ 
TestLMStudioProviderAvailabilityProbing.test_server_availability_detection 
__self = &amp;lt;MagicMock 
name=&amp;#x27;_require_lmstudio().sync_api.models.list&amp;#x27; 
id=&amp;#x27;5454249472&amp;#x27;&amp;gt;    def assert_called_once(self):      
&amp;quot;&amp;quot;&amp;quot;assert that the mock was called only once.        
&amp;quot;&amp;quot;&amp;quot;        if not self.call_count == 1:            
msg = (&amp;quot;Expected &amp;#x27;%s&amp;#x27; to have been called once. 
Called %s times.%s&amp;quot;                   % (self._mock_name or 
&amp;#x27;mock&amp;#x27;,                      self.call_count,                 
self._calls_repr()))&amp;gt;           raise AssertionError(msg)E           
AssertionError: Expected &amp;#x27;list&amp;#x27; to have been called once. 
Called 0 
times./opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versi
ons/3.12/lib/python3.12/unittest/mock.py:928: AssertionErrorDuring handling of 
the above exception, another exception occurred:self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x123842c00&amp;gt;    @pytest.mark.fast    def 
test_server_availability_detection(self):        
&amp;quot;&amp;quot;&amp;quot;Test detection of LM Studio server 
availability.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                # Mock successful model
list response            mock_lmstudio.sync_api.models.list.return_value = [    
{&amp;quot;id&amp;quot;: &amp;quot;model1&amp;quot;, &amp;quot;object&amp;quot;:
&amp;quot;model&amp;quot;},                {&amp;quot;id&amp;quot;: 
&amp;quot;model2&amp;quot;, &amp;quot;object&amp;quot;: 
&amp;quot;model&amp;quot;},            ]                provider = 
LMStudioProvider(config)                # Should probe server availability on 
initialization&amp;gt;           
mock_lmstudio.sync_api.models.list.assert_called_once()E           
AssertionError: Expected &amp;#x27;list&amp;#x27; to have been called once. 
Called 0 
times./Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_l
mstudio_provider.py:470: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:27:29,488 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,492 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,496 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,497 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,499 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,499 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507___ 
TestLMStudioProviderAvailabilityProbing.test_server_unavailable_handling ___self
= &amp;lt;test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object 
at 0x123843020&amp;gt;    @pytest.mark.fast    def 
test_server_unavailable_handling(self):        
&amp;quot;&amp;quot;&amp;quot;Test handling when LM Studio server is 
unavailable.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                # Mock connection error
from requests.exceptions import ConnectionError                
mock_lmstudio.sync_api.models.list.side_effect = ConnectionError(               
&amp;quot;Connection refused&amp;quot;            )                provider = 
LMStudioProvider(config)                # Should handle gracefully and set 
server_unavailable flag&amp;gt;           assert hasattr(provider, 
&amp;quot;server_unavailable&amp;quot;)E           AssertionError: assert FalseE
+  where False = 
hasattr(&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider 
object at 0x1450a0c20&amp;gt;, 
&amp;#x27;server_unavailable&amp;#x27;)/Users/caitlyn/Projects/github.com/raveno
ak/devsynth/tests/unit/llm/test_lmstudio_provider.py:493: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,556 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,559 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,562 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,562 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,564 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,565 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507______ 
TestLMStudioProviderAvailabilityProbing.test_model_list_retrieval _______self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x1238434d0&amp;gt;    @pytest.mark.fast    def test_model_list_retrieval(self):
&amp;quot;&amp;quot;&amp;quot;Test retrieval of available models 
list.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
available_models = [            {&amp;quot;id&amp;quot;: 
&amp;quot;llama-2-7b&amp;quot;, &amp;quot;object&amp;quot;: 
&amp;quot;model&amp;quot;},            {&amp;quot;id&amp;quot;: 
&amp;quot;codellama-7b&amp;quot;, &amp;quot;object&amp;quot;: 
&amp;quot;model&amp;quot;},            {&amp;quot;id&amp;quot;: 
&amp;quot;mistral-7b&amp;quot;, &amp;quot;object&amp;quot;: 
&amp;quot;model&amp;quot;},        ]            with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                
mock_lmstudio.sync_api.models.list.return_value = available_models              
provider = LMStudioProvider(config)                # Should retrieve and store 
model list&amp;gt;           models = provider.get_available_models()           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: 
&amp;#x27;LMStudioProvider&amp;#x27; object has no attribute 
&amp;#x27;get_available_models&amp;#x27;. Did you mean: 
&amp;#x27;list_available_models&amp;#x27;?/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/llm/test_lmstudio_provider.py:518: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,584 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,587 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,591 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,591 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,593 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,593 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507______ 
TestLMStudioProviderConfiguration.test_configuration_with_defaults ______self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderConfiguration object at 
0x123843ad0&amp;gt;    @pytest.mark.fast    def 
test_configuration_with_defaults(self):        
&amp;quot;&amp;quot;&amp;quot;Test configuration with default 
values.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                provider = 
LMStudioProvider(config)                assert provider.temperature == 0.7  # 
Default&amp;gt;           assert provider.max_tokens == 4096  # Default         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E           assert 1024 == 4096E            + 
where 1024 = &amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider
object at 
0x145385550&amp;gt;.max_tokens/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/llm/test_lmstudio_provider.py:561: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,658 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,662 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,664 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,665 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,668 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,668 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_______ 
TestLMStudioProviderErrorHandling.test_invalid_temperature_range _______self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderErrorHandling object at 
0x123861700&amp;gt;    @pytest.mark.fast    def 
test_invalid_temperature_range(self):        &amp;quot;&amp;quot;&amp;quot;Test 
error handling for invalid temperature range.&amp;quot;&amp;quot;&amp;quot;     
config = {&amp;quot;base_url&amp;quot;: 
&amp;quot;http://localhost:1234/v1&amp;quot;}            with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                provider = 
LMStudioProvider(config)                with pytest.raises(DevSynthError) as 
exc_info:                provider.generate(&amp;quot;Hello&amp;quot;, 
{&amp;quot;temperature&amp;quot;: -0.1})    &amp;gt;           assert 
&amp;quot;temperature must be between&amp;quot; in str(exc_info.value)E         
AssertionError: assert &amp;#x27;temperature must be between&amp;#x27; in 
&amp;#x27;LM Studio API error: Network access disabled during tests (httpx). 
Check that LM Studio is running and accessible.&amp;#x27;E            +  where 
&amp;#x27;LM Studio API error: Network access disabled during tests (httpx). 
Check that LM Studio is running and accessible.&amp;#x27; = 
str(LMStudioConnectionError(&amp;#x27;LM Studio API error: Network access 
disabled during tests (httpx). Check that LM Studio is running and 
accessible.&amp;#x27;))E            +    where 
LMStudioConnectionError(&amp;#x27;LM Studio API error: Network access disabled 
during tests (httpx). Check that LM Studio is running and accessible.&amp;#x27;)
= &amp;lt;ExceptionInfo LMStudioConnectionError(&amp;#x27;LM Studio API error: 
Network access disabled during tests (httpx). Check that LM Studio is running 
and accessible.&amp;#x27;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_lmstudio_provider.py:684: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,801 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,804 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,807 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,807 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,809 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,809 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-25072025-10-28 09:27:29,811 - 
AsyncWebsocketThread - INFO - {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x1451e1f10&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-32&amp;quot;}2025-10-28 09:27:29,820 - 
devsynth.application.llm.lmstudio_provider - ERROR - LM Studio API error: 
Network access disabled during tests (httpx). Check that LM Studio is running 
and accessible.------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507INFO     
AsyncWebsocketThread:_ws_thread.py:138 {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x1451e1f10&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-32&amp;quot;}ERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio API 
error: Network access disabled during tests (httpx). Check that LM Studio is 
running and accessible.__________ 
TestLMStudioProviderErrorHandling.test_invalid_max_tokens ___________self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderErrorHandling object at 
0x123861b50&amp;gt;    @pytest.mark.fast    def test_invalid_max_tokens(self):  
&amp;quot;&amp;quot;&amp;quot;Test error handling for invalid 
max_tokens.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                provider = 
LMStudioProvider(config)                with pytest.raises(DevSynthError) as 
exc_info:                provider.generate(&amp;quot;Hello&amp;quot;, 
{&amp;quot;max_tokens&amp;quot;: 0})    &amp;gt;           assert 
&amp;quot;max_tokens must be positive&amp;quot; in str(exc_info.value)E         
AssertionError: assert &amp;#x27;max_tokens must be positive&amp;#x27; in 
&amp;#x27;LM Studio API error: Network access disabled during tests (httpx). 
Check that LM Studio is running and accessible.&amp;#x27;E            +  where 
&amp;#x27;LM Studio API error: Network access disabled during tests (httpx). 
Check that LM Studio is running and accessible.&amp;#x27; = 
str(LMStudioConnectionError(&amp;#x27;LM Studio API error: Network access 
disabled during tests (httpx). Check that LM Studio is running and 
accessible.&amp;#x27;))E            +    where 
LMStudioConnectionError(&amp;#x27;LM Studio API error: Network access disabled 
during tests (httpx). Check that LM Studio is running and accessible.&amp;#x27;)
= &amp;lt;ExceptionInfo LMStudioConnectionError(&amp;#x27;LM Studio API error: 
Network access disabled during tests (httpx). Check that LM Studio is running 
and accessible.&amp;#x27;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_lmstudio_provider.py:702: 
AssertionError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:29,840 - AsyncWebsocketThread - 
INFO - {&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client 
object at 0x1451e1f10&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: 
&amp;quot;Websocket thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-32&amp;quot;}------------------------------ Captured log setup 
------------------------------INFO     AsyncWebsocketThread:_ws_thread.py:149 
{&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client object 
at 0x1451e1f10&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket
thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-32&amp;quot;}----------------------------- Captured stdout call
-----------------------------2025-10-28 09:27:29,849 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,854 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,856 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,856 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,858 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,858 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-25072025-10-28 09:27:29,859 - 
AsyncWebsocketThread - INFO - {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x1451e3680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-33&amp;quot;}2025-10-28 09:27:29,866 - 
devsynth.application.llm.lmstudio_provider - ERROR - LM Studio API error: 
Network access disabled during tests (httpx). Check that LM Studio is running 
and accessible.------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507INFO     
AsyncWebsocketThread:_ws_thread.py:138 {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x1451e3680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-33&amp;quot;}ERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio API 
error: Network access disabled during tests (httpx). Check that LM Studio is 
running and accessible._________ 
TestLMStudioProviderEdgeCases.test_empty_model_list_handling _________self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x123862a50&amp;gt;    @pytest.mark.fast    def 
test_empty_model_list_handling(self):        &amp;quot;&amp;quot;&amp;quot;Test 
handling of empty model list.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                # Mock empty model list
mock_lmstudio.sync_api.models.list.return_value = []                provider = 
LMStudioProvider(config)                # Should handle empty model list 
gracefully&amp;gt;           models = provider.get_available_models()           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: 
&amp;#x27;LMStudioProvider&amp;#x27; object has no attribute 
&amp;#x27;get_available_models&amp;#x27;. Did you mean: 
&amp;#x27;list_available_models&amp;#x27;?/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/llm/test_lmstudio_provider.py:774: 
AttributeError---------------------------- Captured stdout setup 
-----------------------------2025-10-28 09:27:29,885 - AsyncWebsocketThread - 
INFO - {&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client 
object at 0x1451e3680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: 
&amp;quot;Websocket thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-33&amp;quot;}------------------------------ Captured log setup 
------------------------------INFO     AsyncWebsocketThread:_ws_thread.py:149 
{&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client object 
at 0x1451e3680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket
thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-33&amp;quot;}----------------------------- Captured stdout call
-----------------------------2025-10-28 09:27:29,892 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,895 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,896 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,897 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,898 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,898 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_____________ 
TestLMStudioProviderEdgeCases.test_timeout_handling ______________self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x123862e70&amp;gt;    @pytest.mark.fast    def test_timeout_handling(self):    
&amp;quot;&amp;quot;&amp;quot;Test timeout handling during model 
listing.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                # Mock timeout during 
model listing            import requests                
mock_lmstudio.sync_api.models.list.side_effect = (                
requests.exceptions.Timeout(&amp;quot;Request timed out&amp;quot;)            ) 
provider = LMStudioProvider(config)                # Should handle timeout 
gracefully&amp;gt;           models = provider.get_available_models()           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E           AttributeError: 
&amp;#x27;LMStudioProvider&amp;#x27; object has no attribute 
&amp;#x27;get_available_models&amp;#x27;. Did you mean: 
&amp;#x27;list_available_models&amp;#x27;?/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/llm/test_lmstudio_provider.py:798: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,921 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,924 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,925 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,926 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,927 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,927 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507------------------------------ 
Captured log call -------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507_________ 
TestLMStudioProviderEdgeCases.test_unicode_content_handling __________self = 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x145434380&amp;gt;prompt = &amp;#x27;Say hello in multiple languages&amp;#x27;,
parameters = None    def generate(self, prompt: str, parameters: Dict = None) 
-&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;Generate text from a prompt 
using LM Studio.            Args:            prompt: The prompt to generate text
from            parameters: Additional parameters for the generation            
Returns:            The generated text            Raises:            
LMStudioConnectionError: If there&amp;#x27;s an issue connecting to LM Studio   
LMStudioModelError: If there&amp;#x27;s an issue with the model or response     
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            params = {            
&amp;quot;temperature&amp;quot;: self.temperature,            
&amp;quot;maxTokens&amp;quot;: self.max_tokens,        }        if parameters:  
params.update(parameters)            try:            result = 
self._execute_with_resilience(&amp;gt;               
self._lmstudio.llm(self.model).complete,                
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                prompt,                
config=params,            
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/lmstudio_provider.py:500: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:63: in __call__    return getattr(real, 
self._attr)(*args, **kwargs)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.
12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: in
inner    return func(*args, **kwds)           
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/li
b/python3.12/site-packages/lmstudio/sync_api.py:1753: in llm    return 
get_default_client().llm.model(model_key, ttl=ttl, config=config)           
^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.f
ramework/Versions/3.12/lib/python3.12/contextlib.py:81: in inner    return 
func(*args, **kwds)           
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/li
b/python3.12/site-packages/lmstudio/sync_api.py:1726: in get_default_client    
_default_client._ensure_api_host_is_valid()/Users/caitlyn/Projects/github.com/ra
venoak/devsynth/.venv/lib/python3.12/site-packages/lmstudio/sync_api.py:1598: in
_ensure_api_host_is_valid    api_host = self.find_default_local_api_host()      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frame
works/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: in inner  
return func(*args, **kwds)           
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/li
b/python3.12/site-packages/lmstudio/sync_api.py:1591: in 
find_default_local_api_host    if cls.is_valid_api_host(api_host):       
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Framewor
ks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: in inner    
return func(*args, **kwds)           
^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/li
b/python3.12/site-packages/lmstudio/sync_api.py:1581: in is_valid_api_host    
probe_response = cls._query_probe_url(probe_url)                     
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/.venv/lib/python3.12/site-packages/lmstudio/sync_api.py:1573: in 
_query_probe_url    return httpx.get(url, timeout=1)           
^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.v
env/lib/python3.12/site-packages/httpx/_api.py:195: in get    return 
request(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.1
2/site-packages/httpx/_api.py:109: in request    return client.request(_ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;httpx.Client object at 0x1454f9790&amp;gt;, method = 
&amp;#x27;GET&amp;#x27;url = 
&amp;#x27;http://127.0.0.1:41343/lmstudio-greeting&amp;#x27;, args = ()kwargs = 
{&amp;#x27;auth&amp;#x27;: None, &amp;#x27;content&amp;#x27;: None, 
&amp;#x27;data&amp;#x27;: None, &amp;#x27;files&amp;#x27;: None, ...}host = 
None, host_str = &amp;#x27;&amp;#x27;    def guard_httpx_request(self, method: 
str, url, *args: Any, **kwargs: Any):  # type: ignore        # Allow in-memory 
TestClient requests against the ASGI test server        try:            host = 
getattr(url, &amp;quot;host&amp;quot;, None) or getattr(url, 
&amp;quot;netloc&amp;quot;, None)            if isinstance(host, bytes):        
host = host.decode(&amp;quot;utf-8&amp;quot;, &amp;quot;ignore&amp;quot;)       
host_str = str(host or &amp;quot;&amp;quot;)            if 
host_str.split(&amp;quot;:&amp;quot;)[0] == &amp;quot;testserver&amp;quot;:     
return _orig_client_request(self, method, url, *args, **kwargs)        except 
Exception:            host_str = str(url)            if 
host_str.startswith(&amp;quot;http://testserver&amp;quot;) or 
host_str.startswith(                &amp;quot;https://testserver&amp;quot;      
):                return _orig_client_request(self, method, url, *args, 
**kwargs)&amp;gt;       raise RuntimeError(&amp;quot;Network access disabled 
during tests (httpx)&amp;quot;)E       RuntimeError: Network access disabled 
during tests 
(httpx)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/netwo
rking.py:101: RuntimeErrorDuring handling of the above exception, another 
exception occurred:self = 
&amp;lt;test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x123863320&amp;gt;    @pytest.mark.fast    def 
test_unicode_content_handling(self):        &amp;quot;&amp;quot;&amp;quot;Test 
handling of Unicode content.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;base_url&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;}    
unicode_response = {            &amp;quot;id&amp;quot;: 
&amp;quot;chatcmpl-unicode&amp;quot;,            &amp;quot;object&amp;quot;: 
&amp;quot;chat.completion&amp;quot;,            &amp;quot;created&amp;quot;: 
1677652288,            &amp;quot;model&amp;quot;: 
&amp;quot;test-model&amp;quot;,            &amp;quot;choices&amp;quot;: [       
{                    &amp;quot;index&amp;quot;: 0,                    
&amp;quot;message&amp;quot;: {                        &amp;quot;role&amp;quot;: 
&amp;quot;assistant&amp;quot;,                        
&amp;quot;content&amp;quot;: &amp;quot;Hello! ! Hola! &amp;quot;,        
},                    &amp;quot;finish_reason&amp;quot;: 
&amp;quot;stop&amp;quot;,                }            ],            
&amp;quot;usage&amp;quot;: {&amp;quot;prompt_tokens&amp;quot;: 5, 
&amp;quot;completion_tokens&amp;quot;: 10, &amp;quot;total_tokens&amp;quot;: 
15},        }            with patch(            
&amp;quot;devsynth.application.llm.lmstudio_provider._require_lmstudio&amp;quot;
) as mock_require:            mock_lmstudio = MagicMock()            
mock_require.return_value = mock_lmstudio                
mock_lmstudio.sync_api.chat.completions.create.return_value = (                
unicode_response            )                provider = 
LMStudioProvider(config)&amp;gt;           response = 
provider.generate(&amp;quot;Say hello in multiple languages&amp;quot;)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/gith
ub.com/ravenoak/devsynth/tests/unit/llm/test_lmstudio_provider.py:835: _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x145434380&amp;gt;prompt = &amp;#x27;Say hello in multiple languages&amp;#x27;,
parameters = None    def generate(self, prompt: str, parameters: Dict = None) 
-&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;Generate text from a prompt 
using LM Studio.            Args:            prompt: The prompt to generate text
from            parameters: Additional parameters for the generation            
Returns:            The generated text            Raises:            
LMStudioConnectionError: If there&amp;#x27;s an issue connecting to LM Studio   
LMStudioModelError: If there&amp;#x27;s an issue with the model or response     
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            params = {            
&amp;quot;temperature&amp;quot;: self.temperature,            
&amp;quot;maxTokens&amp;quot;: self.max_tokens,        }        if parameters:  
params.update(parameters)            try:            result = 
self._execute_with_resilience(                
self._lmstudio.llm(self.model).complete,                prompt,                
config=params,            )            content = getattr(result, 
&amp;quot;content&amp;quot;, None)            if isinstance(content, str) and 
content:                return content            raise 
LMStudioModelError(&amp;quot;Invalid response from LM Studio&amp;quot;)        
except LMStudioModelError:            raise        except 
LMStudioTokenLimitError:            raise  # Re-raise token limit errors as-is  
except LMStudioConnectionError:            raise  # Re-raise connection errors 
as-is        except Exception as e:  # noqa: BLE001            error_msg = 
f&amp;quot;LM Studio API error: {str(e)}. Check that LM Studio is running and 
accessible.&amp;quot;            logger.error(error_msg)&amp;gt;           raise
LMStudioConnectionError(error_msg)E           
devsynth.application.llm.lmstudio_provider.LMStudioConnectionError: LM Studio 
API error: Network access disabled during tests (httpx). Check that LM Studio is
running and 
accessible./Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/app
lication/llm/lmstudio_provider.py:517: 
LMStudioConnectionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:29,954 - 
devsynth.application.llm.lmstudio_provider - INFO - LM Studio resource disabled;
skipping default client configuration2025-10-28 09:27:29,958 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:29,961 
- devsynth.application.llm.lmstudio_provider - ERROR - Failed to connect to LM 
Studio: Network access disabled during tests2025-10-28 09:27:29,961 - 
devsynth.application.llm.lmstudio_provider - WARNING - Could not verify 
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection2025-10-28 
09:27:29,963 - devsynth.application.llm.lmstudio_provider - ERROR - Failed to 
connect to LM Studio: Network access disabled during tests2025-10-28 
09:27:29,964 - devsynth.application.llm.lmstudio_provider - WARNING - Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-25072025-10-28 09:27:29,965 - 
AsyncWebsocketThread - INFO - {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x145434680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-34&amp;quot;}2025-10-28 09:27:29,974 - 
devsynth.application.llm.lmstudio_provider - ERROR - LM Studio API error: 
Network access disabled during tests (httpx). Check that LM Studio is running 
and accessible.------------------------------ Captured log call 
-------------------------------INFO     
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio 
resource disabled; skipping default client configurationWARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not verify
specified model &amp;#x27;stub-model&amp;#x27;: Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selectionERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed to 
connect to LM Studio: Network access disabled during testsWARNING  
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could not 
connect to LM Studio: Failed to connect to LM Studio: Network access disabled 
during tests. Using fallback: qwen/qwen3-4b-2507INFO     
AsyncWebsocketThread:_ws_thread.py:138 {&amp;quot;client&amp;quot;: 
&amp;quot;&amp;lt;lmstudio.sync_api.Client object at 
0x145434680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket 
handling thread started&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-34&amp;quot;}ERROR    
devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM Studio API 
error: Network access disabled during tests (httpx). Check that LM Studio is 
running and accessible.--------------------------- Captured stdout teardown 
---------------------------2025-10-28 09:27:30,124 - AsyncWebsocketThread - INFO
- {&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client object
at 0x145434680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket
thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-34&amp;quot;}---------------------------- Captured log teardown
-----------------------------INFO     AsyncWebsocketThread:_ws_thread.py:149 
{&amp;quot;client&amp;quot;: &amp;quot;&amp;lt;lmstudio.sync_api.Client object 
at 0x145434680&amp;gt;&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;Websocket
thread terminated&amp;quot;, &amp;quot;thread_id&amp;quot;: 
&amp;quot;Thread-34&amp;quot;}___ 
TestOpenAIProviderInitialization.test_initialization_with_default_model ____self
= &amp;lt;test_openai_provider.TestOpenAIProviderInitialization object at 
0x12387d7c0&amp;gt;    @pytest.mark.fast    def 
test_initialization_with_default_model(self):        
&amp;quot;&amp;quot;&amp;quot;Test initialization with default 
model.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}            provider 
= OpenAIProvider(config)    &amp;gt;       assert provider.model == 
&amp;quot;gpt-3.5-turbo&amp;quot;  # Default model        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: assert 
&amp;#x27;stub-model&amp;#x27; == &amp;#x27;gpt-3.5-turbo&amp;#x27;E         E  
- gpt-3.5-turboE         + 
stub-model/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/te
st_openai_provider.py:72: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:27:30,223 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:30,245 
- devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-model________ 
TestOpenAIProviderErrorHandling.test_invalid_temperature_range ________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderErrorHandling object at 
0x12387e6c0&amp;gt;    @pytest.mark.fast    def 
test_invalid_temperature_range(self):        &amp;quot;&amp;quot;&amp;quot;Test 
error handling for invalid temperature range.&amp;quot;&amp;quot;&amp;quot;     
config = {&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}        
provider = OpenAIProvider(config)            with pytest.raises(DevSynthError) 
as exc_info:            provider.generate(&amp;quot;Hello&amp;quot;, 
{&amp;quot;temperature&amp;quot;: -0.1})    &amp;gt;       assert 
&amp;quot;temperature must be between&amp;quot; in str(exc_info.value)E       
AssertionError: assert &amp;#x27;temperature must be between&amp;#x27; in 
&amp;#x27;OpenAI temperature must be a number between 0.0 and 2.0, got 
-0.1&amp;#x27;E        +  where &amp;#x27;OpenAI temperature must be a number 
between 0.0 and 2.0, got -0.1&amp;#x27; = 
str(OpenAIConfigurationError(&amp;#x27;OpenAI temperature must be a number 
between 0.0 and 2.0, got -0.1&amp;#x27;))E        +    where 
OpenAIConfigurationError(&amp;#x27;OpenAI temperature must be a number between 
0.0 and 2.0, got -0.1&amp;#x27;) = &amp;lt;ExceptionInfo 
OpenAIConfigurationError(&amp;#x27;OpenAI temperature must be a number between 
0.0 and 2.0, got -0.1&amp;#x27;) 
tblen=3&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openai_provider.py:462: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,334 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:30,354 
- devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-model___________ 
TestOpenAIProviderErrorHandling.test_invalid_max_tokens ____________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderErrorHandling object at 
0x12387eae0&amp;gt;    @pytest.mark.fast    def test_invalid_max_tokens(self):  
&amp;quot;&amp;quot;&amp;quot;Test error handling for invalid 
max_tokens.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}        provider = 
OpenAIProvider(config)            with pytest.raises(DevSynthError) as exc_info:
provider.generate(&amp;quot;Hello&amp;quot;, {&amp;quot;max_tokens&amp;quot;: 
0})    &amp;gt;       assert &amp;quot;max_tokens must be positive&amp;quot; in 
str(exc_info.value)E       AssertionError: assert &amp;#x27;max_tokens must be 
positive&amp;#x27; in &amp;#x27;OpenAI max_tokens must be a positive integer, 
got 0&amp;#x27;E        +  where &amp;#x27;OpenAI max_tokens must be a positive 
integer, got 0&amp;#x27; = str(OpenAIConfigurationError(&amp;#x27;OpenAI 
max_tokens must be a positive integer, got 0&amp;#x27;))E        +    where 
OpenAIConfigurationError(&amp;#x27;OpenAI max_tokens must be a positive integer,
got 0&amp;#x27;) = &amp;lt;ExceptionInfo 
OpenAIConfigurationError(&amp;#x27;OpenAI max_tokens must be a positive integer,
got 0&amp;#x27;) 
tblen=3&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openai_provider.py:473: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,385 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:30,407 
- devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-model______________ 
TestOpenAIProviderHeaders.test_correct_headers_set ______________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderHeaders object at 
0x12388e0c0&amp;gt;    @pytest.mark.fast    def test_correct_headers_set(self): 
&amp;quot;&amp;quot;&amp;quot;Test that correct headers are set for 
OpenAI.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}        provider = 
OpenAIProvider(config)            expected_headers = {            
&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;,         
&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer test-key&amp;quot;,        }
# Verify headers are set correctly        for key, value in 
expected_headers.items():&amp;gt;           assert provider.headers == value    
^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;OpenAIProvider&amp;#x27; 
object has no attribute 
&amp;#x27;headers&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/llm/test_openai_provider.py:653: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,744 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:30,772 
- devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-model_____________ 
TestOpenAIProviderHeaders.test_custom_api_key_header _____________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderHeaders object at 
0x12388e570&amp;gt;    @pytest.mark.fast    def 
test_custom_api_key_header(self):        &amp;quot;&amp;quot;&amp;quot;Test 
custom API key header configuration.&amp;quot;&amp;quot;&amp;quot;        config
= {&amp;quot;api_key&amp;quot;: &amp;quot;custom-key&amp;quot;}        provider 
= OpenAIProvider(config)    &amp;gt;       assert 
provider.headers[&amp;quot;Authorization&amp;quot;] == &amp;quot;Bearer 
custom-key&amp;quot;               ^^^^^^^^^^^^^^^^E       AttributeError: 
&amp;#x27;OpenAIProvider&amp;#x27; object has no attribute 
&amp;#x27;headers&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/llm/test_openai_provider.py:661: 
AttributeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,800 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:30,825 
- devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-model___________ 
TestOpenAIProviderEdgeCases.test_empty_response_handling ___________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderEdgeCases object at 
0x12388ea20&amp;gt;    @pytest.mark.fast    def 
test_empty_response_handling(self):        &amp;quot;&amp;quot;&amp;quot;Test 
handling of empty responses.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}            
empty_response = {            &amp;quot;id&amp;quot;: 
&amp;quot;chatcmpl-empty&amp;quot;,            &amp;quot;object&amp;quot;: 
&amp;quot;chat.completion&amp;quot;,            &amp;quot;created&amp;quot;: 
1677652288,            &amp;quot;model&amp;quot;: 
&amp;quot;gpt-3.5-turbo&amp;quot;,            &amp;quot;choices&amp;quot;: [],  
}            with responses.RequestsMock() as rsps:            rsps.add(        
responses.POST,                
&amp;quot;https://api.openai.com/v1/chat/completions&amp;quot;,                
json=empty_response,                status=200,            )                
provider = OpenAIProvider(config)                with 
pytest.raises(DevSynthError) as exc_info:                
provider.generate(&amp;quot;Hello&amp;quot;)    &amp;gt;           assert 
&amp;quot;Invalid response&amp;quot; in str(exc_info.value)E           assert 
&amp;#x27;Invalid response&amp;#x27; in &amp;quot;OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot;E            +  where &amp;quot;OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot; = str(OpenAIConnectionError(&amp;quot;OpenAI API error:
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot;))E            +    where 
OpenAIConnectionError(&amp;quot;OpenAI API error: &amp;#x27;NoneType&amp;#x27; 
object has no attribute &amp;#x27;choices&amp;#x27;. Check your API key and 
model configuration.&amp;quot;) = &amp;lt;ExceptionInfo 
OpenAIConnectionError(&amp;quot;OpenAI API error: &amp;#x27;NoneType&amp;#x27; 
object has no attribute &amp;#x27;choices&amp;#x27;. Check your API key and 
model configuration.&amp;quot;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openai_provider.py:693: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,852 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:27:30,877 - 
devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model2025-10-28 09:27:30,877 - 
devsynth.application.llm.openai_provider - ERROR - OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-modelERROR    
devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration._________ 
TestOpenAIProviderEdgeCases.test_malformed_response_handling _________self = 
&amp;lt;test_openai_provider.TestOpenAIProviderEdgeCases object at 
0x12388e1e0&amp;gt;    @pytest.mark.fast    def 
test_malformed_response_handling(self):        
&amp;quot;&amp;quot;&amp;quot;Test handling of malformed 
responses.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}            
malformed_response = {&amp;quot;invalid&amp;quot;: &amp;quot;response&amp;quot;,
&amp;quot;structure&amp;quot;: True}            with responses.RequestsMock() as
rsps:            rsps.add(                responses.POST,                
&amp;quot;https://api.openai.com/v1/chat/completions&amp;quot;,                
json=malformed_response,                status=200,            )                
provider = OpenAIProvider(config)                with 
pytest.raises(DevSynthError) as exc_info:                
provider.generate(&amp;quot;Hello&amp;quot;)    &amp;gt;           assert 
&amp;quot;Invalid response&amp;quot; in str(exc_info.value)E           assert 
&amp;#x27;Invalid response&amp;#x27; in &amp;quot;OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot;E            +  where &amp;quot;OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot; = str(OpenAIConnectionError(&amp;quot;OpenAI API error:
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.&amp;quot;))E            +    where 
OpenAIConnectionError(&amp;quot;OpenAI API error: &amp;#x27;NoneType&amp;#x27; 
object has no attribute &amp;#x27;choices&amp;#x27;. Check your API key and 
model configuration.&amp;quot;) = &amp;lt;ExceptionInfo 
OpenAIConnectionError(&amp;quot;OpenAI API error: &amp;#x27;NoneType&amp;#x27; 
object has no attribute &amp;#x27;choices&amp;#x27;. Check your API key and 
model configuration.&amp;quot;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openai_provider.py:715: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,912 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:27:30,934 - 
devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model2025-10-28 09:27:30,934 - 
devsynth.application.llm.openai_provider - ERROR - OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-modelERROR    
devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.______________ TestOpenAIProviderEdgeCases.test_unicode_handling 
_______________self = 
&amp;lt;devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x144be3e00&amp;gt;prompt = &amp;#x27;Say hello in multiple languages&amp;#x27;,
parameters = None    def generate(self, prompt: str, parameters: Dict = None) 
-&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;Generate text from a prompt 
using OpenAI.            Args:            prompt: The prompt to generate text 
from            parameters: Additional parameters for the generation            
Returns:            The generated text            Raises:            
OpenAIConnectionError: If there&amp;#x27;s an issue connecting to OpenAI        
OpenAIModelError: If there&amp;#x27;s an issue with the model or response       
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Validate runtime parameters        
self._validate_runtime_parameters(parameters or {})            # Merge default 
parameters with provided parameters        params = {            
&amp;quot;temperature&amp;quot;: self.temperature,            
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            try:            response = 
self._execute_with_resilience(                
self.client.chat.completions.create,                model=self.model,           
messages=messages,                **params,            )&amp;gt;           
message = getattr(response.choices[0], &amp;quot;message&amp;quot;, None)       
^^^^^^^^^^^^^^^^E           AttributeError: &amp;#x27;NoneType&amp;#x27; object 
has no attribute 
&amp;#x27;choices&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/application/llm/openai_provider.py:346: AttributeErrorDuring 
handling of the above exception, another exception occurred:self = 
&amp;lt;test_openai_provider.TestOpenAIProviderEdgeCases object at 
0x12388ec30&amp;gt;    @pytest.mark.fast    def test_unicode_handling(self):    
&amp;quot;&amp;quot;&amp;quot;Test handling of Unicode 
content.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;api_key&amp;quot;: &amp;quot;test-key&amp;quot;}            
unicode_response = {            &amp;quot;id&amp;quot;: 
&amp;quot;chatcmpl-unicode&amp;quot;,            &amp;quot;object&amp;quot;: 
&amp;quot;chat.completion&amp;quot;,            &amp;quot;created&amp;quot;: 
1677652288,            &amp;quot;model&amp;quot;: 
&amp;quot;gpt-3.5-turbo&amp;quot;,            &amp;quot;choices&amp;quot;: [    
{                    &amp;quot;index&amp;quot;: 0,                    
&amp;quot;message&amp;quot;: {                        &amp;quot;role&amp;quot;: 
&amp;quot;assistant&amp;quot;,                        
&amp;quot;content&amp;quot;: &amp;quot;Hello! ! Hola! &amp;quot;,        
},                    &amp;quot;finish_reason&amp;quot;: 
&amp;quot;stop&amp;quot;,                }            ],            
&amp;quot;usage&amp;quot;: {&amp;quot;prompt_tokens&amp;quot;: 5, 
&amp;quot;completion_tokens&amp;quot;: 10, &amp;quot;total_tokens&amp;quot;: 
15},        }            with responses.RequestsMock() as rsps:            
rsps.add(                responses.POST,                
&amp;quot;https://api.openai.com/v1/chat/completions&amp;quot;,                
json=unicode_response,                status=200,            )                
provider = OpenAIProvider(config)&amp;gt;           response = 
provider.generate(&amp;quot;Say hello in multiple languages&amp;quot;)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/gith
ub.com/ravenoak/devsynth/tests/unit/llm/test_openai_provider.py:749: _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x144be3e00&amp;gt;prompt = &amp;#x27;Say hello in multiple languages&amp;#x27;,
parameters = None    def generate(self, prompt: str, parameters: Dict = None) 
-&amp;gt; str:        &amp;quot;&amp;quot;&amp;quot;Generate text from a prompt 
using OpenAI.            Args:            prompt: The prompt to generate text 
from            parameters: Additional parameters for the generation            
Returns:            The generated text            Raises:            
OpenAIConnectionError: If there&amp;#x27;s an issue connecting to OpenAI        
OpenAIModelError: If there&amp;#x27;s an issue with the model or response       
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Validate runtime parameters        
self._validate_runtime_parameters(parameters or {})            # Merge default 
parameters with provided parameters        params = {            
&amp;quot;temperature&amp;quot;: self.temperature,            
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            try:            response = 
self._execute_with_resilience(                
self.client.chat.completions.create,                model=self.model,           
messages=messages,                **params,            )            message = 
getattr(response.choices[0], &amp;quot;message&amp;quot;, None)            
content = getattr(message, &amp;quot;content&amp;quot;, None)            if 
content is None:                raise OpenAIModelError(&amp;quot;Invalid 
response from OpenAI&amp;quot;)            return content        except 
OpenAIModelError:            raise        except OpenAITokenLimitError:         
raise  # Re-raise token limit errors as-is        except OpenAIConnectionError: 
raise  # Re-raise connection errors as-is        except Exception as e:         
error_msg = f&amp;quot;OpenAI API error: {str(e)}. Check your API key and model 
configuration.&amp;quot;            logger.error(error_msg)&amp;gt;           
raise OpenAIConnectionError(error_msg)E           
devsynth.application.llm.openai_provider.OpenAIConnectionError: OpenAI API 
error: &amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration./Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/
application/llm/openai_provider.py:360: 
OpenAIConnectionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:30,962 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:27:30,991 - 
devsynth.application.llm.openai_provider - INFO - Initialized OpenAI provider 
with model: stub-model2025-10-28 09:27:30,992 - 
devsynth.application.llm.openai_provider - ERROR - OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model 
configuration.------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://api.openai.com/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openai_provider:logging_setup.py:615 Initialized OpenAI
provider with model: stub-modelERROR    
devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI API error: 
&amp;#x27;NoneType&amp;#x27; object has no attribute 
&amp;#x27;choices&amp;#x27;. Check your API key and model configuration._ 
TestOpenRouterProviderInitialization.test_initialization_without_api_key_raises_
error _self = 
&amp;lt;test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x1238bd340&amp;gt;    @pytest.mark.fast    def 
test_initialization_without_api_key_raises_error(self):        
&amp;quot;&amp;quot;&amp;quot;Test that initialization fails without API 
key.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_model&amp;quot;: 
&amp;quot;google/gemini-flash-1.5&amp;quot;}    &amp;gt;       with 
pytest.raises(OpenRouterConnectionError) as exc_info:             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       Failed: DID NOT RAISE 
&amp;lt;class 
&amp;#x27;devsynth.application.llm.openrouter_provider.OpenRouterConnectionError
&amp;#x27;&amp;gt;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/uni
t/llm/test_openrouter_provider.py:64: Failed----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:27:31,101 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:31,121 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.5------------------------------ 
Captured log call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5_ 
TestOpenRouterProviderInitialization.test_initialization_with_httpx_unavailable 
_self = &amp;lt;test_openrouter_provider.TestOpenRouterProviderInitialization 
object at 0x1238bd9d0&amp;gt;    @pytest.mark.fast    def 
test_initialization_with_httpx_unavailable(self):        
&amp;quot;&amp;quot;&amp;quot;Test initialization when httpx is not 
available.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}          
with 
patch(&amp;quot;devsynth.application.llm.openrouter_provider.httpx&amp;quot;, 
None):            provider = OpenRouterProvider(config)    &amp;gt;       assert
provider.sync_client is NoneE       assert &amp;lt;httpx.Client object at 
0x1454a59a0&amp;gt; is NoneE        +  where &amp;lt;httpx.Client object at 
0x1454a59a0&amp;gt; = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 
0x145466ff0&amp;gt;.sync_client/Users/caitlyn/Projects/github.com/ravenoak/devsy
nth/tests/unit/llm/test_openrouter_provider.py:86: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:31,244 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:31,263 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.5------------------------------ 
Captured log call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5______ 
TestOpenRouterProviderErrorHandling.test_invalid_temperature_range ______self = 
&amp;lt;test_openrouter_provider.TestOpenRouterProviderErrorHandling object at 
0x1238be450&amp;gt;    @pytest.mark.fast    def 
test_invalid_temperature_range(self):        &amp;quot;&amp;quot;&amp;quot;Test 
error handling for invalid temperature range.&amp;quot;&amp;quot;&amp;quot;     
config = {&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;} 
provider = OpenRouterProvider(config)            with 
pytest.raises(OpenRouterConnectionError) as exc_info:            
provider.generate(&amp;quot;Hello&amp;quot;, {&amp;quot;temperature&amp;quot;: 
-0.1})    &amp;gt;       assert &amp;quot;temperature must be between 0 and 
2&amp;quot; in str(exc_info.value)E       AssertionError: assert 
&amp;#x27;temperature must be between 0 and 2&amp;#x27; in &amp;#x27;OpenRouter 
API error: Circuit breaker for _api_call is open&amp;#x27;E        +  where 
&amp;#x27;OpenRouter API error: Circuit breaker for _api_call is open&amp;#x27; 
= str(OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker 
for _api_call is open&amp;#x27;))E        +    where 
OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker for 
_api_call is open&amp;#x27;) = &amp;lt;ExceptionInfo 
OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker for 
_api_call is open&amp;#x27;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openrouter_provider.py:428: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:31,310 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:31,329 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.52025-10-28 09:27:31,331 - fallback -
WARNING - Retry attempt 1/3 after 2.52s delay2025-10-28 09:27:31,331 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 2.52s)2025-10-28 09:27:33,859 - fallback - WARNING - Retry attempt 2/3 
after 6.88s delay2025-10-28 09:27:33,860 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 6.88s)2025-10-28 09:27:40,743 - circuit_breaker - WARNING - Circuit 
breaker for _api_call transitioned to OPEN due to failure2025-10-28 09:27:40,745
- fallback - WARNING - Retry attempt 3/3 after 11.90s delay2025-10-28 
09:27:40,745 - devsynth.application.llm.openrouter_provider - WARNING - Retrying
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 11.90s)2025-10-28 09:27:52,653 - circuit_breaker - WARNING - Circuit 
breaker for _api_call is OPEN, failing fast2025-10-28 09:27:52,655 - fallback - 
WARNING - Circuit open - aborting retries for _wrapped2025-10-28 09:27:52,655 - 
devsynth.application.llm.openrouter_provider - ERROR - OpenRouter API error: 
Circuit breaker for _api_call is open------------------------------ Captured log
call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5WARNING  
fallback:logging_setup.py:615 Retry attempt 1/3 after 2.52s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 2.52s)WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 
6.88s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 6.88s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call transitioned to OPEN due to failureWARNING  
fallback:logging_setup.py:615 Retry attempt 3/3 after 11.90s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 11.90s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker 
for _api_call is OPEN, failing fastWARNING  fallback:logging_setup.py:615 
Circuit open - aborting retries for _wrappedERROR    
devsynth.application.llm.openrouter_provider:logging_setup.py:615 OpenRouter API
error: Circuit breaker for _api_call is open_________ 
TestOpenRouterProviderErrorHandling.test_invalid_max_tokens __________self = 
&amp;lt;test_openrouter_provider.TestOpenRouterProviderErrorHandling object at 
0x1238be870&amp;gt;    @pytest.mark.fast    def test_invalid_max_tokens(self):  
&amp;quot;&amp;quot;&amp;quot;Test error handling for invalid 
max_tokens.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}        
provider = OpenRouterProvider(config)            with 
pytest.raises(OpenRouterConnectionError) as exc_info:            
provider.generate(&amp;quot;Hello&amp;quot;, {&amp;quot;max_tokens&amp;quot;: 
0})    &amp;gt;       assert &amp;quot;max_tokens must be positive&amp;quot; in 
str(exc_info.value)E       AssertionError: assert &amp;#x27;max_tokens must be 
positive&amp;#x27; in &amp;#x27;OpenRouter API error: Circuit breaker for 
_api_call is open&amp;#x27;E        +  where &amp;#x27;OpenRouter API error: 
Circuit breaker for _api_call is open&amp;#x27; = 
str(OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker 
for _api_call is open&amp;#x27;))E        +    where 
OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker for 
_api_call is open&amp;#x27;) = &amp;lt;ExceptionInfo 
OpenRouterConnectionError(&amp;#x27;OpenRouter API error: Circuit breaker for 
_api_call is open&amp;#x27;) 
tblen=2&amp;gt;.value/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/llm/test_openrouter_provider.py:439: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:27:52,692 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:27:52,709 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.52025-10-28 09:27:52,710 - fallback -
WARNING - Retry attempt 1/3 after 1.85s delay2025-10-28 09:27:52,710 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.85s)2025-10-28 09:27:54,563 - fallback - WARNING - Retry attempt 2/3 
after 4.20s delay2025-10-28 09:27:54,563 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.20s)2025-10-28 09:27:58,770 - circuit_breaker - WARNING - Circuit 
breaker for _api_call transitioned to OPEN due to failure2025-10-28 09:27:58,771
- fallback - WARNING - Retry attempt 3/3 after 11.77s delay2025-10-28 
09:27:58,771 - devsynth.application.llm.openrouter_provider - WARNING - Retrying
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 11.77s)2025-10-28 09:28:10,541 - circuit_breaker - WARNING - Circuit 
breaker for _api_call is OPEN, failing fast2025-10-28 09:28:10,543 - fallback - 
WARNING - Circuit open - aborting retries for _wrapped2025-10-28 09:28:10,544 - 
devsynth.application.llm.openrouter_provider - ERROR - OpenRouter API error: 
Circuit breaker for _api_call is open------------------------------ Captured log
call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5WARNING  
fallback:logging_setup.py:615 Retry attempt 1/3 after 1.85s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.85s)WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 
4.20s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.20s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call transitioned to OPEN due to failureWARNING  
fallback:logging_setup.py:615 Retry attempt 3/3 after 11.77s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 11.77s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker 
for _api_call is OPEN, failing fastWARNING  fallback:logging_setup.py:615 
Circuit open - aborting retries for _wrappedERROR    
devsynth.application.llm.openrouter_provider:logging_setup.py:615 OpenRouter API
error: Circuit breaker for _api_call is open_____ 
TestOpenRouterProviderConfiguration.test_configuration_with_defaults _____self =
&amp;lt;test_openrouter_provider.TestOpenRouterProviderConfiguration object at 
0x1238bf6b0&amp;gt;    @pytest.mark.fast    def 
test_configuration_with_defaults(self):        
&amp;quot;&amp;quot;&amp;quot;Test configuration with default 
values.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}          
provider = OpenRouterProvider(config)            assert provider.temperature == 
0.7  # Default&amp;gt;       assert provider.max_tokens == 4096  # Default      
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       assert 1024 == 4096E        +  where 
1024 = &amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider 
object at 
0x144f71a30&amp;gt;.max_tokens/Users/caitlyn/Projects/github.com/ravenoak/devsyn
th/tests/unit/llm/test_openrouter_provider.py:482: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:10,630 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:28:10,647 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.5------------------------------ 
Captured log call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5___________ 
TestOpenRouterProviderHeaders.test_custom_referer_header ___________self = 
&amp;lt;test_openrouter_provider.TestOpenRouterProviderHeaders object at 
0x1238ce030&amp;gt;    @pytest.mark.fast    def 
test_custom_referer_header(self):        &amp;quot;&amp;quot;&amp;quot;Test 
custom referer header configuration.&amp;quot;&amp;quot;&amp;quot;        config
= {            &amp;quot;openrouter_api_key&amp;quot;: 
&amp;quot;test-key&amp;quot;,            &amp;quot;http_referer&amp;quot;: 
&amp;quot;https://custom-app.com&amp;quot;,        }        provider = 
OpenRouterProvider(config)    &amp;gt;       assert 
provider.headers[&amp;quot;HTTP-Referer&amp;quot;] == 
&amp;quot;https://custom-app.com&amp;quot;E       AssertionError: assert 
&amp;#x27;https://devsynth.dev&amp;#x27; == 
&amp;#x27;https://custom-app.com&amp;#x27;E         E         - 
https://custom-app.comE         + 
https://devsynth.dev/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/llm/test_openrouter_provider.py:634: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:10,938 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access disabled 
during tests. Falling back to approximate token counting2025-10-28 09:28:10,957 
- devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.5------------------------------ 
Captured log call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Network access 
disabled during tests. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5_________ 
TestOpenRouterProviderEdgeCases.test_empty_response_handling _________self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x1453910d0&amp;gt;prompt = &amp;#x27;Hello&amp;#x27;, parameters = None    
def generate(self, prompt: str, parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:&amp;gt;           return self._execute_with_resilience(_api_call)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/raven
oak/devsynth/src/devsynth/application/llm/openrouter_provider.py:219: _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience    return _wrapped()  
^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fall
back.py:297: in wrapper    result = wrapped_func(*args, **kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/src/devsynth/application/llm/openrouter_provider.py:166: in _wrapped    return
future.result(timeout=self.timeout)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Fram
eworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py
:456: in result    return self.__get_result()           
^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.fr
amework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401: in 
__get_result    raise 
self._exception/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framew
ork/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:59: in run    
result = self.fn(*self.args, **self.kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.fallback.CircuitBreaker object at 0x145444230&amp;gt;func = 
&amp;lt;function OpenRouterProvider.generate.&amp;lt;locals&amp;gt;._api_call at
0x14546da80&amp;gt;args = (), kwargs = {}, state = &amp;#x27;OPEN&amp;#x27;, 
last_failure_time = 1761668897.713435    def call(self, func: Callable[P, T], 
*args: P.args, **kwargs: P.kwargs) -&amp;gt; T:        
&amp;quot;&amp;quot;&amp;quot;        Call a function with circuit breaker 
protection.            Parameters        ----------        func : Callable      
The function to call        *args: Any            Positional arguments for the 
function        **kwargs: Any            Keyword arguments for the function     
Returns        -------        Any            The result of the function call    
Raises        ------        DevSynthError            If the circuit is open     
&amp;quot;&amp;quot;&amp;quot;        # Check if the circuit is open        with
self.lock:            state = self.state            last_failure_time = 
self.last_failure_time        if state == self.OPEN:            if time.time() -
last_failure_time &amp;gt;= self.recovery_timeout:                with 
self.lock:                    self.state = self.HALF_OPEN                    
self.test_calls_remaining = self.test_calls                
inc_circuit_breaker_state(func.__name__, self.HALF_OPEN)                
self.logger.info(                    f&amp;quot;Circuit breaker for 
{func.__name__} transitioned from OPEN to HALF_OPEN&amp;quot;,                  
function=func.__name__,                    state=self.state,                )   
self._safe_hook(self.on_half_open, func.__name__)            else:              
inc_circuit_breaker_state(func.__name__, self.OPEN)                
self.logger.warning(                    f&amp;quot;Circuit breaker for 
{func.__name__} is OPEN, failing fast&amp;quot;,                    
function=func.__name__,                    state=state,                    
recovery_time_remaining=self.recovery_timeout                    - (time.time() 
- last_failure_time),                )&amp;gt;           raise DevSynthError(   
f&amp;quot;Circuit breaker for {func.__name__} is open&amp;quot;,               
error_code=&amp;quot;CIRCUIT_OPEN&amp;quot;,                details={           
&amp;quot;function&amp;quot;: func.__name__,                    
&amp;quot;recovery_time_remaining&amp;quot;: self.recovery_timeout              
- (time.time() - last_failure_time),                },            )E           
devsynth.exceptions.DevSynthError: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.p
y:926: DevSynthErrorDuring handling of the above exception, another exception 
occurred:self = &amp;lt;test_openrouter_provider.TestOpenRouterProviderEdgeCases
object at 0x1238ce570&amp;gt;    @pytest.mark.fast    def 
test_empty_response_handling(self):        &amp;quot;&amp;quot;&amp;quot;Test 
handling of empty responses.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}          
empty_response = {            &amp;quot;id&amp;quot;: 
&amp;quot;chatcmpl-empty&amp;quot;,            &amp;quot;object&amp;quot;: 
&amp;quot;chat.completion&amp;quot;,            &amp;quot;created&amp;quot;: 
1677652288,            &amp;quot;model&amp;quot;: 
&amp;quot;google/gemini-flash-1.5&amp;quot;,            
&amp;quot;choices&amp;quot;: [],        }            with 
responses.RequestsMock() as rsps:            rsps.add(                
responses.POST,                
&amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;,              
json=empty_response,                status=200,            )                
provider = OpenRouterProvider(config)                with 
pytest.raises(OpenRouterModelError) as exc_info:&amp;gt;               
provider.generate(&amp;quot;Hello&amp;quot;)/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/llm/test_openrouter_provider.py:664: _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x1453910d0&amp;gt;prompt = &amp;#x27;Hello&amp;#x27;, parameters = None    
def generate(self, prompt: str, parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:            return self._execute_with_resilience(_api_call)        except 
Exception as e:            error_msg = f&amp;quot;OpenRouter API error: 
{str(e)}&amp;quot;            logger.error(error_msg)&amp;gt;           raise 
OpenRouterConnectionError(error_msg)E           
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError: 
OpenRouter API error: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applicatio
n/llm/openrouter_provider.py:223: 
OpenRouterConnectionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:10,978 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:28:10,997 - 
devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.52025-10-28 09:28:10,998 - fallback -
WARNING - Retry attempt 1/3 after 2.01s delay2025-10-28 09:28:10,998 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 2.01s)2025-10-28 09:28:13,012 - fallback - WARNING - Retry attempt 2/3 
after 4.70s delay2025-10-28 09:28:13,013 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.70s)2025-10-28 09:28:17,713 - circuit_breaker - WARNING - Circuit 
breaker for _api_call transitioned to OPEN due to failure2025-10-28 09:28:17,714
- fallback - WARNING - Retry attempt 3/3 after 13.04s delay2025-10-28 
09:28:17,714 - devsynth.application.llm.openrouter_provider - WARNING - Retrying
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 13.04s)2025-10-28 09:28:30,763 - circuit_breaker - WARNING - Circuit 
breaker for _api_call is OPEN, failing fast2025-10-28 09:28:30,765 - fallback - 
WARNING - Circuit open - aborting retries for _wrapped2025-10-28 09:28:30,765 - 
devsynth.application.llm.openrouter_provider - ERROR - OpenRouter API error: 
Circuit breaker for _api_call is open------------------------------ Captured log
call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5WARNING  
fallback:logging_setup.py:615 Retry attempt 1/3 after 2.01s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 2.01s)WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 
4.70s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.70s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call transitioned to OPEN due to failureWARNING  
fallback:logging_setup.py:615 Retry attempt 3/3 after 13.04s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 13.04s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker 
for _api_call is OPEN, failing fastWARNING  fallback:logging_setup.py:615 
Circuit open - aborting retries for _wrappedERROR    
devsynth.application.llm.openrouter_provider:logging_setup.py:615 OpenRouter API
error: Circuit breaker for _api_call is open_______ 
TestOpenRouterProviderEdgeCases.test_malformed_response_handling _______self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x1451db050&amp;gt;prompt = &amp;#x27;Hello&amp;#x27;, parameters = None    
def generate(self, prompt: str, parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:&amp;gt;           return self._execute_with_resilience(_api_call)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/raven
oak/devsynth/src/devsynth/application/llm/openrouter_provider.py:219: _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience    return _wrapped()  
^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fall
back.py:297: in wrapper    result = wrapped_func(*args, **kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/src/devsynth/application/llm/openrouter_provider.py:166: in _wrapped    return
future.result(timeout=self.timeout)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Fram
eworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py
:456: in result    return self.__get_result()           
^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.fr
amework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401: in 
__get_result    raise 
self._exception/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framew
ork/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:59: in run    
result = self.fn(*self.args, **self.kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.fallback.CircuitBreaker object at 0x1451dbfe0&amp;gt;func = 
&amp;lt;function OpenRouterProvider.generate.&amp;lt;locals&amp;gt;._api_call at
0x14546c0e0&amp;gt;args = (), kwargs = {}, state = &amp;#x27;OPEN&amp;#x27;, 
last_failure_time = 1761668917.144001    def call(self, func: Callable[P, T], 
*args: P.args, **kwargs: P.kwargs) -&amp;gt; T:        
&amp;quot;&amp;quot;&amp;quot;        Call a function with circuit breaker 
protection.            Parameters        ----------        func : Callable      
The function to call        *args: Any            Positional arguments for the 
function        **kwargs: Any            Keyword arguments for the function     
Returns        -------        Any            The result of the function call    
Raises        ------        DevSynthError            If the circuit is open     
&amp;quot;&amp;quot;&amp;quot;        # Check if the circuit is open        with
self.lock:            state = self.state            last_failure_time = 
self.last_failure_time        if state == self.OPEN:            if time.time() -
last_failure_time &amp;gt;= self.recovery_timeout:                with 
self.lock:                    self.state = self.HALF_OPEN                    
self.test_calls_remaining = self.test_calls                
inc_circuit_breaker_state(func.__name__, self.HALF_OPEN)                
self.logger.info(                    f&amp;quot;Circuit breaker for 
{func.__name__} transitioned from OPEN to HALF_OPEN&amp;quot;,                  
function=func.__name__,                    state=self.state,                )   
self._safe_hook(self.on_half_open, func.__name__)            else:              
inc_circuit_breaker_state(func.__name__, self.OPEN)                
self.logger.warning(                    f&amp;quot;Circuit breaker for 
{func.__name__} is OPEN, failing fast&amp;quot;,                    
function=func.__name__,                    state=state,                    
recovery_time_remaining=self.recovery_timeout                    - (time.time() 
- last_failure_time),                )&amp;gt;           raise DevSynthError(   
f&amp;quot;Circuit breaker for {func.__name__} is open&amp;quot;,               
error_code=&amp;quot;CIRCUIT_OPEN&amp;quot;,                details={           
&amp;quot;function&amp;quot;: func.__name__,                    
&amp;quot;recovery_time_remaining&amp;quot;: self.recovery_timeout              
- (time.time() - last_failure_time),                },            )E           
devsynth.exceptions.DevSynthError: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.p
y:926: DevSynthErrorDuring handling of the above exception, another exception 
occurred:self = &amp;lt;test_openrouter_provider.TestOpenRouterProviderEdgeCases
object at 0x1238ce9c0&amp;gt;    @pytest.mark.fast    def 
test_malformed_response_handling(self):        
&amp;quot;&amp;quot;&amp;quot;Test handling of malformed 
responses.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}          
malformed_response = {&amp;quot;invalid&amp;quot;: &amp;quot;response&amp;quot;,
&amp;quot;structure&amp;quot;: True}            with responses.RequestsMock() as
rsps:            rsps.add(                responses.POST,                
&amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;,              
json=malformed_response,                status=200,            )                
provider = OpenRouterProvider(config)                with 
pytest.raises(OpenRouterModelError) as exc_info:&amp;gt;               
provider.generate(&amp;quot;Hello&amp;quot;)/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/llm/test_openrouter_provider.py:686: _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x1451db050&amp;gt;prompt = &amp;#x27;Hello&amp;#x27;, parameters = None    
def generate(self, prompt: str, parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:            return self._execute_with_resilience(_api_call)        except 
Exception as e:            error_msg = f&amp;quot;OpenRouter API error: 
{str(e)}&amp;quot;            logger.error(error_msg)&amp;gt;           raise 
OpenRouterConnectionError(error_msg)E           
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError: 
OpenRouter API error: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applicatio
n/llm/openrouter_provider.py:223: 
OpenRouterConnectionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:30,842 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:28:30,854 - 
devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.52025-10-28 09:28:30,855 - fallback -
WARNING - Retry attempt 1/3 after 1.81s delay2025-10-28 09:28:30,855 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.81s)2025-10-28 09:28:32,667 - fallback - WARNING - Retry attempt 2/3 
after 4.47s delay2025-10-28 09:28:32,667 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.47s)2025-10-28 09:28:37,144 - circuit_breaker - WARNING - Circuit 
breaker for _api_call transitioned to OPEN due to failure2025-10-28 09:28:37,144
- fallback - WARNING - Retry attempt 3/3 after 7.25s delay2025-10-28 
09:28:37,145 - devsynth.application.llm.openrouter_provider - WARNING - Retrying
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 7.25s)2025-10-28 09:28:44,401 - circuit_breaker - WARNING - Circuit 
breaker for _api_call is OPEN, failing fast2025-10-28 09:28:44,402 - fallback - 
WARNING - Circuit open - aborting retries for _wrapped2025-10-28 09:28:44,402 - 
devsynth.application.llm.openrouter_provider - ERROR - OpenRouter API error: 
Circuit breaker for _api_call is open------------------------------ Captured log
call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5WARNING  
fallback:logging_setup.py:615 Retry attempt 1/3 after 1.81s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.81s)WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 
4.47s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 4.47s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call transitioned to OPEN due to failureWARNING  
fallback:logging_setup.py:615 Retry attempt 3/3 after 7.25s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 7.25s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call is OPEN, failing fastWARNING  fallback:logging_setup.py:615 Circuit 
open - aborting retries for _wrappedERROR    
devsynth.application.llm.openrouter_provider:logging_setup.py:615 OpenRouter API
error: Circuit breaker for _api_call is open____________ 
TestOpenRouterProviderEdgeCases.test_unicode_handling _____________self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x145106360&amp;gt;prompt = &amp;#x27;Say hello in multiple 
languages&amp;#x27;, parameters = None    def generate(self, prompt: str, 
parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:&amp;gt;           return self._execute_with_resilience(_api_call)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/raven
oak/devsynth/src/devsynth/application/llm/openrouter_provider.py:219: _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:168: in _execute_with_resilience    return _wrapped()  
^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fall
back.py:297: in wrapper    result = wrapped_func(*args, **kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak/devsynt
h/src/devsynth/application/llm/openrouter_provider.py:166: in _wrapped    return
future.result(timeout=self.timeout)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Fram
eworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py
:456: in result    return self.__get_result()           
^^^^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.fr
amework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401: in 
__get_result    raise 
self._exception/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framew
ork/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:59: in run    
result = self.fn(*self.args, **self.kwargs)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.fallback.CircuitBreaker object at 0x145aebf20&amp;gt;func = 
&amp;lt;function OpenRouterProvider.generate.&amp;lt;locals&amp;gt;._api_call at
0x14546eac0&amp;gt;args = (), kwargs = {}, state = &amp;#x27;OPEN&amp;#x27;, 
last_failure_time = 1761668927.300715    def call(self, func: Callable[P, T], 
*args: P.args, **kwargs: P.kwargs) -&amp;gt; T:        
&amp;quot;&amp;quot;&amp;quot;        Call a function with circuit breaker 
protection.            Parameters        ----------        func : Callable      
The function to call        *args: Any            Positional arguments for the 
function        **kwargs: Any            Keyword arguments for the function     
Returns        -------        Any            The result of the function call    
Raises        ------        DevSynthError            If the circuit is open     
&amp;quot;&amp;quot;&amp;quot;        # Check if the circuit is open        with
self.lock:            state = self.state            last_failure_time = 
self.last_failure_time        if state == self.OPEN:            if time.time() -
last_failure_time &amp;gt;= self.recovery_timeout:                with 
self.lock:                    self.state = self.HALF_OPEN                    
self.test_calls_remaining = self.test_calls                
inc_circuit_breaker_state(func.__name__, self.HALF_OPEN)                
self.logger.info(                    f&amp;quot;Circuit breaker for 
{func.__name__} transitioned from OPEN to HALF_OPEN&amp;quot;,                  
function=func.__name__,                    state=self.state,                )   
self._safe_hook(self.on_half_open, func.__name__)            else:              
inc_circuit_breaker_state(func.__name__, self.OPEN)                
self.logger.warning(                    f&amp;quot;Circuit breaker for 
{func.__name__} is OPEN, failing fast&amp;quot;,                    
function=func.__name__,                    state=state,                    
recovery_time_remaining=self.recovery_timeout                    - (time.time() 
- last_failure_time),                )&amp;gt;           raise DevSynthError(   
f&amp;quot;Circuit breaker for {func.__name__} is open&amp;quot;,               
error_code=&amp;quot;CIRCUIT_OPEN&amp;quot;,                details={           
&amp;quot;function&amp;quot;: func.__name__,                    
&amp;quot;recovery_time_remaining&amp;quot;: self.recovery_timeout              
- (time.time() - last_failure_time),                },            )E           
devsynth.exceptions.DevSynthError: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/fallback.p
y:926: DevSynthErrorDuring handling of the above exception, another exception 
occurred:self = &amp;lt;test_openrouter_provider.TestOpenRouterProviderEdgeCases
object at 0x1238cee70&amp;gt;    @pytest.mark.fast    def 
test_unicode_handling(self):        &amp;quot;&amp;quot;&amp;quot;Test handling 
of Unicode content.&amp;quot;&amp;quot;&amp;quot;        config = 
{&amp;quot;openrouter_api_key&amp;quot;: &amp;quot;test-key&amp;quot;}          
unicode_response = {            &amp;quot;id&amp;quot;: 
&amp;quot;chatcmpl-unicode&amp;quot;,            &amp;quot;object&amp;quot;: 
&amp;quot;chat.completion&amp;quot;,            &amp;quot;created&amp;quot;: 
1677652288,            &amp;quot;model&amp;quot;: 
&amp;quot;google/gemini-flash-1.5&amp;quot;,            
&amp;quot;choices&amp;quot;: [                {                    
&amp;quot;index&amp;quot;: 0,                    &amp;quot;message&amp;quot;: { 
&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;,                        
&amp;quot;content&amp;quot;: &amp;quot;Hello! ! Hola! &amp;quot;,        
},                    &amp;quot;finish_reason&amp;quot;: 
&amp;quot;stop&amp;quot;,                }            ],            
&amp;quot;usage&amp;quot;: {&amp;quot;prompt_tokens&amp;quot;: 5, 
&amp;quot;completion_tokens&amp;quot;: 10, &amp;quot;total_tokens&amp;quot;: 
15},        }            with responses.RequestsMock() as rsps:            
rsps.add(                responses.POST,                
&amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;,              
json=unicode_response,                status=200,            )                
provider = OpenRouterProvider(config)&amp;gt;           response = 
provider.generate(&amp;quot;Say hello in multiple languages&amp;quot;)          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/gith
ub.com/ravenoak/devsynth/tests/unit/llm/test_openrouter_provider.py:722: _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
&amp;lt;devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x145106360&amp;gt;prompt = &amp;#x27;Say hello in multiple 
languages&amp;#x27;, parameters = None    def generate(self, prompt: str, 
parameters: Dict = None) -&amp;gt; str:        
&amp;quot;&amp;quot;&amp;quot;Generate text from a prompt using OpenRouter.     
Args:            prompt: The prompt to generate text from            parameters:
Additional parameters for the generation            Returns:            The 
generated text            Raises:            OpenRouterConnectionError: If 
there&amp;#x27;s an issue connecting to OpenRouter            
OpenRouterModelError: If there&amp;#x27;s an issue with the model or response   
TokenLimitExceededError: If the prompt exceeds the token limit        
&amp;quot;&amp;quot;&amp;quot;        # Ensure the prompt doesn&amp;#x27;t 
exceed token limits        self.token_tracker.ensure_token_limit(prompt, 
self.max_tokens)            # Merge default parameters with provided parameters 
params = {            &amp;quot;temperature&amp;quot;: self.temperature,        
&amp;quot;max_tokens&amp;quot;: self.max_tokens,        }        if parameters: 
params.update(parameters)            # Prepare the request payload        
messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
&amp;quot;content&amp;quot;: prompt}]            def _api_call():            if 
self.sync_client is None:                raise 
OpenRouterConnectionError(&amp;quot;HTTP client not available&amp;quot;)        
payload = {                &amp;quot;model&amp;quot;: self.model,               
&amp;quot;messages&amp;quot;: messages,                **params,            }   
response = self.sync_client.post(&amp;quot;/chat/completions&amp;quot;, 
json=payload)            response.raise_for_status()            response_data = 
response.json()                if &amp;quot;choices&amp;quot; in response_data 
and len(response_data[&amp;quot;choices&amp;quot;]) &amp;gt; 0:                
return 
response_data[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;
quot;content&amp;quot;]            else:                raise 
OpenRouterModelError(&amp;quot;Invalid response from OpenRouter&amp;quot;)      
try:            return self._execute_with_resilience(_api_call)        except 
Exception as e:            error_msg = f&amp;quot;OpenRouter API error: 
{str(e)}&amp;quot;            logger.error(error_msg)&amp;gt;           raise 
OpenRouterConnectionError(error_msg)E           
devsynth.application.llm.openrouter_provider.OpenRouterConnectionError: 
OpenRouter API error: Circuit breaker for _api_call is 
open/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/applicatio
n/llm/openrouter_provider.py:223: 
OpenRouterConnectionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:44,472 - 
devsynth.application.utils.token_tracker - WARNING - Failed to load tiktoken 
encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection refused by 
Responses - the call doesn&amp;#x27;t match any registered mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token counting2025-10-28 09:28:44,491 - 
devsynth.application.llm.openrouter_provider - INFO - Initialized OpenRouter 
provider with model: google/gemini-flash-1.52025-10-28 09:28:44,491 - fallback -
WARNING - Retry attempt 1/3 after 1.04s delay2025-10-28 09:28:44,491 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.04s)2025-10-28 09:28:45,528 - fallback - WARNING - Retry attempt 2/3 
after 1.77s delay2025-10-28 09:28:45,529 - 
devsynth.application.llm.openrouter_provider - WARNING - Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 1.77s)2025-10-28 09:28:47,300 - circuit_breaker - WARNING - Circuit 
breaker for _api_call transitioned to OPEN due to failure2025-10-28 09:28:47,301
- fallback - WARNING - Retry attempt 3/3 after 2.19s delay2025-10-28 
09:28:47,301 - devsynth.application.llm.openrouter_provider - WARNING - Retrying
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 2.19s)2025-10-28 09:28:49,492 - circuit_breaker - WARNING - Circuit 
breaker for _api_call is OPEN, failing fast2025-10-28 09:28:49,494 - fallback - 
WARNING - Circuit open - aborting retries for _wrapped2025-10-28 09:28:49,494 - 
devsynth.application.llm.openrouter_provider - ERROR - OpenRouter API error: 
Circuit breaker for _api_call is open------------------------------ Captured log
call -------------------------------WARNING  
devsynth.application.utils.token_tracker:logging_setup.py:615 Failed to load 
tiktoken encoding for model &amp;#x27;gpt-3.5-turbo&amp;#x27;: Connection 
refused by Responses - the call doesn&amp;#x27;t match any registered 
mock.Request: - GET 
https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktokenAvailab
le matches:- POST https://openrouter.ai/api/v1/chat/completions Method does not 
match. Falling back to approximate token countingINFO     
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Initialized 
OpenRouter provider with model: google/gemini-flash-1.5WARNING  
fallback:logging_setup.py:615 Retry attempt 1/3 after 1.04s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
1, delay 1.04s)WARNING  fallback:logging_setup.py:615 Retry attempt 2/3 after 
1.77s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
2, delay 1.77s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call transitioned to OPEN due to failureWARNING  
fallback:logging_setup.py:615 Retry attempt 3/3 after 2.19s delayWARNING  
devsynth.application.llm.openrouter_provider:logging_setup.py:615 Retrying 
OpenRouterProvider due to Network access disabled during tests (httpx) (attempt 
3, delay 2.19s)WARNING  circuit_breaker:logging_setup.py:615 Circuit breaker for
_api_call is OPEN, failing fastWARNING  fallback:logging_setup.py:615 Circuit 
open - aborting retries for _wrappedERROR    
devsynth.application.llm.openrouter_provider:logging_setup.py:615 OpenRouter API
error: Circuit breaker for _api_call is open____________ 
test_configure_logging_invokes_directory_creation_once 
____________logging_setup_module = &amp;lt;module 
&amp;#x27;devsynth.logging_setup&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_configure_logging_invokes0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458ab3e0&amp;gt;    
@pytest.mark.fast    def test_configure_logging_invokes_directory_creation_once(
logging_setup_module: ModuleType,        tmp_path: Path,        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: LOG-CONF-05A  ensure_log_dir_exists 
executes only on first configuration.&amp;quot;&amp;quot;&amp;quot;            
logging_setup = logging_setup_module        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(tmp_path))     
monkeypatch.delenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, raising=False) 
calls: list = []            def fake_ensure(path: str) -&amp;gt; str:           
calls.append(path)            return path            
monkeypatch.setattr(logging_setup, &amp;quot;ensure_log_dir_exists&amp;quot;, 
fake_ensure)            
logging_setup.configure_logging(log_dir=&amp;quot;logs&amp;quot;)        
logging_setup.configure_logging(log_dir=&amp;quot;logs&amp;quot;)    &amp;gt;   
assert calls == E       AssertionError: assert 
[&amp;#x27;/private/va...nvokes0/logs&amp;#x27;] == 
[&amp;#x27;/private/va...nvokes0/logs&amp;#x27;]E         E         Left 
contains 3 more items, first extra item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_invokes0/logs&amp;#x27;E         Use -v 
to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test
_logging_setup_configure_logging.py:183: 
AssertionError----------------------------- Captured stdout call 
-----------------------------WARNING: File logging failed - 2025-10-28 
09:28:49,960 - root - WARNING - Failed to set up file logging: [Errno 2] No such
file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_invokes0/logs/devsynth.log&amp;#x27;WARN
ING: File logging failed - 2025-10-28 09:28:49,960 - root - INFO - Logging 
configured for console output only (no file logging).WARNING: File logging 
failed - 2025-10-28 09:28:49,961 - root - WARNING - Failed to set up file 
logging: [Errno 2] No such file or directory: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_invokes0/logs/devsynth.log&amp;#x27;WARN
ING: File logging failed - 2025-10-28 09:28:49,961 - root - INFO - Logging 
configured for console output only (no file logging).______ 
test_configure_logging_reenables_file_handler_after_console_toggle 
______logging_setup_module = &amp;lt;module 
&amp;#x27;devsynth.logging_setup&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_configure_logging_reenabl0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145385880&amp;gt;    
@pytest.mark.fast    def 
test_configure_logging_reenables_file_handler_after_console_toggle(        
logging_setup_module: ModuleType,        tmp_path: Path,        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: LOG-CONF-09  toggling create_dir back on 
reinstates JSON handler.            Issue: issues/coverage-below-threshold.md   
&amp;quot;&amp;quot;&amp;quot;            logging_setup = logging_setup_module  
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(tmp_path))     
monkeypatch.delenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, raising=False) 
logging_setup.configure_logging(log_dir=&amp;quot;retention&amp;quot;, 
create_dir=False)            assert all(            not isinstance(handler, 
logging.FileHandler)            for handler in logging.getLogger().handlers     
), &amp;quot;Initial console-only run should not attach a file 
handler.&amp;quot;            calls: list = []            def 
track_directory(path: str) -&amp;gt; str:            calls.append(path)         
Path(path).mkdir(parents=True, exist_ok=True)            return path            
monkeypatch.setattr(logging_setup, &amp;quot;ensure_log_dir_exists&amp;quot;, 
track_directory)            
logging_setup.configure_logging(log_dir=&amp;quot;retention&amp;quot;, 
create_dir=True)    &amp;gt;       assert calls == E       AssertionError: 
assert [&amp;#x27;/private/va...l0/retention&amp;#x27;] == 
[&amp;#x27;/private/va...l0/retention&amp;#x27;]E         E         Left 
contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_reenabl0/retention&amp;#x27;E         
Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test
_logging_setup_configure_logging.py:353: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:50,039 - root - INFO - Logging 
configured for console output only (no file logging).2025-10-28 09:28:50,041 - 
root - INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_configure_logging_reenabl0/retention/devsynth.log_____________ 
test_configure_logging_retention_matrix ______________logging_setup_module = 
&amp;lt;module &amp;#x27;devsynth.logging_setup&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_configure_logging_retenti0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145446e70&amp;gt;create_dir = True, no_file_env = None, expected_effective = 
True    @pytest.mark.fast    @pytest.mark.parametrize(        
(&amp;quot;create_dir&amp;quot;, &amp;quot;no_file_env&amp;quot;, 
&amp;quot;expected_effective&amp;quot;),        [            pytest.param(True, 
None, True, id=&amp;quot;create-dir&amp;quot;),            pytest.param(True, 
&amp;quot;1&amp;quot;, False, id=&amp;quot;no-file-env&amp;quot;),            
pytest.param(False, None, False, id=&amp;quot;create-dir-disabled&amp;quot;),   
pytest.param(False, &amp;quot;1&amp;quot;, False, 
id=&amp;quot;no-file-env-create-dir-disabled&amp;quot;),        ],    )    def 
test_configure_logging_retention_matrix(        logging_setup_module: 
ModuleType,        tmp_path: Path,        monkeypatch: pytest.MonkeyPatch,      
create_dir: bool,        no_file_env: Optional,        expected_effective: bool,
) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Exercise retention 
decisions across create_dir and environment flags.&amp;quot;&amp;quot;&amp;quot;
logging_setup = logging_setup_module        project_dir = tmp_path / 
&amp;quot;retention_project&amp;quot;        project_dir.mkdir(parents=True, 
exist_ok=True)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(project_dir))  
monkeypatch.delenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, raising=False) 
if no_file_env is not None:            
monkeypatch.setenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, no_file_env)   
log_dir_argument = &amp;quot;relative/logs&amp;quot;        ensure_calls: 
list[Optional] = []        real_ensure = logging_setup.ensure_log_dir_exists    
def tracking(log_dir: Optional = None) -&amp;gt; str:            
ensure_calls.append(log_dir)            return real_ensure(log_dir)            
monkeypatch.setattr(logging_setup, &amp;quot;ensure_log_dir_exists&amp;quot;, 
tracking)            logging_setup.configure_logging(log_dir=log_dir_argument, 
create_dir=create_dir)            expected_dir = os.path.join(str(project_dir), 
log_dir_argument)        expected_file = os.path.join(            expected_dir, 
os.environ.get(&amp;quot;DEVSYNTH_LOG_FILENAME&amp;quot;, 
logging_setup.DEFAULT_LOG_FILENAME),        )            assert 
logging_setup._configured_log_dir == expected_dir        assert 
logging_setup._configured_log_file == expected_file        assert 
logging_setup._last_effective_config[0] == expected_dir        assert 
logging_setup._last_effective_config[1] == expected_file        assert 
logging_setup._last_effective_config[3] is expected_effective            if 
expected_effective:&amp;gt;           assert ensure_calls == E           
AssertionError: assert [&amp;#x27;/private/va...elative/logs&amp;#x27;] == 
[&amp;#x27;/private/va...elative/logs&amp;#x27;]E             E             Left
contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_retenti0/retention_project/relative/logs
&amp;#x27;E             Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test
_logging_setup_retention.py:108: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:28:50,227 - root
- INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_configure_logging_retenti0/retention_project/relative/logs/devsynth.
log________ test_configure_logging_relocates_absolute_paths 
________logging_setup_module = &amp;lt;module 
&amp;#x27;devsynth.logging_setup&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_configure_logging_relocat0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1454456a0&amp;gt;log_dir_input = 
PosixPath(&amp;#x27;/Users/caitlyn/devsynth/logs&amp;#x27;)log_file_name = 
&amp;#x27;home.json&amp;#x27;    @pytest.mark.fast    @pytest.mark.parametrize( 
(&amp;quot;log_dir_input&amp;quot;, &amp;quot;log_file_name&amp;quot;),        [
pytest.param(                Path.home() / &amp;quot;devsynth&amp;quot; / 
&amp;quot;logs&amp;quot;, &amp;quot;home.json&amp;quot;, 
id=&amp;quot;home-absolute&amp;quot;            ),            pytest.param(     
Path(&amp;quot;/var/tmp/devsynth/logs&amp;quot;), 
&amp;quot;system.json&amp;quot;, id=&amp;quot;non-home-absolute&amp;quot;       
),        ],    )    def test_configure_logging_relocates_absolute_paths(       
logging_setup_module: ModuleType,        tmp_path: Path,        monkeypatch: 
pytest.MonkeyPatch,        log_dir_input: Path,        log_file_name: str,    ) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Absolute paths are 
redirected into the sandboxed project directory.&amp;quot;&amp;quot;&amp;quot;  
logging_setup = logging_setup_module        project_dir = tmp_path / 
&amp;quot;sandbox_relocation&amp;quot;        project_dir.mkdir(parents=True, 
exist_ok=True)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(project_dir))  
monkeypatch.delenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, raising=False) 
log_file_input = log_dir_input / log_file_name            ensure_calls: 
list[Optional] = []        real_ensure = logging_setup.ensure_log_dir_exists    
def tracking(log_dir: Optional = None) -&amp;gt; str:            
ensure_calls.append(log_dir)            return real_ensure(log_dir)            
monkeypatch.setattr(logging_setup, &amp;quot;ensure_log_dir_exists&amp;quot;, 
tracking)            logging_setup.configure_logging(            
log_dir=str(log_dir_input),            log_file=str(log_file_input),            
create_dir=True,        )            home_prefix = str(Path.home())            
def expected_relative(path: Path) -&amp;gt; str:            path_str = str(path)
if path_str.startswith(home_prefix):                return 
path_str.replace(home_prefix, &amp;quot;&amp;quot;, 
1).lstrip(&amp;quot;/\\&amp;quot;)            return 
str(path.relative_to(path.anchor))            expected_dir = 
os.path.join(str(project_dir), expected_relative(log_dir_input))        
expected_file = os.path.join(str(project_dir), 
expected_relative(log_file_input))    &amp;gt;       assert ensure_calls == E   
AssertionError: assert [&amp;#x27;/private/va...evsynth/logs&amp;#x27;] == 
[&amp;#x27;/private/va...evsynth/logs&amp;#x27;]E         E         Left 
contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_relocat0/sandbox_relocation/Users/caitly
n/devsynth/logs&amp;#x27;E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test
_logging_setup_retention.py:189: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:28:50,297 - root
- INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_configure_logging_relocat0/sandbox_relocation/Users/caitlyn/devsynth
/logs/home.json______ test_configure_logging_relocates_absolute_paths 
______logging_setup_module = &amp;lt;module 
&amp;#x27;devsynth.logging_setup&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logg
ing_setup.py&amp;#x27;&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_configure_logging_relocat1&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1451e8fb0&amp;gt;log_dir_input = 
PosixPath(&amp;#x27;/var/tmp/devsynth/logs&amp;#x27;)log_file_name = 
&amp;#x27;system.json&amp;#x27;    @pytest.mark.fast    
@pytest.mark.parametrize(        (&amp;quot;log_dir_input&amp;quot;, 
&amp;quot;log_file_name&amp;quot;),        [            pytest.param(           
Path.home() / &amp;quot;devsynth&amp;quot; / &amp;quot;logs&amp;quot;, 
&amp;quot;home.json&amp;quot;, id=&amp;quot;home-absolute&amp;quot;            
),            pytest.param(                
Path(&amp;quot;/var/tmp/devsynth/logs&amp;quot;), 
&amp;quot;system.json&amp;quot;, id=&amp;quot;non-home-absolute&amp;quot;       
),        ],    )    def test_configure_logging_relocates_absolute_paths(       
logging_setup_module: ModuleType,        tmp_path: Path,        monkeypatch: 
pytest.MonkeyPatch,        log_dir_input: Path,        log_file_name: str,    ) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Absolute paths are 
redirected into the sandboxed project directory.&amp;quot;&amp;quot;&amp;quot;  
logging_setup = logging_setup_module        project_dir = tmp_path / 
&amp;quot;sandbox_relocation&amp;quot;        project_dir.mkdir(parents=True, 
exist_ok=True)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROJECT_DIR&amp;quot;, str(project_dir))  
monkeypatch.delenv(&amp;quot;DEVSYNTH_NO_FILE_LOGGING&amp;quot;, raising=False) 
log_file_input = log_dir_input / log_file_name            ensure_calls: 
list[Optional] = []        real_ensure = logging_setup.ensure_log_dir_exists    
def tracking(log_dir: Optional = None) -&amp;gt; str:            
ensure_calls.append(log_dir)            return real_ensure(log_dir)            
monkeypatch.setattr(logging_setup, &amp;quot;ensure_log_dir_exists&amp;quot;, 
tracking)            logging_setup.configure_logging(            
log_dir=str(log_dir_input),            log_file=str(log_file_input),            
create_dir=True,        )            home_prefix = str(Path.home())            
def expected_relative(path: Path) -&amp;gt; str:            path_str = str(path)
if path_str.startswith(home_prefix):                return 
path_str.replace(home_prefix, &amp;quot;&amp;quot;, 
1).lstrip(&amp;quot;/\\&amp;quot;)            return 
str(path.relative_to(path.anchor))            expected_dir = 
os.path.join(str(project_dir), expected_relative(log_dir_input))        
expected_file = os.path.join(str(project_dir), 
expected_relative(log_file_input))    &amp;gt;       assert ensure_calls == E   
AssertionError: assert [&amp;#x27;/private/va...evsynth/logs&amp;#x27;] == 
[&amp;#x27;/private/va...evsynth/logs&amp;#x27;]E         E         Left 
contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devs
ynth/logs&amp;#x27;E         Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test
_logging_setup_retention.py:189: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:28:50,323 - root
- INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devsynth/logs/
system.json___________ TestSyncManagerProtocol.test_sync_manager_initialization 
___________self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x123973320&amp;gt;    def test_sync_manager_initialization(self):    
&amp;quot;&amp;quot;&amp;quot;Test SyncManager initializes correctly with 
required stores.&amp;quot;&amp;quot;&amp;quot;        store1 = MockMemoryStore()
store2 = MockMemoryStore()    &amp;gt;       sync_manager = SyncManager(        
stores={&amp;quot;tinydb&amp;quot;: store1, &amp;quot;store2&amp;quot;: store2},
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:43: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x1451eb140&amp;gt;, &amp;#x27;s...MemoryStore object at 0x1451e9dc0&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError________ 
TestSyncManagerProtocol.test_sync_manager_write_to_all_stores _________self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397c920&amp;gt;    def 
test_sync_manager_write_to_all_stores(self):        
&amp;quot;&amp;quot;&amp;quot;Test write operation propagates to all configured 
stores.&amp;quot;&amp;quot;&amp;quot;        store1 = MockMemoryStore()        
store2 = MockMemoryStore()    &amp;gt;       sync_manager = SyncManager(        
stores={&amp;quot;tinydb&amp;quot;: store1, &amp;quot;store2&amp;quot;: store2},
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:62: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145aebd10&amp;gt;, &amp;#x27;s...MemoryStore object at 0x145af96d0&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError_______ 
TestSyncManagerProtocol.test_sync_manager_read_from_first_store ________self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397cdd0&amp;gt;    def 
test_sync_manager_read_from_first_store(self):        
&amp;quot;&amp;quot;&amp;quot;Test read operation returns from first store 
containing the key.&amp;quot;&amp;quot;&amp;quot;        store1 = 
MockMemoryStore()        store2 = MockMemoryStore()    &amp;gt;       
sync_manager = SyncManager(            stores={&amp;quot;tinydb&amp;quot;: 
store1, &amp;quot;store2&amp;quot;: store2}, 
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:76: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145ae9640&amp;gt;, &amp;#x27;s...MemoryStore object at 0x1457ce6c0&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError___ 
TestSyncManagerProtocol.test_sync_manager_read_fallback_to_second_store ____self
= &amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397d280&amp;gt;    def 
test_sync_manager_read_fallback_to_second_store(self):        
&amp;quot;&amp;quot;&amp;quot;Test read operation falls back to second store if 
first doesn&amp;#x27;t have key.&amp;quot;&amp;quot;&amp;quot;        store1 = 
MockMemoryStore()        store2 = MockMemoryStore()    &amp;gt;       
sync_manager = SyncManager(            stores={&amp;quot;tinydb&amp;quot;: 
store1, &amp;quot;store2&amp;quot;: store2}, 
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:89: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145ae8620&amp;gt;, &amp;#x27;s...MemoryStore object at 0x1458a9b20&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError_ 
TestSyncManagerProtocol.test_sync_manager_read_raises_keyerror_if_not_found 
__self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397d730&amp;gt;    def 
test_sync_manager_read_raises_keyerror_if_not_found(self):        
&amp;quot;&amp;quot;&amp;quot;Test read operation raises KeyError if key not 
found in any store.&amp;quot;&amp;quot;&amp;quot;        store1 = 
MockMemoryStore()        store2 = MockMemoryStore()    &amp;gt;       
sync_manager = SyncManager(            stores={&amp;quot;tinydb&amp;quot;: 
store1, &amp;quot;store2&amp;quot;: store2}, 
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:102: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145aebcb0&amp;gt;, &amp;#x27;s...MemoryStore object at 0x1453fba70&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError_________ 
TestSyncManagerProtocol.test_sync_manager_transaction_commit _________self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397dbe0&amp;gt;    def test_sync_manager_transaction_commit(self):
&amp;quot;&amp;quot;&amp;quot;Test transaction commits changes to all 
stores.&amp;quot;&amp;quot;&amp;quot;        store1 = MockMemoryStore()        
store2 = MockMemoryStore()    &amp;gt;       sync_manager = SyncManager(        
stores={&amp;quot;tinydb&amp;quot;: store1, &amp;quot;store2&amp;quot;: store2},
required_stores={&amp;quot;tinydb&amp;quot;}        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_syn
c_manager_protocol.py:114: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;tinydb&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145ae8bc0&amp;gt;, &amp;#x27;s...MemoryStore object at 0x14542c6e0&amp;gt;}, 
required_stores={&amp;#x27;tinydb&amp;#x27;}, 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:            raise ValueError(f&amp;quot;Missing 
stores: {&amp;#x27;, &amp;#x27;.join(sorted(missing))}&amp;quot;)            
allowed = self.required_stores | self.optional_stores        unexpected = 
configured - allowed        if unexpected:&amp;gt;           raise ValueError(  
&amp;quot;Unexpected stores configured: &amp;quot; f&amp;quot;{&amp;#x27;, 
&amp;#x27;.join(sorted(unexpected))}&amp;quot;            )E           
ValueError: Unexpected stores configured: 
store2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:88: ValueError_ 
TestSyncManagerProtocol.test_sync_manager_transaction_rollback_on_exception 
__self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397e090&amp;gt;    def 
test_sync_manager_transaction_rollback_on_exception(self):        
&amp;quot;&amp;quot;&amp;quot;Test transaction rolls back changes if exception 
occurs.&amp;quot;&amp;quot;&amp;quot;        store1 = MockMemoryStore()        
store2 = MockMemoryStore()    &amp;gt;       sync_manager = 
SyncManager(stores={&amp;quot;store1&amp;quot;: store1, 
&amp;quot;store2&amp;quot;: store2})                       
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/
github.com/ravenoak/devsynth/tests/unit/memory/test_sync_manager_protocol.py:133
: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ &amp;lt;string&amp;gt;:6: in __init__    ???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
SyncManager(stores={&amp;#x27;store1&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145ae9a30&amp;gt;, &amp;#x27;s... object at 0x1454011f0&amp;gt;}, 
required_stores=frozenset({&amp;#x27;tinydb&amp;#x27;}), 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:&amp;gt;           raise 
ValueError(f&amp;quot;Missing stores: {&amp;#x27;, 
&amp;#x27;.join(sorted(missing))}&amp;quot;)E           ValueError: Missing 
stores: 
tinydb/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:83: ValueError_________ 
TestSyncManagerProtocol.test_sync_manager_with_generic_type __________self = 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x12397e9f0&amp;gt;    def test_sync_manager_with_generic_type(self): 
&amp;quot;&amp;quot;&amp;quot;Test SyncManager works with different value 
types.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       sync_manager = 
SyncManager(stores={&amp;quot;mock&amp;quot;: MockMemoryStore()})               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/gith
ub.com/ravenoak/devsynth/tests/unit/memory/test_sync_manager_protocol.py:162: _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/typing.py:1184: in __call__    result = self.__origin__(*args,
**kwargs)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&amp;lt;string&amp;gt;:6: 
in __init__    ???_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ self = SyncManager(stores={&amp;#x27;mock&amp;#x27;: 
&amp;lt;tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x145a1d490&amp;gt;}, required_stores=frozenset({&amp;#x27;tinydb&amp;#x27;}), 
optional_stores=frozenset({&amp;#x27;kuzu&amp;#x27;, &amp;#x27;lmdb&amp;#x27;, 
&amp;#x27;duckdb&amp;#x27;}))    def __post_init__(self) -&amp;gt; None:        
configured = frozenset(self.stores)        missing = self.required_stores - 
configured        if missing:&amp;gt;           raise 
ValueError(f&amp;quot;Missing stores: {&amp;#x27;, 
&amp;#x27;.join(sorted(missing))}&amp;quot;)E           ValueError: Missing 
stores: 
tinydb/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/s
ync_manager.py:83: ValueError_____________________ 
test_reasoning_loop_records_results ______________________mocker = 
&amp;lt;pytest_mock.plugin.MockerFixture object at 0x145814050&amp;gt;    
@pytest.mark.fast    def test_reasoning_loop_records_results(mocker) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;It stores results through the 
coordinator.            ReqID: DR-1        &amp;quot;&amp;quot;&amp;quot;       
coordinator = mocker.create_autospec(EDRRCoordinator, instance=True)        
result = 
build_dialectical_sequence(status=&amp;quot;completed&amp;quot;)&amp;gt;       
mocker.patch(            
&amp;quot;devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoning&
amp;quot;,            return_value=result,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/tes
t_dialectical_reasoning.py:21: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__    return 
self._start_patch(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib
/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch    mocked: 
MockType = p.start()                       
^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Ve
rsions/3.12/lib/python3.12/unittest/mock.py:1624: in start    result = 
self.__enter__()             
^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.frame
work/Versions/3.12/lib/python3.12/unittest/mock.py:1467: in __enter__    
original, local = self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = &amp;lt;unittest.mock._patch object at 0x1458476e0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module &amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; does not have the attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/opt/homebrew/Cellar/python@3.12
/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.
py:1437: AttributeError__________________ 
test_reasoning_loop_logs_consensus_failure __________________mocker = 
&amp;lt;pytest_mock.plugin.MockerFixture object at 0x1458cc950&amp;gt;    
@pytest.mark.fast    def test_reasoning_loop_logs_consensus_failure(mocker) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;It delegates consensus 
failures to the coordinator.            ReqID: DR-2        
&amp;quot;&amp;quot;&amp;quot;            coordinator = 
mocker.create_autospec(EDRRCoordinator, instance=True)            class 
DummyConsensusError(ConsensusError):            def __init__(self, message: 
str):                Exception.__init__(self, message)    &amp;gt;       
mocker.patch(            
&amp;quot;devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoning&
amp;quot;,            side_effect=DummyConsensusError(&amp;quot;no 
consensus&amp;quot;),        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/tes
t_dialectical_reasoning.py:46: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__    return 
self._start_patch(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib
/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch    mocked: 
MockType = p.start()                       
^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Ve
rsions/3.12/lib/python3.12/unittest/mock.py:1624: in start    result = 
self.__enter__()             
^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.frame
work/Versions/3.12/lib/python3.12/unittest/mock.py:1467: in __enter__    
original, local = self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = &amp;lt;unittest.mock._patch object at 0x145a1de20&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module &amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; does not have the attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/opt/homebrew/Cellar/python@3.12
/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.
py:1437: AttributeError__________________ 
test_reasoning_loop_persists_phase_results __________________mocker = 
&amp;lt;pytest_mock.plugin.MockerFixture object at 0x145ae9040&amp;gt;    
@pytest.mark.fast    def test_reasoning_loop_persists_phase_results(mocker) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;It stores results using the
memory manager.            ReqID: DR-3        &amp;quot;&amp;quot;&amp;quot;    
memory_manager = mocker.Mock()        coordinator = 
EDRRCoordinator(memory_manager)        result = 
build_dialectical_sequence(status=&amp;quot;completed&amp;quot;)&amp;gt;       
mocker.patch(            
&amp;quot;devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoning&
amp;quot;,            return_value=result,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/tes
t_dialectical_reasoning.py:67: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_mock/plugin.py:448: in __call__    return 
self._start_patch(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib
/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch    mocked: 
MockType = p.start()                       
^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Ve
rsions/3.12/lib/python3.12/unittest/mock.py:1624: in start    result = 
self.__enter__()             
^^^^^^^^^^^^^^^^/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.frame
work/Versions/3.12/lib/python3.12/unittest/mock.py:1467: in __enter__    
original, local = self.get_original()                      ^^^^^^^^^^^^^^^^^^^_ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = &amp;lt;unittest.mock._patch object at 0x145ae99d0&amp;gt;    def 
get_original(self):        target = self.getter()        name = self.attribute  
original = DEFAULT        local = False            try:            original = 
target.__dict__        except (AttributeError, KeyError):            original = 
getattr(target, name, DEFAULT)        else:            local = True            
if name in _builtins and isinstance(target, ModuleType):            self.create 
= True            if not self.create and original is DEFAULT:&amp;gt;           
raise AttributeError(                &amp;quot;%s does not have the attribute 
%r&amp;quot; % (target, name)            )E           AttributeError: 
&amp;lt;module &amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; does not have the attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/opt/homebrew/Cellar/python@3.12
/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.
py:1437: AttributeError___________________ 
test_reasoning_loop_runs_until_complete ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145845c40&amp;gt;    
@pytest.mark.fast    def test_reasoning_loop_runs_until_complete(monkeypatch):  
&amp;quot;&amp;quot;&amp;quot;It continues until the reasoning process is 
complete.            ReqID: DR-4        &amp;quot;&amp;quot;&amp;quot;          
calls = []            def fake_apply(team, task, critic, memory):            
calls.append(task.get(&amp;quot;solution&amp;quot;))            if len(calls) ==
1:                return {&amp;quot;status&amp;quot;: 
&amp;quot;in_progress&amp;quot;, &amp;quot;synthesis&amp;quot;: 
&amp;quot;next&amp;quot;}            return {&amp;quot;status&amp;quot;: 
&amp;quot;completed&amp;quot;, &amp;quot;synthesis&amp;quot;: 
&amp;quot;final&amp;quot;}    &amp;gt;       monkeypatch.setattr(rl, 
&amp;quot;_apply_dialectical_reasoning&amp;quot;, fake_apply)E       
AttributeError: &amp;lt;module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/methodology/test_dialectical_reasoning_loop.py:3
2: AttributeError__________________ test_reasoning_loop_logs_consensus_failure 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x1458444a0&amp;gt;caplog = &amp;lt;_pytest.logging.LogCaptureFixture object 
at 0x14584aed0&amp;gt;    @pytest.mark.fast    def 
test_reasoning_loop_logs_consensus_failure(monkeypatch, caplog):        
&amp;quot;&amp;quot;&amp;quot;It logs and swallows consensus failures.          
ReqID: DR-5        &amp;quot;&amp;quot;&amp;quot;            def 
fail_apply(team, task, critic, memory):            raise 
DummyConsensusError(&amp;quot;no consensus&amp;quot;)    &amp;gt;       
monkeypatch.setattr(rl, &amp;quot;_apply_dialectical_reasoning&amp;quot;, 
fail_apply)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/methodology/test_dialectical_reasoning_loop.py:4
9: AttributeError_________________ test_reasoning_loop_respects_max_iterations 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x145844e90&amp;gt;    @pytest.mark.fast    def 
test_reasoning_loop_respects_max_iterations(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;It stops after reaching the iteration limit.      
ReqID: DR-6        &amp;quot;&amp;quot;&amp;quot;            calls = []         
def fake_apply(team, task, critic, memory):            calls.append(1)          
return {&amp;quot;status&amp;quot;: &amp;quot;in_progress&amp;quot;}    &amp;gt;
monkeypatch.setattr(rl, &amp;quot;_apply_dialectical_reasoning&amp;quot;, 
fake_apply)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/methodology/test_dialectical_reasoning_loop.py:7
1: AttributeError________________ test_reasoning_loop_respects_total_time_budget
________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145402b10&amp;gt;    @pytest.mark.fast    @pytest.mark.unit    def 
test_reasoning_loop_respects_total_time_budget(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;It stops when the total time budget is exhausted. 
ReqID: DR-6        &amp;quot;&amp;quot;&amp;quot;    &amp;gt;       
monkeypatch.setattr(rl, &amp;quot;_apply_dialectical_reasoning&amp;quot;, 
_slow_apply)E       AttributeError: &amp;lt;module 
&amp;#x27;devsynth.methodology.edrr.reasoning_loop&amp;#x27; from 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/meth
odology/edrr/reasoning_loop.py&amp;#x27;&amp;gt; has no attribute 
&amp;#x27;_apply_dialectical_reasoning&amp;#x27;/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/methodology/test_reasoning_loop_time_budget.py:2
5: AttributeError________________________ test_ceremony_mapping_to_phase 
________________________    @pytest.mark.fast    def 
test_ceremony_mapping_to_phase():        
&amp;quot;&amp;quot;&amp;quot;Configured ceremonies map to the correct EDRR 
phases.&amp;quot;&amp;quot;&amp;quot;        config = {            
&amp;quot;settings&amp;quot;: {                
&amp;quot;ceremonyMapping&amp;quot;: {                    
&amp;quot;planning&amp;quot;: &amp;quot;retrospect.iteration_planning&amp;quot;,
&amp;quot;dailyStandup&amp;quot;: 
&amp;quot;phase_progression_tracking&amp;quot;,                    
&amp;quot;review&amp;quot;: &amp;quot;refine.outputs_review&amp;quot;,          
&amp;quot;retrospective&amp;quot;: 
&amp;quot;retrospect.process_evaluation&amp;quot;,                }            }
}        adapter = SprintAdapter(config)        assert 
adapter.get_ceremony_phase(&amp;quot;planning&amp;quot;) == Phase.RETROSPECT    
assert adapter.get_ceremony_phase(&amp;quot;review&amp;quot;) == Phase.REFINE   
assert adapter.get_ceremony_phase(&amp;quot;retrospective&amp;quot;) == 
Phase.RETROSPECT&amp;gt;       assert 
adapter.get_ceremony_phase(&amp;quot;dailyStandup&amp;quot;) is NoneE       
AssertionError: assert &amp;lt;Phase.DIFFERENTIATE: 
&amp;#x27;differentiate&amp;#x27;&amp;gt; is NoneE        +  where 
&amp;lt;Phase.DIFFERENTIATE: &amp;#x27;differentiate&amp;#x27;&amp;gt; = 
get_ceremony_phase(&amp;#x27;dailyStandup&amp;#x27;)E        +    where 
get_ceremony_phase = &amp;lt;devsynth.methodology.sprint.SprintAdapter object at
0x1458a9880&amp;gt;.get_ceremony_phase/Users/caitlyn/Projects/github.com/ravenoa
k/devsynth/tests/unit/methodology/test_sprint_adapter.py:63: 
AssertionError_____________________ test_map_ceremony_to_phase_defaults 
______________________    @pytest.mark.fast    def 
test_map_ceremony_to_phase_defaults():        
&amp;quot;&amp;quot;&amp;quot;Common ceremonies resolve to their default EDRR 
phases.&amp;quot;&amp;quot;&amp;quot;&amp;gt;       assert 
map_ceremony_to_phase(&amp;quot;planning&amp;quot;) == Phase.RETROSPECTE       
AssertionError: assert &amp;lt;Phase.EXPAND: &amp;#x27;expand&amp;#x27;&amp;gt; 
== &amp;lt;Phase.RETROSPECT: &amp;#x27;retrospect&amp;#x27;&amp;gt;E        +  
where &amp;lt;Phase.EXPAND: &amp;#x27;expand&amp;#x27;&amp;gt; = 
map_ceremony_to_phase(&amp;#x27;planning&amp;#x27;)E        +  and   
&amp;lt;Phase.RETROSPECT: &amp;#x27;retrospect&amp;#x27;&amp;gt; = 
Phase.RETROSPECT/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
methodology/test_sprint_hooks.py:11: AssertionError_____________________ 
test_adapter_uses_ceremony_defaults ______________________    @pytest.mark.fast 
def test_adapter_uses_ceremony_defaults():        
&amp;quot;&amp;quot;&amp;quot;SprintAdapter falls back to default phase mapping 
for bare ceremony names.&amp;quot;&amp;quot;&amp;quot;        config = {        
&amp;quot;settings&amp;quot;: {                
&amp;quot;ceremonyMapping&amp;quot;: {                    
&amp;quot;planning&amp;quot;: &amp;quot;planning&amp;quot;,                    
&amp;quot;review&amp;quot;: &amp;quot;review&amp;quot;,                    
&amp;quot;retrospective&amp;quot;: &amp;quot;retrospective&amp;quot;,           
}            }        }        adapter = SprintAdapter(config)&amp;gt;       
assert adapter.get_ceremony_phase(&amp;quot;planning&amp;quot;) == 
Phase.RETROSPECTE       AssertionError: assert &amp;lt;Phase.EXPAND: 
&amp;#x27;expand&amp;#x27;&amp;gt; == &amp;lt;Phase.RETROSPECT: 
&amp;#x27;retrospect&amp;#x27;&amp;gt;E        +  where &amp;lt;Phase.EXPAND: 
&amp;#x27;expand&amp;#x27;&amp;gt; = 
get_ceremony_phase(&amp;#x27;planning&amp;#x27;)E        +    where 
get_ceremony_phase = &amp;lt;devsynth.methodology.sprint.SprintAdapter object at
0x145814a10&amp;gt;.get_ceremony_phaseE        +  and   
&amp;lt;Phase.RETROSPECT: &amp;#x27;retrospect&amp;#x27;&amp;gt; = 
Phase.RETROSPECT/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
methodology/test_sprint_hooks.py:31: AssertionError_______________________ 
test_graph_transitions_complete ________________________engine = 
&amp;lt;devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngin
e object at 0x1458fe0c0&amp;gt;    @pytest.mark.fast    def 
test_graph_transitions_complete(engine):        wf = 
engine.create_workflow(&amp;quot;wf&amp;quot;, &amp;quot;desc&amp;quot;)        
step1 = WorkflowStep(id=&amp;quot;s1&amp;quot;, name=&amp;quot;Step 1&amp;quot;,
description=&amp;quot;d1&amp;quot;, agent_type=&amp;quot;t&amp;quot;)        
step2 = WorkflowStep(id=&amp;quot;s2&amp;quot;, name=&amp;quot;Step 2&amp;quot;,
description=&amp;quot;d2&amp;quot;, agent_type=&amp;quot;t&amp;quot;)        wf 
= engine.add_step(wf, step1)        wf = engine.add_step(wf, step2)            #
Patch orchestration to be a no-op that appends a message        class 
FakeService:            def process_step(self, state, step):                
state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, 
&amp;quot;content&amp;quot;: f&amp;quot;ran {step.id}&amp;quot;})               
return state            with patch(            
&amp;quot;devsynth.orchestration.step_executor.OrchestrationService&amp;quot;, 
FakeService        ):            result = engine.execute_workflow(wf, 
context={})    &amp;gt;       assert result.status == WorkflowStatus.COMPLETEDE 
AssertionError: assert &amp;lt;WorkflowStatus.FAILED: 
&amp;#x27;failed&amp;#x27;&amp;gt; == &amp;lt;WorkflowStatus.COMPLETED: 
&amp;#x27;completed&amp;#x27;&amp;gt;E        +  where 
&amp;lt;WorkflowStatus.FAILED: &amp;#x27;failed&amp;#x27;&amp;gt; = 
Workflow(id=&amp;#x27;5e2ecae9-31fc-4707-a4e8-c24dd9626970&amp;#x27;, 
name=&amp;#x27;wf&amp;#x27;, description=&amp;#x27;desc&amp;#x27;, 
steps=[WorkflowStep(id=&amp;#x27;s1&amp;#x27;, 
name=&amp;#x27;...d_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 206816), 
updated_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 207119)).statusE        + 
and   &amp;lt;WorkflowStatus.COMPLETED: &amp;#x27;completed&amp;#x27;&amp;gt; = 
WorkflowStatus.COMPLETED/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/orchestration/test_graph_transitions_and_controls.py:33: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:52,207 - 
devsynth.adapters.orchestration.langgraph_adapter - INFO - Error executing 
workflow: object() takes no arguments------------------------------ Captured log
call -------------------------------INFO     
devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 Error 
executing workflow: object() takes no arguments_________________ 
test_retry_branch_succeeds_with_max_retries __________________engine = 
&amp;lt;devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngin
e object at 0x1458101a0&amp;gt;    @pytest.mark.fast    def 
test_retry_branch_succeeds_with_max_retries(engine):        wf = 
engine.create_workflow(&amp;quot;wf&amp;quot;, &amp;quot;desc&amp;quot;)        
step1 = WorkflowStep(id=&amp;quot;s1&amp;quot;, name=&amp;quot;Step 1&amp;quot;,
description=&amp;quot;d1&amp;quot;, agent_type=&amp;quot;t&amp;quot;)        wf 
= engine.add_step(wf, step1)            calls = {&amp;quot;n&amp;quot;: 0}      
class FlakyService:            def process_step(self, state, step):             
calls[&amp;quot;n&amp;quot;] += 1                if calls[&amp;quot;n&amp;quot;]
== 1:                    raise RuntimeError(&amp;quot;transient&amp;quot;)      
state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, 
&amp;quot;content&amp;quot;: &amp;quot;ok&amp;quot;})                return 
state            with patch(            
&amp;quot;devsynth.orchestration.step_executor.OrchestrationService&amp;quot;, 
FlakyService        ):            # Allow one retry            result = 
engine.execute_workflow(wf, context={&amp;quot;max_retries&amp;quot;: 1})    
&amp;gt;       assert calls[&amp;quot;n&amp;quot;] == 2E       assert 0 == 
2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/t
est_graph_transitions_and_controls.py:76: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:52,231 - 
devsynth.adapters.orchestration.langgraph_adapter - INFO - Error executing 
workflow: object() takes no arguments------------------------------ Captured log
call -------------------------------INFO     
devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 Error 
executing workflow: object() takes no arguments________________________ 
test_streaming_callback_called ________________________engine = 
&amp;lt;devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngin
e object at 0x1458844d0&amp;gt;    @pytest.mark.fast    def 
test_streaming_callback_called(engine):        wf = 
engine.create_workflow(&amp;quot;wf&amp;quot;, &amp;quot;desc&amp;quot;)        
step1 = WorkflowStep(id=&amp;quot;s1&amp;quot;, name=&amp;quot;Step 1&amp;quot;,
description=&amp;quot;d1&amp;quot;, agent_type=&amp;quot;t&amp;quot;)        wf 
= engine.add_step(wf, step1)            class Service:            def 
process_step(self, state, step):                
state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;agent&amp;quot;, 
&amp;quot;content&amp;quot;: &amp;quot;result&amp;quot;})                return 
state            stream_events = []            def stream_cb(evt):            
stream_events.append(evt)            with 
patch(&amp;quot;devsynth.orchestration.step_executor.OrchestrationService&amp;qu
ot;, Service):            result = engine.execute_workflow(wf, 
context={&amp;quot;stream_callback&amp;quot;: stream_cb})    &amp;gt;       
assert result.status == WorkflowStatus.COMPLETEDE       AssertionError: assert 
&amp;lt;WorkflowStatus.FAILED: &amp;#x27;failed&amp;#x27;&amp;gt; == 
&amp;lt;WorkflowStatus.COMPLETED: &amp;#x27;completed&amp;#x27;&amp;gt;E        
+  where &amp;lt;WorkflowStatus.FAILED: &amp;#x27;failed&amp;#x27;&amp;gt; = 
Workflow(id=&amp;#x27;c642e516-09a3-4f8b-90b0-81d0b8bf40c8&amp;#x27;, 
name=&amp;#x27;wf&amp;#x27;, description=&amp;#x27;desc&amp;#x27;, 
steps=[WorkflowStep(id=&amp;#x27;s1&amp;#x27;, 
name=&amp;#x27;...d_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 246799), 
updated_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 246867)).statusE        + 
and   &amp;lt;WorkflowStatus.COMPLETED: &amp;#x27;completed&amp;#x27;&amp;gt; = 
WorkflowStatus.COMPLETED/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/orchestration/test_graph_transitions_and_controls.py:99: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:52,246 - 
devsynth.adapters.orchestration.langgraph_adapter - INFO - Error executing 
workflow: object() takes no arguments------------------------------ Captured log
call -------------------------------INFO     
devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 Error 
executing workflow: object() takes no arguments__________________ 
test_cancellation_pauses_before_first_step __________________engine = 
&amp;lt;devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngin
e object at 0x1458874d0&amp;gt;    @pytest.mark.fast    def 
test_cancellation_pauses_before_first_step(engine):        wf = 
engine.create_workflow(&amp;quot;wf&amp;quot;, &amp;quot;desc&amp;quot;)        
step1 = WorkflowStep(id=&amp;quot;s1&amp;quot;, name=&amp;quot;Step 1&amp;quot;,
description=&amp;quot;d1&amp;quot;, agent_type=&amp;quot;t&amp;quot;)        wf 
= engine.add_step(wf, step1)            spy = MagicMock()            class 
SpyService:            def process_step(self, state, step):                spy()
return state            with 
patch(&amp;quot;devsynth.orchestration.step_executor.OrchestrationService&amp;qu
ot;, SpyService):            result = engine.execute_workflow(wf, 
context={&amp;quot;is_cancelled&amp;quot;: lambda: True})    &amp;gt;       
assert result.status == WorkflowStatus.PAUSEDE       AssertionError: assert 
&amp;lt;WorkflowStatus.FAILED: &amp;#x27;failed&amp;#x27;&amp;gt; == 
&amp;lt;WorkflowStatus.PAUSED: &amp;#x27;paused&amp;#x27;&amp;gt;E        +  
where &amp;lt;WorkflowStatus.FAILED: &amp;#x27;failed&amp;#x27;&amp;gt; = 
Workflow(id=&amp;#x27;6a71d70f-5b3d-4bf4-b0a3-dc42ec569ae5&amp;#x27;, 
name=&amp;#x27;wf&amp;#x27;, description=&amp;#x27;desc&amp;#x27;, 
steps=[WorkflowStep(id=&amp;#x27;s1&amp;#x27;, 
name=&amp;#x27;...d_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 267139), 
updated_at=datetime.datetime(2025, 10, 28, 9, 28, 52, 267253)).statusE        + 
and   &amp;lt;WorkflowStatus.PAUSED: &amp;#x27;paused&amp;#x27;&amp;gt; = 
WorkflowStatus.PAUSED/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/orchestration/test_graph_transitions_and_controls.py:122: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:28:52,267 - 
devsynth.adapters.orchestration.langgraph_adapter - INFO - Error executing 
workflow: object() takes no arguments------------------------------ Captured log
call -------------------------------INFO     
devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 Error 
executing workflow: object() takes no arguments__________________ 
test_adapter_openai_provider_stub_offline ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458ff3e0&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_adapter_openai_provider_stub_offline(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;        When DEVSYNTH_OFFLINE=true and normalized 
stubs are applied (default),        adapter-level OpenAIProvider should return 
deterministic responses.        &amp;quot;&amp;quot;&amp;quot;        # Ensure 
offline and that provider availability flags don&amp;#x27;t accidentally enable 
real backend        monkeypatch.setenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, 
&amp;quot;true&amp;quot;)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_PROVIDER&amp;quot;, 
&amp;quot;stub&amp;quot;)            # Import locally to ensure stub application
via tests/conftest autouse fixture has run        import 
devsynth.adapters.provider_system as provider_system  # type: ignore    &amp;gt;
p = provider_system.OpenAIProvider()            
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       TypeError: OpenAIProvider.__init__() 
missing 1 required positional argument: 
&amp;#x27;api_key&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
tests/unit/providers/test_provider_stub_offline.py:20: 
TypeError________________________ test_generate_arguments_sorted 
________________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch 
object at 0x1457d33b0&amp;gt;    @pytest.mark.fast    def 
test_generate_arguments_sorted(monkeypatch):        # Reverse order in text, 
expect sorted by position/content        text = (            &amp;quot;Argument 
2\n&amp;quot;            &amp;quot;Position: AGAINST\n&amp;quot;            
&amp;quot;Content: zeta\n&amp;quot;            &amp;quot;Counterargument: 
aaa\n\n&amp;quot;            &amp;quot;Argument 1\n&amp;quot;            
&amp;quot;Position: FOR\n&amp;quot;            &amp;quot;Content: 
alpha\n&amp;quot;            &amp;quot;Counterargument: bbb\n&amp;quot;        )
class LL(StubLLM):            def query(self, prompt: str) -&amp;gt; str:       
if &amp;quot;Arguments&amp;quot; in prompt or &amp;quot;arguments&amp;quot; in 
prompt:                    return text                if &amp;quot;Determine if 
the following reasoning&amp;quot; in prompt:                    return 
&amp;quot;yes&amp;quot;                return &amp;quot;ok&amp;quot;            
service = make_service(llm=LL())        change = 
RequirementChange(requirement_id=uuid4(), change_type=ChangeType.ADD)        
change.new_state = Requirement(title=&amp;quot;T&amp;quot;, 
description=&amp;quot;D&amp;quot;)        args = 
service._generate_arguments(change, &amp;quot;t&amp;quot;, 
&amp;quot;a&amp;quot;)        # After sorting, FOR/alpha should come before 
AGAINST/zeta&amp;gt;       assert args[0][&amp;quot;content&amp;quot;].lower() 
== &amp;quot;alpha&amp;quot;               ^^^^^^^^^^^^^^^^^^E       TypeError: 
&amp;#x27;ParsedDialecticalArgument&amp;#x27; object is not 
subscriptable/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/req
uirements/test_dialectical_reasoner_determinism.py:185: TypeError___________ 
TestRecommendationGeneration.test_calculates_percentages ___________self = 
&amp;lt;tests.unit.scripts.test_analyze_test_dependencies.TestRecommendationGene
ration object at 0x123b1dcd0&amp;gt;    def test_calculates_percentages(self):  
&amp;quot;&amp;quot;&amp;quot;Test percentage calculations in 
recommendations.&amp;quot;&amp;quot;&amp;quot;        analysis_results = [      
{&amp;quot;has_isolation_marker&amp;quot;: True, 
&amp;quot;safe_for_parallel&amp;quot;: True, &amp;quot;risk_score&amp;quot;: 0},
{&amp;quot;has_isolation_marker&amp;quot;: True, 
&amp;quot;safe_for_parallel&amp;quot;: True, &amp;quot;risk_score&amp;quot;: 1},
{                &amp;quot;has_isolation_marker&amp;quot;: True,                
&amp;quot;safe_for_parallel&amp;quot;: False,                
&amp;quot;risk_score&amp;quot;: 10,            },            {                
&amp;quot;has_isolation_marker&amp;quot;: True,                
&amp;quot;safe_for_parallel&amp;quot;: False,                
&amp;quot;risk_score&amp;quot;: 15,            },        ]    &amp;gt;       
recommendations = generate_recommendations(analysis_results)                    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/rav
enoak/devsynth/tests/unit/scripts/test_analyze_test_dependencies.py:235: _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
analysis_results = [{&amp;#x27;has_isolation_marker&amp;#x27;: True, 
&amp;#x27;risk_score&amp;#x27;: 0, &amp;#x27;safe_for_parallel&amp;#x27;: True},
{&amp;#x27;has_isolation_marker&amp;#x27;: True, 
&amp;#x27;risk_scor..._score&amp;#x27;: 10, 
&amp;#x27;safe_for_parallel&amp;#x27;: False}, 
{&amp;#x27;has_isolation_marker&amp;#x27;: True, &amp;#x27;risk_score&amp;#x27;:
15, &amp;#x27;safe_for_parallel&amp;#x27;: False}]    def 
generate_recommendations(analysis_results: List[Dict]) -&amp;gt; Dict:        
&amp;quot;&amp;quot;&amp;quot;Generate recommendations based on analysis 
results.&amp;quot;&amp;quot;&amp;quot;        total_files = 
len(analysis_results)        files_with_isolation = sum(            1 for r in 
analysis_results if r.get(&amp;quot;has_isolation_marker&amp;quot;, False)      
)        safe_for_removal = sum(            1            for r in 
analysis_results            if r.get(&amp;quot;safe_for_parallel&amp;quot;, 
False) and r.get(&amp;quot;has_isolation_marker&amp;quot;, False)        )      
# Categorize by risk level        low_risk = [            r            for r in 
analysis_results            if r.get(&amp;quot;risk_score&amp;quot;, 0) 
&amp;lt;= 2 and r.get(&amp;quot;has_isolation_marker&amp;quot;, False)        ] 
medium_risk = [            r            for r in analysis_results            if 
3 &amp;lt;= r.get(&amp;quot;risk_score&amp;quot;, 0) &amp;lt;= 8 and 
r.get(&amp;quot;has_isolation_marker&amp;quot;, False)        ]        high_risk
= [            r            for r in analysis_results            if 
r.get(&amp;quot;risk_score&amp;quot;, 0) &amp;gt; 8 and 
r.get(&amp;quot;has_isolation_marker&amp;quot;, False)        ]            
return {            &amp;quot;summary&amp;quot;: {                
&amp;quot;total_test_files&amp;quot;: total_files,                
&amp;quot;files_with_isolation_markers&amp;quot;: files_with_isolation,         
&amp;quot;safe_for_removal&amp;quot;: safe_for_removal,                
&amp;quot;removal_percentage&amp;quot;: round(                    (             
(safe_for_removal / files_with_isolation * 100)                        if 
files_with_isolation &amp;gt; 0                        else 0                   
),                    1,                ),            },            
&amp;quot;risk_categories&amp;quot;: {                
&amp;quot;low_risk&amp;quot;: {                    &amp;quot;count&amp;quot;: 
len(low_risk),&amp;gt;                   &amp;quot;files&amp;quot;: 
[r[&amp;quot;relative_path&amp;quot;] for r in low_risk],                       
^^^^^^^^^^^^^^^^^^                },                
&amp;quot;medium_risk&amp;quot;: {                    &amp;quot;count&amp;quot;:
len(medium_risk),                    &amp;quot;files&amp;quot;: 
[r[&amp;quot;relative_path&amp;quot;] for r in medium_risk],                },  
&amp;quot;high_risk&amp;quot;: {                    &amp;quot;count&amp;quot;: 
len(high_risk),                    &amp;quot;files&amp;quot;: 
[r[&amp;quot;relative_path&amp;quot;] for r in high_risk],                },    
},            &amp;quot;recommendations&amp;quot;: {                
&amp;quot;immediate_removal&amp;quot;: [r[&amp;quot;relative_path&amp;quot;] for
r in low_risk],                &amp;quot;careful_review&amp;quot;: 
[r[&amp;quot;relative_path&amp;quot;] for r in medium_risk],                
&amp;quot;keep_isolation&amp;quot;: [r[&amp;quot;relative_path&amp;quot;] for r 
in high_risk],            },        }E       KeyError: 
&amp;#x27;relative_path&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/scripts/analyze_test_dependencies.py:313: KeyError____________ 
TestTestExecutionBenchmark.test_run_benchmark_success _____________self = 
&amp;lt;tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchm
ark object at 0x123b75490&amp;gt;mock_run = &amp;lt;MagicMock 
name=&amp;#x27;run&amp;#x27; id=&amp;#x27;5463322896&amp;#x27;&amp;gt;    
@patch(&amp;quot;subprocess.run&amp;quot;)    def 
test_run_benchmark_success(self, mock_run):        
&amp;quot;&amp;quot;&amp;quot;Test successful benchmark 
execution.&amp;quot;&amp;quot;&amp;quot;        # Mock successful subprocess 
result        mock_result = MagicMock()        mock_result.returncode = 0       
mock_result.stdout = &amp;quot;===== 5 passed, 0 failed, 2 skipped in 1.23s 
=====&amp;quot;        mock_result.stderr = &amp;quot;&amp;quot;        
mock_run.return_value = mock_result            benchmark = 
TestExecutionBenchmark()            with patch(&amp;quot;time.time&amp;quot;, 
side_effect=[1000.0, 1001.23]):  # 1.23 second duration            result = 
benchmark.run_benchmark(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;, 2)            assert 
result[&amp;quot;target&amp;quot;] == &amp;quot;unit-tests&amp;quot;        
assert result[&amp;quot;speed&amp;quot;] == &amp;quot;fast&amp;quot;        
assert result[&amp;quot;workers&amp;quot;] == 2&amp;gt;       assert 
result[&amp;quot;duration&amp;quot;] == 1.23E       assert 1.2300000000000182 ==
1.23/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test
_benchmark_test_execution.py:54: AssertionError----------------------------- 
Captured stdout call -----------------------------  Running unit-tests (speed: 
fast, workers: 2)...____________ 
TestTestExecutionBenchmark.test_run_benchmark_failure _____________self = 
&amp;lt;tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchm
ark object at 0x123b75d90&amp;gt;mock_run = &amp;lt;MagicMock 
name=&amp;#x27;run&amp;#x27; id=&amp;#x27;5460831808&amp;#x27;&amp;gt;    
@patch(&amp;quot;subprocess.run&amp;quot;)    def 
test_run_benchmark_failure(self, mock_run):        
&amp;quot;&amp;quot;&amp;quot;Test benchmark failure 
handling.&amp;quot;&amp;quot;&amp;quot;        # Mock subprocess failure        
mock_result = MagicMock()        mock_result.returncode = 1        
mock_result.stdout = &amp;quot;===== 2 passed, 3 failed in 2.45s =====&amp;quot;
mock_result.stderr = &amp;quot;Some error&amp;quot;        mock_run.return_value
= mock_result            benchmark = TestExecutionBenchmark()            with 
patch(&amp;quot;time.time&amp;quot;, side_effect=[1000.0, 1002.45]):            
result = benchmark.run_benchmark(&amp;quot;unit-tests&amp;quot;, None, 1)       
assert result[&amp;quot;success&amp;quot;] == False&amp;gt;       assert 
result[&amp;quot;passed&amp;quot;] == 2E       assert 0 == 
2/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_be
nchmark_test_execution.py:93: AssertionError----------------------------- 
Captured stdout call -----------------------------  Running unit-tests (speed: 
all, workers: 1)..._____________________ test_parametrize_speed_marker_parity 
_____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_parametrize_speed_marker_0&amp;#x27;)    
@pytest.mark.fast    def test_parametrize_speed_marker_parity(tmp_path: Path):  
# Create a synthetic test file that uses only pytest.param speed markers        
test_code = textwrap.dedent(            &amp;quot;&amp;quot;&amp;quot;          
import pytest                @pytest.mark.parametrize(                
&amp;quot;x&amp;quot;,                [                    pytest.param(1, 
marks=pytest.mark.medium),                    pytest.param(2, 
marks=pytest.mark.medium),                ],            )            def 
test_derived_speed_from_params(x):                assert x in (1, 2)            
&amp;quot;&amp;quot;&amp;quot;        )        file_path = tmp_path / 
&amp;quot;test_sample_param_speed.py&amp;quot;        
file_path.write_text(test_code)            # Load enhanced_test_parser from 
scripts        etp = _import_module_from_path(            
&amp;quot;enhanced_test_parser&amp;quot;,            Path(__file__).parents[3] /
&amp;quot;scripts&amp;quot; / &amp;quot;enhanced_test_parser.py&amp;quot;,      
)        vtm = _import_module_from_path(            
&amp;quot;verify_test_markers&amp;quot;,            Path(__file__).parents[3] / 
&amp;quot;scripts&amp;quot; / &amp;quot;verify_test_markers.py&amp;quot;,       
)            # Parse using enhanced test parser        parsed = 
etp.parse_test_file(str(file_path))        # Find our test        target = 
[&amp;gt;           t for t in parsed[&amp;quot;tests&amp;quot;] if 
t[&amp;quot;name&amp;quot;] == 
&amp;quot;test_derived_speed_from_params&amp;quot;                       
^^^^^^^^^^^^^^^        ]E       TypeError: list indices must be integers or 
slices, not 
str/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_
enhanced_test_parser_marker_parity.py:54: TypeError__________________ 
test_returns_error_when_syntax_is_invalid ___________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_returns_error_when_syntax0&amp;#x27;)    def 
test_returns_error_when_syntax_is_invalid(tmp_path: Path) -&amp;gt; None:       
bad_file = tmp_path / &amp;quot;bad.py&amp;quot;        
bad_file.write_text(&amp;quot;def broken(:\n    pass\n&amp;quot;, 
encoding=&amp;quot;utf-8&amp;quot;)            result = run_script(tmp_path)    
&amp;gt;       assert result.returncode == 1E       AssertionError: assert 0 == 
1E        +  where 0 = 
CompletedProcess(args=[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/.venv/bin/python&amp;#x27;, 
&amp;#x27;/Users/caitlyn/Projec...-of-caitlyn/pytest-1428/test_returns_error_whe
n_syntax0&amp;#x27;], returncode=0, stdout=&amp;#x27; OK\n&amp;#x27;, 
stderr=&amp;#x27;&amp;#x27;).returncode/Users/caitlyn/Projects/github.com/raveno
ak/devsynth/tests/unit/scripts/test_find_syntax_errors.py:32: 
AssertionError_______________________ test_returns_zero_with_no_errors 
_______________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_returns_zero_with_no_erro0&amp;#x27;)    def 
test_returns_zero_with_no_errors(tmp_path: Path) -&amp;gt; None:        
good_file = tmp_path / &amp;quot;good.py&amp;quot;        
good_file.write_text(&amp;quot;print(&amp;#x27;ok&amp;#x27;)\n&amp;quot;, 
encoding=&amp;quot;utf-8&amp;quot;)            result = run_script(tmp_path)    
assert result.returncode == 0&amp;gt;       assert &amp;quot;No syntax errors 
found&amp;quot; in result.stdoutE       AssertionError: assert &amp;#x27;No 
syntax errors found&amp;#x27; in &amp;#x27; OK\n&amp;#x27;E        +  where 
&amp;#x27; OK\n&amp;#x27; = 
CompletedProcess(args=[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/.venv/bin/python&amp;#x27;, 
&amp;#x27;/Users/caitlyn/Projec...-of-caitlyn/pytest-1428/test_returns_zero_with
_no_erro0&amp;#x27;], returncode=0, stdout=&amp;#x27; OK\n&amp;#x27;, 
stderr=&amp;#x27;&amp;#x27;).stdout/Users/caitlyn/Projects/github.com/ravenoak/d
evsynth/tests/unit/scripts/test_find_syntax_errors.py:43: AssertionError_____ 
TestQualityReportGenerator.test_quality_score_with_missing_mutation ______self =
&amp;lt;tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerat
or object at 0x123baa570&amp;gt;    def 
test_quality_score_with_missing_mutation(self):        
&amp;quot;&amp;quot;&amp;quot;Test quality score calculation when mutation 
testing is skipped.&amp;quot;&amp;quot;&amp;quot;        metrics = {            
&amp;quot;coverage&amp;quot;: {&amp;quot;line_coverage&amp;quot;: 90.0},        
&amp;quot;mutation&amp;quot;: {&amp;quot;mutation_score&amp;quot;: None},  # 
Skipped            &amp;quot;property&amp;quot;: 
{&amp;quot;total_property_tests&amp;quot;: 5, &amp;quot;enabled&amp;quot;: 
False},            &amp;quot;organization&amp;quot;: 
{&amp;quot;marker_compliance&amp;quot;: 100.0},            
&amp;quot;performance&amp;quot;: {&amp;quot;parallel_speedup&amp;quot;: 3.5},   
}            score = calculate_overall_quality_score(metrics)            # 
Should handle None mutation score gracefully&amp;gt;       assert 60 &amp;lt;= 
score &amp;lt;= 85E       assert 60 &amp;lt;= 
58.5/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test
_generate_quality_report.py:124: AssertionError_______ 
TestQualityReportGenerator.test_recommendations_for_good_metrics _______self = 
&amp;lt;tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerat
or object at 0x123baaa20&amp;gt;    def 
test_recommendations_for_good_metrics(self):        
&amp;quot;&amp;quot;&amp;quot;Test recommendations when metrics are already 
good.&amp;quot;&amp;quot;&amp;quot;        metrics = {            
&amp;quot;coverage&amp;quot;: {&amp;quot;line_coverage&amp;quot;: 95.0},        
&amp;quot;mutation&amp;quot;: {&amp;quot;skipped&amp;quot;: False, 
&amp;quot;mutation_score&amp;quot;: 85.0},            
&amp;quot;property&amp;quot;: {&amp;quot;total_property_tests&amp;quot;: 20, 
&amp;quot;enabled&amp;quot;: True},            &amp;quot;organization&amp;quot;:
{&amp;quot;marker_compliance&amp;quot;: 98.0},            
&amp;quot;performance&amp;quot;: {&amp;quot;parallel_speedup&amp;quot;: 5.0},   
}            recommendations = generate_quality_recommendations(metrics)        
# Should have fewer recommendations for good metrics        assert 
len(recommendations) &amp;lt;= 2        # Should have positive 
reinforcement&amp;gt;       assert any(&amp;quot;excellent&amp;quot; in 
rec.lower() for rec in recommendations)E       assert FalseE        +  where 
False = any(&amp;lt;generator object 
TestQualityReportGenerator.test_recommendations_for_good_metrics.&amp;lt;locals&
amp;gt;.&amp;lt;genexpr&amp;gt; at 
0x145b7ee90&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/scripts/test_generate_quality_report.py:142: 
AssertionError__________________ test_verify_test_markers_collection_error 
___________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_verify_test_markers_colle0&amp;#x27;)    
@pytest.mark.fast    def test_verify_test_markers_collection_error(tmp_path: 
Path) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Reports collection 
errors. ReqID: QA-02&amp;quot;&amp;quot;&amp;quot;        test_file = tmp_path /
&amp;quot;test_bad.py&amp;quot;        test_file.write_text(            
&amp;quot;import pytest\nimport nonexistent_module\n\n@pytest.mark.fast\ndef 
test_fail():\n    pass\n&amp;quot;,            
encoding=&amp;quot;utf-8&amp;quot;,        )            
vtm.PERSISTENT_CACHE.clear()        vtm.FILE_SIGNATURES.clear()            
result = vtm.verify_directory_markers(str(tmp_path))&amp;gt;       assert 
result[&amp;quot;files_with_issues&amp;quot;] == 1E       assert 0 == 
1/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ve
rify_test_markers.py:43: AssertionError_________________________ 
test_audit_detects_violation _________________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_audit_detects_violation0&amp;#x27;)    
@pytest.mark.fast    def test_audit_detects_violation(tmp_path: Path) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;A file with forbidden patterns should
be reported.&amp;quot;&amp;quot;&amp;quot;        config = tmp_path / 
&amp;quot;unsafe.cfg&amp;quot;        
config.write_text(&amp;quot;password=secret&amp;quot;)&amp;gt;       results = 
policy_audit.audit()                  ^^^^^^^^^^^^^^^^^^E       AttributeError: 
module &amp;#x27;policy_audit&amp;#x27; has no attribute 
&amp;#x27;audit&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/security/test_policy_audit.py:18: 
AttributeError_________________________ test_audit_passes_clean_file 
_________________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_audit_passes_clean_file0&amp;#x27;)    
@pytest.mark.fast    def test_audit_passes_clean_file(tmp_path: Path) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;A clean file should yield no 
findings.&amp;quot;&amp;quot;&amp;quot;        config = tmp_path / 
&amp;quot;safe.cfg&amp;quot;        
config.write_text(&amp;quot;value=1&amp;quot;)&amp;gt;       assert 
policy_audit.audit() == []               ^^^^^^^^^^^^^^^^^^E       
AttributeError: module &amp;#x27;policy_audit&amp;#x27; has no attribute 
&amp;#x27;audit&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/te
sts/unit/security/test_policy_audit.py:27: 
AttributeError_________________________ test_run_requires_pre_deploy 
_________________________mock_policy = &amp;lt;MagicMock 
name=&amp;#x27;main&amp;#x27; 
id=&amp;#x27;5464296096&amp;#x27;&amp;gt;mock_safety = &amp;lt;MagicMock 
name=&amp;#x27;run_safety&amp;#x27; 
id=&amp;#x27;5464300080&amp;#x27;&amp;gt;mock_bandit = &amp;lt;MagicMock 
name=&amp;#x27;run_bandit&amp;#x27; 
id=&amp;#x27;5464302720&amp;#x27;&amp;gt;monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1463ee240&amp;gt;    
@patch(&amp;quot;security_audit.audit.run_bandit&amp;quot;)    
@patch(&amp;quot;security_audit.audit.run_safety&amp;quot;)    
@patch(&amp;quot;security_audit.verify_security_policy.main&amp;quot;, 
return_value=0)    @pytest.mark.fast    def test_run_requires_pre_deploy(       
mock_policy: MagicMock,        mock_safety: MagicMock,        mock_bandit: 
MagicMock,        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;The audit aborts if pre-deploy approval is 
missing.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.delenv(&amp;quot;DEVSYNTH_PRE_DEPLOY_APPROVED&amp;quot;, 
raising=False)&amp;gt;       with pytest.raises(RuntimeError):             
^^^^^^^^^^^^^^^^^^^^^^^^^^^E       Failed: DID NOT RAISE &amp;lt;class 
&amp;#x27;RuntimeError&amp;#x27;&amp;gt;/Users/caitlyn/Projects/github.com/raven
oak/devsynth/tests/unit/security/test_security_audit.py:105: 
Failed----------------------------- Captured stdout call 
----------------------------- Environment: DEVSYNTH_VERSION=0.1.0a1, 
DEVSYNTH_ENV=alpha Pre-deploy policy checks not approved, but allowing for alpha
release_________________ test_mvuu_config_schema_and_sample_validate 
__________________    @pytest.mark.fast    def 
test_mvuu_config_schema_and_sample_validate():        
&amp;quot;&amp;quot;&amp;quot;Ensure the MVUU config schema is valid and the 
sample conforms to it.            This test is intentionally strict but 
local-only:        - validates the schema structure using 
Draft7Validator.check_schema        - validates the sample instance against the 
schema using jsonschema.validate        &amp;quot;&amp;quot;&amp;quot;        
repo_root = Path(__file__).resolve().parents[3]        schema_path = repo_root /
&amp;quot;docs/specifications/mvuu_config.schema.json&amp;quot;        
sample_path = repo_root / 
&amp;quot;docs/specifications/mvuu_config.sample.json&amp;quot;            
assert schema_path.exists(), f&amp;quot;Schema file not found: 
{schema_path}&amp;quot;        assert sample_path.exists(), f&amp;quot;Sample 
config file not found: {sample_path}&amp;quot;            schema = 
json.loads(schema_path.read_text(encoding=&amp;quot;utf-8&amp;quot;))        
sample = json.loads(sample_path.read_text(encoding=&amp;quot;utf-8&amp;quot;))  
# 1) Validate the schema is itself a valid Draft-07 schema        
Draft7Validator.check_schema(schema)            # 2) Validate the sample 
instance against the schema&amp;gt;       validate(instance=sample, 
schema=schema)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/sp
ecifications/test_mvuu_config_schema_validation.py:44: _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ instance = 
{&amp;#x27;$schema&amp;#x27;: &amp;#x27;./mvuu_config.schema.json&amp;#x27;, 
&amp;#x27;issues&amp;#x27;: {}, &amp;#x27;schema&amp;#x27;: 
&amp;#x27;docs/specifications/mvuuschema.json&amp;#x27;, 
&amp;#x27;storage&amp;#x27;: {&amp;#x27;format&amp;#x27;: 
&amp;#x27;json&amp;#x27;, &amp;#x27;path&amp;#x27;: 
&amp;#x27;docs/specifications/mvuu_database.json&amp;#x27;}}schema = 
{&amp;#x27;$schema&amp;#x27;: 
&amp;#x27;http://json-schema.org/draft-07/schema#&amp;#x27;, 
&amp;#x27;additionalProperties&amp;#x27;: False, &amp;#x27;properties&amp;#x27;:
{&amp;#x27;issues&amp;#x27;: {&amp;#x27;addit...ype&amp;#x27;: 
&amp;#x27;string&amp;#x27;}}, &amp;#x27;required&amp;#x27;: 
[&amp;#x27;path&amp;#x27;, &amp;#x27;format&amp;#x27;], 
&amp;#x27;type&amp;#x27;: &amp;#x27;object&amp;#x27;}}, 
&amp;#x27;required&amp;#x27;: [&amp;#x27;schema&amp;#x27;, 
&amp;#x27;storage&amp;#x27;, &amp;#x27;issues&amp;#x27;], ...}cls = 
&amp;lt;class &amp;#x27;jsonschema.validators.Draft7Validator&amp;#x27;&amp;gt;,
args = (), kwargs = {}validator = 
Draft7Validator(schema={&amp;#x27;$schema&amp;#x27;: 
&amp;#x27;http://json-...ft-07/schema#&amp;#x27;, 
&amp;#x27;additionalProperties&amp;#x27;: False, &amp;#x27;properties&amp;#x27;:
{&amp;#x27;issu...uired&amp;#x27;: [&amp;#x27;path&amp;#x27;, 
&amp;#x27;format&amp;#x27;], &amp;#x27;type&amp;#x27;: 
&amp;#x27;object&amp;#x27;}}, &amp;#x27;required&amp;#x27;: 
[&amp;#x27;schema&amp;#x27;, &amp;#x27;storage&amp;#x27;, 
&amp;#x27;issues&amp;#x27;], ...}, format_checker=None)error = 
&amp;lt;ValidationError: &amp;quot;Additional properties are not allowed 
(&amp;#x27;$schema&amp;#x27; was unexpected)&amp;quot;&amp;gt;    def 
validate(instance, schema, cls=None, *args, **kwargs):  # noqa: D417        
&amp;quot;&amp;quot;&amp;quot;        Validate an instance under the given 
schema.                &amp;gt;&amp;gt;&amp;gt; validate([2, 3, 4], 
{&amp;quot;maxItems&amp;quot;: 2})            Traceback (most recent call last):
...            ValidationError: [2, 3, 4] is too long            
:func:`~jsonschema.validators.validate` will first verify that the        
provided schema is itself valid, since not doing so can lead to less        
obvious error messages and fail in less obvious or consistent ways.            
If you know you have a valid schema already, especially        if you intend to 
validate multiple instances with        the same schema, you likely would prefer
using the        `jsonschema.protocols.Validator.validate` method directly on a 
specific validator (e.g. ``Draft202012Validator.validate``).                
Arguments:                instance:                    The instance to validate 
schema:                    The schema to validate with                cls 
(jsonschema.protocols.Validator):                    The class that will be used
to validate the instance.            If the ``cls`` argument is not provided, 
two things will happen        in accordance with the specification. First, if 
the schema has a        :kw:`$schema` keyword containing a known meta-schema _ 
then the        proper validator will be used. The specification recommends that
all schemas contain :kw:`$schema` properties for this reason. If no        
:kw:`$schema` property is found, the default validator class is the        
latest released draft.            Any other provided positional and keyword 
arguments will be passed        on when instantiating the ``cls``.            
Raises:                `jsonschema.exceptions.ValidationError`:                 
if the instance is invalid                `jsonschema.exceptions.SchemaError`:  
if the schema itself is invalid            .. rubric:: Footnotes        ..  
known by a validator registered with            
`jsonschema.validators.validates`            &amp;quot;&amp;quot;&amp;quot;     
if cls is None:            cls = validator_for(schema)            
cls.check_schema(schema)        validator = cls(schema, *args, **kwargs)        
error = exceptions.best_match(validator.iter_errors(instance))        if error 
is not None:&amp;gt;           raise errorE           
jsonschema.exceptions.ValidationError: Additional properties are not allowed 
(&amp;#x27;$schema&amp;#x27; was unexpected)E           E           Failed 
validating &amp;#x27;additionalProperties&amp;#x27; in schema:E               
{&amp;#x27;$schema&amp;#x27;: 
&amp;#x27;http://json-schema.org/draft-07/schema#&amp;#x27;,E                
&amp;#x27;title&amp;#x27;: &amp;#x27;DevSynth MVUU Config Schema&amp;#x27;,E    
&amp;#x27;type&amp;#x27;: &amp;#x27;object&amp;#x27;,E                
&amp;#x27;required&amp;#x27;: [&amp;#x27;schema&amp;#x27;, 
&amp;#x27;storage&amp;#x27;, &amp;#x27;issues&amp;#x27;],E                
&amp;#x27;additionalProperties&amp;#x27;: False,E                
&amp;#x27;properties&amp;#x27;: {&amp;#x27;schema&amp;#x27;: 
{&amp;#x27;type&amp;#x27;: &amp;#x27;string&amp;#x27;,E                         
&amp;#x27;description&amp;#x27;: &amp;#x27;Path to the MVUU record &amp;#x27;E  
&amp;#x27;schema (JSON Schema file).&amp;#x27;,E                                
&amp;#x27;default&amp;#x27;: 
&amp;#x27;docs/specifications/mvuuschema.json&amp;#x27;},E                      
&amp;#x27;storage&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;object&amp;#x27;,E                                           
&amp;#x27;required&amp;#x27;: [&amp;#x27;path&amp;#x27;, 
&amp;#x27;format&amp;#x27;],E                                           
&amp;#x27;additionalProperties&amp;#x27;: False,E                               
&amp;#x27;properties&amp;#x27;: {&amp;#x27;path&amp;#x27;: 
{&amp;#x27;type&amp;#x27;: &amp;#x27;string&amp;#x27;,E                         
&amp;#x27;description&amp;#x27;: &amp;#x27;Location &amp;#x27;E                 
&amp;#x27;for &amp;#x27;E                                                       
&amp;#x27;storing &amp;#x27;E                                                   
&amp;#x27;MVUU &amp;#x27;E                                                      
&amp;#x27;records &amp;#x27;E                                                   
&amp;#x27;(e.g., &amp;#x27;E                                                    
&amp;#x27;JSON &amp;#x27;E                                                      
&amp;#x27;database).&amp;#x27;,E                                                
&amp;#x27;default&amp;#x27;: 
&amp;#x27;docs/specifications/mvuu_database.json&amp;#x27;},E                   
&amp;#x27;format&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;string&amp;#x27;,E                                                    
&amp;#x27;enum&amp;#x27;: [&amp;#x27;json&amp;#x27;],E                          
&amp;#x27;description&amp;#x27;: &amp;#x27;Storage &amp;#x27;E                  
&amp;#x27;format. &amp;#x27;E                                                   
&amp;#x27;Currently &amp;#x27;E                                                 
&amp;#x27;only &amp;#x27;E                                                      
&amp;quot;&amp;#x27;json&amp;#x27; &amp;quot;E                                  
&amp;#x27;is &amp;#x27;E                                                        
&amp;#x27;supported.&amp;#x27;,E                                                
&amp;#x27;default&amp;#x27;: &amp;#x27;json&amp;#x27;}}},E                      
&amp;#x27;issues&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;object&amp;#x27;,E                                          
&amp;#x27;required&amp;#x27;: [],E                                          
&amp;#x27;additionalProperties&amp;#x27;: False,E                               
&amp;#x27;properties&amp;#x27;: {&amp;#x27;github&amp;#x27;: 
{&amp;#x27;type&amp;#x27;: &amp;#x27;object&amp;#x27;,E                         
&amp;#x27;required&amp;#x27;: [&amp;#x27;base_url&amp;#x27;,E                   
&amp;#x27;token&amp;#x27;],E                                                    
&amp;#x27;additionalProperties&amp;#x27;: False,E                               
&amp;#x27;properties&amp;#x27;: {&amp;#x27;base_url&amp;#x27;: 
{&amp;#x27;type&amp;#x27;: &amp;#x27;string&amp;#x27;},E                        
&amp;#x27;token&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;string&amp;#x27;}}},E                                                 
&amp;#x27;jira&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;object&amp;#x27;,E                                                    
&amp;#x27;required&amp;#x27;: [&amp;#x27;base_url&amp;#x27;,E                   
&amp;#x27;token&amp;#x27;],E                                                    
&amp;#x27;additionalProperties&amp;#x27;: False,E                               
&amp;#x27;properties&amp;#x27;: {&amp;#x27;base_url&amp;#x27;: 
{&amp;#x27;type&amp;#x27;: &amp;#x27;string&amp;#x27;},E                        
&amp;#x27;token&amp;#x27;: {&amp;#x27;type&amp;#x27;: 
&amp;#x27;string&amp;#x27;}}}}}}}E           E           On instance:E          
{&amp;#x27;$schema&amp;#x27;: &amp;#x27;./mvuu_config.schema.json&amp;#x27;,E   
&amp;#x27;schema&amp;#x27;: 
&amp;#x27;docs/specifications/mvuuschema.json&amp;#x27;,E                
&amp;#x27;storage&amp;#x27;: {&amp;#x27;path&amp;#x27;: 
&amp;#x27;docs/specifications/mvuu_database.json&amp;#x27;,E                    
&amp;#x27;format&amp;#x27;: &amp;#x27;json&amp;#x27;},E                
&amp;#x27;issues&amp;#x27;: 
{}}/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/sit
e-packages/jsonschema/validators.py:1332: ValidationError____________ 
test_collect_behavior_tests_fallback_when_no_tests_ran ____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145c16360&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_behavior_tests_fa0&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_behavior_tests_fallback_when_no_tests_ran(monkeypatch, tmp_path):  
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-01  Behavior/integration fallback 
when no tests ran.&amp;quot;&amp;quot;&amp;quot;        # Simulate the 
behavior/integration fallback branch when a speed_category is        # provided 
and the initial collection yields &amp;quot;no tests ran&amp;quot;. The function
# should retry with a relaxed marker expression and return the second set of    
# collected node ids.            calls: list[dict] = []            def fake_run(
cmd, check=False, capture_output=False, text=False        ):  # type: ignore    
# Record the call for assertions            
calls.append({&amp;quot;cmd&amp;quot;: cmd[:]})            joined = &amp;quot; 
&amp;quot;.join(cmd)            if &amp;quot;--collect-only&amp;quot; in cmd and
&amp;quot;-m&amp;quot; in cmd:                # First path: the pre-check for 
behavior/integration when                # speed_category is set                
if &amp;quot;tests/behavior/&amp;quot; in joined or cmd[-1] == 
&amp;quot;.&amp;quot;:                    # Return a signal equivalent to no 
tests collected under this                    # filter                    return
_CP(stdout=&amp;quot;no tests ran\n&amp;quot;, returncode=0)            # The 
subsequent actual collect_cmd should run with either relaxed            # marker
or same path. Return a couple of synthetic node ids to be            # 
sanitized.            out = (                
&amp;quot;tests/behavior/test_fake.py::test_case\n&amp;quot;                
&amp;quot;tests/behavior/test_other.py::test_ok\n&amp;quot;            )        
return _CP(stdout=out, returncode=0)            
monkeypatch.setattr(&amp;quot;subprocess.run&amp;quot;, fake_run)            # 
Use a temporary cache dir so we don&amp;#x27;t affect the real project cache    
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;1&amp;quot;)        # Force cache directory to tmp by changing CWD 
since function writes relative path        monkeypatch.chdir(tmp_path)          
# Create a minimal behavior tests tree so pruning-by-existence keeps our ids    
(tmp_path / &amp;quot;tests&amp;quot; / 
&amp;quot;behavior&amp;quot;).mkdir(parents=True, exist_ok=True)        
(tmp_path / &amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;test_fake.py&amp;quot;).write_text(            &amp;quot;def 
test_case():\n    assert True\n&amp;quot;        )        (tmp_path / 
&amp;quot;tests&amp;quot; / &amp;quot;behavior&amp;quot; / 
&amp;quot;test_other.py&amp;quot;).write_text(            &amp;quot;def 
test_ok():\n    assert True\n&amp;quot;        )    &amp;gt;       result = 
collect_tests_with_cache(target=&amp;quot;behavior-tests&amp;quot;, 
speed_category=&amp;quot;fast&amp;quot;)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/c
aitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_collect_beh
avior_fallback.py:63: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = _collect_via_pytest(_ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
def _collect_via_pytest(        *,        target: str,        test_path: str,   
category_expr: str,        normalized_filter: str | None,        
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_collect_behavior_tests_fallback_when_no_tests_ran.&amp;lt;locals&amp;gt;.fa
ke_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:28:59,169 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=behavior-tests (fast)  collecting via 
pytest------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=behavior-tests (fast)  collecting via pytest_________ 
test_collect_tests_with_cache_prunes_nonexistent_and_caches __________tmp_path =
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14644aa20&amp;gt;    
@pytest.mark.fast    def 
test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path, 
monkeypatch):        # Create a fake tests directory with a simple structure    
tests_dir = tmp_path / &amp;quot;tests&amp;quot;        unit_dir = tests_dir / 
&amp;quot;unit&amp;quot;        unit_dir.mkdir(parents=True)        # Create 
some dummy test files        t1 = unit_dir / &amp;quot;test_a.py&amp;quot;      
t1.write_text(&amp;quot;def test_a():\n    assert True\n&amp;quot;)        t2 = 
unit_dir / &amp;quot;test_b.py&amp;quot;        t2.write_text(&amp;quot;def 
test_b():\n    assert True\n&amp;quot;)            # Monkeypatch TARGET_PATHS 
lookup by pretending &amp;#x27;all-tests&amp;#x27; maps to our tmp tests dir    
from devsynth.testing import run_tests as rt            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;all-tests&amp;quot;, 
str(tests_dir))            # Simulate pytest --collect-only -q output lines via 
subprocess.run        class FakeProc:            def __init__(self, stdout: str,
returncode: int = 0, stderr: str = &amp;quot;&amp;quot;):                
self.stdout = stdout                self.returncode = returncode                
self.stderr = stderr            def fake_run(            cmd, check=False, 
capture_output=True, text=True, timeout=None        ):  # noqa: D401            
# Emit node ids including a non-existent file and a line-number suffix          
lines = [                f&amp;quot;{t1}::test_a&amp;quot;,                
f&amp;quot;{t2}:42&amp;quot;,  # should be sanitized to path only then pruned 
check happens on path                
f&amp;quot;{tests_dir}/missing_test.py::test_missing&amp;quot;,            ]    
return FakeProc(&amp;quot;\n&amp;quot;.join(lines), 0, &amp;quot;&amp;quot;)    
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Ensure os.path.exists behaves normally but returns False for missing_test.py  
real_exists = os.path.exists            def fake_exists(path):            if 
str(path).endswith(&amp;quot;missing_test.py&amp;quot;):                return 
False            return real_exists(path)            
monkeypatch.setattr(rt.os.path, &amp;quot;exists&amp;quot;, fake_exists)        
# Point cache directory to tmp so we don&amp;#x27;t touch repo state        
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(tmp_path /
&amp;quot;.cache&amp;quot;))    &amp;gt;       out = 
collect_tests_with_cache(&amp;quot;all-tests&amp;quot;)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/ravenoak
/devsynth/tests/unit/testing/test_collect_cache_sanitize.py:80: _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = 
&amp;#x27;all-tests&amp;#x27;, speed_category = None    def 
collect_tests_with_cache(        target: str,        speed_category: str | None 
= None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError_____________ 
test_collect_tests_with_cache_synthesizes_when_empty _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643eb40&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_1&amp;#x27;)    def 
test_collect_tests_with_cache_synthesizes_when_empty(monkeypatch, tmp_path):    
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-02  Synthesize minimal list when 
collection is empty.            When both primary and fallback collections 
return no node ids and no cache exists,        collect_tests_with_cache should 
synthesize a minimal file list by scanning the        filesystem under the 
target path for test_*.py files.        &amp;quot;&amp;quot;&amp;quot;        # 
Arrange: create an isolated temporary test tree        test_root = tmp_path / 
&amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot;        
test_root.mkdir(parents=True)        dummy = test_root / 
&amp;quot;test_dummy.py&amp;quot;        dummy.write_text(&amp;quot;def 
test_ok():\n    assert True\n&amp;quot;)            # Import module under test  
from devsynth.testing import run_tests as rt            # Point the target 
mapping to our isolated test directory        old_target = 
rt.TARGET_PATHS.get(&amp;quot;unit-tests&amp;quot;)        
rt.TARGET_PATHS[&amp;quot;unit-tests&amp;quot;] = str(test_root)            # 
Ensure cwd is the temp directory so the cache dir is local and empty        
monkeypatch.chdir(tmp_path)            # Monkeypatch subprocess.run to simulate 
empty collection outputs for both        # the primary and fallback collectors. 
def _fake_run(            cmd, check=False, capture_output=True, text=True      
):  # type: ignore            return types.SimpleNamespace(returncode=0, 
stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;)            
monkeypatch.setattr(subprocess, &amp;quot;run&amp;quot;, _fake_run)            #
Act: call collect with speed_category=None to exercise the synthesize 
path&amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, None)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/testing/test_collect_synthesize_on_empty.py:44: _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = _collect_via_pytest(_ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
def _collect_via_pytest(        *,        target: str,        test_path: str,   
category_expr: str,        normalized_filter: str | None,        
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_collect_tests_with_cache_synthesizes_when_empty.&amp;lt;locals&amp;gt;._fak
e_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:28:59,276 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (all)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (all)  collecting via pytest____________________ 
test_collect_tests_with_cache_bad_json ____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_2&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643e7b0&amp;gt;    
@pytest.mark.fast    def test_collect_tests_with_cache_bad_json(tmp_path, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;Malformed cache file triggers
regeneration.            ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(tmp_path))
cache = tmp_path / &amp;quot;unit-tests_all_tests.json&amp;quot;        
cache.write_text(&amp;quot;{bad json}&amp;quot;)            class Res:          
stdout = &amp;quot;tests/unit/sample_test.py::test_a\n&amp;quot;            
returncode = 0            monkeypatch.setattr(subprocess, 
&amp;quot;run&amp;quot;, lambda *a, **k: Res())&amp;gt;       out = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, None)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_bad_json.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
target = &amp;#x27;unit-tests&amp;#x27;, speed_category = None    def 
collect_tests_with_cache(        target: str,        speed_category: str | None 
= None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError____________________ 
test_cache_invalidation_on_file_change ____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cache_invalidation_on_fil0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643cec0&amp;gt;    
@pytest.mark.fast    def test_cache_invalidation_on_file_change(        
tmp_path: Path, monkeypatch: pytest.MonkeyPatch    ) -&amp;gt; None:        # 
Arrange: create an isolated tests directory        tests_dir = tmp_path / 
&amp;quot;isolated_tests&amp;quot;        tests_dir.mkdir(parents=True, 
exist_ok=True)            test_file = tests_dir / 
&amp;quot;test_sample.py&amp;quot;        test_file.write_text(            
&amp;quot;&amp;quot;&amp;quot;    import pytest        @pytest.mark.fast    def 
test_example():        assert 1 + 1 == 2    &amp;quot;&amp;quot;&amp;quot;      
)            # Redirect cache dir to tmp        cache_dir = tmp_path / 
&amp;quot;.cache&amp;quot;        
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;999999&amp;quot;)        # Patch globals on the module for cache dir 
and target path        import devsynth.testing.run_tests as rt            
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setitem(TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir) + &amp;quot;/&amp;quot;)         
outputs = [&amp;quot;test_sample.py::test_example\n&amp;quot;]        call_index
= {&amp;quot;value&amp;quot;: 0}            def fake_run(            cmd,       
check=False,            capture_output=True,            text=True,            
timeout=None,            cwd=None,            env=None,        ):  # noqa: 
ANN001            idx = min(call_index[&amp;quot;value&amp;quot;], len(outputs) 
- 1)            call_index[&amp;quot;value&amp;quot;] += 1            return 
SimpleNamespace(stdout=outputs, stderr=&amp;quot;&amp;quot;, returncode=0)      
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Act: initial collection (populates cache)&amp;gt;       first = 
collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_invalidation.py:5
7: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = 
&amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str,   
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError___________________ 
test_cache_invalidation_on_marker_change ___________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cache_invalidation_on_mar0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643d280&amp;gt;    
@pytest.mark.fast    def test_cache_invalidation_on_marker_change(        
tmp_path: Path, monkeypatch: pytest.MonkeyPatch    ) -&amp;gt; None:        
tests_dir = tmp_path / &amp;quot;isolated_tests_marker&amp;quot;        
tests_dir.mkdir(parents=True, exist_ok=True)            test_file = tests_dir / 
&amp;quot;test_marker.py&amp;quot;        test_file.write_text(            
&amp;quot;&amp;quot;&amp;quot;    import pytest        @pytest.mark.fast    def 
test_fast_case():        assert True    &amp;quot;&amp;quot;&amp;quot;        ) 
cache_dir = tmp_path / &amp;quot;.cache&amp;quot;        
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;999999&amp;quot;)        import devsynth.testing.run_tests as rt      
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setitem(TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir) + &amp;quot;/&amp;quot;)         
outputs = [            &amp;quot;test_marker.py::test_fast_case\n&amp;quot;,    
&amp;quot;test_marker.py::test_medium_case\n&amp;quot;,        ]        
call_index = {&amp;quot;value&amp;quot;: 0}            def fake_run(            
cmd,            check=False,            capture_output=True,            
text=True,            timeout=None,            cwd=None,            env=None,   
):  # noqa: ANN001            idx = min(call_index[&amp;quot;value&amp;quot;], 
len(outputs) - 1)            call_index[&amp;quot;value&amp;quot;] += 1         
return SimpleNamespace(stdout=outputs, stderr=&amp;quot;&amp;quot;, 
returncode=0)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_run)            # Populate cache for fast&amp;gt; 
fast_list = collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_invalidation.py:1
33: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = 
&amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str,   
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError________________ 
test_cache_invalidation_on_target_path_change _________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cache_invalidation_on_tar0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643f2f0&amp;gt;    
@pytest.mark.fast    def test_cache_invalidation_on_target_path_change(        
tmp_path: Path, monkeypatch: pytest.MonkeyPatch    ) -&amp;gt; None:        
tests_dir_one = tmp_path / &amp;quot;suite_one&amp;quot;        
tests_dir_one.mkdir(parents=True, exist_ok=True)        (tests_dir_one / 
&amp;quot;test_alpha.py&amp;quot;).write_text(            &amp;quot;import 
pytest\n\n@pytest.mark.fast\ndef test_alpha():\n    assert True\n&amp;quot;     
)            tests_dir_two = tmp_path / &amp;quot;suite_two&amp;quot;        
tests_dir_two.mkdir(parents=True, exist_ok=True)        (tests_dir_two / 
&amp;quot;test_beta.py&amp;quot;).write_text(            &amp;quot;import 
pytest\n\n@pytest.mark.fast\ndef test_beta():\n    assert True\n&amp;quot;      
)            cache_dir = tmp_path / &amp;quot;.cache&amp;quot;        import 
devsynth.testing.run_tests as rt            monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(cache_dir))        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir_one))        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999)            calls = 
{&amp;quot;count&amp;quot;: 0}            class FakeProc:            def 
__init__(self, stdout: str) -&amp;gt; None:                self.stdout = stdout 
self.stderr = &amp;quot;&amp;quot;                self.returncode = 0           
def fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):            assert 
&amp;quot;--collect-only&amp;quot; in cmd            
calls[&amp;quot;count&amp;quot;] += 1            cwd = os.getcwd()            if
cwd == str(tests_dir_one):                return 
FakeProc(&amp;quot;test_alpha.py::test_alpha\n&amp;quot;)            if cwd == 
str(tests_dir_two):                return 
FakeProc(&amp;quot;test_beta.py::test_beta\n&amp;quot;)            raise 
AssertionError(f&amp;quot;Unexpected cwd {cwd}&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)    
&amp;gt;       first = collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_invalidation.py:2
15: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = 
&amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str,   
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError_____________ 
test_cache_uses_fresh_cache_without_subprocess_call ______________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cache_uses_fresh_cache_wi0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145a59e80&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_cache_uses_fresh_cache_without_subprocess_call(tmp_path, monkeypatch):     
&amp;quot;&amp;quot;&amp;quot;ReqID: CACHE-TTL-1&amp;quot;&amp;quot;&amp;quot;  
# Arrange isolated tests dir and cache dir        tests_dir = tmp_path / 
&amp;quot;isolated_tests_fresh&amp;quot;        tests_dir.mkdir(parents=True, 
exist_ok=True)        t1 = tests_dir / &amp;quot;test_alpha.py&amp;quot;        
# File existence matters for pruning; create it        t1.write_text(           
&amp;quot;import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n&amp;quot;        )            # Redirect module globals        import 
devsynth.testing.run_tests as rt            monkeypatch.setitem(rt.TARGET_PATHS,
&amp;quot;unit-tests&amp;quot;, str(tests_dir) + &amp;quot;/&amp;quot;)        
cache_dir = tmp_path / &amp;quot;.cache&amp;quot;        monkeypatch.setattr(rt,
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(cache_dir))            # Stable 
mtime for fingerprint match        monkeypatch.setattr(rt.os.path, 
&amp;quot;getmtime&amp;quot;, lambda p: 123.456)            # Prepare a fresh 
cache file that should be reused        cache_dir.mkdir(parents=True, 
exist_ok=True)        cache_key = &amp;quot;unit-tests_fast&amp;quot;        
cache_file = cache_dir / f&amp;quot;{cache_key}_tests.json&amp;quot;        
payload = {            &amp;quot;timestamp&amp;quot;: 
datetime.now().isoformat(),  # fresh            &amp;quot;tests&amp;quot;: ,    
&amp;quot;fingerprint&amp;quot;: {                
&amp;quot;latest_mtime&amp;quot;: 123.456,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tests_dir) + &amp;quot;/&amp;quot;,                
&amp;quot;node_set_hash&amp;quot;: 111,            },        }        
cache_file.write_text(json.dumps(payload))            # If subprocess.run is 
called, fail the test (cache should be used)        def forbid_run(*args, 
**kwargs):  # pragma: no cover - should not be hit            raise 
AssertionError(&amp;quot;subprocess.run should not be called for fresh 
cache&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, forbid_run)            # Act&amp;gt;       out = 
collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_ttl.py:55: _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target
= &amp;#x27;unit-tests&amp;#x27;, speed_category = &amp;#x27;fast&amp;#x27;    
def collect_tests_with_cache(        target: str,        speed_category: str | 
None = None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError____________ 
test_cache_ttl_expired_triggers_subprocess_and_refresh ____________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cache_ttl_expired_trigger0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457d85c0&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_cache_ttl_expired_triggers_subprocess_and_refresh(tmp_path, monkeypatch):  
&amp;quot;&amp;quot;&amp;quot;ReqID: CACHE-TTL-2&amp;quot;&amp;quot;&amp;quot;  
# Arrange isolated tests dir and cache dir        tests_dir = tmp_path / 
&amp;quot;isolated_tests_ttl&amp;quot;        tests_dir.mkdir(parents=True, 
exist_ok=True)        t1 = tests_dir / &amp;quot;test_beta.py&amp;quot;        
t1.write_text(            &amp;quot;import pytest\n\n@pytest.mark.fast\ndef 
test_beta():\n    assert True\n&amp;quot;        )            import 
devsynth.testing.run_tests as rt            monkeypatch.setitem(rt.TARGET_PATHS,
&amp;quot;unit-tests&amp;quot;, str(tests_dir) + &amp;quot;/&amp;quot;)        
cache_dir = tmp_path / &amp;quot;.cache&amp;quot;        monkeypatch.setattr(rt,
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(cache_dir))            # Keep 
fingerprint matching except TTL        monkeypatch.setattr(rt.os.path, 
&amp;quot;getmtime&amp;quot;, lambda p: 999.0)            # Expire quickly: TTL 
= 1 second        
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;1&amp;quot;)        # Re-import TTL constant by reloading module-level
var        # Note: the module reads TTL at import; our code in run_tests.py 
guards        # ValueError and sets default.        # For simplicity we 
won&amp;#x27;t force re-import. collect_tests_with_cache reads        # the env 
only at import time for TTL, but it uses the module-level int        # 
COLLECTION_CACHE_TTL_SECONDS.        # We&amp;#x27;ll monkeypatch that directly 
for this test.        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 1)            # Prepare an old
cache file whose timestamp is older than TTL        
cache_dir.mkdir(parents=True, exist_ok=True)        cache_key = 
&amp;quot;unit-tests_fast&amp;quot;        cache_file = cache_dir / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        old_time = (datetime.now() - 
timedelta(seconds=5)).isoformat()        payload = {            
&amp;quot;timestamp&amp;quot;: old_time,            &amp;quot;tests&amp;quot;: ,
&amp;quot;fingerprint&amp;quot;: {                
&amp;quot;latest_mtime&amp;quot;: 999.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tests_dir) + &amp;quot;/&amp;quot;,                
&amp;quot;node_set_hash&amp;quot;: 222,            },        }        
cache_file.write_text(json.dumps(payload))            # Track subprocess 
invocations and emit a predictable collection        calls = 
{&amp;quot;count&amp;quot;: 0}            class FakeProc:            def 
__init__(self, stdout: str, returncode: int = 0, stderr: str = 
&amp;quot;&amp;quot;):                self.stdout = stdout                
self.returncode = returncode                self.stderr = stderr            def 
fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):            
calls[&amp;quot;count&amp;quot;] += 1            lines =             return 
FakeProc(&amp;quot;\n&amp;quot;.join(lines), 0, &amp;quot;&amp;quot;)           
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Act: TTL expired so it should call subprocess and refresh cache&amp;gt;       
out = collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com
/ravenoak/devsynth/tests/unit/testing/test_collect_tests_cache_ttl.py:135: _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target
= &amp;#x27;unit-tests&amp;#x27;, speed_category = &amp;#x27;fast&amp;#x27;    
def collect_tests_with_cache(        target: str,        speed_category: str | 
None = None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError______________ 
test_collect_tests_with_cache_respects_ttl_expiry _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145aa5ac0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_3&amp;#x27;)    
@pytest.mark.fast    def test_collect_tests_with_cache_respects_ttl_expiry(     
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: CTC-01  TTL expiry invalidates stale cache
entries.&amp;quot;&amp;quot;&amp;quot;            tests_dir = tmp_path / 
&amp;quot;suite_ttl&amp;quot;        tests_dir.mkdir()        node = tests_dir /
&amp;quot;test_ttl.py&amp;quot;        node.write_text(&amp;quot;def 
test_ttl():\n    assert True\n&amp;quot;)            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        cache_dir = tmp_path / &amp;quot;.cache_ttl&amp;quot;    
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setattr(rt.os.path, 
&amp;quot;getmtime&amp;quot;, lambda path: 42.0)        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 0)            
cache_dir.mkdir(parents=True, exist_ok=True)        cache_file = cache_dir / 
&amp;quot;unit-tests_fast_tests.json&amp;quot;        payload = {            
&amp;quot;timestamp&amp;quot;: (datetime.now() - 
timedelta(seconds=5)).isoformat(),            &amp;quot;tests&amp;quot;: ,      
&amp;quot;fingerprint&amp;quot;: {                
&amp;quot;latest_mtime&amp;quot;: 42.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tests_dir),                &amp;quot;node_set_hash&amp;quot;: 1,            
},        }        cache_file.write_text(json.dumps(payload))            calls: 
list[list] = []            def fake_run(            cmd,            check=False,
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            
calls.append(cmd)            return SimpleNamespace(                
returncode=0,                stdout=f&amp;quot;{node}::test_ttl\n&amp;quot;,    
stderr=&amp;quot;&amp;quot;,            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)    
&amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/testing/test_collect_tests_with_cache_additiona
l_paths.py:64: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = 
&amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str,   
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError______ 
test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch 
_______monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1457db410&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_4&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: CTC-02  Fingerprint mismatch forces a 
fresh collection.&amp;quot;&amp;quot;&amp;quot;            tests_dir = tmp_path 
/ &amp;quot;suite_fingerprint&amp;quot;        tests_dir.mkdir()        node = 
tests_dir / &amp;quot;test_fp.py&amp;quot;        node.write_text(&amp;quot;def 
test_fp():\n    assert True\n&amp;quot;)            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        cache_dir = tmp_path / &amp;quot;.cache_fp&amp;quot;     
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        cache_dir.mkdir(parents=True, exist_ok=True)            
monkeypatch.setattr(rt.os.path, &amp;quot;getmtime&amp;quot;, lambda path: 
321.0)        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 3600)            cache_file = 
cache_dir / &amp;quot;unit-tests_fast_tests.json&amp;quot;        payload = {   
&amp;quot;timestamp&amp;quot;: datetime.now().isoformat(),            
&amp;quot;tests&amp;quot;: ,            &amp;quot;fingerprint&amp;quot;: {      
&amp;quot;latest_mtime&amp;quot;: 111.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tests_dir),                &amp;quot;node_set_hash&amp;quot;: 5,            
},        }        cache_file.write_text(json.dumps(payload))            calls: 
list[list] = []            def fake_run(            cmd,            check=False,
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            
calls.append(cmd)            return SimpleNamespace(                
returncode=0,                stdout=f&amp;quot;{node}::test_fp\n&amp;quot;,     
stderr=&amp;quot;&amp;quot;,            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)    
&amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/testing/test_collect_tests_with_cache_additiona
l_paths.py:124: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = 
&amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str,   
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError___ 
test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty 
____monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1457db380&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_5&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: CTC-03  Empty collections trigger fallback
to pruned cache.&amp;quot;&amp;quot;&amp;quot;            tests_dir = tmp_path /
&amp;quot;suite_fallback&amp;quot;        tests_dir.mkdir()        keep = 
tests_dir / &amp;quot;test_keep.py&amp;quot;        
keep.write_text(&amp;quot;def test_keep():\n    assert True\n&amp;quot;)        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        cache_dir = tmp_path / &amp;quot;.cache_fb&amp;quot;     
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 0)        
cache_dir.mkdir(parents=True, exist_ok=True)            cache_file = cache_dir /
&amp;quot;unit-tests_all_tests.json&amp;quot;        payload = {            
&amp;quot;timestamp&amp;quot;: (datetime.now() - 
timedelta(seconds=10)).isoformat(),            &amp;quot;tests&amp;quot;: [     
f&amp;quot;{keep}::test_ok&amp;quot;,                f&amp;quot;{tests_dir / 
&amp;#x27;test_missing.py&amp;#x27;}::test_missing&amp;quot;,            ],     
&amp;quot;fingerprint&amp;quot;: {                
&amp;quot;latest_mtime&amp;quot;: 10.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;not memory_intensive&amp;quot;,    
&amp;quot;test_path&amp;quot;: str(tests_dir),                
&amp;quot;node_set_hash&amp;quot;: 10,            },        }        
cache_file.write_text(json.dumps(payload))            calls: list[list] = []    
def fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            
calls.append(cmd)            return SimpleNamespace(returncode=0, 
stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
real_exists = rt.os.path.exists            def fake_exists(path: str) -&amp;gt; 
bool:            if path == str(keep):                return True            if 
path == str(tests_dir / &amp;quot;test_missing.py&amp;quot;):                
return False            if path == str(cache_file):                return True  
return real_exists(path)            monkeypatch.setattr(rt.os.path, 
&amp;quot;exists&amp;quot;, fake_exists)    &amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, None)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/testing/test_collect_tests_with_cache_additional_
paths.py:194: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = None    
def collect_tests_with_cache(        target: str,        speed_category: str | 
None = None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError________ 
test_collect_tests_with_cache_synthesizes_and_caches_node_ids 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1457d05f0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_6&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_tests_with_cache_synthesizes_and_caches_node_ids(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: CTC-04  Synthesized node list persists to 
cache when empty.&amp;quot;&amp;quot;&amp;quot;            tests_dir = tmp_path 
/ &amp;quot;suite_synth&amp;quot;        tests_dir.mkdir(parents=True)        
file_a = tests_dir / &amp;quot;test_alpha.py&amp;quot;        
file_a.write_text(&amp;quot;def test_alpha():\n    assert True\n&amp;quot;)     
nested = tests_dir / &amp;quot;nested&amp;quot;        nested.mkdir()        
file_b = nested / &amp;quot;test_beta.py&amp;quot;        
file_b.write_text(&amp;quot;def test_beta():\n    assert True\n&amp;quot;)      
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        cache_dir = tmp_path / &amp;quot;.cache_synth&amp;quot;  
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))            def fake_run(            cmd,            check=False,
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            return 
SimpleNamespace(returncode=0, stdout=&amp;quot;&amp;quot;, 
stderr=&amp;quot;&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_run)    &amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, None)               
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.co
m/ravenoak/devsynth/tests/unit/testing/test_collect_tests_with_cache_additional_
paths.py:232: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, speed_category = None    
def collect_tests_with_cache(        target: str,        speed_category: str | 
None = None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError__________ 
test_collect_uses_cached_and_prunes_when_collection_empty ___________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457d2630&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_uses_cached_and_p0&amp;#x27;)&amp;gt;   
???/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_
collect_tests_with_cache_fallback.py:69: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;,
speed_category = None    def collect_tests_with_cache(        target: str,      
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError_______ 
test_collect_falls_back_to_unfiltered_and_returns_sanitized_ids 
________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1457d0710&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_falls_back_to_unf0&amp;#x27;)&amp;gt;   
???/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_
collect_tests_with_cache_fallback.py:119: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = 
&amp;#x27;unit-tests&amp;#x27;, speed_category = None    def 
collect_tests_with_cache(        target: str,        speed_category: str | None 
= None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError____________ 
test_html_report_artifacts_created_with_stable_naming _____________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_html_report_artifacts_cre0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14551af90&amp;gt;    
@pytest.mark.fast    def 
test_html_report_artifacts_created_with_stable_naming(tmp_path, monkeypatch):   
&amp;quot;&amp;quot;&amp;quot;        Verify that --report produces an HTML 
report under test_reports/ with a        timestamped folder and target 
subdirectory, and that the naming is stable        enough for tooling 
(YYYYMMDD_HHMMSS/target/report.html).        &amp;quot;&amp;quot;&amp;quot;     
# Arrange: create a minimal passing test in an isolated directory        
test_file = tmp_path / &amp;quot;test_passes.py&amp;quot;        
test_file.write_text(            &amp;quot;&amp;quot;&amp;quot;    import pytest
@pytest.mark.fast    def test_will_pass():        assert True    
&amp;quot;&amp;quot;&amp;quot;        )        # Route the unit-tests target to 
our isolated directory        monkeypatch.setitem(TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))            # Act: run tests with 
HTML report generation enabled        success, output = 
run_tests(&amp;quot;unit-tests&amp;quot;, [&amp;quot;fast&amp;quot;], 
report=True, parallel=False)            # Assert: tests should pass and a report
should be created in test_reports/&amp;gt;       assert success, 
f&amp;quot;Expected success, got failure. Output was:\n{output}&amp;quot;E      
AssertionError: Expected success, got failure. Output was:E         ERROR: file 
or directory not found: test_passes.py::test_will_passE         E         
============================= test session starts 
==============================E         platform darwin -- Python 3.12.12, 
pytest-8.4.2, pluggy-1.6.0E         benchmark: 5.1.0 (defaults: 
timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 
max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)E   
rootdir: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_html_report_artifacts_cre0/projectE         plugins: mock-3.15.1, 
asyncio-1.2.0, anyio-4.11.0, html-4.1.1, xdist-3.8.0, langsmith-0.4.37, 
metadata-3.1.1, Faker-37.11.0, benchmark-5.1.0, hypothesis-6.142.3, bdd-8.1.0, 
rerunfailures-16.1, cov-7.0.0E         asyncio: mode=Mode.STRICT, debug=False, 
asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=functionE         collected 0 itemsE         E  
============================ no tests ran in 0.29s 
=============================E         E         Pytest exited with code 4. 
Command: /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python 
-m pytest test_passes.py::test_will_pass -m not memory_intensive and fast and 
not gui --cov=src/devsynth --cov-report=json:test_reports/coverage.json 
--cov-report=html:htmlcov --cov-appendE         Troubleshooting tips:E         -
Smoke mode: reduce third-party plugin surface to isolate issues:E           
poetry run devsynth run-tests --smoke --speed=fast --no-parallel --maxfail=1E   
- Marker discipline: default is &amp;#x27;-m not memory_intensive&amp;#x27;.E   
Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.E         - Plugin 
autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using --smoke; plugin 
options may fail otherwise.E         - Diagnostics: run &amp;#x27;poetry run 
devsynth doctor&amp;#x27; for a quick environment check.E         - Narrow 
scope: use &amp;#x27;-k &amp;lt;expr&amp;gt;&amp;#x27; and 
&amp;#x27;-vv&amp;#x27; to focus a failure.E         - Segment large suites to 
localize failures and flakes:E           devsynth run-tests --target unit-tests 
--speed=fast --segment --segment-size=50E         - Limit failures early to 
speed iteration:E           poetry run devsynth run-tests --target unit-tests 
--speed=fast --maxfail=1E         - Disable parallelism if xdist interaction is 
suspected:E           devsynth run-tests --target unit-tests --speed=fast 
--no-parallelE         - Generate an HTML report for context (saved under 
test_reports/):E           devsynth run-tests --target unit-tests --speed=fast 
--reportE         E       assert 
False/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/tes
t_html_report_artifacts.py:33: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:00,210 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:02,535 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact generation 
skipped: data file missing______________________ 
test_integration_mutation_workflow ______________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_integration_mutation_work0&amp;#x27;)    
@pytest.mark.fast    def test_integration_mutation_workflow(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;Test complete mutation testing 
workflow.&amp;quot;&amp;quot;&amp;quot;        # Create a simple source file    
source_dir = tmp_path / &amp;quot;src&amp;quot;        source_dir.mkdir()       
source_file = source_dir / &amp;quot;calculator.py&amp;quot;        
source_file.write_text(            &amp;quot;&amp;quot;&amp;quot;    def add(x, 
y):        return x + y        def is_positive(x):        return x &amp;gt; 0   
def is_equal(x, y):        return x == y    &amp;quot;&amp;quot;&amp;quot;      
)            # Create corresponding tests        test_dir = tmp_path / 
&amp;quot;tests&amp;quot;        test_dir.mkdir()        test_file = test_dir / 
&amp;quot;test_calculator.py&amp;quot;        test_file.write_text(            
&amp;quot;&amp;quot;&amp;quot;    import sys    
sys.path.append(&amp;#x27;../src&amp;#x27;)    from calculator import add, 
is_positive, is_equal        def test_add():        assert add(2, 3) == 5       
assert add(-1, 1) == 0        def test_is_positive():        assert 
is_positive(5) == True        assert is_positive(-5) == False        def 
test_is_equal():        assert is_equal(5, 5) == True        assert is_equal(5, 
6) == False    &amp;quot;&amp;quot;&amp;quot;        )            # Mock 
subprocess to avoid actually running tests        with 
patch(&amp;quot;subprocess.run&amp;quot;) as mock_run:            # Simulate 
that mutations are caught (tests fail)            mock_result = MagicMock()     
mock_result.returncode = 1  # Test failed (mutation killed)            
mock_result.stdout = &amp;quot;FAILED&amp;quot;            mock_result.stderr = 
&amp;quot;&amp;quot;            mock_run.return_value = mock_result             
tester = MutationTester(timeout_seconds=5)                # Run with limited 
mutations for testing&amp;gt;           report = 
tester.run_mutations(str(source_dir), str(test_dir), max_mutations=5)           
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/cait
lyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_mutation_testi
ng.py:425: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/mutati
on_testing.py:452: in run_mutations    file_key = 
str(Path(result.file_path).relative_to(Path.cwd()))                   
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_integration_mutation_work0/src/calculator.py&amp
;#x27;)other = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_integration_mutation_work0/project&amp;#x27;)wal
k_up = False, _deprecated = (), step = 0    def relative_to(self, other, /, 
*_deprecated, walk_up=False):        &amp;quot;&amp;quot;&amp;quot;Return the 
relative path to another path identified by the passed        arguments.  If the
operation is not possible (because this is not        related to the other 
path), raise ValueError.            The *walk_up* parameter controls whether 
`..` may be used to resolve        the path.        
&amp;quot;&amp;quot;&amp;quot;        if _deprecated:            msg = 
(&amp;quot;support for supplying more than one positional argument &amp;quot;   
&amp;quot;to pathlib.PurePath.relative_to() is deprecated and &amp;quot;        
&amp;quot;scheduled for removal in Python {remove}&amp;quot;)            
warnings._deprecated(&amp;quot;pathlib.PurePath.relative_to(*args)&amp;quot;, 
msg,                                 remove=(3, 14))        other = 
self.with_segments(other, *_deprecated)        for step, path in enumerate( + 
list(other.parents)):            if self.is_relative_to(path):                
break            elif not walk_up:&amp;gt;               raise 
ValueError(f&amp;quot;{str(self)!r} is not in the subpath of 
{str(other)!r}&amp;quot;)E               ValueError: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_integration_mutation_work0/src/calculator.py&amp;#x27; is 
not in the subpath of 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_integration_mutation_work0/project&amp;#x27;/opt/homebrew/
Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3
.12/pathlib.py:682: ValueError----------------------------- Captured stdout call
-----------------------------2025-10-28 09:29:02,759 - 
devsynth.testing.mutation_testing - INFO - Starting mutation testing: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_integration_mutation_work0/src -&amp;gt; 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_integration_mutation_work0/tests2025-10-28 09:29:02,760 - 
devsynth.testing.mutation_testing - INFO - Found 1 Python files to 
mutate2025-10-28 09:29:02,766 - devsynth.testing.mutation_testing - INFO - 
Generated 8 total mutations2025-10-28 09:29:02,766 - 
devsynth.testing.mutation_testing - INFO - Limited to 5 
mutations------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.mutation_testing:logging_setup.py:615 Starting mutation 
testing: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_integration_mutation_work0/src -&amp;gt; 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-1428/test_integration_mutation_work0/testsINFO     
devsynth.testing.mutation_testing:logging_setup.py:615 Found 1 Python files to 
mutateINFO     devsynth.testing.mutation_testing:logging_setup.py:615 Generated 
8 total mutationsINFO     devsynth.testing.mutation_testing:logging_setup.py:615
Limited to 5 mutations___________________ 
test_run_tests_keyword_filter_no_matches ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14524bcb0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_keyword_filter_0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_keyword_filter_no_matches(monkeypatch, 
tmp_path):        &amp;quot;&amp;quot;&amp;quot;ReqID: FR-11.2  Keyword path 
handles empty collection gracefully.&amp;quot;&amp;quot;&amp;quot;        # 
Ensure test path exists to satisfy chdir logic        tests_dir = tmp_path / 
&amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot;        
tests_dir.mkdir(parents=True)        monkeypatch.chdir(tmp_path)            # 
Patch target path mapping to our tmp tests dir for unit-tests        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))            # Mock subprocess.run for collection to return no 
node ids        def fake_run(            cmd,            check=False,           
capture_output=False,            text=False,            timeout=None,           
cwd=None,            env=None,        ):            # Simulate --collect-only 
output with no matching lines            return 
_DummyCompleted(stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;, 
returncode=0)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_run)            ok, output = rt.run_tests(        
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        )        assert ok is True&amp;gt;       assert &amp;quot;No tests 
matched&amp;quot; in outputE       AssertionError: assert &amp;#x27;No tests 
matched&amp;#x27; in &amp;#x27;Marker fallback 
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0 
_______________\n\n============================ no tests ran in 0.33s 
=============================\n&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/testing/test_run_tests.py:121: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:02,860 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:02,862 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (medium)  collecting via pytest2025-10-28 09:29:02,863 - 
devsynth.testing.run_tests - INFO - marker fallback triggered for 
target=unit-tests (speeds=fast,medium)2025-10-28 09:29:04,765 - 
devsynth.testing.run_tests - INFO - Coverage data file detected at .coverage 
(53248 bytes)2025-10-28 09:29:04,768 - devsynth.testing.run_tests - WARNING - 
Coverage artifact generation skipped: no measured files 
present------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (medium)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 marker fallback triggered for 
target=unit-tests (speeds=fast,medium)INFO     
devsynth.testing.run_tests:logging_setup.py:615 Coverage data file detected at 
.coverage (53248 bytes)WARNING  devsynth.testing.run_tests:logging_setup.py:615 
Coverage artifact generation skipped: no measured files present___________ 
test_collect_tests_with_cache_writes_cache_and_sanitizes ___________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14524a600&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_7&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_tests_with_cache_writes_cache_and_sanitizes(monkeypatch, tmp_path):
&amp;quot;&amp;quot;&amp;quot;ReqID: FR-11.2  Collection cache stores sanitized
node ids.&amp;quot;&amp;quot;&amp;quot;        tests_dir = tmp_path / 
&amp;quot;tests&amp;quot;        (tests_dir / 
&amp;quot;unit&amp;quot;).mkdir(parents=True)        # Create dummy test files 
to satisfy synthesized fallback if needed        (tests_dir / 
&amp;quot;unit&amp;quot; / 
&amp;quot;test_x.py&amp;quot;).write_text(&amp;quot;\n&amp;quot;)        
monkeypatch.chdir(tmp_path)            monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir / &amp;quot;unit&amp;quot;))      
# First run: provide some noisy output with line numbers and duplicates        
noisy = [            &amp;quot;tests/unit/test_x.py:10&amp;quot;,            
&amp;quot;tests/unit/test_x.py::test_ok&amp;quot;,            
&amp;quot;tests/unit/test_x.py:10&amp;quot;,  # duplicate        ]            
def fake_run(            cmd,            check=False,            
capture_output=False,            text=False,            timeout=None,           
cwd=None,            env=None,        ):            return 
_DummyCompleted(stdout=&amp;quot;\n&amp;quot;.join(noisy), 
stderr=&amp;quot;&amp;quot;, returncode=0)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
out = rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)        # Sanitized and deduped: line numbers removed 
when no &amp;#x27;::&amp;#x27;&amp;gt;       assert out[0] == 
&amp;quot;tests/unit/test_x.py&amp;quot;E       AssertionError: assert 
&amp;#x27;tests/unit/t...x.py::test_ok&amp;#x27; == 
&amp;#x27;tests/unit/test_x.py&amp;#x27;E         E         - 
tests/unit/test_x.pyE         + tests/unit/test_x.py::test_okE         ?        
+++++++++/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing
/test_run_tests.py:215: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-10-28 09:29:04,809 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest________________ 
test_collect_tests_with_cache_handles_timeout _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457d84d0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_8&amp;#x27;)    def 
test_collect_tests_with_cache_handles_timeout(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Timeouts during collection yield tips but no 
crash.            ReqID: coverage-run-tests        
&amp;quot;&amp;quot;&amp;quot;            monkeypatch.chdir(tmp_path)        
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;1&amp;quot;)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, &amp;quot;tests&amp;quot;)        (tmp_path / 
&amp;quot;tests&amp;quot;).mkdir()            def fake_run(            cmd,     
check=False,            capture_output=True,            text=True,            
timeout=None,            cwd=None,            env=None,        ):  # noqa: 
ANN001            return SimpleNamespace(stdout=&amp;quot;&amp;quot;, 
stderr=&amp;quot;&amp;quot;, returncode=-1)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)    
&amp;gt;       collected = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
speed_category=&amp;quot;fast&amp;quot;)                    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/P
rojects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_additiona
l_coverage.py:180: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = _collect_via_pytest(_ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
def _collect_via_pytest(        *,        target: str,        test_path: str,   
category_expr: str,        normalized_filter: str | None,        
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;            result = subprocess.run(            
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )        if result.returncode != 0:            if 
result.stderr:                error_message = f&amp;quot;Test collection failed:
{result.stderr}&amp;quot;            else:                error_message = 
f&amp;quot;Test collection failed with exit code {result.returncode}&amp;quot;  
# Log more details for debugging            logger.warning(                
error_message,                extra={                    
&amp;quot;event&amp;quot;: &amp;quot;test_collection_failed&amp;quot;,          
&amp;quot;target&amp;quot;: target,                    
&amp;quot;returncode&amp;quot;: result.returncode,                    
&amp;quot;stdout&amp;quot;: (                        result.stdout[:500] if 
result.stdout else None                    ),  # First 500 chars                
&amp;quot;stderr&amp;quot;: (                        result.stderr[:500] if 
result.stderr else None                    ),  # First 500 chars                
&amp;quot;speed_category&amp;quot;: category_expr,                },            
)&amp;gt;           raise RuntimeError(error_message)E           RuntimeError: 
Test collection failed with exit code 
-1/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_
tests.py:1367: RuntimeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:04,886 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:04,886 - 
devsynth.testing.run_tests - WARNING - Test collection failed with exit code 
-1------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Test collection failed with exit
code -1__________ test_collect_tests_with_cache_handles_subprocess_exception 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145593080&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_9&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x1458870b0&amp;gt;    
@pytest.mark.fast    def 
test_collect_tests_with_cache_handles_subprocess_exception(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path, caplog: pytest.LogCaptureFixture    ) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;ReqID: RT-ERR-01  
Collection errors log tips and return empty list.            Issue: 
issues/coverage-below-threshold.md        &amp;quot;&amp;quot;&amp;quot;        
tests_dir = tmp_path / &amp;quot;tests_root&amp;quot;        tests_dir.mkdir()  
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        caplog.set_level(logging.ERROR)            def 
boom(*_args, **_kwargs):  # noqa: ANN002            raise 
RuntimeError(&amp;quot;collection failed&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, boom)    &amp;gt;   
result = rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
speed_category=&amp;quot;fast&amp;quot;)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/P
rojects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_additiona
l_error_paths.py:31: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _args = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_9/tests_root&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)_kwargs = 
{&amp;#x27;capture_output&amp;#x27;: True, &amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: 
{&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;...on.debugpy-2025.14.1-darwin-arm64/bu
ndled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...}, &amp;#x27;text&amp;#x27;: True, ...}    def boom(*_args, **_kwargs):  # 
noqa: ANN002&amp;gt;       raise RuntimeError(&amp;quot;collection 
failed&amp;quot;)E       RuntimeError: collection 
failed/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/te
st_run_tests_additional_error_paths.py:27: RuntimeError______________ 
test_run_tests_handles_unexpected_execution_error _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145591eb0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_handles_unexpec0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_handles_unexpected_execution_error(     
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RT-ERR-02  Unexpected execution errors 
surface troubleshooting tips.            Issue: 
issues/coverage-below-threshold.md        &amp;quot;&amp;quot;&amp;quot;        
tests_dir = tmp_path / &amp;quot;tests_exec&amp;quot;        tests_dir.mkdir()  
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))            monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)            def 
failing_popen(*_args, **_kwargs):  # noqa: ANN002            raise 
RuntimeError(&amp;quot;boom popen&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, failing_popen)    
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=None,        )            assert success is False        assert 
&amp;quot;boom popen&amp;quot; in output&amp;gt;       assert 
&amp;quot;Troubleshooting tips&amp;quot; in outputE       AssertionError: assert
&amp;#x27;Troubleshooting tips&amp;#x27; in &amp;#x27;boom 
popen&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/t
esting/test_run_tests_additional_error_paths.py:72: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,036 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest__________________ 
test_run_tests_segment_merges_extra_marker __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458878c0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_segment_merges_0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_segment_merges_extra_marker(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RT-ERR-03  Segmented runs combine marker 
filters with extra expressions.            Issue: 
issues/coverage-below-threshold.md        &amp;quot;&amp;quot;&amp;quot;        
tests_dir = tmp_path / &amp;quot;tests_segment&amp;quot;        
tests_dir.mkdir()        (tests_dir / 
&amp;quot;test_demo.py&amp;quot;).write_text(&amp;quot;def test_ok():\n    
assert True\n&amp;quot;)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir))            node_ids = 
&amp;quot;test_demo.py::test_ok\n&amp;quot;            def fake_run(            
cmd,            check=False,            capture_output=True,            
text=True,            timeout=None,            cwd=None,            env=None,   
):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot; in cmd  
marker_index = len(cmd) - 1 - cmd[::-1].index(&amp;quot;-m&amp;quot;)           
expr = cmd            assert &amp;quot;fast&amp;quot; in expr and &amp;quot;not 
memory_intensive&amp;quot; in expr            assert &amp;quot;not 
slow&amp;quot; in expr            return SimpleNamespace(returncode=0, 
stdout=node_ids, stderr=&amp;quot;&amp;quot;)            captured_batches: 
list[list] = []            class FakePopen:            def __init__(            
self, cmd, stdout=None, stderr=None, text=False, env=None            ):  # noqa:
ANN001                captured_batches.append(cmd)                
self.returncode = 0                def communicate(self):                return 
(&amp;quot;ok\n&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)        
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopen)    
&amp;gt;       success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=None,            extra_marker=&amp;quot;not 
slow&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_additional_error_paths.py:122: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ cmd = 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lbs...23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_run_tests_segment_merges_0/tests_segment&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...]check = False, 
capture_output = True, text = True, timeout = 60.0cwd = 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;env = 
{&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi&amp;#x27;, 
&amp;#x27;BUNDLED_DEBUGPY_PATH&amp;#x27;: 
&amp;#x27;/Users/caitlyn/.cursor/extensions/ms-python.debugpy-2025.14.1-darwin-a
rm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...}    def fake_run(        cmd,        check=False,        
capture_output=True,        text=True,        timeout=None,        cwd=None,    
env=None,    ):  # noqa: ANN001        assert &amp;quot;--collect-only&amp;quot;
in cmd        marker_index = len(cmd) - 1 - 
cmd[::-1].index(&amp;quot;-m&amp;quot;)        expr = cmd        assert 
&amp;quot;fast&amp;quot; in expr and &amp;quot;not memory_intensive&amp;quot; in
expr&amp;gt;       assert &amp;quot;not slow&amp;quot; in exprE       
AssertionError: assert &amp;#x27;not slow&amp;#x27; in &amp;#x27;fast and not 
memory_intensive&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/testing/test_run_tests_additional_error_paths.py:104: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,049 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest______________ 
test_coverage_artifacts_status_detects_empty_html _______________coverage_paths 
= 
(PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/py
test-of-caitlyn/pytest-1428/test_coverage_artif...ar/folders/2v/lbss3by10y51bg7c
07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1428/test_coverage_artifacts_status5/h
tmlcov&amp;#x27;))    @pytest.mark.fast    def 
test_coverage_artifacts_status_detects_empty_html(        coverage_paths: 
tuple[Path, Path],    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;HTML reports that note missing data should trigger
remediation.&amp;quot;&amp;quot;&amp;quot;            coverage_json, html_dir = 
coverage_paths        coverage_json.parent.mkdir(parents=True, exist_ok=True)   
coverage_json.write_text(json.dumps({&amp;quot;totals&amp;quot;: 
{&amp;quot;percent_covered&amp;quot;: 90.0}}))        
html_dir.mkdir(parents=True, exist_ok=True)        (html_dir / 
&amp;quot;index.html&amp;quot;).write_text(&amp;quot;No coverage data 
available&amp;quot;)            ok, reason = 
run_tests_module.coverage_artifacts_status()        assert ok is False&amp;gt;  
assert reason is not None and &amp;quot;No coverage data&amp;quot; in reasonE   
AssertionError: assert (&amp;#x27;Coverage HTML indicates no recorded 
data&amp;#x27; is not None and &amp;#x27;No coverage data&amp;#x27; in 
&amp;#x27;Coverage HTML indicates no recorded 
data&amp;#x27;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/t
esting/test_run_tests_artifacts.py:264: AssertionError__________________ 
test_failure_tips_includes_command_context __________________    
@pytest.mark.fast    def test_failure_tips_includes_command_context() -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Failure tips prefix the command and 
retain segmentation guidance.&amp;quot;&amp;quot;&amp;quot;            cmd = 
[&amp;quot;python&amp;quot;, &amp;quot;-m&amp;quot;, &amp;quot;pytest&amp;quot;,
&amp;quot;tests/unit&amp;quot;]        tips = run_tests_module._failure_tips(2, 
cmd)            assert tips.startswith(            &amp;quot;\nPytest exited 
with code 2. Command: python -m pytest tests/unit&amp;quot;        )        
assert &amp;quot;Troubleshooting tips:&amp;quot; in tips        assert 
&amp;quot;Segment large suites&amp;quot; in tips&amp;gt;       assert 
&amp;quot;Re-run failing segments&amp;quot; in tipsE       AssertionError: 
assert &amp;#x27;Re-run failing segments&amp;#x27; in &amp;#x27;\nPytest exited 
with code 2. Command: python -m pytest tests/unit\nTroubleshooting tips:\n- 
Smoke mode: reduce third-...HTML report for context (saved under 
test_reports/):\n  devsynth run-tests --target unit-tests --speed=fast 
--report\n&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_artifacts.py:294: AssertionError____________ 
test_segmented_run_treats_benchmark_warning_as_success ____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457dad80&amp;gt;    
@pytest.mark.fast    def 
test_segmented_run_treats_benchmark_warning_as_success(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;        ReqID: FR-11.2        When running in 
segmented mode, if a batch returns a nonzero exit code but        stderr 
contains PytestBenchmarkWarning, the batch should be treated as        
successful. This test simulates that path deterministically.        
&amp;quot;&amp;quot;&amp;quot;            # Simulate collection returning two 
node ids        class DummyCompleted:            def __init__(self, stdout: str 
= &amp;quot;&amp;quot;, stderr: str = &amp;quot;&amp;quot;) -&amp;gt; None:     
self.stdout = stdout                self.stderr = stderr                
self.returncode = 0            def fake_run(            cmd: list, check: bool, 
capture_output: bool, text: bool        ):  # type: ignore            # Return 
minimal collect-only output resembling pytest&amp;#x27;s -q --collect-only      
# Use python path style entries so _sanitize_node_ids accepts them            
out = &amp;quot;\n&amp;quot;.join(                [                    
&amp;quot;tests/unit/sample_test.py::test_one&amp;quot;,                    
&amp;quot;tests/unit/sample_test.py::test_two&amp;quot;,                ]       
)            return DummyCompleted(stdout=out, stderr=&amp;quot;&amp;quot;)     
monkeypatch.setattr(&amp;quot;subprocess.run&amp;quot;, fake_run)            # 
Simulate pytest execution batches: nonzero returncode        # but with a 
benchmark warning in stderr        class DummyPopen:            def __init__(   
self,                cmd: list,                stdout: Any,                
stderr: Any,                text: bool,                env: dict,            ): 
# noqa: D401                # store for potential assertions/debug              
self.cmd = cmd                # nonzero return code; handled as success due to 
warning in stderr                self._returncode = 1                def 
communicate(self):                stdout = &amp;quot;&amp;quot;                
stderr = &amp;quot;PytestBenchmarkWarning: benchmark plugin present\n&amp;quot; 
return stdout, stderr                @property            def returncode(self) 
-&amp;gt; int:                return self._returncode            
monkeypatch.setattr(&amp;quot;subprocess.Popen&amp;quot;, DummyPopen)    
&amp;gt;       success, output = run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            segment=True,           
segment_size=1,  # force two batches            parallel=False,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_benchmark_warning.py:66: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_segmented_run_treats_benchmark_warning_as_success.&amp;lt;locals&amp;gt;.fa
ke_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:05,284 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_________ 
test_collect_tests_with_cache_prunes_nonexistent_and_caches __________tmp_path =
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_10&amp;#x27;)monkeypatc
h = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145815790&amp;gt;    
@pytest.mark.fast    def 
test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path, 
monkeypatch):        # Direct cache into a temp directory        import 
devsynth.testing.run_tests as rt            monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(tmp_path / 
&amp;quot;cache&amp;quot;))            # Simulate pytest --collect-only output 
with one non-existent and one existent file        existing = 
&amp;quot;tests/unit/synthetic_test_file.py::test_ok&amp;quot;        missing = 
&amp;quot;tests/unit/missing_test_file.py::test_missing&amp;quot;            
stdout = f&amp;quot;{missing}\n{existing}\n&amp;quot;            def fake_run(  
cmd,            check=False,            capture_output=True,            
text=True,            timeout=None,            cwd=None,            env=None,   
):  # noqa: ANN001            return SimpleNamespace(returncode=0, 
stdout=stdout, stderr=&amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Make os.path.exists return True only for the existing path part        
real_exists = os.path.exists            def fake_exists(path):  # noqa: ANN001  
if isinstance(path, str) and path.startswith(                
&amp;quot;tests/unit/synthetic_test_file.py&amp;quot;            ):             
return True            if isinstance(path, str) and 
path.startswith(&amp;quot;tests/unit/missing_test_file.py&amp;quot;):           
return False            return real_exists(path)            
monkeypatch.setattr(rt.os.path, &amp;quot;exists&amp;quot;, fake_exists)        
results = collect_tests_with_cache(target=&amp;quot;unit-tests&amp;quot;, 
speed_category=None)        assert existing in results&amp;gt;       assert 
all(missing not in r for r in results)E       assert FalseE        +  where 
False = all(&amp;lt;generator object 
test_collect_tests_with_cache_prunes_nonexistent_and_caches.&amp;lt;locals&amp;g
t;.&amp;lt;genexpr&amp;gt; at 
0x1457fe260&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_cache_prune_and_tips.py:65: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,391 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (all)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (all)  collecting via pytest_________________ 
test_prunes_nonexistent_paths_and_uses_cache _________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_prunes_nonexistent_paths_0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458bdf10&amp;gt;    
@pytest.mark.fast    def test_prunes_nonexistent_paths_and_uses_cache(tmp_path, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;ReqID: 
CACHE-PRUNE-1&amp;quot;&amp;quot;&amp;quot;        # Prepare a fake cache with a
nonexistent test path and an existent one        
os.makedirs(COLLECTION_CACHE_DIR, exist_ok=True)        cache_file = 
os.path.join(COLLECTION_CACHE_DIR, 
&amp;quot;unit-tests_all_tests.json&amp;quot;)        existent = 
&amp;quot;tests/unit/test_example.py::test_ok&amp;quot;        nonexistent = 
&amp;quot;tests/unit/test_deleted.py::test_gone&amp;quot;        # Ensure the 
existent path exists on filesystem for the pruning check        exist_dir = 
Path(&amp;quot;tests/unit&amp;quot;)        exist_dir.mkdir(parents=True, 
exist_ok=True)        exist_file = exist_dir / 
&amp;quot;test_example.py&amp;quot;        if not exist_file.exists():          
exist_file.write_text(                &amp;quot;import 
pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert True\n&amp;quot;        
)            cache_payload = {            &amp;quot;timestamp&amp;quot;: 
&amp;quot;2099-01-01T00:00:00&amp;quot;,            &amp;quot;tests&amp;quot;: ,
&amp;quot;fingerprint&amp;quot;: {                
&amp;quot;latest_mtime&amp;quot;: 0.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;not memory_intensive&amp;quot;,    
&amp;quot;test_path&amp;quot;: &amp;quot;tests/&amp;quot;,                
&amp;quot;node_set_hash&amp;quot;: 123,            },        }        with 
open(cache_file, &amp;quot;w&amp;quot;) as f:            
json.dump(cache_payload, f)            # Monkeypatch TTL to be huge so cache 
would be used if fingerprint matches        
monkeypatch.setenv(&amp;quot;DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 
&amp;quot;999999&amp;quot;)            def fake_run(            cmd,            
check=False,            capture_output=True,            text=True,            
timeout=None,            cwd=None,            env=None,        ):  # noqa: 
ANN001            &amp;quot;&amp;quot;&amp;quot;Return the cached node ids 
without invoking pytest.&amp;quot;&amp;quot;&amp;quot;                assert 
&amp;quot;--collect-only&amp;quot; in cmd, cmd            return 
SimpleNamespace(                returncode=0,                
stdout=&amp;quot;\n&amp;quot;.join(),                
stderr=&amp;quot;&amp;quot;,            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Force fingerprint mismatch by changing latest_mtime via        # 
monkeypatching os.path.getmtime        original_getmtime = os.path.getmtime     
def fake_getmtime(path):            return 1.0            
monkeypatch.setattr(os.path, &amp;quot;getmtime&amp;quot;, fake_getmtime)       
# Now call collection; it should regenerate and prune the nonexistent path      
out = collect_tests_with_cache(target=&amp;quot;all-tests&amp;quot;, 
speed_category=None)        assert existent in out&amp;gt;       assert 
all(nonexistent != nid for nid in out)E       assert FalseE        +  where 
False = all(&amp;lt;generator object 
test_prunes_nonexistent_paths_and_uses_cache.&amp;lt;locals&amp;gt;.&amp;lt;gene
xpr&amp;gt; at 
0x144d57c60&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_cache_pruning.py:77: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,401 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=all-tests (all)  decomposing all-tests into dependent targets2025-10-28 
09:29:05,401 - devsynth.testing.run_tests - INFO - test collection cache miss 
for target=unit-tests (all)  collecting via pytest2025-10-28 09:29:05,401 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=integration-tests (all)  collecting via pytest2025-10-28 09:29:05,402 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=behavior-tests (all)  collecting via 
pytest------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=all-tests (all)  decomposing all-tests into dependent targetsINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (all)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=integration-tests (all)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=behavior-tests (all)  collecting via pytest____________ 
test_segmented_batch_exception_emits_tips_and_plugins _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458100b0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_batch_exception0&amp;#x27;)    
@pytest.mark.fast    def test_segmented_batch_exception_emits_tips_and_plugins( 
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,    ) -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-SEGMENT-CLI-2  Exceptions 
surface tips and preserve plugins.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;-q&amp;quot;)  
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_segment.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_fail():\n    assert True\n&amp;quot;)            def fake_collect(         
cmd, check=False, capture_output=True, text=True, timeout=None, **kwargs        
):  # noqa: ANN001            return SimpleNamespace(                
returncode=0, stdout=f&amp;quot;{test_file}::test_fail\n&amp;quot;, 
stderr=&amp;quot;&amp;quot;            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
captured_envs: list[dict] = []            def exploding_batch(            cmd, 
stdout=None, stderr=None, text=True, env=None        ):  # noqa: ANN001         
captured_envs.append(dict(env or {}))            raise 
RuntimeError(&amp;quot;segmented batch crashed&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, exploding_batch)  
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=3,            extra_marker=None,        )    
assert success is False        assert &amp;quot;segmented batch 
crashed&amp;quot; in output&amp;gt;       assert 
output.count(&amp;quot;Troubleshooting tips:&amp;quot;) == 2E       
AssertionError: assert 0 == 2E        +  where 0 = &amp;lt;built-in method count
of str object at 0x14632b2d0&amp;gt;(&amp;#x27;Troubleshooting tips:&amp;#x27;)E
+    where &amp;lt;built-in method count of str object at 0x14632b2d0&amp;gt; = 
&amp;#x27;Failed to run tests: segmented batch 
crashed&amp;#x27;.count/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/testing/test_run_tests_cli_helpers_focus.py:186: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,430 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:05,430 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:05,430 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support2025-10-28 09:29:05,431 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,431 - 
devsynth.testing.run_tests - INFO - Running 1 tests in 1 segments of size 1 for 
target=unit-tests2025-10-28 09:29:05,431 - devsynth.testing.run_tests - INFO - 
Running segment 1/1 (1 tests)2025-10-28 09:29:05,431 - 
devsynth.testing.run_tests - ERROR - Failed to run tests: segmented batch 
crashed------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportINFO    
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)ERROR    devsynth.testing.run_tests:logging_setup.py:615 Failed to run 
tests: segmented batch crashed_______________ 
test_segmented_batches_reinject_when_env_mutates _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458162a0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_batches_reinjec0&amp;#x27;)    
@pytest.mark.fast    def test_segmented_batches_reinject_when_env_mutates(      
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,    ) -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;Segments reapply plugin directives even if 
previous runs stripped them.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;-q&amp;quot;)  
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            first = tmp_path / 
&amp;quot;test_first.py&amp;quot;        second = tmp_path / 
&amp;quot;test_second.py&amp;quot;        first.write_text(&amp;quot;def 
test_one():\n    assert True\n&amp;quot;)        second.write_text(&amp;quot;def
test_two():\n    assert True\n&amp;quot;)            def fake_collect(          
cmd, check=False, capture_output=True, text=True, timeout=None, **kwargs        
):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot; in cmd  
return SimpleNamespace(                returncode=0,                
stdout=&amp;quot;\n&amp;quot;.join(),                
stderr=&amp;quot;&amp;quot;,            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
popen_envs: list[dict] = []            class MutatingPopen:            
call_index = 0                def __init__(                self,                
cmd,                stdout=None,                stderr=None,                
text=True,                env=None,            ):  # noqa: ANN001               
MutatingPopen.call_index += 1                env_map = dict(env or {})          
popen_envs.append(env_map)                _assert_plugins_in_env(env_map)       
if env is not None:                    tokens = 
env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;).split()       
filtered = [                        token                        for token in 
tokens                        if token not in {&amp;quot;-p&amp;quot;, 
&amp;quot;pytest_cov&amp;quot;, &amp;quot;pytest_bdd.plugin&amp;quot;}          
]                    env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot; 
&amp;quot;.join(filtered)                    self.returncode = 0                
self._stdout = f&amp;quot;segment {MutatingPopen.call_index} ok&amp;quot;       
self._stderr = &amp;quot;&amp;quot;                def communicate(self):  # 
noqa: D401 - subprocess API emulation                
&amp;quot;&amp;quot;&amp;quot;Return the stubbed stdout/stderr 
pair.&amp;quot;&amp;quot;&amp;quot;                    return self._stdout, 
self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, MutatingPopen)            success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=None,            extra_marker=None,        ) 
assert success is True        assert &amp;quot;segment 1 ok&amp;quot; in output 
assert &amp;quot;segment 2 ok&amp;quot; in output        assert len(popen_envs) 
== 2        for env_snapshot in popen_envs:            
_assert_plugins_in_env(env_snapshot)&amp;gt;       
_assert_plugins_in_env(dict(os.environ))/Users/caitlyn/Projects/github.com/raven
oak/devsynth/tests/unit/testing/test_run_tests_cli_helpers_focus.py:280: _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:39: in _assert_plugins_in_env    
_assert_plugins_in_addopts(env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, 
&amp;quot;&amp;quot;))_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ addopts = &amp;#x27;-q&amp;#x27;    def 
_assert_plugins_in_addopts(addopts: str) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Assert pytest-cov and pytest-bdd plugins are 
present in addopts.&amp;quot;&amp;quot;&amp;quot;            normalized = 
addopts.strip()        assert normalized, &amp;quot;PYTEST_ADDOPTS should not be
empty when plugins are injected&amp;quot;&amp;gt;       assert (            
&amp;quot;-p pytest_cov&amp;quot; in normalized or 
&amp;quot;-ppytest_cov&amp;quot; in normalized        ), f&amp;quot;pytest-cov 
plugin missing: {normalized}&amp;quot;E       AssertionError: pytest-cov plugin 
missing: -qE       assert (&amp;#x27;-p pytest_cov&amp;#x27; in 
&amp;#x27;-q&amp;#x27; or &amp;#x27;-ppytest_cov&amp;#x27; in 
&amp;#x27;-q&amp;#x27;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/testing/test_run_tests_cli_helpers_focus.py:28: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,454 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:05,455 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:05,455 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support2025-10-28 09:29:05,455 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,456 - 
devsynth.testing.run_tests - INFO - Running 2 tests in 2 segments of size 1 for 
target=unit-tests2025-10-28 09:29:05,456 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (1 tests)2025-10-28 09:29:05,456 - 
devsynth.testing.run_tests - INFO - Running segment 2/2 (1 
tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportINFO    
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/2 (1 tests)_________ 
test_run_tests_env_var_propagation_retains_existing_addopts 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145816180&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_env_var_propaga0&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_env_var_propagation_retains_existing_addopts(        monkeypatch:
pytest.MonkeyPatch,        tmp_path: Path,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-ENV-1  CLI helper preserves 
existing PYTEST_ADDOPTS.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)        
monkeypatch.setenv(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;-q&amp;quot;)  
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_env.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_env():\n    assert True\n&amp;quot;)            def fake_collect(          
cmd, check=False, capture_output=True, text=True, timeout=None, **kwargs        
):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot; in cmd  
return SimpleNamespace(returncode=0, 
stdout=f&amp;quot;{test_file}::test_env&amp;quot;, stderr=&amp;quot;&amp;quot;) 
recorded: list[tuple[list, dict]] = []            class FakePopen:            
def __init__(                self, cmd, stdout=None, stderr=None, text=True, 
env=None            ):  # noqa: ANN001                
recorded.append((list(cmd), dict(env or {})))                self.returncode = 0
self._stdout = &amp;quot;pass&amp;quot;                self._stderr = 
&amp;quot;&amp;quot;                def communicate(self):  # noqa: D401 - mimic
subprocess API                &amp;quot;&amp;quot;&amp;quot;Return deterministic
stdout/stderr.&amp;quot;&amp;quot;&amp;quot;                    return 
self._stdout, self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, FakePopen)            success, output = rt.run_tests(
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=False,            
maxfail=1,            extra_marker=None,        )            assert success is 
True        assert output == &amp;quot;pass&amp;quot;        assert recorded, 
&amp;quot;Expected subprocess invocation to be recorded&amp;quot;            
process_addopts = os.environ.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, 
&amp;quot;&amp;quot;)        assert &amp;quot;-q&amp;quot; in 
process_addopts&amp;gt;       
_assert_plugins_in_addopts(process_addopts)/Users/caitlyn/Projects/github.com/ra
venoak/devsynth/tests/unit/testing/test_run_tests_cli_helpers_focus.py:340: _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
addopts = &amp;#x27;-q&amp;#x27;    def _assert_plugins_in_addopts(addopts: str)
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Assert pytest-cov and 
pytest-bdd plugins are present in addopts.&amp;quot;&amp;quot;&amp;quot;        
normalized = addopts.strip()        assert normalized, &amp;quot;PYTEST_ADDOPTS 
should not be empty when plugins are injected&amp;quot;&amp;gt;       assert (  
&amp;quot;-p pytest_cov&amp;quot; in normalized or 
&amp;quot;-ppytest_cov&amp;quot; in normalized        ), f&amp;quot;pytest-cov 
plugin missing: {normalized}&amp;quot;E       AssertionError: pytest-cov plugin 
missing: -qE       assert (&amp;#x27;-p pytest_cov&amp;#x27; in 
&amp;#x27;-q&amp;#x27; or &amp;#x27;-ppytest_cov&amp;#x27; in 
&amp;#x27;-q&amp;#x27;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/test
s/unit/testing/test_run_tests_cli_helpers_focus.py:28: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,483 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:05,483 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:05,483 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support2025-10-28 09:29:05,484 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportINFO    
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_____________ 
test_run_tests_option_wiring_includes_expected_flags _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145816210&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_option_wiring_i0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_option_wiring_includes_expected_flags(  
monkeypatch: pytest.MonkeyPatch,        tmp_path: Path,    ) -&amp;gt; None:    
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-PYTEST-OPTS-1  Command wiring 
emits coverage/report args.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_opts.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_opts():\n    assert True\n&amp;quot;)            class FakeDT:            
@staticmethod            def now(tz=None) -&amp;gt; SimpleNamespace:  # type: 
ignore                return SimpleNamespace(                    
isoformat=lambda: &amp;quot;2025-01-02T00:00:00&amp;quot;,                    
strftime=lambda fmt: &amp;quot;20250102_000000&amp;quot;,                )      
monkeypatch.setattr(rt, &amp;quot;datetime&amp;quot;, FakeDT)            def 
fake_collect(            cmd, check=False, capture_output=True, text=True, 
timeout=None, **kwargs        ):  # noqa: ANN001            assert 
&amp;quot;--collect-only&amp;quot; in cmd            return SimpleNamespace(    
returncode=0, stdout=f&amp;quot;{test_file}::test_opts&amp;quot;, 
stderr=&amp;quot;&amp;quot;            )            recorded: list[list] = []   
class FakePopen:            def __init__(                self, cmd, stdout=None,
stderr=None, text=True, env=None            ):  # noqa: ANN001                
recorded.append(list(cmd))                self.returncode = 0                
self._stdout = &amp;quot;opts&amp;quot;                self._stderr = 
&amp;quot;&amp;quot;                def communicate(self):                return
self._stdout, self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, FakePopen)            success, output = rt.run_tests(
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=True,           
report=True,            parallel=False,            segment=False,            
maxfail=3,            extra_marker=None,        )            assert success is 
True        assert &amp;quot;opts&amp;quot; in output  # The main output should 
contain &amp;quot;opts&amp;quot;        assert recorded, &amp;quot;Expected 
pytest command to be recorded&amp;quot;            cmd = recorded[0]&amp;gt;    
assert &amp;quot;--maxfail=3&amp;quot; in cmdE       AssertionError: assert 
&amp;#x27;--maxfail=3&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lbs...-1428/test_run_tests_option_wiring_i0/te
st_opts.py::test_opts&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not 
memory_intensive and fast and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_cli_helpers_focus.py:412: AssertionError-----------------------------
Captured stdout call -----------------------------2025-10-28 09:29:05,507 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,508 - 
devsynth.testing.run_tests - WARNING - Skipping release graph publication: 
Coverage JSON missing at 
test_reports/coverage.json------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json_______________ 
test_cli_marker_expression_includes_extra_marker _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145814f80&amp;gt;    
@pytest.mark.fast    def test_cli_marker_expression_includes_extra_marker(      
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-ARGS-1  CLI marker flag 
augments default filters.&amp;quot;&amp;quot;&amp;quot;            commands: 
list[list] = []            class DummyProcess:            def __init__(         
self,                cmd: list,                stdout=None,                
stderr=None,                text: bool = False,                env: dict | None 
= None,            ) -&amp;gt; None:                commands.append(cmd)        
self.returncode = 0                def communicate(self) -&amp;gt; tuple:       
return (&amp;quot;collected 1 item&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, DummyProcess)    
&amp;gt;       success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            extra_marker=&amp;quot;custom_marker&amp;quot;,       
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_cli_invocation.py:62: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_cli_marker_expression_includes_extra_marker.&amp;lt;locals&amp;gt;.DummyPro
cess.__init__() got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,537 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest__________________ 
test_cli_failure_surfaces_actionable_tips ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458153d0&amp;gt;    
@pytest.mark.fast    def test_cli_failure_surfaces_actionable_tips(        
monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-ERROR-1  CLI surfaces 
troubleshooting tips.&amp;quot;&amp;quot;&amp;quot;            commands: 
list[list] = []            class FailingProcess:            def __init__(       
self,                cmd: list,                stdout=None,                
stderr=None,                text: bool = False,                env: dict | None 
= None,            ) -&amp;gt; None:                commands.append(cmd)        
raise RuntimeError(&amp;quot;simulated failure&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FailingProcess)   
&amp;gt;       success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_cli_invocation.py:107: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_cli_failure_surfaces_actionable_tips.&amp;lt;locals&amp;gt;.FailingProcess.
__init__() got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,650 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest________________ 
test_cli_segment_failure_emits_aggregate_tips _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145817f50&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_segment_failure_emits0&amp;#x27;)    
@pytest.mark.fast    def test_cli_segment_failure_emits_aggregate_tips(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-SEGMENT-2  failing batch 
surfaces aggregate tips.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_segmented.py&amp;quot;        test_file.write_text(            
&amp;quot;def test_batch_one():\n    assert True\n\n&amp;quot;            
&amp;quot;def test_batch_two():\n    assert True\n&amp;quot;        )           
node_ids = [            f&amp;quot;{test_file}::test_batch_one&amp;quot;,       
f&amp;quot;{test_file}::test_batch_two&amp;quot;,        ]            def 
fake_collect(            cmd: list,            check: bool = False,            
capture_output: bool = True,            text: bool = True,            timeout: 
float | None = None,            cwd: str | None = None,            env: dict | 
None = None,        ) -&amp;gt; SimpleNamespace:            return 
SimpleNamespace(stdout=&amp;quot;\n&amp;quot;.join(node_ids), 
stderr=&amp;quot;&amp;quot;, returncode=0)            batch_commands: list[list]
= []        responses = [            (&amp;quot;batch one ok&amp;quot;, 
&amp;quot;&amp;quot;, 0),            (&amp;quot;batch two fail&amp;quot;, 
&amp;quot;collected errors&amp;quot;, 2),        ]        tips_record: 
list[tuple[int, tuple, str]] = []            def fake_failure_tips(returncode: 
int, cmd: list) -&amp;gt; str:            tip = f&amp;quot;&amp;quot;           
tips_record.append((returncode, tuple(cmd), tip))            return tip         
call_index = {&amp;quot;value&amp;quot;: 0}            class DummyBatchProcess: 
def __init__(                self,                cmd: list,                
stdout=None,                stderr=None,                text: bool = False,     
env: dict | None = None,            ) -&amp;gt; None:                idx = 
call_index[&amp;quot;value&amp;quot;]                batch_commands.append(cmd) 
stdout_payload, stderr_payload, returncode = responses                
self._stdout = stdout_payload                self._stderr = stderr_payload      
self.returncode = returncode                
call_index[&amp;quot;value&amp;quot;] += 1                def communicate(self) 
-&amp;gt; tuple:                return (self._stdout, self._stderr)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, DummyBatchProcess)
monkeypatch.setattr(rt, &amp;quot;_failure_tips&amp;quot;, fake_failure_tips)   
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,        )            assert success is False        assert 
len(batch_commands) == 2, &amp;quot;Segmented execution should spawn two 
batches&amp;quot;        assert tips_record, &amp;quot;Expected failure tips to 
be generated&amp;quot;            failing_tip = tips_record[0]&amp;gt;       
aggregate_tip = tips_record[1]                        ^^^^^^^^^^^^^^E       
IndexError: list index out of 
range/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/tes
t_run_tests_cli_invocation.py:281: IndexError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:05,781 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,782 - 
devsynth.testing.run_tests - INFO - Running 2 tests in 2 segments of size 1 for 
target=unit-tests2025-10-28 09:29:05,782 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (1 tests)2025-10-28 09:29:05,783 - 
devsynth.testing.run_tests - INFO - Running segment 2/2 (1 
tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/2 (1 tests)__________________ test_cli_marker_filters_merge_extra_marker 
__________________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x1458cc7a0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_marker_filters_merge_0&amp;#x27;)    
@pytest.mark.fast    def test_cli_marker_filters_merge_extra_marker(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-ARGS-3  speed markers 
combine with extra filter.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_filters.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_placeholder():\n    assert True\n&amp;quot;)            
collect_invocations: list[list] = []            def fake_collect(            
cmd: list,            check: bool = False,            capture_output: bool = 
True,            text: bool = True,            timeout: float | None = None,    
cwd: str | None = None,            env: dict | None = None,        ) -&amp;gt; 
SimpleNamespace:            collect_invocations.append(cmd)            return 
SimpleNamespace(                
stdout=f&amp;quot;{test_file}::test_placeholder\n&amp;quot;, 
stderr=&amp;quot;&amp;quot;, returncode=0            )            run_commands: 
list[list] = []            class DummyProcess:            def __init__(         
self,                cmd: list,                stdout=None,                
stderr=None,                text: bool = False,                env: dict | None 
= None,            ) -&amp;gt; None:                run_commands.append(cmd)    
self.returncode = 0                def communicate(self) -&amp;gt; tuple:       
return (&amp;quot;ok&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, DummyProcess)     
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;, &amp;quot;slow&amp;quot;],          
verbose=False,            report=False,            parallel=False,            
segment=False,            extra_marker=&amp;quot;custom_marker&amp;quot;,       
)            assert success is True        assert run_commands, 
&amp;quot;Expected run commands for each speed&amp;quot;        assert 
collect_invocations, &amp;quot;Expected collection to occur&amp;quot;        
assert output.count(&amp;quot;ok&amp;quot;) == len(run_commands)            
collect_strings = [&amp;quot; &amp;quot;.join(cmd) for cmd in 
collect_invocations]&amp;gt;       assert any(            &amp;quot;(fast and 
not memory_intensive) and (custom_marker)&amp;quot; in cmd            for cmd in
collect_strings        )E       assert FalseE        +  where False = 
any(&amp;lt;generator object 
test_cli_marker_filters_merge_extra_marker.&amp;lt;locals&amp;gt;.&amp;lt;genexp
r&amp;gt; at 
0x14514be00&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_cli_invocation.py:437: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,824 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,825 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (slow)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (slow)  collecting via pytest___________________ 
test_cli_report_mode_adds_html_argument ____________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458bf440&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_report_mode_adds_html0&amp;#x27;)    
@pytest.mark.fast    def test_cli_report_mode_adds_html_argument(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-REPORT-1  report flag 
appends HTML output options.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;all-tests&amp;quot;, 
str(tmp_path))            test_file = tmp_path / 
&amp;quot;test_report.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_report():\n    assert True\n&amp;quot;)            def fake_collect(       
cmd: list,            check: bool = False,            capture_output: bool = 
True,            text: bool = True,            timeout: float | None = None,    
cwd: str | None = None,            env: dict | None = None,        ) -&amp;gt; 
SimpleNamespace:            return SimpleNamespace(                
stdout=f&amp;quot;{test_file.name}::test_report\n&amp;quot;, 
stderr=&amp;quot;&amp;quot;, returncode=0            )            run_commands: 
list[list] = []            class DummyProcess:            def __init__(         
self,                cmd: list,                stdout=None,                
stderr=None,                text: bool = False,                env: dict | None 
= None,            ) -&amp;gt; None:                run_commands.append(cmd)    
self.returncode = 0                def communicate(self) -&amp;gt; tuple:       
return (&amp;quot;report ok&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, DummyProcess)     
monkeypatch.setattr(rt, &amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: 
None)        monkeypatch.setattr(rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: None)            
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=True,            parallel=False,        )      
assert success is True        assert &amp;quot;report ok&amp;quot; in output    
assert run_commands, &amp;quot;Expected report-mode command to run&amp;quot;    
html_args = [arg for arg in run_commands[0] if 
arg.startswith(&amp;quot;--html=&amp;quot;)]&amp;gt;       assert html_args, 
run_commands[0]E       AssertionError: 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;test_report.py::test_report&amp;#x27;, &amp;#x27;-m&amp;#x27;, 
&amp;#x27;not memory_intensive and (fast or medium) and not gui&amp;#x27;, ...]E
assert 
[]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_r
un_tests_cli_invocation.py:508: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:05,853 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,853 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (medium)  collecting via pytest2025-10-28 09:29:05,854 - 
devsynth.testing.run_tests - WARNING - Skipping release graph publication: 
Coverage JSON missing at 
test_reports/coverage.json------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (medium)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json___________ 
test_cli_keyword_filter_returns_success_when_no_matches ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457d0b30&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_cli_keyword_filter_return0&amp;#x27;)    
@pytest.mark.fast    def 
test_cli_keyword_filter_returns_success_when_no_matches(        monkeypatch: 
pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-CLI-ARGS-4  keyword fallback 
exits cleanly when empty.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))            lifecycle: list = []   
def fake_reset() -&amp;gt; None:            
lifecycle.append(&amp;quot;reset&amp;quot;)            def fake_ensure() 
-&amp;gt; None:            lifecycle.append(&amp;quot;ensure&amp;quot;)         
monkeypatch.setattr(rt, &amp;quot;_reset_coverage_artifacts&amp;quot;, 
fake_reset)        monkeypatch.setattr(rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, fake_ensure)            
collect_commands: list[list] = []            def fake_collect(            cmd: 
list,            check: bool = False,            capture_output: bool = True,   
text: bool = True,            timeout: float | None = None,            cwd: str 
| None = None,            env: dict | None = None,        ) -&amp;gt; 
SimpleNamespace:            collect_commands.append(cmd)            return 
SimpleNamespace(stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;, 
returncode=0)            def fail_popen(*args, **kwargs):            raise 
AssertionError(&amp;quot;Popen should not be invoked when no tests 
match&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, fail_popen)            success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        )            assert collect_commands, &amp;quot;Expected keyword-filter
collection to execute&amp;quot;        collect_tokens = &amp;quot; 
&amp;quot;.join(collect_commands[0])        assert &amp;quot;-k 
lmstudio&amp;quot; in collect_tokens    &amp;gt;       assert success is TrueE  
assert False is 
True/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_cli_invocation.py:722: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:05,918 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:05,925 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (medium)  collecting via pytest2025-10-28 09:29:05,926 - 
devsynth.testing.run_tests - INFO - marker fallback triggered for 
target=unit-tests (speeds=fast,medium)2025-10-28 09:29:05,926 - 
devsynth.testing.run_tests - ERROR - Failed to run tests: Popen should not be 
invoked when no tests match------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (medium)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 marker fallback triggered for 
target=unit-tests (speeds=fast,medium)ERROR    
devsynth.testing.run_tests:logging_setup.py:615 Failed to run tests: Popen 
should not be invoked when no tests match____________ 
test_run_tests_generates_artifacts_for_normal_profile _____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14551bc20&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_generates_artif0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_generates_artifacts_for_normal_profile( 
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Normal run writes `.coverage`, JSON, and HTML 
artifacts via the harness.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            coverage_json = tmp_path / 
&amp;quot;reports&amp;quot; / &amp;quot;coverage.json&amp;quot;        html_dir 
= tmp_path / &amp;quot;htmlcov&amp;quot;        monkeypatch.setattr(rt, 
&amp;quot;COVERAGE_JSON_PATH&amp;quot;, coverage_json)        
monkeypatch.setattr(rt, &amp;quot;COVERAGE_HTML_DIR&amp;quot;, html_dir)        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
&amp;quot;tests/unit&amp;quot;)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, &amp;quot;tests&amp;quot;)            
monkeypatch.setattr(            rt, 
&amp;quot;collect_tests_with_cache&amp;quot;, lambda *_: 
[&amp;quot;tests/unit/test_ok.py::test_one&amp;quot;]        )            
lifecycle: list = []        monkeypatch.setattr(            rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: 
lifecycle.append(&amp;quot;reset&amp;quot;)        )        monkeypatch.setattr(
rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
lifecycle.append(&amp;quot;ensure&amp;quot;)        )            popen_envs: 
list[dict] = []            def fake_single_batch(            config: 
rt.SingleBatchRequest,        ) -&amp;gt; rt.BatchExecutionResult:            
popen_envs.append(dict(config.env))            
tmp_path.joinpath(&amp;quot;.coverage&amp;quot;).write_text(&amp;quot;data&amp;q
uot;)            html_dir.mkdir(parents=True, exist_ok=True)            
(html_dir / 
&amp;quot;index.html&amp;quot;).write_text(&amp;quot;&amp;lt;html&amp;gt;ok&amp;
lt;/html&amp;gt;&amp;quot;)            coverage_json.parent.mkdir(parents=True, 
exist_ok=True)            
coverage_json.write_text(json.dumps({&amp;quot;totals&amp;quot;: 
{&amp;quot;percent_covered&amp;quot;: 98.7}}))            return True, 
&amp;quot;batch ok&amp;quot;, 
build_batch_metadata(&amp;quot;batch-cli-artifacts&amp;quot;)            
monkeypatch.setattr(rt, &amp;quot;_run_single_test_batch&amp;quot;, 
fake_single_batch)            success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=True,            parallel=False,        )            assert success is 
True&amp;gt;       assert output == &amp;quot;batch ok&amp;quot;E       
AssertionError: assert &amp;#x27;batch ok\n[k...hold=70.00%\n&amp;#x27; == 
&amp;#x27;batch ok&amp;#x27;E         E         - batch okE         + batch okE 
?         +E         +  coverage gate pass  QualityGate 
6f892712-c31d-480b-99bb-c93003b7e672 (new), TestRun 
acee87dd-e69d-4346-aef0-d4ec4b2963f6 (new), Evidence  via networkx; 
coverage=98.70% 
threshold=70.00%/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
testing/test_run_tests_cli_invocation.py:779: AssertionError__________ 
test_run_tests_generates_artifacts_with_autoload_disabled ___________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145aa61b0&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_generates_artif1&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_generates_artifacts_with_autoload_disabled(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Smoke-style environments still create coverage 
artifacts with plugin injection.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.chdir(tmp_path)            coverage_json = tmp_path / 
&amp;quot;reports&amp;quot; / &amp;quot;coverage.json&amp;quot;        html_dir 
= tmp_path / &amp;quot;htmlcov&amp;quot;        monkeypatch.setattr(rt, 
&amp;quot;COVERAGE_JSON_PATH&amp;quot;, coverage_json)        
monkeypatch.setattr(rt, &amp;quot;COVERAGE_HTML_DIR&amp;quot;, html_dir)        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
&amp;quot;tests/unit&amp;quot;)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, &amp;quot;tests&amp;quot;)            
monkeypatch.setattr(            rt, 
&amp;quot;collect_tests_with_cache&amp;quot;, lambda *_: 
[&amp;quot;tests/unit/test_ok.py::test_one&amp;quot;]        )            
monkeypatch.setattr(rt, &amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: 
None)        monkeypatch.setattr(rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: None)            
captured_envs: list[dict] = []            def fake_single_batch(            
config: rt.SingleBatchRequest,        ) -&amp;gt; rt.BatchExecutionResult:      
captured_envs.append(dict(config.env))            
tmp_path.joinpath(&amp;quot;.coverage&amp;quot;).write_text(&amp;quot;data&amp;q
uot;)            html_dir.mkdir(parents=True, exist_ok=True)            
(html_dir / 
&amp;quot;index.html&amp;quot;).write_text(&amp;quot;&amp;lt;html&amp;gt;smoke&a
mp;lt;/html&amp;gt;&amp;quot;)            
coverage_json.parent.mkdir(parents=True, exist_ok=True)            
coverage_json.write_text(json.dumps({&amp;quot;totals&amp;quot;: 
{&amp;quot;percent_covered&amp;quot;: 94.2}}))            return True, 
&amp;quot;smoke ok&amp;quot;, 
build_batch_metadata(&amp;quot;batch-cli-smoke&amp;quot;)            
monkeypatch.setattr(rt, &amp;quot;_run_single_test_batch&amp;quot;, 
fake_single_batch)            env = 
{&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;: &amp;quot;1&amp;quot;}     
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=True,           
report=True,            parallel=True,            env=env,        )            
assert success is True&amp;gt;       assert output == &amp;quot;smoke 
ok&amp;quot;E       AssertionError: assert &amp;#x27;smoke 
ok\n[k...hold=70.00%\n&amp;#x27; == &amp;#x27;smoke ok&amp;#x27;E         E     
- smoke okE         + smoke okE         ?         +E         +  coverage gate 
pass  QualityGate 6ef51634-c5a1-4d95-b687-a6fd68782120 (new), TestRun 
38421d8f-3858-40e3-929f-8124eefa1a80 (new), Evidence 
[9b99b49c-ee7f-4ffb-a0d1-7d4a54c39ecb (new), 
461128ef-6092-497b-9f36-9da1e737f305 (new)] via networkx; coverage=94.20% 
threshold=70.00%/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/
testing/test_run_tests_cli_invocation.py:838: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:05,977 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:05,978 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:05,978 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test 
support_________ test_ensure_coverage_artifacts_skips_when_module_unavailable 
_________coverage_test_environment = 
namespace(coverage_json=PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51
bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-1...olders/2v/lbss3by10y51bg7c07nd
23rh0000gn/T/pytest-of-caitlyn/pytest-1428/test_ensure_coverage_artifacts4/legac
y/html&amp;#x27;))monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object 
at 0x145a5ba40&amp;gt;caplog = &amp;lt;_pytest.logging.LogCaptureFixture object 
at 0x1459a1c40&amp;gt;    @pytest.mark.fast    def 
test_ensure_coverage_artifacts_skips_when_module_unavailable(        
coverage_test_environment: SimpleNamespace,        monkeypatch: 
pytest.MonkeyPatch,        caplog: pytest.LogCaptureFixture,    ) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;ReqID: COV-ART-02A  Missing coverage
module logs debug message and exits.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.delitem(sys.modules, &amp;quot;coverage&amp;quot;, raising=False)   
original_import = builtins.__import__            def fake_import(name, *args, 
**kwargs):  # noqa: ANN001            if name == &amp;quot;coverage&amp;quot;:  
raise ImportError(&amp;quot;coverage module unavailable&amp;quot;)            
return original_import(name, *args, **kwargs)            
monkeypatch.setattr(builtins, &amp;quot;__import__&amp;quot;, fake_import)      
with caplog.at_level(logging.DEBUG, 
logger=&amp;quot;devsynth.testing.run_tests&amp;quot;):            
rt._ensure_coverage_artifacts()    &amp;gt;       assert any(&amp;quot;coverage 
library unavailable&amp;quot; in message for message in caplog.messages)E       
assert FalseE        +  where False = any(&amp;lt;generator object 
test_ensure_coverage_artifacts_skips_when_module_unavailable.&amp;lt;locals&amp;
gt;.&amp;lt;genexpr&amp;gt; at 
0x144fb13c0&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_coverage_artifacts.py:218: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:06,088 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact generation 
skipped: data file missing________________ 
test_keyword_filter_no_matches_returns_success ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1459a3110&amp;gt;    
@pytest.mark.fast    def 
test_keyword_filter_no_matches_returns_success(monkeypatch) -&amp;gt; None:     
&amp;quot;&amp;quot;&amp;quot;ReqID: RT-01  keyword filter with no matches 
returns success and message.&amp;quot;&amp;quot;&amp;quot;            # Simulate
collect-only returning no matching node ids under keyword filter        def 
fake_run(            cmd,  # noqa: ANN001            check: bool = False,       
capture_output: bool = False,            text: bool = False,        ):  # type: 
ignore            class R:                def __init__(self) -&amp;gt; None:    
self.returncode = 0                    # nothing that matches the node id regex 
self.stdout = &amp;quot;\n&amp;quot;                    self.stderr = 
&amp;quot;&amp;quot;                return R()            # Ensure Popen is not 
invoked if there are no node ids        def fail_popen(*args, **kwargs):  # 
type: ignore            pytest.fail(&amp;quot;Popen should not be called when no
node ids match&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_run)        monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, fail_popen)    &amp;gt;       success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            segment_size=50,           
maxfail=None,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_extra.py:33: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_keyword_filter_no_matches_returns_success.&amp;lt;locals&amp;gt;.fake_run()
got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:06,251 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_________________ 
test_failure_tips_appended_on_nonzero_return _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145452ae0&amp;gt;    
@pytest.mark.fast    def 
test_failure_tips_appended_on_nonzero_return(monkeypatch) -&amp;gt; None:       
&amp;quot;&amp;quot;&amp;quot;ReqID: RT-02  non-zero exit appends 
troubleshooting tips.&amp;quot;&amp;quot;&amp;quot;            # Make the simple
&amp;#x27;-m not memory_intensive&amp;#x27; path run and return a non-zero code 
class DummyProc:            def __init__(self) -&amp;gt; None:                
self.returncode = 2                def communicate(self) -&amp;gt; tuple:       
return (&amp;quot;&amp;quot;, &amp;quot;boom&amp;quot;)            def 
fake_popen(            args,  # noqa: ANN001            stdout=None,  # noqa: 
ANN001            stderr=None,  # noqa: ANN001            text=None,  # noqa: 
ANN001            env=None,  # noqa: ANN001        ):  # type: ignore           
return DummyProc()            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, fake_popen)    &amp;gt;       success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            segment_size=50,           
maxfail=None,            extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_extra.py:72: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_failure_tips_appended_on_nonzero_return.&amp;lt;locals&amp;gt;.fake_popen()
got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:06,327 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest___________ 
test_keyword_filter_lmstudio_no_matches_returns_success ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14500edb0&amp;gt;    
@pytest.mark.fast    def 
test_keyword_filter_lmstudio_no_matches_returns_success(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-08  Keyword 
&amp;#x27;lmstudio&amp;#x27; no-match returns success.            When 
extra_marker includes requires_resource(&amp;#x27;lmstudio&amp;#x27;) and 
collection        yields no matching node ids, run_tests should return 
success=True with a        helpful message without attempting to execute pytest 
on any node ids.        &amp;quot;&amp;quot;&amp;quot;            class 
DummyRunResult:            def __init__(self):                self.stdout = 
&amp;quot;&amp;quot;  # no node ids collected                self.stderr = 
&amp;quot;&amp;quot;                self.returncode = 0            calls = {    
&amp;quot;run&amp;quot;: [],            &amp;quot;popen&amp;quot;: [],        } 
def fake_run(            cmd, check=False, capture_output=True, text=True       
):  # type: ignore            calls[&amp;quot;run&amp;quot;].append(cmd)        
return DummyRunResult()            class FakePopen:            def __init__(    
self, cmd, stdout=None, stderr=None, text=False, env=None            ):  # type:
ignore                calls[&amp;quot;popen&amp;quot;].append(cmd)              
# This path should not be reached because no node ids =&amp;gt; early return    
self._stdout = &amp;quot;&amp;quot;                self._stderr = 
&amp;quot;&amp;quot;                self.returncode = 0                def 
communicate(self):                return self._stdout, self._stderr            
import subprocess            monkeypatch.setattr(subprocess, 
&amp;quot;run&amp;quot;, fake_run)        monkeypatch.setattr(subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            segment_size=50,           
maxfail=None,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_extra_marker.py:50: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_keyword_filter_lmstudio_no_matches_returns_success.&amp;lt;locals&amp;gt;.f
ake_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:06,414 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest__________________ 
test_extra_marker_merges_into_m_expression __________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1457975c0&amp;gt;    
@pytest.mark.fast    def 
test_extra_marker_merges_into_m_expression(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-09  Non-keyword extra_marker merges 
into -m expression.&amp;quot;&amp;quot;&amp;quot;        
&amp;quot;&amp;quot;&amp;quot;        When extra_marker does not invoke the 
keyword fallback, ensure it is merged        into the -m expression and 
subprocess.Popen is called once; run_tests returns        success if the process
returncode is 0.        &amp;quot;&amp;quot;&amp;quot;            class 
FakePopen:            last_cmd = None                def __init__(              
self, cmd, stdout=None, stderr=None, text=False, env=None            ):  # type:
ignore                FakePopen.last_cmd = cmd                self.returncode = 
0                self._stdout = &amp;quot;pytest ok\n&amp;quot;                
self._stderr = &amp;quot;&amp;quot;                def communicate(self):       
return self._stdout, self._stderr            import subprocess            def 
fake_run(            cmd, check=False, capture_output=True, text=True        ): 
# type: ignore            # Not used in this path; ensure it&amp;#x27;s not 
called unnecessarily            raise AssertionError(&amp;quot;subprocess.run 
should not be called for non-keyword path&amp;quot;)            
monkeypatch.setattr(subprocess, &amp;quot;Popen&amp;quot;, FakePopen)        
monkeypatch.setattr(subprocess, &amp;quot;run&amp;quot;, fake_run)            
extra = &amp;quot;slow or 
(requires_resource(&amp;#x27;codebase&amp;#x27;))&amp;quot;&amp;gt;       
success, output = run_tests(            target=&amp;quot;unit-tests&amp;quot;,  
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            segment_size=50,           
maxfail=None,            extra_marker=extra,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_extra_marker.py:112: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_extra_marker_merges_into_m_expression.&amp;lt;locals&amp;gt;.fake_run() got
an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:06,471 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_________ 
test_run_tests_merges_extra_marker_into_category_expression 
__________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1452485f0&amp;gt;    @pytest.mark.fast    def 
test_run_tests_merges_extra_marker_into_category_expression(monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-09  Merge extra_marker into -m 
expression.            When speed_categories is None and extra_marker is a 
normal expression        (not a requires_resource(&amp;#x27;lmstudio&amp;#x27;) 
keyword case), run_tests should pass        a combined -m expression that 
includes both the base filter and the extra        marker.        
&amp;quot;&amp;quot;&amp;quot;        import devsynth.testing.run_tests as rt   
captured_cmd = {}            class FakePopen:            def __init__(          
self, cmd, stdout=None, stderr=None, text=False, env=None            ):  # noqa:
ANN001                # The command should include &amp;#x27;-m&amp;#x27; with 
the merged expression                assert &amp;quot;-m&amp;quot; in cmd, 
f&amp;quot;-m missing in: {cmd}&amp;quot;                # There are two 
&amp;#x27;-m&amp;#x27; flags: Python module and pytest marker; use the last     
idx = len(cmd) - 1 - cmd[::-1].index(&amp;quot;-m&amp;quot;)                expr
= cmd                assert &amp;quot;not memory_intensive&amp;quot; in expr    
assert &amp;quot;(not slow)&amp;quot; in expr or &amp;quot;not slow&amp;quot; in
expr                captured_cmd[&amp;quot;cmd&amp;quot;] = cmd                
self.returncode = 0                def communicate(self):                return 
(&amp;quot;ok\n&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopen)    
&amp;gt;       success, output = run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=&amp;quot;not slow&amp;quot;,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_extra_marker_passthrough.py:38: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_run_tests_merges_extra_marker_into_category_expression.&amp;lt;locals&amp;g
t;.FakePopen.__init__() got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:06,547 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_______________ 
test_collect_fallback_on_behavior_speed_no_tests _______________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_fallback_on_behav0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14524a960&amp;gt;    
@pytest.mark.fast    def 
test_collect_fallback_on_behavior_speed_no_tests(tmp_path, monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;When behavior-tests with a speed filter yields no 
tests, fallback to marker_expr.            We simulate the preliminary check 
returning a &amp;#x27;no tests ran&amp;#x27; message and assert that        the 
second collection call (fallback) is invoked and its items are returned.        
ReqID: C3 (coverage of fallback branch)        &amp;quot;&amp;quot;&amp;quot;   
# Arrange        from devsynth.testing import run_tests as rt            # 
Redirect cache dir and target path to isolated temp locations        cache_dir =
tmp_path / &amp;quot;.cache&amp;quot;        cache_dir.mkdir()        
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;behavior-tests&amp;quot;, str(tmp_path))        # Create a real file 
that the pruning check will accept        real_file = tmp_path / 
&amp;quot;test_example.py&amp;quot;        real_file.write_text(            
&amp;quot;import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n&amp;quot;        )            # Prepare fake subprocess.run that returns 
two different responses:        calls = {&amp;quot;invocations&amp;quot;: []}   
class FakeCompleted:            def __init__(self, stdout: str, returncode: int 
= 0, stderr: str = &amp;quot;&amp;quot;):                self.stdout = stdout   
self.returncode = returncode                self.stderr = stderr            def 
fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            
calls[&amp;quot;invocations&amp;quot;].append(cmd)            # First invocation
is the preliminary check (with same category_expr)            if 
len(calls[&amp;quot;invocations&amp;quot;]) == 1:                return 
FakeCompleted(stdout=&amp;quot;no tests ran\n&amp;quot;, returncode=0)          
# Second invocation should be the fallback collection using marker_expr only    
# Return a couple of node ids            return 
FakeCompleted(stdout=f&amp;quot;{real_file}::test_ok\n&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Act&amp;gt;       out = 
rt.collect_tests_with_cache(target=&amp;quot;behavior-tests&amp;quot;, 
speed_category=&amp;quot;fast&amp;quot;)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/User
s/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_test
s_extra_paths.py:55: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ target = &amp;#x27;behavior-tests&amp;#x27;, speed_category 
= &amp;#x27;fast&amp;#x27;    def collect_tests_with_cache(        target: str, 
speed_category: str | None = None,        *,        keyword_filter: str | None =
None,        _allow_all_target_decomposition: bool = True,        
_timeout_override: float | None = None,        _propagate_timeout: bool = False,
) -&amp;gt; list:        &amp;quot;&amp;quot;&amp;quot;Collect tests for the 
given target and speed category.            Args:            target: Logical 
test target such as ``unit-tests`` or ``all-tests``.            speed_category: 
Optional speed marker used to scope collection.            keyword_filter: 
Optional ``-k`` expression applied during collection.            Returns:       
A list of pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError___________________ 
test_collect_malformed_cache_regenerates ___________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_malformed_cache_r0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145248bf0&amp;gt;    
@pytest.mark.fast    def test_collect_malformed_cache_regenerates(tmp_path, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;Malformed JSON cache should 
be ignored and collection regenerated.            ReqID: C3 (coverage of 
malformed cache read path)        &amp;quot;&amp;quot;&amp;quot;        from 
devsynth.testing import run_tests as rt            # Point cache dir and target 
path        cache_dir = tmp_path / &amp;quot;.cache&amp;quot;        
cache_dir.mkdir()        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(cache_dir))        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;all-tests&amp;quot;, 
str(tmp_path))        # Create a real file for existence pruning        real2 = 
tmp_path / &amp;quot;test_a.py&amp;quot;        real2.write_text(            
&amp;quot;import pytest\n\n@pytest.mark.fast\ndef test_b():\n    assert 
True\n&amp;quot;        )            # Write malformed JSON to the expected 
cache file for key all-tests_all        cache_file = cache_dir / 
&amp;quot;all-tests_all_tests.json&amp;quot;        
cache_file.write_text(&amp;quot;{ not-json }&amp;quot;)            # Fake 
subprocess.run to return one id        class FakeCompleted:            def 
__init__(self, stdout: str, returncode: int = 0, stderr: str = 
&amp;quot;&amp;quot;):                self.stdout = stdout                
self.returncode = returncode                self.stderr = stderr            
monkeypatch.setattr(            rt.subprocess,            
&amp;quot;run&amp;quot;,            lambda *a, **k: 
FakeCompleted(stdout=f&amp;quot;{real2}::test_b\n&amp;quot;),        )    
&amp;gt;       out = 
rt.collect_tests_with_cache(target=&amp;quot;all-tests&amp;quot;, 
speed_category=None)              
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitl
yn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_extra
_paths.py:99: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ target = &amp;#x27;all-tests&amp;#x27;, speed_category = None    
def collect_tests_with_cache(        target: str,        speed_category: str | 
None = None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError__________ 
test_run_tests_lmstudio_extra_marker_keyword_early_success __________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_lmstudio_extra_0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145813f50&amp;gt;    
@pytest.mark.fast    def 
test_run_tests_lmstudio_extra_marker_keyword_early_success(tmp_path, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;With extra_marker 
requires_resource(&amp;#x27;lmstudio&amp;#x27;) and no matches, run_tests should
perform keyword-based collection and return success with a friendly message.    
ReqID: C3 (coverage of extra_marker keyword path with early success)        
&amp;quot;&amp;quot;&amp;quot;        from devsynth.testing import run_tests as 
rt            # Point the unit-tests target to an isolated path (not used 
directly here)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))            # Fake subprocess.run 
for the collect-only path to return no matching node IDs        class 
FakeCompleted:            def __init__(self, stdout: str, returncode: int = 0, 
stderr: str = &amp;quot;&amp;quot;):                self.stdout = stdout        
self.returncode = returncode                self.stderr = stderr            def 
fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            # Simulate 
collect-only returning nothing for &amp;#x27;-k lmstudio&amp;#x27;            
return FakeCompleted(stdout=&amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
parallel=False,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        )            assert success is True&amp;gt;       assert &amp;quot;No 
tests matched&amp;quot; in outputE       AssertionError: assert &amp;#x27;No 
tests matched&amp;#x27; in &amp;#x27;Marker fallback 
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0 
_______________\n\n============================ no tests ran in 0.31s 
=============================\n&amp;#x27;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/tests/unit/testing/test_run_tests_extra_paths.py:144: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:06,718 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:06,719 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (medium)  collecting via pytest2025-10-28 09:29:06,719 - 
devsynth.testing.run_tests - INFO - marker fallback triggered for 
target=unit-tests (speeds=fast,medium)2025-10-28 09:29:08,481 - 
devsynth.testing.run_tests - INFO - Coverage data file detected at .coverage 
(53248 bytes)2025-10-28 09:29:08,484 - devsynth.testing.run_tests - WARNING - 
Coverage artifact generation skipped: no measured files 
present------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (medium)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 marker fallback triggered for 
target=unit-tests (speeds=fast,medium)INFO     
devsynth.testing.run_tests:logging_setup.py:615 Coverage data file detected at 
.coverage (53248 bytes)WARNING  devsynth.testing.run_tests:logging_setup.py:615 
Coverage artifact generation skipped: no measured files 
present____________________ test_failure_tips_include_common_flags 
____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_failure_tips_include_comm0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145811fd0&amp;gt;    
@pytest.mark.fast    def test_failure_tips_include_common_flags(tmp_path, 
monkeypatch):        &amp;quot;&amp;quot;&amp;quot;ReqID: FR-59 Ensure failure 
tips include common flags examples.            We create a minimal failing test 
and assert that the output contains        actionable hints for --smoke, 
--segment/--segment-size, --maxfail,        --no-parallel, and --report.        
&amp;quot;&amp;quot;&amp;quot;        test_file = tmp_path / 
&amp;quot;test_fails.py&amp;quot;        test_file.write_text(            
&amp;quot;&amp;quot;&amp;quot;    import pytest        @pytest.mark.fast    def 
test_will_fail():        assert False    &amp;quot;&amp;quot;&amp;quot;        )
# Point unit-tests target to our tmp_path        
monkeypatch.setitem(TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, str(tmp_path))
monkeypatch.setattr(rt, &amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: 
None)        monkeypatch.setattr(rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: None)            def 
fake_collect(cmd, check=False, capture_output=True, text=True):  # noqa: ANN001 
assert &amp;quot;--collect-only&amp;quot; in cmd            return 
SimpleNamespace(                returncode=0,                
stdout=f&amp;quot;{test_file}::test_will_fail&amp;quot;,                
stderr=&amp;quot;&amp;quot;,            )            class FakePopen:           
def __init__(                self, cmd, stdout=None, stderr=None, text=True, 
env=None            ):  # noqa: ANN001                self.cmd = list(cmd)      
self.returncode = 1                self._stdout = &amp;quot;&amp;quot;          
self._stderr = &amp;quot;FAIL Required test coverage of 90% not 
reached.&amp;quot;                def communicate(self):  # noqa: D401 - mimic 
subprocess signature                &amp;quot;&amp;quot;&amp;quot;Return 
predetermined stdout/stderr.&amp;quot;&amp;quot;&amp;quot;                    
return self._stdout, self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(&amp;quot;unit-tests&amp;quot;, [&amp;quot;fast&amp;quot;], 
parallel=False)                          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/testing/test_run_tests_failure_tips.py:58: _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_failure_tips_include_common_flags.&amp;lt;locals&amp;gt;.fake_collect() got
an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:08,511 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest____________ 
test_keyword_filter_no_matches_returns_success_message ____________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_keyword_filter_no_matches1&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14643c500&amp;gt;    
@pytest.mark.fast    def 
test_keyword_filter_no_matches_returns_success_message(tmp_path, monkeypatch):  
&amp;quot;&amp;quot;&amp;quot;ReqID: FR-11.2  Keyword filter path returns 
success when no matches.            Triggers the lmstudio keyword path and 
expects a friendly message.        &amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)            def fake_collect(cmd, check=False, capture_output=True, 
text=True):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot;
in cmd            assert &amp;quot;-k&amp;quot; in cmd and 
&amp;quot;lmstudio&amp;quot; in cmd            return 
SimpleNamespace(returncode=0, stdout=&amp;quot;&amp;quot;, 
stderr=&amp;quot;&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)            # Use a very specific marker 
expression to trigger keyword path.&amp;gt;       success, output = run_tests(  
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_keyword_filter.py:29: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_keyword_filter_no_matches_returns_success_message.&amp;lt;locals&amp;gt;.fa
ke_collect() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:08,601 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest________ 
test_keyword_filter_honors_report_flag_and_creates_report_dir 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458bf050&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_keyword_filter_honors_rep0&amp;#x27;)    
@pytest.mark.fast    def 
test_keyword_filter_honors_report_flag_and_creates_report_dir(        
monkeypatch, tmp_path    ):        &amp;quot;&amp;quot;&amp;quot;ReqID: FR-11.2 
 Report flag creates deterministic report directory.            Use keyword 
path with report=True and patch datetime to assert directory path.        
&amp;quot;&amp;quot;&amp;quot;            class FakeDT:            @staticmethod
def now():                # Fixed timestamp for stable directory path           
class _DT:                    def strftime(self, fmt: str) -&amp;gt; str:       
return &amp;quot;20250101_000000&amp;quot;                    return _DT()      
monkeypatch.setattr(rt, &amp;quot;datetime&amp;quot;, FakeDT)            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)            test_file = tmp_path / &amp;quot;test_lmstudio.py&amp;quot;    
test_file.write_text(&amp;quot;def test_placeholder():\n    assert 
True\n&amp;quot;)            def fake_collect(cmd, check=False, 
capture_output=True, text=True):  # noqa: ANN001            assert 
&amp;quot;--collect-only&amp;quot; in cmd            assert 
&amp;quot;-k&amp;quot; in cmd and &amp;quot;lmstudio&amp;quot; in cmd           
return SimpleNamespace(                returncode=0,                
stdout=f&amp;quot;{test_file}::test_placeholder&amp;quot;,                
stderr=&amp;quot;&amp;quot;,            )            class FakePopen:           
def __init__(                self, cmd, stdout=None, stderr=None, text=True, 
env=None            ):  # noqa: ANN001                self.cmd = list(cmd)      
self.returncode = 0                self._stdout = &amp;quot;ok&amp;quot;        
self._stderr = &amp;quot;&amp;quot;                def communicate(self):  # 
noqa: D401 - mimic subprocess API                
&amp;quot;&amp;quot;&amp;quot;Return deterministic 
stdout/stderr.&amp;quot;&amp;quot;&amp;quot;                    return 
self._stdout, self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=True,        
parallel=False,            segment=False,            
extra_marker=&amp;#x27;requires_resource(&amp;quot;lmstudio&amp;quot;)&amp;#x27;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_keyword_filter.py:97: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_keyword_filter_honors_report_flag_and_creates_report_dir.&amp;lt;locals&amp
;gt;.fake_collect() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:08,687 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest____ 
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success 
____monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458be150&amp;gt;    @pytest.mark.fast    def 
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success(monkeypat
ch):        &amp;quot;&amp;quot;&amp;quot;        ReqID: C3-05        When 
extra_marker requires_resource(&amp;#x27;lmstudio&amp;#x27;) is provided and the
keyword-filtered        collection yields no tests, run_tests should 
short-circuit and return success=True        with a friendly message instead of 
attempting to invoke pytest with empty args.        
&amp;quot;&amp;quot;&amp;quot;            # Simulate `pytest --collect-only -q 
-m &amp;lt;expr&amp;gt; -k lmstudio` returning no node IDs.        class 
DummyCompleted:            def __init__(self, stdout: str = 
&amp;quot;&amp;quot;, returncode: int = 0):                self.stdout = stdout 
self.stderr = &amp;quot;&amp;quot;                self.returncode = returncode  
def fake_run(            cmd: list,            check: bool = False,            
capture_output: bool = True,            text: bool = True,        ):  # type: 
ignore            # Ensure we&amp;#x27;re calling a python -m pytest command 
with &amp;#x27;--collect-only&amp;#x27;            assert cmd[:3] == , cmd      
assert &amp;quot;--collect-only&amp;quot; in cmd            # Return empty 
stdout to indicate no matched tests            return 
DummyCompleted(stdout=&amp;quot;&amp;quot;, returncode=0)            
monkeypatch.setenv(&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;, 
&amp;quot;1&amp;quot;)  # keep hermetic and fast        
monkeypatch.setenv(&amp;quot;DEVSYNTH_OFFLINE&amp;quot;, 
&amp;quot;true&amp;quot;)            
monkeypatch.setattr(&amp;quot;subprocess.run&amp;quot;, fake_run)            # 
Call run_tests with an lmstudio resource marker expression&amp;gt;       
success, output = run_tests(            target=&amp;quot;unit-tests&amp;quot;,  
speed_categories=None,  # triggers the single-pass branch            
verbose=False,            report=False,            parallel=False,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_keyword_filter_empty.py:42: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success.&amp;lt;l
ocals&amp;gt;.fake_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:08,771 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:08,771 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:08,771 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support2025-10-28 09:29:08,772 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportINFO    
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest___________________ 
test_collect_tests_with_cache_uses_cache ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145b76e10&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_11&amp;#x27;)    
@typed_freeze_time(&amp;quot;2025-01-01&amp;quot;)    @pytest.mark.fast    def 
test_collect_tests_with_cache_uses_cache(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-COLL-1  collect_tests_with_cache
uses the cache.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR        
cache_dir.mkdir()        cache_file = cache_dir / 
&amp;quot;unit-tests_fast_tests.json&amp;quot;        cached_data = {           
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-01T00:00:00.000000&amp;quot;,  
&amp;quot;tests&amp;quot;: ,            &amp;quot;fingerprint&amp;quot;: {      
&amp;quot;latest_mtime&amp;quot;: 1.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tmp_path),            },        }        with open(cache_file, 
&amp;quot;w&amp;quot;) as f:            json.dump(cached_data, f)            
(tmp_path / &amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def 
test_example(): pass&amp;quot;)        with (            
patch.object(rt.os.path, &amp;quot;getmtime&amp;quot;, return_value=1.0),       
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run,            
patch.object(rt, &amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999),     
):&amp;gt;           tests = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)                    
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/testing/test_run_tests_logic.py:77: _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = _collect_via_pytest(_ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
def _collect_via_pytest(        *,        target: str,        test_path: str,   
category_expr: str,        normalized_filter: str | None,        
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;            result = subprocess.run(            
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )        if result.returncode != 0:            if 
result.stderr:                error_message = f&amp;quot;Test collection failed:
{result.stderr}&amp;quot;            else:                error_message = 
f&amp;quot;Test collection failed with exit code {result.returncode}&amp;quot;  
# Log more details for debugging            logger.warning(                
error_message,                extra={                    
&amp;quot;event&amp;quot;: &amp;quot;test_collection_failed&amp;quot;,          
&amp;quot;target&amp;quot;: target,                    
&amp;quot;returncode&amp;quot;: result.returncode,                    
&amp;quot;stdout&amp;quot;: (                        result.stdout[:500] if 
result.stdout else None                    ),  # First 500 chars                
&amp;quot;stderr&amp;quot;: (                        result.stderr[:500] if 
result.stderr else None                    ),  # First 500 chars                
&amp;quot;speed_category&amp;quot;: category_expr,                },            
)&amp;gt;           raise RuntimeError(error_message)E           RuntimeError: 
Test collection failed: &amp;lt;MagicMock name=&amp;#x27;run().stderr&amp;#x27; 
id=&amp;#x27;5455638224&amp;#x27;&amp;gt;/Users/caitlyn/Projects/github.com/rave
noak/devsynth/src/devsynth/testing/run_tests.py:1367: 
RuntimeError----------------------------- Captured stdout call 
-----------------------------2024-12-31 16:00:00,000 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2024-12-31 16:00:00,000 - 
devsynth.testing.run_tests - WARNING - Test collection failed: &amp;lt;MagicMock
name=&amp;#x27;run().stderr&amp;#x27; 
id=&amp;#x27;5455638224&amp;#x27;&amp;gt;------------------------------ Captured
log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Test collection failed: 
&amp;lt;MagicMock name=&amp;#x27;run().stderr&amp;#x27; 
id=&amp;#x27;5455638224&amp;#x27;&amp;gt;____________ 
test_collect_tests_with_cache_regenerates_when_expired ____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458bc0e0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_12&amp;#x27;)    
@typed_freeze_time(&amp;quot;2025-01-02&amp;quot;)    @pytest.mark.fast    def 
test_collect_tests_with_cache_regenerates_when_expired(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-COLL-2  collect_tests_with_cache
regenerates when expired.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR        
cache_dir.mkdir()        cache_file = cache_dir / 
&amp;quot;unit-tests_fast_tests.json&amp;quot;        cached_data = {           
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-01T00:00:00.000000&amp;quot;,  
&amp;quot;tests&amp;quot;: ,            &amp;quot;fingerprint&amp;quot;: {      
&amp;quot;latest_mtime&amp;quot;: 0.5,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tmp_path),            },        }        with open(cache_file, 
&amp;quot;w&amp;quot;) as f:            json.dump(cached_data, f)            
(tmp_path / &amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def 
test_example(): pass&amp;quot;)        with (            
patch.object(rt.os.path, &amp;quot;getmtime&amp;quot;, return_value=1.0),       
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run,            
patch.object(rt, &amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999),     
):            mock_run.return_value = SimpleNamespace(                
stdout=os.path.join(str(tmp_path), &amp;quot;new_test.py&amp;quot;) + 
&amp;quot;\n&amp;quot;,                returncode=0,                
stderr=&amp;quot;&amp;quot;,            )            tests = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)    &amp;gt;       assert tests == E       
AssertionError: assert [] == [&amp;#x27;/private/va.../new_test.py&amp;#x27;]E  
E         Right contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_with_cache_12/new_test.py&amp;#x27;E        
 Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_logic.py:118: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-01-01 16:00:00,000 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest______________________ 
test_collect_tests_with_cache_miss ______________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458bf8c0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_13&amp;#x27;)    
@typed_freeze_time(&amp;quot;2025-01-01&amp;quot;)    @pytest.mark.fast    def 
test_collect_tests_with_cache_miss(        monkeypatch: pytest.MonkeyPatch, 
tmp_path: Path    ) -&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;ReqID: 
RUN-TESTS-COLL-3  collect_tests_with_cache handles a cache 
miss.&amp;quot;&amp;quot;&amp;quot;        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))        (tmp_path / 
&amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def test_example(): 
pass&amp;quot;)            with (            patch.object(rt.os.path, 
&amp;quot;getmtime&amp;quot;, return_value=1.0),            
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run,            
patch.object(rt, &amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999),     
):            mock_run.return_value = SimpleNamespace(                
stdout=os.path.join(str(tmp_path), &amp;quot;test_file.py&amp;quot;) + 
&amp;quot;\n&amp;quot;,                returncode=0,                
stderr=&amp;quot;&amp;quot;,            )            tests = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)    &amp;gt;       assert tests == E       
AssertionError: assert [] == [&amp;#x27;/private/va...test_file.py&amp;#x27;]E  
E         Right contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_with_cache_13/test_file.py&amp;#x27;E       
  Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_logic.py:143: AssertionError----------------------------- Captured 
stdout call -----------------------------2024-12-31 16:00:00,000 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest______________ 
test_collect_tests_with_cache_invalidated_by_mtime ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458a8230&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_14&amp;#x27;)    
@typed_freeze_time(&amp;quot;2025-01-01&amp;quot;)    @pytest.mark.fast    def 
test_collect_tests_with_cache_invalidated_by_mtime(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-COLL-4  collect_tests_with_cache
is invalidated by mtime.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR        
cache_dir.mkdir()        cache_file = cache_dir / 
&amp;quot;unit-tests_fast_tests.json&amp;quot;        cached_data = {           
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-01T00:00:00.000000&amp;quot;,  
&amp;quot;tests&amp;quot;: ,            &amp;quot;fingerprint&amp;quot;: {      
&amp;quot;latest_mtime&amp;quot;: 0.5,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tmp_path),            },        }        with open(cache_file, 
&amp;quot;w&amp;quot;) as f:            json.dump(cached_data, f)            
(tmp_path / &amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def 
test_example(): pass&amp;quot;)        with (            
patch.object(rt.os.path, &amp;quot;getmtime&amp;quot;, return_value=1.0),       
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run,            
patch.object(rt, &amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999),     
):            mock_run.return_value = SimpleNamespace(                
stdout=os.path.join(str(tmp_path), &amp;quot;new_test.py&amp;quot;) + 
&amp;quot;\n&amp;quot;,                returncode=0,                
stderr=&amp;quot;&amp;quot;,            )            with 
freeze_time(&amp;quot;2025-01-02&amp;quot;):                tests = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;fast&amp;quot;)    &amp;gt;       assert tests == E       
AssertionError: assert [] == [&amp;#x27;/private/va.../new_test.py&amp;#x27;]E  
E         Right contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_with_cache_14/new_test.py&amp;#x27;E        
 Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_logic.py:183: AssertionError----------------------------- Captured 
stdout call -----------------------------2025-01-01 16:00:00,000 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_____________ 
test_collect_tests_with_cache_invalidated_by_marker ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14532b500&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_15&amp;#x27;)    
@typed_freeze_time(&amp;quot;2025-01-01&amp;quot;)    @pytest.mark.fast    def 
test_collect_tests_with_cache_invalidated_by_marker(        monkeypatch: 
pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-COLL-5  collect_tests_with_cache
is invalidated by marker.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR        
cache_dir.mkdir()        cache_file = cache_dir / 
&amp;quot;unit-tests_fast_tests.json&amp;quot;        cached_data = {           
&amp;quot;timestamp&amp;quot;: &amp;quot;2025-01-01T00:00:00.000000&amp;quot;,  
&amp;quot;tests&amp;quot;: ,            &amp;quot;fingerprint&amp;quot;: {      
&amp;quot;latest_mtime&amp;quot;: 1.0,                
&amp;quot;category_expr&amp;quot;: &amp;quot;fast and not 
memory_intensive&amp;quot;,                &amp;quot;test_path&amp;quot;: 
str(tmp_path),            },        }        with open(cache_file, 
&amp;quot;w&amp;quot;) as f:            json.dump(cached_data, f)            
(tmp_path / &amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def 
test_example(): pass&amp;quot;)        with (            
patch.object(rt.os.path, &amp;quot;getmtime&amp;quot;, return_value=1.0),       
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run,            
patch.object(rt, &amp;quot;COLLECTION_CACHE_TTL_SECONDS&amp;quot;, 999999),     
):            mock_run.return_value = SimpleNamespace(                
stdout=os.path.join(str(tmp_path), &amp;quot;new_test.py&amp;quot;) + 
&amp;quot;\n&amp;quot;,                returncode=0,                
stderr=&amp;quot;&amp;quot;,            )            with 
freeze_time(&amp;quot;2025-01-01&amp;quot;):                tests = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, 
&amp;quot;slow&amp;quot;)    &amp;gt;       assert tests == E       
AssertionError: assert [] == [&amp;#x27;/private/va.../new_test.py&amp;#x27;]E  
E         Right contains one more item: 
&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cai
tlyn/pytest-1428/test_collect_tests_with_cache_15/new_test.py&amp;#x27;E        
 Use -v to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_logic.py:223: AssertionError----------------------------- Captured 
stdout call -----------------------------2024-12-31 16:00:00,000 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (slow)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (slow)  collecting via pytest______________________ 
test_run_tests_verbose_and_report _______________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_verbose_and_rep0&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145324800&amp;gt;    @pytest.mark.fast    def 
test_run_tests_verbose_and_report(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests with verbose and report 
flags.&amp;quot;&amp;quot;&amp;quot;        monkeypatch.setitem(rt.TARGET_PATHS,
&amp;quot;unit-tests&amp;quot;, str(tmp_path))        test_file = tmp_path / 
&amp;quot;test_example.py&amp;quot;        test_file.write_text(&amp;quot;def 
test_pass(): pass&amp;quot;)            monkeypatch.setattr(            rt,     
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )        monkeypatch.setattr(            rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: None        )  # Prevent
actual artifact generation        monkeypatch.setattr(            rt, 
&amp;quot;enforce_coverage_threshold&amp;quot;, lambda *args, **kwargs: 90.0    
)  # Mock coverage enforcement        custom_env = 
{&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;: &amp;quot;1&amp;quot;}  # 
Enable plugin injection        success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;, verbose=True, report=True, env=custom_env
)            assert success is True        assert len(mock_subprocess_popen) == 
1        cmd, env = mock_subprocess_popen[0]        assert 
&amp;quot;-v&amp;quot; in cmd        assert 
f&amp;quot;--cov={rt.COVERAGE_TARGET}&amp;quot; in cmd        assert 
f&amp;quot;--cov-report=json:{rt.COVERAGE_JSON_PATH}&amp;quot; in cmd        
assert f&amp;quot;--cov-report=html:{rt.COVERAGE_HTML_DIR}&amp;quot; in cmd     
assert &amp;quot;--cov-append&amp;quot; in cmd        assert 
&amp;quot;PYTEST_ADDOPTS&amp;quot; in env  # Should contain plugin 
injection&amp;gt;       del 
os.environ[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;]  # Clean up env  
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.com/r
avenoak/devsynth/tests/unit/testing/test_run_tests_main_logic.py:228: _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = 
environ({&amp;#x27;LOCAL_SHARE&amp;#x27;: 
&amp;#x27;/Users/caitlyn/.local/share&amp;#x27;, &amp;#x27;PWD&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
...n/T/pytest-of-caitlyn/pytest-1428/test_run_tests_verbose_and_rep0/home/.local
/share&amp;#x27;, &amp;#x27;DEVSYNTH_NO_FILE_LOGGING&amp;#x27;: 
&amp;#x27;1&amp;#x27;})key = 
&amp;#x27;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;#x27;&amp;gt;   ???E   KeyError: 
&amp;#x27;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;#x27;&amp;lt;frozen os&amp;gt;:730:
KeyError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:09,547 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_cov into PYTEST_ADDOPTS 
to preserve coverage instrumentation2025-10-28 09:29:09,547 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_bdd.plugin into 
PYTEST_ADDOPTS to preserve pytest-bdd hooks2025-10-28 09:29:09,547 - 
devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into 
PYTEST_ADDOPTS to preserve async test support2025-10-28 09:29:09,548 - 
devsynth.testing.run_tests - WARNING - Skipping release graph publication: 
Coverage JSON missing at 
test_reports/coverage.json------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov into 
PYTEST_ADDOPTS to preserve coverage instrumentationINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_bdd.plugin 
into PYTEST_ADDOPTS to preserve pytest-bdd hooksINFO     
devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test supportWARNING 
devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json________________
test_run_tests_with_markers_and_keyword_filter ________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_with_markers_an0&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145324d70&amp;gt;    @pytest.mark.fast    def 
test_run_tests_with_markers_and_keyword_filter(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests with extra_marker and 
keyword_filter.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_file = tmp_path / &amp;quot;test_example.py&amp;quot;
test_file.write_text(&amp;quot;def test_pass(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )            success, output = rt.run_tests(           
target=&amp;quot;unit-tests&amp;quot;, extra_marker=&amp;quot;slow&amp;quot;, 
keyword_filter=&amp;quot;example&amp;quot;        )            assert success is
True        assert len(mock_subprocess_popen) == 1        cmd, env = 
mock_subprocess_popen[0]        assert &amp;quot;-m&amp;quot; in cmd&amp;gt;    
assert &amp;quot;not memory_intensive and (slow)&amp;quot; in cmdE       
AssertionError: assert &amp;#x27;not memory_intensive and (slow)&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lbs...markers_an0/test_example.py::test_pass&a
mp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not memory_intensive and (fast or 
medium) and (slow) and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_main_logic.py:257: AssertionError_______________ 
test_run_tests_collection_failure_returns_false ________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_collection_fail0&amp;#x27;)mock_subpro
cess_run = [], mock_subprocess_popen = []monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145324aa0&amp;gt;    
@pytest.mark.fast    def test_run_tests_collection_failure_returns_false(       
tmp_path: Path,        mock_subprocess_run: list[list],        
mock_subprocess_popen: list[tuple[list, dict]],        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests returns False on test collection 
failure.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))            def failing_collect(target, speed_category):          
raise RuntimeError(&amp;quot;Collection failed&amp;quot;)            
monkeypatch.setattr(rt, &amp;quot;collect_tests_with_cache&amp;quot;, 
failing_collect)            success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;)            assert success is
False&amp;gt;       assert output == &amp;quot;Test collection failed&amp;quot;E
AssertionError: assert &amp;#x27;Collection failed&amp;#x27; == &amp;#x27;Test 
collection failed&amp;#x27;E         E         - Test collection failedE        
? ^^^^^^E         + Collection failedE         ? 
^/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_main_logic.py:341: AssertionError_________ 
test_run_tests_no_tests_collected_returns_true_with_message __________tmp_path =
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_no_tests_collec0&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145326f60&amp;gt;    @pytest.mark.fast    def 
test_run_tests_no_tests_collected_returns_true_with_message(        tmp_path: 
Path,        mock_subprocess_run: list[list],        mock_subprocess_popen: 
list[tuple[list, dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;Test run_tests returns True and 
message when no tests are collected.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))            monkeypatch.setattr(            rt, 
&amp;quot;collect_tests_with_cache&amp;quot;, lambda target, speed_category: [] 
)            success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;)            assert success is
True&amp;gt;       assert &amp;quot;No tests collected&amp;quot; in outputE     
AssertionError: assert &amp;#x27;No tests collected&amp;#x27; in 
&amp;#x27;Marker fallback 
executed.\n&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/
unit/testing/test_run_tests_main_logic.py:362: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:09,640 - 
devsynth.testing.run_tests - INFO - marker fallback triggered for 
target=unit-tests (speeds=fast,medium)------------------------------ Captured 
log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 marker fallback triggered for 
target=unit-tests (speeds=fast,medium)______________________ 
test_run_tests_segmented_execution ______________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_segmented_execu1&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145325100&amp;gt;    @pytest.mark.fast    def 
test_run_tests_segmented_execution(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests with segmented 
execution.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_a = tmp_path / &amp;quot;test_a.py&amp;quot;        
test_b = tmp_path / &amp;quot;test_b.py&amp;quot;        test_c = tmp_path / 
&amp;quot;test_c.py&amp;quot;        test_a.write_text(&amp;quot;def test_a(): 
pass&amp;quot;)        test_b.write_text(&amp;quot;def test_b(): pass&amp;quot;)
test_c.write_text(&amp;quot;def test_c(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: [                f&amp;quot;{test_a}::test_a&amp;quot;,         
f&amp;quot;{test_b}::test_b&amp;quot;,                
f&amp;quot;{test_c}::test_c&amp;quot;,            ],        )            # Mock 
Popen to return success for all batches        class 
FakePopenSuccess(rt.subprocess.Popen):            def __init__(self, *args: Any,
**kwargs: Any) -&amp;gt; None:                # Append to the fixture&amp;#x27;s
recorded_calls                mock_subprocess_popen.append((args[0], 
kwargs.get(&amp;quot;env&amp;quot;, {})))                super().__init__(*args,
**kwargs)                self.returncode = 0                self._stdout = 
&amp;quot;ok&amp;quot;                self._stderr = &amp;quot;&amp;quot;       
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopenSuccess) 
success, output = rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, 
segment=True, segment_size=1)            assert success is True&amp;gt;       
assert len(mock_subprocess_popen) == 3  # Three batches        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: assert 6 == 3E    
+  where 6 = 
len([([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/
python&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/t
est_run_tests_main_logic.py:439: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:09,682 - 
devsynth.testing.run_tests - INFO - Running 3 tests in 3 segments of size 1 for 
target=unit-tests2025-10-28 09:29:09,682 - devsynth.testing.run_tests - INFO - 
Running segment 1/3 (1 tests)2025-10-28 09:29:09,683 - 
devsynth.testing.run_tests - INFO - Running segment 2/3 (1 tests)2025-10-28 
09:29:09,683 - devsynth.testing.run_tests - INFO - Running segment 3/3 (1 
tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 3 tests in 3 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/3 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/3 (1 tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 
segment 3/3 (1 tests)_______________ 
test_run_tests_segmented_execution_with_failure ________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_segmented_execu2&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145326cf0&amp;gt;    @pytest.mark.fast    def 
test_run_tests_segmented_execution_with_failure(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests with segmented execution where one 
batch fails.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_a = tmp_path / &amp;quot;test_a.py&amp;quot;        
test_b = tmp_path / &amp;quot;test_b.py&amp;quot;        
test_a.write_text(&amp;quot;def test_a(): pass&amp;quot;)        
test_b.write_text(&amp;quot;def test_b(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: [                f&amp;quot;{test_a}::test_a&amp;quot;,         
f&amp;quot;{test_b}::test_b&amp;quot;,            ],        )            # Mock 
Popen to simulate failure in the first batch        class 
FakePopenMixed(rt.subprocess.Popen):            call_count = 0                
def __init__(self, *args: Any, **kwargs: Any) -&amp;gt; None:                # 
Append to the fixture&amp;#x27;s recorded_calls                
mock_subprocess_popen.append((args[0], kwargs.get(&amp;quot;env&amp;quot;, {})))
super().__init__(*args, **kwargs)                FakePopenMixed.call_count += 1 
if FakePopenMixed.call_count == 1:                    self.returncode = 1  # 
Fail first batch                    self._stdout = &amp;quot;fail&amp;quot;     
self._stderr = &amp;quot;error&amp;quot;                else:                   
self.returncode = 0                    self._stdout = &amp;quot;ok&amp;quot;    
self._stderr = &amp;quot;&amp;quot;            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopenMixed)   
success, output = rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, 
segment=True, segment_size=1)            assert success is False        assert 
&amp;quot;Troubleshooting tips&amp;quot; in output  # Should include failure 
tips&amp;gt;       assert len(mock_subprocess_popen) == 2  # Both batches should
run        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^E       AssertionError: assert 
4 == 2E        +  where 4 = 
len([([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/
python&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/t
est_run_tests_main_logic.py:492: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:09,704 - 
devsynth.testing.run_tests - INFO - Running 2 tests in 2 segments of size 1 for 
target=unit-tests2025-10-28 09:29:09,704 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (1 tests)2025-10-28 09:29:09,704 - 
devsynth.testing.run_tests - INFO - Running segment 2/2 (1 
tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/2 (1 tests)______________________ test_run_tests_parallel_execution 
_______________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_parallel_execut0&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145326ab0&amp;gt;    @pytest.mark.fast    def 
test_run_tests_parallel_execution(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test run_tests with parallel 
execution.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_file = tmp_path / &amp;quot;test_example.py&amp;quot;
test_file.write_text(&amp;quot;def test_pass(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )            success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, parallel=True)            
assert success is True        assert len(mock_subprocess_popen) == 1        cmd,
env = mock_subprocess_popen[0]&amp;gt;       assert &amp;quot;-n auto&amp;quot; 
in cmd  # Should add parallel flag        ^^^^^^^^^^^^^^^^^^^^^^^E       
AssertionError: assert &amp;#x27;-n auto&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lbs...tests_parallel_execut0/test_example.py::
test_pass&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not memory_intensive and 
(fast or medium) and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_main_logic.py:520: AssertionError____________ 
test_run_tests_parallel_execution_disabled_by_segment _____________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_parallel_execut1&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x145324560&amp;gt;    @pytest.mark.fast    def 
test_run_tests_parallel_execution_disabled_by_segment(        tmp_path: Path,   
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test parallel execution is disabled when segment 
is True.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_file = tmp_path / &amp;quot;test_example.py&amp;quot;
test_file.write_text(&amp;quot;def test_pass(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )            success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, parallel=True, segment=True)
assert success is True        assert len(mock_subprocess_popen) == 1  # Only one
batch due to segment=True        cmd, env = mock_subprocess_popen[0]        
assert &amp;quot;-n auto&amp;quot; not in cmd  # Parallel should be 
disabled&amp;gt;       assert  == cmd[            3:        ]  # Check if 
node_ids are correctly passedE       AssertionError: assert 
[&amp;#x27;/private/va...y::test_pass&amp;#x27;] == 
[&amp;#x27;/private/va...htmlcov&amp;#x27;, ...]E         E         Right 
contains 8 more items, first extra item: &amp;#x27;-m&amp;#x27;E         Use -v 
to get more 
diff/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_main_logic.py:548: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:09,755 - 
devsynth.testing.run_tests - INFO - Running 1 tests in 1 segments of size 50 for
target=unit-tests2025-10-28 09:29:09,755 - devsynth.testing.run_tests - INFO - 
Running segment 1/1 (1 tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 segments of
size 50 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)___________________ test_run_tests_with_env_var_propagation 
____________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_with_env_var_pr0&amp;#x27;)mock_subpro
cess_run = [], mock_subprocess_popen = []monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145324710&amp;gt;    
@pytest.mark.fast    def test_run_tests_with_env_var_propagation(        
tmp_path: Path,        mock_subprocess_run: list[list],        
mock_subprocess_popen: list[tuple[list, dict]],        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        # Set some initial environment 
variables        os.environ[&amp;quot;EXISTING_VAR&amp;quot;] = 
&amp;quot;initial_value&amp;quot;        
os.environ[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-q&amp;quot;         
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )            custom_env = {&amp;quot;NEW_VAR&amp;quot;:
&amp;quot;new_value&amp;quot;, &amp;quot;PYTEST_ADDOPTS&amp;quot;: 
&amp;quot;--strict-markers&amp;quot;}    &amp;gt;       success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, env=custom_env)             
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.
com/ravenoak/devsynth/tests/unit/testing/test_run_tests_main_logic.py:572: _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests    nodes = collect_callable(target, category)         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, 
speed_category = &amp;#x27;fast&amp;#x27;&amp;gt;       lambda target, 
speed_category: ,                                           ^^^^^^^^^    )E   
NameError: name &amp;#x27;test_file&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/t
est_run_tests_main_logic.py:567: NameError_____________ 
test_run_tests_with_empty_speed_categories_uses_all ______________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_with_empty_spee0&amp;#x27;)mock_subpro
cess_run = [], mock_subprocess_popen = []monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14530e1b0&amp;gt;    
@pytest.mark.fast    def test_run_tests_with_empty_speed_categories_uses_all(   
tmp_path: Path,        mock_subprocess_run: list[list],        
mock_subprocess_popen: list[tuple[list, dict]],        monkeypatch: 
pytest.MonkeyPatch,    ) -&amp;gt; None:        monkeypatch.setattr(            
rt,            &amp;quot;collect_tests_with_cache&amp;quot;,            lambda 
target, speed_category: ,        )    &amp;gt;       success, output = 
rt.run_tests(target=&amp;quot;unit-tests&amp;quot;, speed_categories=[])        
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/gi
thub.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_main_logic.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests    nodes = collect_callable(target, category)         
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = &amp;#x27;unit-tests&amp;#x27;, 
speed_category = &amp;#x27;fast&amp;#x27;&amp;gt;       lambda target, 
speed_category: ,                                           ^^^^^^^^^    )E   
NameError: name &amp;#x27;test_file&amp;#x27; is not 
defined/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/t
est_run_tests_main_logic.py:614: NameError________________ 
test_run_tests_with_specific_speed_categories _________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_with_specific_s0&amp;#x27;)mock_subpro
cess_run = []mock_subprocess_popen = 
[([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pyth
on&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/l...extensions/ms-python.debugpy-2025.14.1-dar
win-arm64/bundled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...})]monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1458bcf20&amp;gt;    @pytest.mark.fast    def 
test_run_tests_with_specific_speed_categories(        tmp_path: Path,        
mock_subprocess_run: list[list],        mock_subprocess_popen: list[tuple[list, 
dict]],        monkeypatch: pytest.MonkeyPatch,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;Test that specific speed_categories are correctly 
applied.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        test_file = tmp_path / &amp;quot;test_example.py&amp;quot;
test_file.write_text(&amp;quot;def test_pass(): pass&amp;quot;)            
monkeypatch.setattr(            rt,            
&amp;quot;collect_tests_with_cache&amp;quot;,            lambda target, 
speed_category: ,        )            success, output = rt.run_tests(           
target=&amp;quot;unit-tests&amp;quot;, 
speed_categories=[&amp;quot;fast&amp;quot;, &amp;quot;medium&amp;quot;]        )
assert success is True        assert len(mock_subprocess_popen) == 1        cmd,
env = mock_subprocess_popen[0]        assert &amp;quot;-m&amp;quot; in 
cmd&amp;gt;       assert &amp;quot;not memory_intensive and (fast or 
medium)&amp;quot; in cmdE       AssertionError: assert &amp;#x27;not 
memory_intensive and (fast or medium)&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lbs...tests_with_specific_s0/test_example.py::
test_pass&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not memory_intensive and 
(fast or medium) and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_main_logic.py:654: AssertionError__________ 
test_collect_tests_with_cache_uses_cache_and_respects_ttl ___________monkeypatch
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458be2a0&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_19&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_tests_with_cache_uses_cache_and_respects_ttl(        monkeypatch: 
pytest.MonkeyPatch, tmp_path    ):        &amp;quot;&amp;quot;&amp;quot;ReqID: 
RTM-02  collect_tests_with_cache caches and reuses results        respecting 
TTL and fingerprint.&amp;quot;&amp;quot;&amp;quot;        # Point TARGET_PATHS 
to tmp tests dir        tests_dir = tmp_path / &amp;quot;tests&amp;quot; / 
&amp;quot;unit&amp;quot;        tests_dir.mkdir(parents=True)        (tests_dir 
/ &amp;quot;test_sample.py&amp;quot;).write_text(&amp;quot;def test_ok():\n    
assert True\n&amp;quot;)            # Monkeypatch TARGET_PATHS and subprocess to
avoid invoking real pytest        monkeypatch.setattr(            rt, 
&amp;quot;TARGET_PATHS&amp;quot;, {&amp;quot;unit-tests&amp;quot;: 
str(tests_dir), &amp;quot;all-tests&amp;quot;: str(tests_dir)}        )         
class DummyProc:            def __init__(self, out: str):                
self.stdout = out                self.returncode = 0            def fake_run(   
cmd: list, check: bool, capture_output: bool, text: bool        ):  # type: 
ignore            assert &amp;quot;--collect-only&amp;quot; in cmd            # 
emulate -q output: one per line            return 
DummyProc(out=f&amp;quot;{tests_dir}/test_sample.py\n&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
# Speed None path&amp;gt;       ids1 = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;, speed_category=None)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Pro
jects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_module.py:7
6: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache    node_ids = _collect_via_pytest(_ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     
def _collect_via_pytest(        *,        target: str,        test_path: str,   
category_expr: str,        normalized_filter: str | None,        
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_collect_tests_with_cache_uses_cache_and_respects_ttl.&amp;lt;locals&amp;gt;
.fake_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,027 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (all)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (all)  collecting via pytest___________ 
test_run_tests_translates_args_and_handles_return_codes ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1458bd2b0&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_translates_args0&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_translates_args_and_handles_return_codes(        monkeypatch: 
pytest.MonkeyPatch, tmp_path    ):        &amp;quot;&amp;quot;&amp;quot;ReqID: 
RTM-03  run_tests translates args, treats code 0/5 as success,        and omits
-n when parallel=False.&amp;quot;&amp;quot;&amp;quot;        # Arrange base to 
avoid plugin interactions and filesystem writes        tests_dir = tmp_path / 
&amp;quot;tests&amp;quot;        tests_dir.mkdir()        (tests_dir / 
&amp;quot;test_x.py&amp;quot;).write_text(&amp;quot;def test_one():\n    assert 
True\n&amp;quot;)        monkeypatch.setattr(            rt, 
&amp;quot;TARGET_PATHS&amp;quot;, {&amp;quot;unit-tests&amp;quot;: 
str(tests_dir), &amp;quot;all-tests&amp;quot;: str(tests_dir)}        )         
# Capture the command built for the run path (no speed_categories provided)     
captured = {&amp;quot;cmd&amp;quot;: None, &amp;quot;env&amp;quot;: None}       
class CollectResult:            def __init__(self, out: str) -&amp;gt; None:    
self.stdout = out                self.stderr = &amp;quot;&amp;quot;             
self.returncode = 0            def fake_run(            cmd: list,            
check: bool,            capture_output: bool,            text: bool,        ) 
-&amp;gt; CollectResult:  # type: ignore            assert 
&amp;quot;--collect-only&amp;quot; in cmd            return 
CollectResult(f&amp;quot;{tests_dir}/test_x.py::test_one\n&amp;quot;)           
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
class P:            def __init__(self, cmd: list, code: int, out: str = 
&amp;quot;ok\n&amp;quot;, err: str = &amp;quot;&amp;quot;):                
self.args = cmd                self._code = code                self._out = out 
self._err = err                def communicate(self, *_args: Any, **_kwargs: 
Any):                return self._out, self._err                @property       
def returncode(self) -&amp;gt; int:                return self._code            
def __enter__(self) -&amp;gt; &amp;quot;P&amp;quot;:                return self 
def __exit__(                self,                _exc_type: Any,               
_exc: Any,                _tb: Any,            ) -&amp;gt; bool:                
return False                def kill(self) -&amp;gt; None:  # pragma: no cover -
subprocess.run compatibility                pass                def wait(self) 
-&amp;gt; int:  # pragma: no cover - subprocess.run compatibility               
return self._code                def poll(self) -&amp;gt; int:  # pragma: no 
cover - subprocess.run compatibility                return self._code           
def fake_popen(            cmd: list,            stdout,            stderr,     
text: bool,            env: dict | None = None,        ):  # type: ignore       
captured[&amp;quot;cmd&amp;quot;] = cmd            
captured[&amp;quot;env&amp;quot;] = env            # Succeed with code 0 first  
return P(cmd, 0)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, fake_popen)    &amp;gt;       ok, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;, 
speed_categories=[&amp;quot;fast&amp;quot;], parallel=False        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_module.py:174: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_run_tests_translates_args_and_handles_return_codes.&amp;lt;locals&amp;gt;.f
ake_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,079 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest___________ 
test_run_tests_keyword_filter_for_extra_marker_lmstudio ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14530e780&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_keyword_filter_1&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_keyword_filter_for_extra_marker_lmstudio(        monkeypatch: 
pytest.MonkeyPatch, tmp_path    ):        &amp;quot;&amp;quot;&amp;quot;ReqID: 
RTM-04  extra_marker 
&amp;#x27;requires_resource(&amp;quot;lmstudio&amp;quot;)&amp;#x27; uses        
keyword filter and early success on no matches.&amp;quot;&amp;quot;&amp;quot;   
# Arrange: ensure keyword narrowing path is exercised with no matches -&amp;gt; 
# early success        tests_dir = tmp_path / &amp;quot;tests&amp;quot;        
tests_dir.mkdir()        monkeypatch.setattr(            rt, 
&amp;quot;TARGET_PATHS&amp;quot;, {&amp;quot;unit-tests&amp;quot;: 
str(tests_dir), &amp;quot;all-tests&amp;quot;: str(tests_dir)}        )         
class Dummy:            def __init__(self, stdout: str, returncode: int = 0):   
self.stdout = stdout                self.returncode = returncode            def 
fake_run(            cmd, check: bool, capture_output: bool, text: bool        
):  # type: ignore            # &amp;#x27;--collect-only&amp;#x27; path with 
&amp;#x27;-k lmstudio&amp;#x27; produces no items            assert 
&amp;quot;--collect-only&amp;quot; in cmd            return 
Dummy(stdout=&amp;quot;&amp;quot;)            monkeypatch.setattr(rt.subprocess,
&amp;quot;run&amp;quot;, fake_run)    &amp;gt;       ok, msg = rt.run_tests(    
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
extra_marker=&amp;quot;requires_resource(&amp;#x27;lmstudio&amp;#x27;)&amp;quot;
,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_module.py:236: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_run_tests_keyword_filter_for_extra_marker_lmstudio.&amp;lt;locals&amp;gt;.f
ake_run() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,140 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_________ 
test_run_tests_handles_popen_exception_without_speed_filters 
_________monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x14532b4d0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_handles_popen_e0&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_handles_popen_exception_without_speed_filters(        
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RTM-05  run_tests surfaces subprocess 
errors with guidance.&amp;quot;&amp;quot;&amp;quot;            tests_dir = 
tmp_path / &amp;quot;tests&amp;quot;        tests_dir.mkdir()        
monkeypatch.setattr(            rt, &amp;quot;TARGET_PATHS&amp;quot;, 
{&amp;quot;unit-tests&amp;quot;: str(tests_dir), &amp;quot;all-tests&amp;quot;: 
str(tests_dir)}        )            # ``run_tests`` should not perform 
collection when no speed categories are        # provided. Guard against 
unexpected subprocess.run usage.        def fail_run(*_args: Any, **_kwargs: 
Any) -&amp;gt; None:  # pragma: no cover - safety            raise 
AssertionError(&amp;quot;subprocess.run should not be invoked in this 
branch&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fail_run)            captured: dict[str, list] = {}    
def boom_popen(            cmd: list,            stdout: Any = None,            
stderr: Any = None,            text: bool = True,            env: dict | None = 
None,        ) -&amp;gt; Any:  # pragma: no cover - behavior exercised via 
exception path            captured[&amp;quot;cmd&amp;quot;] = cmd            
raise RuntimeError(&amp;quot;intentional popen failure&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, boom_popen)    
&amp;gt;       success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=None,        
verbose=False,            report=False,            parallel=False,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_module.py:278: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _args = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-1428/test_run_tests_handles_popen_e0/tests&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)_kwargs = 
{&amp;#x27;capture_output&amp;#x27;: True, &amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: 
{&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;...on.debugpy-2025.14.1-darwin-arm64/bu
ndled/libs/debugpy&amp;#x27;, &amp;#x27;CLICOLOR&amp;#x27;: 
&amp;#x27;1&amp;#x27;, &amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, 
...}, &amp;#x27;text&amp;#x27;: True, ...}    def fail_run(*_args: Any, 
**_kwargs: Any) -&amp;gt; None:  # pragma: no cover - safety&amp;gt;       raise
AssertionError(&amp;quot;subprocess.run should not be invoked in this 
branch&amp;quot;)E       AssertionError: subprocess.run should not be invoked in
this 
branch/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/te
st_run_tests_module.py:260: AssertionError----------------------------- Captured
stdout call -----------------------------2025-10-28 09:29:10,208 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_______________ 
test_collect_unknown_target_uses_all_tests_path ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145329eb0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_unknown_target_us0&amp;#x27;)    
@pytest.mark.fast    def 
test_collect_unknown_target_uses_all_tests_path(monkeypatch, tmp_path):        
&amp;quot;&amp;quot;&amp;quot;ReqID: RTM-06  Unknown target falls back to 
all-tests mapping.&amp;quot;&amp;quot;&amp;quot;            tests_dir = tmp_path
/ &amp;quot;some_tests&amp;quot;        tests_dir.mkdir()        cache_dir = 
tmp_path / &amp;quot;cache&amp;quot;        cache_dir.mkdir()        
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, 
str(cache_dir))        monkeypatch.setattr(rt, &amp;quot;TARGET_PATHS&amp;quot;,
{&amp;quot;all-tests&amp;quot;: str(tests_dir)})            observed: list[list]
= []            def fake_run(            cmd,            check=False,           
capture_output=False,            text=False,            timeout=None,           
cwd=None,            env=None,        ):  # noqa: ANN001            
observed.append(cmd[:])            return SimpleNamespace(                
stdout=&amp;quot;test_sample.py::test_ok\n&amp;quot;,                
stderr=&amp;quot;&amp;quot;,                returncode=0,            )          
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
original_isdir = rt.os.path.isdir            def fake_isdir(path: str) -&amp;gt;
bool:            if path == str(tests_dir):                return False         
return original_isdir(path)            monkeypatch.setattr(rt.os.path, 
&amp;quot;isdir&amp;quot;, fake_isdir)            original_exists = 
rt.os.path.exists            def fake_exists(path: str) -&amp;gt; bool:         
if path == &amp;quot;test_sample.py&amp;quot;:                return True       
return original_exists(path)            monkeypatch.setattr(rt.os.path, 
&amp;quot;exists&amp;quot;, fake_exists)    &amp;gt;       result = 
rt.collect_tests_with_cache(&amp;quot;custom-target&amp;quot;, 
speed_category=None)                 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/
Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run_tests_module.p
y:346: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ target = &amp;#x27;custom-target&amp;#x27;, speed_category = None    def 
collect_tests_with_cache(        target: str,        speed_category: str | None 
= None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError_______________ 
test_enforce_coverage_threshold_exit_and_return ________________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_enforce_coverage_threshol2&amp;#x27;)    
@pytest.mark.fast    def 
test_enforce_coverage_threshold_exit_and_return(tmp_path):        
&amp;quot;&amp;quot;&amp;quot;ReqID: RTM-05  Coverage helper returns percent 
and exits on failure.&amp;quot;&amp;quot;&amp;quot;            cov_file = 
tmp_path / &amp;quot;coverage.json&amp;quot;        
cov_file.write_text(json.dumps({&amp;quot;totals&amp;quot;: 
{&amp;quot;percent_covered&amp;quot;: 95.25}}))        percent = 
rt.enforce_coverage_threshold(            coverage_file=cov_file, 
exit_on_failure=False        )        assert percent == pytest.approx(95.25)    
cov_file.write_text(json.dumps({&amp;quot;totals&amp;quot;: 
{&amp;quot;percent_covered&amp;quot;: 81.7}}))&amp;gt;       with 
pytest.raises(RuntimeError):             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E       
Failed: DID NOT RAISE &amp;lt;class 
&amp;#x27;RuntimeError&amp;#x27;&amp;gt;/Users/caitlyn/Projects/github.com/raven
oak/devsynth/tests/unit/testing/test_run_tests_module.py:365: 
Failed----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:10,339 - 
devsynth.testing.run_tests - INFO - Coverage 95.25% meets the 70.00% 
threshold.2025-10-28 09:29:10,340 - devsynth.testing.run_tests - INFO - Coverage
81.70% meets the 70.00% threshold.------------------------------ Captured log 
call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 Coverage 95.25% meets the 70.00%
threshold.INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage 
81.70% meets the 70.00% threshold._______________ 
test_run_tests_segment_appends_aggregation_tips ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1453282c0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_segment_appends0&amp;#x27;)    
@pytest.mark.fast    def test_run_tests_segment_appends_aggregation_tips(       
monkeypatch: pytest.MonkeyPatch, tmp_path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RTM-07  Segmented failures append 
aggregate troubleshooting tips.&amp;quot;&amp;quot;&amp;quot;            
tests_dir = tmp_path / &amp;quot;segmented&amp;quot;        tests_dir.mkdir()   
(tests_dir / &amp;quot;test_one.py&amp;quot;).write_text(&amp;quot;def 
test_one():\n    assert True\n&amp;quot;)        (tests_dir / 
&amp;quot;test_two.py&amp;quot;).write_text(&amp;quot;def test_two():\n    
assert True\n&amp;quot;)            monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)            class CollectProc:            def __init__(self, out: str) 
-&amp;gt; None:                self.stdout = out                self.stderr = 
&amp;quot;&amp;quot;                self.returncode = 0            def fake_run(
cmd,            check=False,            capture_output=True,            
text=True,            timeout=None,            cwd=None,            env=None,   
):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot; in cmd  
stdout = &amp;quot;\n&amp;quot;.join([&amp;quot;test_one.py::test_one&amp;quot;,
&amp;quot;test_two.py::test_two&amp;quot;])            return 
CollectProc(stdout)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_run)            batch_calls: list[list] = []      
class FakeBatchProcess:            def __init__(                self,           
cmd,                stdout=None,                stderr=None,                
text=False,                env=None,            ) -&amp;gt; None:  # noqa: 
ANN001                batch_calls.append(cmd)                self.args = cmd    
index = len(batch_calls) - 1                if index == 0:                    
self._stdout = &amp;quot;batch-1\n&amp;quot;                    self._stderr = 
&amp;quot;&amp;quot;                    self._returncode = 0                
else:                    self._stdout = &amp;quot;batch-2\n&amp;quot;           
self._stderr = &amp;quot;boom&amp;quot;                    self._returncode = 1 
def communicate(self):  # noqa: D401 - simple stub                return 
self._stdout, self._stderr                @property            def 
returncode(self) -&amp;gt; int:                return self._returncode          
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakeBatchProcess) 
success, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=None,            extra_marker=None,        ) 
assert success is False        assert len(batch_calls) == 2            
expected_agg_cmd = [            rt.sys.executable,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
f&amp;quot;--cov={rt.COVERAGE_TARGET}&amp;quot;,            
&amp;quot;--cov-report=term-missing&amp;quot;,            
f&amp;quot;--cov-report=json:{rt.COVERAGE_JSON_PATH}&amp;quot;,            
f&amp;quot;--cov-report=html:{rt.COVERAGE_HTML_DIR}&amp;quot;,            
&amp;quot;--cov-append&amp;quot;,            str(tests_dir),        ]        
aggregate_tip = rt._failure_tips(1, expected_agg_cmd)    &amp;gt;       assert 
aggregate_tip in outputE       AssertionError: assert &amp;#x27;\nPytest exited 
with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m 
pytest...HTML report for context (saved under test_reports/):\n  devsynth 
run-tests --target unit-tests --speed=fast --report\n&amp;#x27; in 
&amp;#x27;batch-1\n\nbatch-2\n\nPytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/...HTML report for 
context (saved under test_reports/):\n  devsynth run-tests --target unit-tests 
--speed=fast 
--report\n&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_module.py:484: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:10,366 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:10,366 - 
devsynth.testing.run_tests - INFO - Running 2 tests in 2 segments of size 1 for 
target=unit-tests2025-10-28 09:29:10,366 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (1 tests)2025-10-28 09:29:10,366 - 
devsynth.testing.run_tests - INFO - Running segment 2/2 (1 
tests)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/2 (1 tests)______________ test_run_tests_completes_without_xdist_assertions 
_______________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_completes_witho0&amp;#x27;)monkeypatch
 = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145152e10&amp;gt;    
@pytest.mark.fast    def 
test_run_tests_completes_without_xdist_assertions(tmp_path, monkeypatch):       
&amp;quot;&amp;quot;&amp;quot;run_tests completes without INTERNALERROR when run
in parallel. ReqID: FR-22&amp;quot;&amp;quot;&amp;quot;        test_file = 
tmp_path / &amp;quot;test_dummy.py&amp;quot;        test_file.write_text(       
&amp;quot;import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n&amp;quot;        )        monkeypatch.setitem(TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;all-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)            def fake_collect(cmd, check=False, capture_output=True, 
text=True):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot;
in cmd            return SimpleNamespace(                returncode=0,          
stdout=f&amp;quot;{test_file}::test_ok&amp;quot;,                
stderr=&amp;quot;&amp;quot;,            )            class FakePopen:           
def __init__(                self, cmd, stdout=None, stderr=None, text=True, 
env=None            ):  # noqa: ANN001                assert 
&amp;quot;-n&amp;quot; in cmd and &amp;quot;auto&amp;quot; in cmd               
self.returncode = 0                self._stdout = &amp;quot;passed&amp;quot;    
self._stderr = &amp;quot;&amp;quot;                def communicate(self):  # 
noqa: D401 - mimic subprocess API                
&amp;quot;&amp;quot;&amp;quot;Return deterministic 
stdout/stderr.&amp;quot;&amp;quot;&amp;quot;                    return 
self._stdout, self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;run&amp;quot;, fake_collect)        monkeypatch.setattr(rt.subprocess,
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(&amp;quot;unit-tests&amp;quot;, [&amp;quot;fast&amp;quot;], 
parallel=True)                          
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^/Users/caitlyn/Projects/github.c
om/ravenoak/devsynth/tests/unit/testing/test_run_tests_no_xdist_assertions.py:46
: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_run_tests_completes_without_xdist_assertions.&amp;lt;locals&amp;gt;.fake_co
llect() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,413 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_________________ 
test_report_flag_adds_html_report_to_command _________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451f97f0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_report_flag_adds_html_rep0&amp;#x27;)    
@pytest.mark.fast    def test_report_flag_adds_html_report_to_command(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-ORCH-2  report=True adds --html 
to the pytest command.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        (tmp_path / 
&amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def test_example(): 
pass&amp;quot;)            recorded_cmds: list[list] = []            class 
FakePopen:            def __init__(self, cmd, *args, **kwargs):                
recorded_cmds.append(list(cmd))                self.returncode = 0              
def communicate(self):                return &amp;quot;ok&amp;quot;, 
&amp;quot;&amp;quot;            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)            with (            
patch.object(rt, &amp;quot;collect_tests_with_cache&amp;quot;, 
return_value=[&amp;quot;test_file.py&amp;quot;]),            patch.object(rt, 
&amp;quot;datetime&amp;quot;) as mock_dt,        ):            
mock_dt.now.return_value.strftime.return_value = 
&amp;quot;20250101_000000&amp;quot;            rt.run_tests(                
target=&amp;quot;unit-tests&amp;quot;,                
speed_categories=[&amp;quot;fast&amp;quot;],                verbose=False,      
report=True,                parallel=False,                segment=False,       
maxfail=None,                extra_marker=None,            )            assert 
recorded_cmds, &amp;quot;run_tests should have invoked Popen&amp;quot;        
pytest_cmd = recorded_cmds[0]&amp;gt;       assert 
any(arg.startswith(&amp;quot;--html=&amp;quot;) for arg in pytest_cmd)E       
assert FalseE        +  where False = any(&amp;lt;generator object 
test_report_flag_adds_html_report_to_command.&amp;lt;locals&amp;gt;.&amp;lt;gene
xpr&amp;gt; at 
0x1470620c0&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_orchestration.py:101: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:10,535 - 
devsynth.testing.run_tests - WARNING - Skipping release graph publication: 
Coverage JSON missing at 
test_reports/coverage.json------------------------------ Captured log call 
-------------------------------WARNING  
devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at 
test_reports/coverage.json___________________ 
test_no_parallel_flag_adds_n0_to_command ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451f98e0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_no_parallel_flag_adds_n0_0&amp;#x27;)    
@pytest.mark.fast    def test_no_parallel_flag_adds_n0_to_command(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-ORCH-3  parallel=False adds -n0 
to the pytest command.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        (tmp_path / 
&amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def test_example(): 
pass&amp;quot;)            recorded_cmds: list[list] = []            class 
FakePopen:            def __init__(self, cmd, *args, **kwargs):                
recorded_cmds.append(list(cmd))                self.returncode = 0              
def communicate(self):                return &amp;quot;ok&amp;quot;, 
&amp;quot;&amp;quot;            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)            with patch.object(rt, 
&amp;quot;collect_tests_with_cache&amp;quot;, 
return_value=[&amp;quot;test_file.py&amp;quot;]):            rt.run_tests(      
target=&amp;quot;unit-tests&amp;quot;,                
speed_categories=[&amp;quot;fast&amp;quot;],                verbose=False,      
report=False,                parallel=False,                segment=False,      
maxfail=None,                extra_marker=None,            )            assert 
recorded_cmds, &amp;quot;run_tests should have invoked Popen&amp;quot;        
pytest_cmd = recorded_cmds[0]&amp;gt;       assert &amp;quot;-n0&amp;quot; in 
pytest_cmdE       AssertionError: assert &amp;#x27;-n0&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;test_file.py&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not 
memory_intensive and fast and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_orchestration.py:138: AssertionError__________________ 
test_maxfail_flag_adds_maxfail_to_command ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451f80e0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_maxfail_flag_adds_maxfail0&amp;#x27;)    
@pytest.mark.fast    def test_maxfail_flag_adds_maxfail_to_command(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-ORCH-4  maxfail=N adds 
--maxfail=N to the pytest command.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        (tmp_path / 
&amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def test_example(): 
pass&amp;quot;)            recorded_cmds: list[list] = []            class 
FakePopen:            def __init__(self, cmd, *args, **kwargs):                
recorded_cmds.append(list(cmd))                self.returncode = 0              
def communicate(self):                return &amp;quot;ok&amp;quot;, 
&amp;quot;&amp;quot;            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)            with patch.object(rt, 
&amp;quot;collect_tests_with_cache&amp;quot;, 
return_value=[&amp;quot;test_file.py&amp;quot;]):            rt.run_tests(      
target=&amp;quot;unit-tests&amp;quot;,                
speed_categories=[&amp;quot;fast&amp;quot;],                verbose=False,      
report=False,                parallel=False,                segment=False,      
maxfail=5,                extra_marker=None,            )            assert 
recorded_cmds, &amp;quot;run_tests should have invoked Popen&amp;quot;        
pytest_cmd = recorded_cmds[0]&amp;gt;       assert 
&amp;quot;--maxfail=5&amp;quot; in pytest_cmdE       AssertionError: assert 
&amp;#x27;--maxfail=5&amp;#x27; in 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;test_file.py&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not 
memory_intensive and fast and not gui&amp;#x27;, 
...]/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_orchestration.py:175: AssertionError___________________ 
test_segment_flags_trigger_segmented_run ___________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451fa780&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segment_flags_trigger_seg0&amp;#x27;)    
@pytest.mark.fast    def test_segment_flags_trigger_segmented_run(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-ORCH-5  segment=True triggers a 
segmented run.&amp;quot;&amp;quot;&amp;quot;        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        (tmp_path / 
&amp;quot;test_file.py&amp;quot;).write_text(&amp;quot;def test_example(): 
pass&amp;quot;)            recorded_cmds: list[list] = []            class 
FakePopen:            def __init__(self, cmd, *args, **kwargs):                
recorded_cmds.append(list(cmd))                self.returncode = 0              
def communicate(self):                return &amp;quot;ok&amp;quot;, 
&amp;quot;&amp;quot;            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)            with 
patch.object(rt.subprocess, &amp;quot;run&amp;quot;) as mock_run:            
mock_run.return_value.stdout = &amp;quot;test_file.py\ntest_file2.py&amp;quot;  
mock_run.return_value.returncode = 0            rt.run_tests(                
target=&amp;quot;unit-tests&amp;quot;,                
speed_categories=[&amp;quot;fast&amp;quot;],                verbose=False,      
report=False,                parallel=False,                segment=True,       
segment_size=1,                maxfail=None,                extra_marker=None,  
)    &amp;gt;       assert len(recorded_cmds) == 2, &amp;quot;Expected two Popen
calls for a segmented run&amp;quot;E       AssertionError: Expected two Popen 
calls for a segmented runE       assert 1 == 2E        +  where 1 = 
len([[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/p
ython&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...ytest-of-caitlyn/pytest-1428/test_segment
_flags_trigger_seg0&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;not 
memory_intensive and fast and not gui&amp;#x27;, 
...]])/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/te
st_run_tests_orchestration.py:213: AssertionError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,616 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:10,617 - 
devsynth.testing.run_tests - INFO - marker fallback triggered for 
target=unit-tests (speeds=fast)------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 marker fallback triggered for 
target=unit-tests (speeds=fast)_______________ 
test_run_tests_parallel_includes_cov_and_n_auto ________________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451fb530&amp;gt;    
@pytest.mark.fast    def 
test_run_tests_parallel_includes_cov_and_n_auto(monkeypatch):        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-PARALLEL-1            When 
parallel=True and no explicit node ids are collected (single-pass branch),      
run_tests should include &amp;#x27;-n auto&amp;#x27; and explicit coverage 
instrumentation in the        pytest command.        
&amp;quot;&amp;quot;&amp;quot;            import devsynth.testing.run_tests as 
rt            # We won&amp;#x27;t validate the collection step here; the 
single-pass branch does not        # pre-collect node ids when speed_categories 
is None.            class FakePopen:            def __init__(                
self, cmd, stdout=None, stderr=None, text=False, env=None            ):  # noqa:
ANN001                # Assert parallel-related flags are present               
assert &amp;quot;-n&amp;quot; in cmd and &amp;quot;auto&amp;quot; in cmd, 
f&amp;quot;parallel flags missing in: {cmd}&amp;quot;                cov_flag = 
f&amp;quot;--cov={rt.COVERAGE_TARGET}&amp;quot;                json_flag = 
f&amp;quot;--cov-report=json:{rt.COVERAGE_JSON_PATH}&amp;quot;                
html_flag = f&amp;quot;--cov-report=html:{rt.COVERAGE_HTML_DIR}&amp;quot;       
assert cov_flag in cmd, f&amp;quot;{cov_flag} missing in: {cmd}&amp;quot;       
assert json_flag in cmd, f&amp;quot;{json_flag} missing in: {cmd}&amp;quot;     
assert html_flag in cmd, f&amp;quot;{html_flag} missing in: {cmd}&amp;quot;     
assert &amp;quot;--cov-append&amp;quot; in cmd, f&amp;quot;--cov-append missing 
in: {cmd}&amp;quot;                self.returncode = 0                def 
communicate(self):                return (&amp;quot;ok\n&amp;quot;, 
&amp;quot;&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,  # triggers non-collection single-pass branch            
verbose=False,            report=False,            parallel=True,            
segment=False,            segment_size=50,            maxfail=None,            
extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_parallel_flags.py:40: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_run_tests_parallel_includes_cov_and_n_auto.&amp;lt;locals&amp;gt;.FakePopen
.__init__() got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:10,651 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_______________ 
test_parallel_injects_cov_reports_and_xdist_auto _______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451f95b0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_parallel_injects_cov_repo0&amp;#x27;)    
@pytest.mark.fast    def 
test_parallel_injects_cov_reports_and_xdist_auto(monkeypatch, tmp_path: Path):  
&amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-11  Parallel path injects -n auto 
with coverage reports.            Verify that when parallel=True, run_tests 
injects xdist flags and preserves        coverage instrumentation so JSON/HTML 
artifacts are generated.        &amp;quot;&amp;quot;&amp;quot;            called
= {}            class FakeCompleted:            def __init__(self, stdout: str =
&amp;quot;&amp;quot;, stderr: str = &amp;quot;&amp;quot;, returncode: int = 0): 
self.stdout = stdout                self.stderr = stderr                
self.returncode = returncode            test_a = tmp_path / 
&amp;quot;test_alpha.py&amp;quot;        test_b = tmp_path / 
&amp;quot;test_beta.py&amp;quot;        test_a.write_text(&amp;quot;def 
test_one():\n    assert True\n&amp;quot;)        test_b.write_text(&amp;quot;def
test_two():\n    assert True\n&amp;quot;)            
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tmp_path))        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;all-tests&amp;quot;, str(tmp_path))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)            def fake_run(            cmd, check=False, 
capture_output=False, text=False        ):  # type: ignore            # Simulate
collection with two node ids; pattern            # 
&amp;quot;.*\\.py(::|$)&amp;quot; will match them.            stdout = 
&amp;quot;\n&amp;quot;.join(                [                    
f&amp;quot;{test_a}::test_one&amp;quot;,                    
f&amp;quot;{test_b}::test_two&amp;quot;,                ]            )          
return FakeCompleted(stdout=stdout, stderr=&amp;quot;&amp;quot;, returncode=0)  
# pragma: no cover - communicate() path is asserted via effects        class 
FakePopen:            def __init__(                self, cmd, stdout=None, 
stderr=None, text=False, env=None            ):  # type: ignore                
called[&amp;quot;cmd&amp;quot;] = cmd                self.returncode = 0        
def communicate(self):  # type: ignore                return 
(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;)            # Patch subprocess in 
module under test        
monkeypatch.setattr(&amp;quot;devsynth.testing.run_tests.subprocess.run&amp;quot
;, fake_run)        
monkeypatch.setattr(&amp;quot;devsynth.testing.run_tests.subprocess.Popen&amp;qu
ot;, FakePopen)    &amp;gt;       success, output = run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=True,            segment=False,            
maxfail=None,            extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_parallel_no_cov.py:63: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_parallel_injects_cov_reports_and_xdist_auto.&amp;lt;locals&amp;gt;.fake_run
() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError----------------------------- 
Captured stdout call -----------------------------2025-10-28 09:29:10,805 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest___________ 
test_collect_tests_with_cache_handles_subprocess_timeout ___________tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_collect_tests_with_cache_20&amp;#x27;)monkeypatc
h = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x1451f3c20&amp;gt;caplog 
= &amp;lt;_pytest.logging.LogCaptureFixture object at 0x145328170&amp;gt;    
@pytest.mark.fast    def 
test_collect_tests_with_cache_handles_subprocess_timeout(        tmp_path: Path,
monkeypatch: pytest.MonkeyPatch,        caplog: pytest.LogCaptureFixture,    ) 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Timeouts during collection 
surface a warning and yield no tests.&amp;quot;&amp;quot;&amp;quot;            
monkeypatch.setattr(rt, &amp;quot;COLLECTION_CACHE_DIR&amp;quot;, tmp_path / 
&amp;quot;.cache&amp;quot;)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tmp_path))        (tmp_path / 
&amp;quot;sample_test.py&amp;quot;).write_text(&amp;quot;def test_sample():\n   
assert True\n&amp;quot;)            def fake_run(*_args: object, **_kwargs: 
object) -&amp;gt; subprocess.CompletedProcess:            raise 
subprocess.TimeoutExpired(                cmd=[&amp;quot;pytest&amp;quot;], 
timeout=rt.DEFAULT_COLLECTION_TIMEOUT_SECONDS            )            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
with caplog.at_level(logging.WARNING, 
logger=&amp;quot;devsynth.testing.run_tests&amp;quot;):            collected = 
rt.collect_tests_with_cache(&amp;quot;unit-tests&amp;quot;)            assert 
collected == []&amp;gt;       assert &amp;quot;Test collection failed&amp;quot; 
in caplog.textE       AssertionError: assert &amp;#x27;Test collection 
failed&amp;#x27; in &amp;#x27;WARNING  
devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout for 
target=unit-tests (all); falling back to path\n&amp;#x27;E        +  where 
&amp;#x27;WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test 
collection timeout for target=unit-tests (all); falling back to path\n&amp;#x27;
= &amp;lt;_pytest.logging.LogCaptureFixture object at 
0x145328170&amp;gt;.text/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tes
ts/unit/testing/test_run_tests_plugin_timeouts.py:35: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:10,936 - 
devsynth.testing.run_tests - WARNING - Test collection timeout for 
target=unit-tests (all); falling back to path------------------------------ 
Captured log call -------------------------------WARNING  
devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout for 
target=unit-tests (all); falling back to path________________ 
test_pytest_plugins_registers_pytest_bdd_once _________________    
@pytest.mark.fast    def test_pytest_plugins_registers_pytest_bdd_once() 
-&amp;gt; None:        &amp;quot;&amp;quot;&amp;quot;Ensure the centralized 
helper exports pytest-bdd exactly once.&amp;quot;&amp;quot;&amp;quot;           
import importlib            registry = 
importlib.import_module(&amp;quot;tests.pytest_plugin_registry&amp;quot;)       
plugin_list = list(registry.PYTEST_PLUGINS)    &amp;gt;       assert 
plugin_list.count(&amp;quot;pytest_bdd.plugin&amp;quot;) == 1E       
AssertionError: assert 0 == 1E        +  where 0 = &amp;lt;built-in method count
of list object at 0x1454f6840&amp;gt;(&amp;#x27;pytest_bdd.plugin&amp;#x27;)E   
+    where &amp;lt;built-in method count of list object at 0x1454f6840&amp;gt; =
[].count/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/
test_run_tests_pytest_plugins_bdd.py:107: AssertionError___________ 
test_run_tests_report_injects_html_args_and_creates_dir ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145382810&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_report_injects_0&amp;#x27;)    
@pytest.mark.fast    def 
test_run_tests_report_injects_html_args_and_creates_dir(monkeypatch, tmp_path): 
&amp;quot;&amp;quot;&amp;quot;        ReqID: TR-RT-12  Report HTML generation 
and directory creation.            Validate that when report=True, run_tests:   
- adds --html=&amp;lt;test_reports/.../target&amp;gt;/report.html and 
--self-contained-html        - creates the report directory path        - 
executes pytest with node ids (non-parallel path)        
&amp;quot;&amp;quot;&amp;quot;            # Arrange a tmp tests dir and map 
unit-tests target to it        tests_dir = tmp_path / &amp;quot;tests&amp;quot; 
/ &amp;quot;unit&amp;quot;        tests_dir.mkdir(parents=True)        
monkeypatch.chdir(tmp_path)        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir))            # Collection returns 
a couple of node ids        collected = [            
&amp;quot;tests/unit/test_alpha.py::test_a&amp;quot;,            
&amp;quot;tests/unit/test_beta.py::test_b&amp;quot;,        ]            def 
fake_run(            cmd,            check=False,            
capture_output=False,            text=False,            timeout=None,           
cwd=None,            env=None,        ):  # noqa: ANN001            if 
&amp;quot;--collect-only&amp;quot; in cmd:                return 
SimpleNamespace(stdout=&amp;quot;\n&amp;quot;.join(collected), 
stderr=&amp;quot;&amp;quot;, returncode=0)            return 
SimpleNamespace(stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;, 
returncode=0)            seen_cmds: list[list] = []            class FakePopen: 
def __init__(                self, cmd, stdout=None, stderr=None, text=True, 
env=None            ):  # noqa: ANN001                seen_cmds.append(cmd)     
self.returncode = 0                def communicate(self):  # noqa: D401         
return (&amp;quot;&amp;quot;, &amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)        
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopen)        
# Act        ok, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            speed_categories=[            
&amp;quot;fast&amp;quot;            ],  # go through segmented-speed path 
without segmentation            verbose=False,            report=True,          
parallel=False,            segment=False,            segment_size=50,           
maxfail=None,            extra_marker=None,        )            # Assert        
assert ok is True&amp;gt;       assert output == &amp;quot;&amp;quot;E       
AssertionError: assert &amp;#x27;\n[knowledge...verage.json\n&amp;#x27; == 
&amp;#x27;&amp;#x27;E         E         + E         +  coverage ingestion 
skipped: Coverage JSON missing at 
test_reports/coverage.json/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/testing/test_run_tests_report.py:78: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:11,119 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:11,119 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing2025-10-28 09:29:11,119 - devsynth.testing.run_tests - WARNING 
- Skipping release graph publication: Coverage JSON missing at 
test_reports/coverage.json------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact generation 
skipped: data file missingWARNING  
devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json_____________ 
test_single_pass_non_keyword_returncode_5_is_success _____________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14545f3e0&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_single_pass_non_keyword_returncode_5_is_success(monkeypatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;ReqID: TR-RT-10  Return code 5 is 
success in single-pass non-keyword path.            In the single-pass, 
non-keyword path (no speed_categories), pytest return        code 5 (no tests 
collected) should be treated as success. This exercises        the branch where 
we do not pre-collect node ids and simply pass a category        expression to 
pytest via &amp;#x27;-m&amp;#x27;.        &amp;quot;&amp;quot;&amp;quot;        
# Force the branch: speed_categories=None, no extra_marker or keyword filter,   
# parallel=False to avoid xdist flags.            class FakePopen:            
def __init__(                self, cmd, stdout=None, stderr=None, text=False, 
env=None            ):  # noqa: ANN001                # Ensure the 
&amp;#x27;-m&amp;#x27; category expression is present and no 
&amp;#x27;-k&amp;#x27; keyword filter                assert 
&amp;quot;-m&amp;quot; in cmd, f&amp;quot;expected -m category expression in 
cmd: {cmd}&amp;quot;                assert &amp;quot;-k&amp;quot; not in cmd, 
f&amp;quot;did not expect -k in cmd: {cmd}&amp;quot;                # Simulate 
pytest exit code 5 (no tests collected)                self.returncode = 5      
def communicate(self):                return (&amp;quot;&amp;quot;, 
&amp;quot;&amp;quot;)            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=None,            verbose=False,            report=False,       
parallel=False,            segment=False,            maxfail=None,            
extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_returncode5_success.py:36: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/dev
synth/testing/run_tests.py:1337: in _collect_via_pytest    result = 
subprocess.run(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ input = None, capture_output = True, timeout = 60.0, check = 
Falsepopenargs = 
([&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/pytho
n&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;/private/var/folders/2v/lb...d23rh0000gn/T/pytest-of-caitlyn/pytest-14
28/test_collect_tests_with_cache_1/tests/unit&amp;#x27;, 
&amp;#x27;--collect-only&amp;#x27;, &amp;#x27;-q&amp;#x27;, ...],)kwargs = 
{&amp;#x27;cwd&amp;#x27;: 
&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth&amp;#x27;, 
&amp;#x27;env&amp;#x27;: {&amp;#x27;BRAVE_SEARCH_API_KEY&amp;#x27;: 
&amp;#x27;BSANtaq4PsTJtfCuz8MtV...5.14.1-darwin-arm64/bundled/libs/debugpy&amp;#
x27;, &amp;#x27;CLICOLOR&amp;#x27;: &amp;#x27;1&amp;#x27;, 
&amp;#x27;COLORFGBG&amp;#x27;: &amp;#x27;15;0&amp;#x27;, ...}, 
&amp;#x27;stderr&amp;#x27;: -1, &amp;#x27;stdout&amp;#x27;: -1, ...}    def 
run(*popenargs,            input=None, capture_output=False, timeout=None, 
check=False, **kwargs):        &amp;quot;&amp;quot;&amp;quot;Run command with 
arguments and return a CompletedProcess instance.            The returned 
instance will have attributes args, returncode, stdout and        stderr. By 
default, stdout and stderr are not captured, and those attributes        will be
None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,        or 
pass capture_output=True to capture both.            If check is True and the 
exit code was non-zero, it raises a        CalledProcessError. The 
CalledProcessError object will have the return code        in the returncode 
attribute, and output &amp;amp; stderr attributes if those streams        were 
captured.            If timeout (seconds) is given and the process takes too 
long,         a TimeoutExpired exception will be raised.            There is an 
optional argument &amp;quot;input&amp;quot;, allowing you to        pass bytes 
or a string to the subprocess&amp;#x27;s stdin.  If you use this argument       
you may not also use the Popen constructor&amp;#x27;s &amp;quot;stdin&amp;quot; 
argument, as        it will be used internally.            By default, all 
communication is in bytes, and therefore any &amp;quot;input&amp;quot; should   
be bytes, and the stdout and stderr will be bytes. If in text mode, any        
&amp;quot;input&amp;quot; should be a string, and stdout and stderr will be 
strings decoded        according to locale encoding, or by 
&amp;quot;encoding&amp;quot; if set. Text mode is        triggered by setting 
any of text, encoding, errors or universal_newlines.            The other 
arguments are the same as for the Popen constructor.        
&amp;quot;&amp;quot;&amp;quot;        if input is not None:            if 
kwargs.get(&amp;#x27;stdin&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdin and input arguments may not both be used.&amp;#x27;) 
kwargs[&amp;#x27;stdin&amp;#x27;] = PIPE            if capture_output:          
if kwargs.get(&amp;#x27;stdout&amp;#x27;) is not None or 
kwargs.get(&amp;#x27;stderr&amp;#x27;) is not None:                raise 
ValueError(&amp;#x27;stdout and stderr arguments may not be used &amp;#x27;     
&amp;#x27;with capture_output.&amp;#x27;)            
kwargs[&amp;#x27;stdout&amp;#x27;] = PIPE            
kwargs[&amp;#x27;stderr&amp;#x27;] = PIPE    &amp;gt;       with 
Popen(*popenargs, **kwargs) as process:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^E
TypeError: 
test_single_pass_non_keyword_returncode_5_is_success.&amp;lt;locals&amp;gt;.Fake
Popen.__init__() got an unexpected keyword argument 
&amp;#x27;cwd&amp;#x27;/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Pytho
n.framework/Versions/3.12/lib/python3.12/subprocess.py:548: 
TypeError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:11,132 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest------------------------------ 
Captured log call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytest_______ 
test_segmented_batches_surface_plugin_fallbacks_and_failure_tips 
_______monkeypatch = &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 
0x1451507d0&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_batches_surface0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x145044680&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_segmented_batches_surface_plugin_fallbacks_and_failure_tips(        
monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-SEGMENTATION-1  Segmented 
failures emit rich diagnostics.            This test simulates a segmented 
execution where the first batch fails with a        coverage gate error. It 
verifies that fallback plugin injection occurs for the        subprocess 
environment, coverage warnings propagate to stdout/stderr, and the        
aggregated failure guidance from :func:`_failure_tips` is appended exactly once.
&amp;quot;&amp;quot;&amp;quot;            caplog.set_level(logging.INFO)        
tests_dir = tmp_path / &amp;quot;segmented&amp;quot;        tests_dir.mkdir()   
test_one = tests_dir / &amp;quot;test_one.py&amp;quot;        test_two = 
tests_dir / &amp;quot;test_two.py&amp;quot;        
test_one.write_text(&amp;quot;def test_one():\n    assert True\n&amp;quot;)     
test_two.write_text(&amp;quot;def test_two():\n    assert True\n&amp;quot;)     
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))        monkeypatch.setattr(rt, 
&amp;quot;COLLECTION_CACHE_DIR&amp;quot;, str(tmp_path / 
&amp;quot;cache&amp;quot;))            # Avoid mutating real coverage artifacts 
while exercising segmentation logic.        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
coverage_calls: list = []        monkeypatch.setattr(            rt, 
&amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
coverage_calls.append(&amp;quot;ensured&amp;quot;)        )            
ensure_calls: list[tuple] = []            def fake_cov(env: dict) -&amp;gt; 
bool:            ensure_calls.append((&amp;quot;cov&amp;quot;, env is 
os.environ, env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;))) 
if env is os.environ:                # Simulate a no-op at the process level so 
the subprocess copy applies the fix.                return False            
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = (                
env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;) + &amp;quot; 
-p pytest_cov&amp;quot;            ).strip()            return True            
def fake_bdd(env: dict) -&amp;gt; bool:            
ensure_calls.append((&amp;quot;bdd&amp;quot;, env is os.environ, 
env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;)))            
if env is os.environ:                return False            
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = (                
env.get(&amp;quot;PYTEST_ADDOPTS&amp;quot;, &amp;quot;&amp;quot;) + &amp;quot; 
-p pytest_bdd.plugin&amp;quot;            ).strip()            return True      
monkeypatch.setattr(rt, &amp;quot;ensure_pytest_cov_plugin_env&amp;quot;, 
fake_cov)        monkeypatch.setattr(rt, 
&amp;quot;ensure_pytest_bdd_plugin_env&amp;quot;, fake_bdd)            
collect_output = &amp;quot;\n&amp;quot;.join(            [                
f&amp;quot;{test_one}::test_one&amp;quot;,                
f&amp;quot;{test_two}::test_two&amp;quot;,            ]        )            def 
fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            assert 
&amp;quot;--collect-only&amp;quot; in cmd, &amp;quot;collection command 
expected&amp;quot;            return SimpleNamespace(returncode=0, 
stdout=collect_output, stderr=&amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)           
batch_plan = iter(            [                {                    
&amp;quot;returncode&amp;quot;: 1,                    
&amp;quot;stdout&amp;quot;: &amp;quot;batch-one\n&amp;quot;,                    
&amp;quot;stderr&amp;quot;: &amp;quot;FAIL Required test coverage of 90% not 
reached.\n&amp;quot;,                },                {                    
&amp;quot;returncode&amp;quot;: 0,                    
&amp;quot;stdout&amp;quot;: &amp;quot;batch-two\n&amp;quot;,                    
&amp;quot;stderr&amp;quot;: &amp;quot;&amp;quot;,                },            ]
)        popen_calls: list[dict] = []            class FakePopen:            def
__init__(                self, cmd, stdout=None, stderr=None, text=False, 
env=None            ):  # noqa: ANN001                
popen_calls.append({&amp;quot;cmd&amp;quot;: list(cmd), &amp;quot;env&amp;quot;:
dict(env or {})})                try:                    result = 
next(batch_plan)                except StopIteration as exc:  # pragma: no cover
- guards test integrity                    raise 
AssertionError(&amp;quot;Unexpected extra Popen invocation&amp;quot;) from exc  
self.returncode = result[&amp;quot;returncode&amp;quot;]                
self._stdout = result[&amp;quot;stdout&amp;quot;]                self._stderr = 
result[&amp;quot;stderr&amp;quot;]                def communicate(self):  # 
noqa: D401 - signature mirrors subprocess API                
&amp;quot;&amp;quot;&amp;quot;Return the stubbed stdout/stderr 
pair.&amp;quot;&amp;quot;&amp;quot;                    return self._stdout, 
self._stderr            monkeypatch.setattr(rt.subprocess, 
&amp;quot;Popen&amp;quot;, FakePopen)    &amp;gt;       success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=None,            extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_segmentation.py:131: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = collect_callable(_ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ target = 
&amp;#x27;unit-tests&amp;#x27;, speed_category = &amp;#x27;fast&amp;#x27;    def
collect_tests_with_cache(        target: str,        speed_category: str | None 
= None,        *,        keyword_filter: str | None = None,        
_allow_all_target_decomposition: bool = True,        _timeout_override: float | 
None = None,        _propagate_timeout: bool = False,    ) -&amp;gt; list:      
&amp;quot;&amp;quot;&amp;quot;Collect tests for the given target and speed 
category.            Args:            target: Logical test target such as 
``unit-tests`` or ``all-tests``.            speed_category: Optional speed 
marker used to scope collection.            keyword_filter: Optional ``-k`` 
expression applied during collection.            Returns:            A list of 
pytest node identifiers matching the requested filters.        
&amp;quot;&amp;quot;&amp;quot;        test_path = TARGET_PATHS.get(target, 
TARGET_PATHS[&amp;quot;all-tests&amp;quot;])            # Build the marker 
expression we&amp;#x27;ll use and compute a simple fingerprint of        # the 
test tree (latest mtime) to detect changes that should invalidate cache.        
marker_expr = &amp;quot;not memory_intensive&amp;quot;        category_expr = 
marker_expr        if speed_category:            category_expr = 
f&amp;quot;{speed_category} and {marker_expr}&amp;quot;            
normalized_filter = _normalize_keyword_filter(keyword_filter)        
latest_mtime = _latest_mtime(test_path)        base_cache_key = 
f&amp;quot;{target}_{speed_category or &amp;#x27;all&amp;#x27;}&amp;quot;       
suffix = _cache_key_suffix(normalized_filter)        cache_key = 
f&amp;quot;{base_cache_key}_{suffix}&amp;quot; if suffix else base_cache_key    
cache_file = (&amp;gt;           COLLECTION_CACHE_DIR / 
f&amp;quot;{cache_key}_tests.json&amp;quot;        )  # Use Path object for 
cache_fileE       TypeError: unsupported operand type(s) for /: 
&amp;#x27;str&amp;#x27; and 
&amp;#x27;str&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/
devsynth/testing/run_tests.py:1450: TypeError______________ 
test_segmented_failure_appends_aggregate_tips_once ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14513f650&amp;gt;tmp_path = 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_segmented_failure_appends0&amp;#x27;)    
@pytest.mark.fast    def 
test_segmented_failure_appends_aggregate_tips_once(monkeypatch, tmp_path):      
&amp;quot;&amp;quot;&amp;quot;        ReqID: RT-11  Aggregated troubleshooting 
tips appended once after segments.        &amp;quot;&amp;quot;&amp;quot;        
# Arrange a fake tests directory and map unit-tests to it        tests_dir = 
tmp_path / &amp;quot;tests&amp;quot; / &amp;quot;unit&amp;quot;        
tests_dir.mkdir(parents=True)        monkeypatch.chdir(tmp_path)        
monkeypatch.setitem(rt.TARGET_PATHS, &amp;quot;unit-tests&amp;quot;, 
str(tests_dir))            # Collected node ids =&amp;gt; 2 batches with size=2 
collected = [            &amp;quot;tests/unit/test_a.py::test_1&amp;quot;,      
&amp;quot;tests/unit/test_a.py::test_2&amp;quot;,            
&amp;quot;tests/unit/test_b.py::test_3&amp;quot;,            
&amp;quot;tests/unit/test_b.py::test_4&amp;quot;,        ]            def 
fake_run(            cmd,            check=False,            
capture_output=False,            text=False,            timeout=None,           
cwd=None,            env=None,        ):            if 
&amp;quot;--collect-only&amp;quot; in cmd:                return 
_DummyCompleted(stdout=&amp;quot;\n&amp;quot;.join(collected), 
stderr=&amp;quot;&amp;quot;, returncode=0)            return 
_DummyCompleted(stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;, 
returncode=0)            # First batch fails (rc=1), second succeeds (rc=0)     
dummy_popen = _DummyPopen(rc_sequence=[1, 0])            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)        
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, dummy_popen)      
# Act        ok, output = rt.run_tests(            
target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=2,            maxfail=1,            extra_marker=None,        )    
# Assert: overall not ok due to failed batch        assert ok is False        # 
The troubleshooting tips block should appear once per failed batch (1) plus     
# one aggregated block at the end =&amp;gt; total 2 occurrences.&amp;gt;       
assert output.count(&amp;quot;Troubleshooting tips:&amp;quot;) == 2E       
AssertionError: assert 1 == 2E        +  where 1 = &amp;lt;built-in method count
of str object at 0x15368b400&amp;gt;(&amp;#x27;Troubleshooting tips:&amp;#x27;)E
+    where &amp;lt;built-in method count of str object at 0x15368b400&amp;gt; = 
&amp;#x27;\nPytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m 
pytest...HTML report for context (saved under test_reports/):\n  devsynth 
run-tests --target unit-tests --speed=fast 
--report\n&amp;#x27;.count/Users/caitlyn/Projects/github.com/ravenoak/devsynth/t
ests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py:104: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:11,503 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:11,504 - 
devsynth.testing.run_tests - INFO - Running 4 tests in 2 segments of size 2 for 
target=unit-tests2025-10-28 09:29:11,504 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (2 tests)2025-10-28 09:29:11,504 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 4 tests in 2 segments of
size 2 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (2 
tests)WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing____________ 
test_segmented_aggregate_tips_command_includes_maxfail ____________monkeypatch =
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x145331190&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_segmented_aggregate_tips_command_includes_maxfail(monkeypatch) -&amp;gt; 
None:        &amp;quot;&amp;quot;&amp;quot;        ReqID: RT-11  When segmented
mode runs and any batch fails, the aggregated        troubleshooting tips are 
generated using a command that includes --maxfail        if maxfail was 
provided.        &amp;quot;&amp;quot;&amp;quot;            collected_ids = 
&amp;quot;tests/unit/sample_test.py::test_a\n&amp;quot;            def fake_run(
cmd,            check=False,            capture_output=True,            
text=True,            timeout=None,            cwd=None,            env=None,   
):  # noqa: ANN001            # collection phase returns one node id (ensures 
one batch)            return SimpleNamespace(returncode=0, stdout=collected_ids,
stderr=&amp;quot;&amp;quot;)            class FailingBatch:            def 
__init__(                self, cmd, stdout=None, stderr=None, text=False, 
env=None            ):  # noqa: ANN001                # Simulate a failing batch
self.returncode = 1                def communicate(self) -&amp;gt; tuple:       
return (&amp;quot;&amp;quot;, &amp;quot;boom&amp;quot;)            captured = {}
def fake_failure_tips(returncode, cmd):  # noqa: ANN001            # Capture the
command used to generate tips for later assertion            
captured[&amp;quot;cmd&amp;quot;] = cmd            return 
&amp;quot;\nTroubleshooting tips: ...\n&amp;quot;            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)        
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FailingBatch)     
monkeypatch.setattr(rt, &amp;quot;_failure_tips&amp;quot;, fake_failure_tips)   
success, output = run_tests(            target=&amp;quot;unit-tests&amp;quot;,  
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=5,            maxfail=2,            extra_marker=None,        )    
assert success is False        assert &amp;quot;Troubleshooting tips&amp;quot; 
in output        # Ensure --maxfail=2 was included in the aggregate cmd, not 
just batch cmd&amp;gt;       assert any(            isinstance(arg, str) and 
arg.startswith(&amp;quot;--maxfail=&amp;quot;) and 
arg.endswith(&amp;quot;2&amp;quot;)            for arg in 
captured.get(&amp;quot;cmd&amp;quot;, [])        ), f&amp;quot;--maxfail not 
propagated in aggregate cmd: {captured}&amp;quot;E       AssertionError: 
--maxfail not propagated in aggregate cmd: {&amp;#x27;cmd&amp;#x27;: 
[&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python
&amp;#x27;, &amp;#x27;-m&amp;#x27;, &amp;#x27;pytest&amp;#x27;, 
&amp;#x27;tests/unit/sample_test.py::test_a&amp;#x27;, &amp;#x27;-m&amp;#x27;, 
&amp;#x27;not memory_intensive and fast and not gui&amp;#x27;, 
&amp;#x27;--cov=src/devsynth&amp;#x27;, 
&amp;#x27;--cov-report=json:test_reports/coverage.json&amp;#x27;, 
&amp;#x27;--cov-report=html:htmlcov&amp;#x27;, &amp;#x27;--cov-append&amp;#x27;,
&amp;#x27;--maxfail&amp;#x27;, &amp;#x27;2&amp;#x27;]}E       assert FalseE     
+  where False = any(&amp;lt;generator object 
test_segmented_aggregate_tips_command_includes_maxfail.&amp;lt;locals&amp;gt;.&a
mp;lt;genexpr&amp;gt; at 
0x14716e340&amp;gt;)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/u
nit/testing/test_run_tests_segmented_aggregate_maxfail.py:68: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:11,515 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:11,515 - 
devsynth.testing.run_tests - INFO - Running 1 tests in 1 segments of size 5 for 
target=unit-tests2025-10-28 09:29:11,515 - devsynth.testing.run_tests - INFO - 
Running segment 1/1 (1 tests)2025-10-28 09:29:11,515 - 
devsynth.testing.run_tests - WARNING - Coverage artifact generation skipped: 
data file missing------------------------------ Captured log call 
-------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 segments of
size 5 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact
generation skipped: data file missing___________ 
test_run_tests_segmented_falls_back_on_empty_collection ____________monkeypatch 
= &amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14513f5f0&amp;gt;tmp_path 
= 
PosixPath(&amp;#x27;/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pyt
est-of-caitlyn/pytest-1428/test_run_tests_segmented_falls0&amp;#x27;)caplog = 
&amp;lt;_pytest.logging.LogCaptureFixture object at 0x1451f8ad0&amp;gt;    
@pytest.mark.fast    def 
test_run_tests_segmented_falls_back_on_empty_collection(        monkeypatch: 
pytest.MonkeyPatch,        tmp_path: Path,        caplog: 
pytest.LogCaptureFixture,    ) -&amp;gt; None:        
&amp;quot;&amp;quot;&amp;quot;ReqID: RUN-TESTS-SEGMENTED-5  Fallback run 
executes when no node ids exist.&amp;quot;&amp;quot;&amp;quot;            
tests_dir = tmp_path / &amp;quot;segmented-empty&amp;quot;        
tests_dir.mkdir()        monkeypatch.setitem(rt.TARGET_PATHS, 
&amp;quot;unit-tests&amp;quot;, str(tests_dir))        monkeypatch.setattr(rt, 
&amp;quot;_reset_coverage_artifacts&amp;quot;, lambda: None)        
monkeypatch.setattr(rt, &amp;quot;_ensure_coverage_artifacts&amp;quot;, lambda: 
None)        monkeypatch.setattr(rt, 
&amp;quot;ensure_pytest_cov_plugin_env&amp;quot;, lambda _env: False)        
monkeypatch.setattr(rt, &amp;quot;ensure_pytest_bdd_plugin_env&amp;quot;, lambda
_env: False)            def fake_collect(cmd, check=False, capture_output=True, 
text=True):  # noqa: ANN001            assert &amp;quot;--collect-only&amp;quot;
in cmd            return SimpleNamespace(returncode=0, 
stdout=&amp;quot;&amp;quot;, stderr=&amp;quot;&amp;quot;)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_collect)       
popen_calls: list[list] = []            class FakePopen:            def 
__init__(                self,                cmd,                stdout=None,  
stderr=None,                text=True,                env=None,            ):  #
noqa: ANN001                popen_calls.append(cmd[:])                
self.returncode = 0                self._stdout = &amp;quot;ok\n&amp;quot;      
self._stderr = &amp;quot;&amp;quot;                def communicate(self):  # 
noqa: D401 - simple stub                return self._stdout, self._stderr       
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, FakePopen)        
caplog.set_level(logging.WARNING)    &amp;gt;       success, output = 
rt.run_tests(            target=&amp;quot;unit-tests&amp;quot;,            
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=10,            maxfail=None,            extra_marker=None,        
)/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_ru
n_tests_segmented_empty_node_ids.py:55: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests    nodes = 
collect_callable(/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsyn
th/testing/run_tests.py:1549: in collect_tests_with_cache    node_ids = 
_collect_via_pytest(_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
_ _ _ _ _ _ _ _ _ _     def _collect_via_pytest(        *,        target: str,  
test_path: str,        category_expr: str,        normalized_filter: str | None,
timeout_seconds: float,    ) -&amp;gt; list:        
&amp;quot;&amp;quot;&amp;quot;Execute ``pytest --collect-only`` and return node 
identifiers.&amp;quot;&amp;quot;&amp;quot;            # Use the same Python 
executable that poetry is using        # Check for poetry&amp;#x27;s active 
python first, then fallback to virtual env python        python_exe = 
os.environ.get(&amp;quot;POETRY_ACTIVE_PYTHON&amp;quot;)        if not 
python_exe:            # Try to find the virtual environment python            
venv_python = Path.cwd() / &amp;quot;.venv&amp;quot; / &amp;quot;bin&amp;quot; /
&amp;quot;python3&amp;quot;            if venv_python.exists():                
python_exe = str(venv_python)            else:                python_exe = 
sys.executable        collect_cmd = [            python_exe,            
&amp;quot;-m&amp;quot;,            &amp;quot;pytest&amp;quot;,            
test_path,            &amp;quot;--collect-only&amp;quot;,            
&amp;quot;-q&amp;quot;,            &amp;quot;-o&amp;quot;,            
&amp;quot;addopts=&amp;quot;,            &amp;quot;-m&amp;quot;,            
category_expr,        ]            if normalized_filter:            
collect_cmd.extend([&amp;quot;-k&amp;quot;, normalized_filter])            # Get
the project root (where pyproject.toml is located)        project_root = 
Path(__file__).parent.parent.parent.parent            # Inherit the full 
environment but override specific variables for collection        env = 
os.environ.copy()        env[&amp;quot;PYTHONPATH&amp;quot;] = str(project_root 
/ &amp;quot;src&amp;quot;)        
env[&amp;quot;PYTEST_DISABLE_PLUGIN_AUTOLOAD&amp;quot;] = &amp;quot;1&amp;quot; 
env[&amp;quot;PYTEST_PLUGINS&amp;quot;] = 
&amp;quot;pytest_bdd.plugin,pytest_cov.plugin&amp;quot;        
env[&amp;quot;PYTEST_ADDOPTS&amp;quot;] = &amp;quot;-p pytest_bdd.plugin -p 
pytest_cov.plugin&amp;quot;    &amp;gt;       result = subprocess.run(          
collect_cmd,            capture_output=True,            text=True,            
timeout=timeout_seconds,            cwd=str(project_root),  # Ensure subprocess 
runs from project root            env=env,  # Inherit environment but override 
key variables        )E       TypeError: 
test_run_tests_segmented_falls_back_on_empty_collection.&amp;lt;locals&amp;gt;.f
ake_collect() got an unexpected keyword argument 
&amp;#x27;timeout&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/devsynth/
src/devsynth/testing/run_tests.py:1337: TypeError_____________ 
test_segment_batch_benchmark_warning_forces_success ______________monkeypatch = 
&amp;lt;_pytest.monkeypatch.MonkeyPatch object at 0x14513ce30&amp;gt;    
@pytest.mark.fast    
@pytest.mark.requires_resource(&amp;quot;codebase&amp;quot;)    def 
test_segment_batch_benchmark_warning_forces_success(monkeypatch) -&amp;gt; None:
&amp;quot;&amp;quot;&amp;quot;ReqID: RT-09  PytestBenchmarkWarning in stderr 
forces success for the batch.&amp;quot;&amp;quot;&amp;quot;            
collected_ids = (            
&amp;quot;tests/unit/mod_x_test.py::test_x1\n&amp;quot; 
&amp;quot;tests/unit/mod_y_test.py::test_y1\n&amp;quot;        )            def 
fake_run(            cmd,            check=False,            
capture_output=True,            text=True,            timeout=None,            
cwd=None,            env=None,        ):  # noqa: ANN001            return 
SimpleNamespace(returncode=0, stdout=collected_ids, stderr=&amp;quot;&amp;quot;)
class WarnBatch:            def __init__(                self, cmd, stdout=None,
stderr=None, text=False, env=None            ):  # noqa: ANN001                
self._stderr = &amp;quot;PytestBenchmarkWarning: calibration&amp;quot;          
self.returncode = 1  # would fail without the special-case handling             
def communicate(self) -&amp;gt; tuple:                return 
(&amp;quot;ok\n&amp;quot;, self._stderr)            
monkeypatch.setattr(rt.subprocess, &amp;quot;run&amp;quot;, fake_run)        
monkeypatch.setattr(rt.subprocess, &amp;quot;Popen&amp;quot;, WarnBatch)        
success, output = run_tests(            target=&amp;quot;unit-tests&amp;quot;,  
speed_categories=[&amp;quot;fast&amp;quot;],            verbose=False,          
report=False,            parallel=False,            segment=True,            
segment_size=1,            maxfail=None,            extra_marker=None,        ) 
&amp;gt;       assert success is TrueE       assert False is 
True/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test
_run_tests_segmented_failure_paths.py:105: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:11,630 - 
devsynth.testing.run_tests - INFO - test collection cache miss for 
target=unit-tests (fast)  collecting via pytest2025-10-28 09:29:11,631 - 
devsynth.testing.run_tests - INFO - Running 2 tests in 2 segments of size 1 for 
target=unit-tests2025-10-28 09:29:11,631 - devsynth.testing.run_tests - INFO - 
Running segment 1/2 (1 tests)2025-10-28 09:29:11,632 - 
devsynth.testing.run_tests - INFO - Running segment 2/2 (1 tests)2025-10-28 
09:29:11,632 - devsynth.testing.run_tests - WARNING - Coverage artifact 
generation skipped: data file missing------------------------------ Captured log
call -------------------------------INFO     
devsynth.testing.run_tests:logging_setup.py:615 test collection cache miss for 
target=unit-tests (fast)  collecting via pytestINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 segments of
size 1 for target=unit-testsINFO     
devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 
2/2 (1 tests)WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage 
artifact generation skipped: data file missing__________ 
test_api_health_and_metrics_startup_without_binding_ports ___________    
@pytest.mark.no_network    @pytest.mark.fast    def 
test_api_health_and_metrics_startup_without_binding_ports():        
&amp;quot;&amp;quot;&amp;quot;Metrics endpoint returns consistent counters 
without binding ports. ReqID: N/A&amp;quot;&amp;quot;&amp;quot;        
pytest.importorskip(&amp;quot;fastapi&amp;quot;)        TestClient = 
pytest.importorskip(&amp;quot;fastapi.testclient&amp;quot;).TestClient          
# Import the app from the lightweight API module that includes /health and 
/metrics        api_mod = __import__(&amp;quot;devsynth.api&amp;quot;, 
fromlist=[&amp;quot;app&amp;quot;])  # type: ignore        app = 
getattr(api_mod, &amp;quot;app&amp;quot;)            client = TestClient(app)   
r_health = client.get(&amp;quot;/health&amp;quot;)        assert 
r_health.status_code == 200        assert 
r_health.json().get(&amp;quot;status&amp;quot;) == &amp;quot;ok&amp;quot;       
# Metrics should be available and consistent        
client.get(&amp;quot;/metrics&amp;quot;)  # prime counter for /metrics        
r_metrics = client.get(&amp;quot;/metrics&amp;quot;)        assert 
r_metrics.status_code == 200        metrics_text = r_metrics.text        assert 
isinstance(metrics_text, str)    &amp;gt;       assert 
&amp;#x27;request_count_total{endpoint=&amp;quot;/health&amp;quot;} 
1.0&amp;#x27; in metrics_textE       assert 
&amp;#x27;request_count_total{endpoint=&amp;quot;/health&amp;quot;} 
1.0&amp;#x27; in &amp;#x27;# HELP python_gc_objects_collected_total Objects 
collected during gc\n# TYPE python_gc_objects_collected_total 
counte...7047e+09\ndevsynth_cli_run_tests_invocations_created{smoke=&amp;quot;fa
lse&amp;quot;,target=&amp;quot;not-a-real-target&amp;quot;} 
1.761668719181504e+09\n&amp;#x27;/Users/caitlyn/Projects/github.com/ravenoak/dev
synth/tests/integration/api/test_api_startup.py:28: 
AssertionError----------------------------- Captured stdout call 
-----------------------------2025-10-28 09:29:15,446 - httpx - INFO - HTTP 
Request: GET http://testserver/health &amp;quot;HTTP/1.1 200 
OK&amp;quot;2025-10-28 09:29:15,457 - httpx - INFO - HTTP Request: GET 
http://testserver/metrics &amp;quot;HTTP/1.1 200 OK&amp;quot;2025-10-28 
09:29:15,465 - httpx - INFO - HTTP Request: GET http://testserver/metrics 
&amp;quot;HTTP/1.1 200 OK&amp;quot;------------------------------ Captured log 
call -------------------------------INFO     httpx:_client.py:1025 HTTP Request:
GET http://testserver/health &amp;quot;HTTP/1.1 200 OK&amp;quot;INFO     
httpx:_client.py:1025 HTTP Request: GET http://testserver/metrics 
&amp;quot;HTTP/1.1 200 OK&amp;quot;INFO     httpx:_client.py:1025 HTTP Request: 
GET http://testserver/metrics &amp;quot;HTTP/1.1 200 
OK&amp;quot;=============================== warnings summary 
===============================.venv/lib/python3.12/site-packages/_pytest/config
/__init__.py:833  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/config/__init__.py:833: PytestAssertRewriteWarning: Module 
already imported so cannot be rewritten; tests.fixtures.optional_deps    
self.import_plugin(import_spec).venv/lib/python3.12/site-packages/astor/op_util.
py:92  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/astor/op_util.py:92: DeprecationWarning: ast.Num is deprecated and will 
be removed in Python 3.14; use ast.Constant instead    precedence_data = 
dict((getattr(ast, x, None), z) for x, y, z in 
op_data).venv/lib/python3.12/site-packages/vbuild/__init__.py:33  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/vbuild/__init__.py:33: DeprecationWarning: 
&amp;#x27;pkgutil.find_loader&amp;#x27; is deprecated and slated for removal in 
Python 3.14; use importlib.util.find_spec() instead    hasLess = 
bool(pkgutil.find_loader(&amp;quot;lesscpy&amp;quot;)).venv/lib/python3.12/site-
packages/vbuild/__init__.py:34  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/vbuild/__init__.py:34: DeprecationWarning: 
&amp;#x27;pkgutil.find_loader&amp;#x27; is deprecated and slated for removal in 
Python 3.14; use importlib.util.find_spec() instead    hasSass = 
bool(pkgutil.find_loader(&amp;quot;scss&amp;quot;)).venv/lib/python3.12/site-pac
kages/vbuild/__init__.py:35  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/vbuild/__init__.py:35: DeprecationWarning: 
&amp;#x27;pkgutil.find_loader&amp;#x27; is deprecated and slated for removal in 
Python 3.14; use importlib.util.find_spec() instead    hasClosure = 
bool(pkgutil.find_loader(&amp;quot;closure&amp;quot;))tests/unit/application/cod
e_analysis/test_ast_workflow_integration.py: 2 
warningstests/unit/application/code_analysis/test_self_analyzer.py: 12971 
warningstests/unit/application/code_analysis/test_transformer.py: 6 warnings  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
de_analysis/analyzer.py:270: DeprecationWarning: ast.Str is deprecated and will 
be removed in Python 3.14; use ast.Constant instead    elif hasattr(ast, 
&amp;quot;Str&amp;quot;) and isinstance(node, getattr(ast, 
&amp;quot;Str&amp;quot;)):tests/unit/application/code_analysis/test_ast_workflow
_integration.py: 2 
warningstests/unit/application/code_analysis/test_self_analyzer.py: 12971 
warningstests/unit/application/code_analysis/test_transformer.py: 6 warnings  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
de_analysis/analyzer.py:273: DeprecationWarning: ast.Num is deprecated and will 
be removed in Python 3.14; use ast.Constant instead    elif hasattr(ast, 
&amp;quot;Num&amp;quot;) and isinstance(node, getattr(ast, 
&amp;quot;Num&amp;quot;)):tests/unit/application/code_analysis/test_ast_workflow
_integration.py: 3 
warningstests/unit/application/code_analysis/test_self_analyzer.py: 11422 
warningstests/unit/application/code_analysis/test_transformer.py: 6 warnings  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
de_analysis/analyzer.py:289: DeprecationWarning: ast.NameConstant is deprecated 
and will be removed in Python 3.14; use ast.Constant instead    elif 
hasattr(ast, &amp;quot;NameConstant&amp;quot;) and 
isinstance(tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoA
nalyzer::test_cli_entry_invokes_repo_analyzertests/unit/cli/test_cli_entry.py::t
est_cli_entry_invokes_run_cli  &amp;lt;frozen runpy&amp;gt;:128: RuntimeWarning:
&amp;#x27;devsynth.cli&amp;#x27; found in sys.modules after import of package 
&amp;#x27;devsynth&amp;#x27;, but prior to execution of 
&amp;#x27;devsynth.cli&amp;#x27;; this may result in unpredictable 
behaviourtests/unit/application/code_analysis/test_self_analyzer.py: 1767 
warnings  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
de_analysis/analyzer.py:236: DeprecationWarning: ast.Str is deprecated and will 
be removed in Python 3.14; use ast.Constant instead    elif hasattr(ast, 
&amp;quot;Str&amp;quot;) and isinstance(node, getattr(ast, 
&amp;quot;Str&amp;quot;)):tests/unit/application/code_analysis/test_transformer.
py::TestUnusedImportRemover::test_remove_unused_imports_succeedstests/unit/appli
cation/code_analysis/test_transformer.py::TestUnusedImportRemover::test_remove_u
nused_imports_succeedstests/unit/application/code_analysis/test_transformer.py::
TestUnusedVariableRemover::test_remove_unused_variables_succeedstests/unit/appli
cation/code_analysis/test_transformer.py::TestUnusedVariableRemover::test_remove
_unused_variables_succeeds  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/astor/code_gen.py:599: DeprecationWarning: ast.Str is deprecated and 
will be removed in Python 3.14; use ast.Constant instead    if isinstance(value,
ast.Str):tests/unit/application/code_analysis/test_transformer.py::TestUnusedImp
ortRemover::test_remove_unused_imports_succeedstests/unit/application/code_analy
sis/test_transformer.py::TestUnusedVariableRemover::test_remove_unused_variables
_succeeds  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/astor/code_gen.py:601: DeprecationWarning: Attribute s is deprecated and
will be removed in Python 3.14; use value instead    
self.write(value.s.replace(&amp;#x27;{&amp;#x27;, 
&amp;#x27;{{&amp;#x27;).replace(&amp;#x27;}&amp;#x27;, 
&amp;#x27;}}&amp;#x27;))tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_mvu.py
:56: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled 
for removal in a future version. Use timezone-aware objects to represent 
datetimes in UTC: datetime.datetime.now(datetime.UTC).    now = 
datetime.utcnow()tests/unit/interface/test_api_endpoints.py::test_enhanced_init_
endpoint_returns_typed_error  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
pi_endpoints.py:452: RuntimeWarning: coroutine &amp;#x27;init_endpoint&amp;#x27;
was never awaited    enhanced_api.init_endpoint(request, init_request, 
token=None)  Enable tracemalloc to get traceback where the object was allocated.
See 
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more 
info.tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests
_skip_by_defaulttests/unit/providers/test_resource_gating_meta.py::test_openai_m
arked_tests_run_when_enabled  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:252: PytestDeprecationWarning: The 
configuration option &amp;quot;asyncio_default_fixture_loop_scope&amp;quot; is 
unset.  The event loop scope for asynchronous fixtures will default to the 
fixture caching scope. Future versions of pytest-asyncio will default the loop 
scope for asynchronous fixtures to function scope. Set the default fixture loop 
scope explicitly in order to avoid unexpected behavior in the future. Valid 
fixture loop scopes are: &amp;quot;function&amp;quot;, 
&amp;quot;class&amp;quot;, &amp;quot;module&amp;quot;, 
&amp;quot;package&amp;quot;, &amp;quot;session&amp;quot;      
warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))tests/
unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_speed_
marker_paritytests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test
_parametrize_speed_marker_parity  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_parse
r.py:484: DeprecationWarning: ast.Str is deprecated and will be removed in 
Python 3.14; use ast.Constant instead    elif isinstance(node, 
ast.Str):tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_par
ametrize_speed_marker_paritytests/unit/scripts/test_enhanced_test_parser_marker_
parity.py::test_parametrize_speed_marker_parity  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_parse
r.py:487: DeprecationWarning: ast.Num is deprecated and will be removed in 
Python 3.14; use ast.Constant instead    elif isinstance(node, 
ast.Num):tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_
clitests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_fea
turestests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_erro
r_for_failures  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/run_all_tests.py:24
: DeprecationWarning: scripts/run_all_tests.py is deprecated; use 
&amp;#x27;devsynth run-tests&amp;#x27; instead.    
warnings.warn(tests/unit/testing/test_run_tests_cli_invocation.py::test_run_test
s_generates_artifacts_for_normal_profiletests/unit/testing/test_run_tests_cli_in
vocation.py::test_run_tests_generates_artifacts_with_autoload_disabled  
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning:   The 
default value will be `edges=&amp;quot;edges&amp;quot; in NetworkX 3.6.    To 
make this warning go away, explicitly set the edges kwarg, e.g.:      
nx.node_link_data(G, edges=&amp;quot;links&amp;quot;) to preserve current 
behavior, or    nx.node_link_data(G, edges=&amp;quot;edges&amp;quot;) for 
forward compatibility.    warnings.warn(-- Docs: 
https://docs.pytest.org/en/stable/how-to/capture-warnings.html==================
============== tests coverage ================================______________ 
coverage: platform darwin, python 3.12.12-final-0 _______________Coverage HTML 
written to dir htmlcovCoverage JSON written to file 
test_reports/coverage.json========================= Test Categorization Summary 
==========================Test Type Distribution:  Unit Tests: 2729  Integration
Tests: 31  Behavior Tests: 0Test Speed Distribution:  Fast Tests (&amp;lt; 1s): 
2738  Medium Tests (1-5s): 11  Slow Tests (&amp;gt; 5s): 
11============================= Top 10 Slowest Tests 
=============================1. 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers: 53.32s2. 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range: 21.38s3. 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling: 19.85s4. 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens: 17.89s5. 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling: 13.63s6. 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error: 12.00s7. 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests: 7.99s8. 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly: 6.87s9. 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error: 5.54s10. 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling: 5.10s=========================== short test summary info 
============================FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_defaultFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_defaultFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailableFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_defaultFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_errorFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_errorFAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enumFAILED 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
importFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_fileFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidanceFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missingFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_covFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errorsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidanceFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzeroFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runnerFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typerFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_previewFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runnerFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifactsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_covFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoloadFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_successFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missingFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errorsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifactsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_pluginFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabledFAILED
 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_underFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collectionFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfullyFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_oneFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_messageFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errorsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missingFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tipsFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjectionFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregateFAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregateFAILED 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_envFAILED 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unsetFAILED 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providersFAILED 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_messageFAILED 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_messageFAILED 
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycleFAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_optionsFAIL
ED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
entFAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabledFAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missingFAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environmentFAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runnerFAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagatedFAILED 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallbackFAILED 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summaryFAILED 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflictsFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaultsFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_configFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initializationFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifestFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limitFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularityFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefitFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limitFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metricsFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hookFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooksFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hookFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooksFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hookFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooksFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_overrideFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_thresholdFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_intFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_thresholdFAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recoveryFAILED 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flagFAILED 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_successFAILED 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failureFAILED 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_thresholdFAILED 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progressionFAILED 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_errorFAILED 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_keyFAILED 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_modelsFAILED 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failureFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_errorFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_configFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_configFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_methodFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_methodFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_successFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failureFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_methodFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_propertyFAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_propertyFAILED 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministicFAILED 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tionFAILED 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
lineFAILED 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
aultFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_executionFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understandingFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarksFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_stateFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_componentsFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalenceFAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behaviorFAILED 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_recordsFAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_taskFAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancementFAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_stateFAILED 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
FAILED 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_recordFAILED 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tupleFAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phaseFAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterargumentsFAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargumentFAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_pathFAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_pathFAILED 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directoryFAILED 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creationFAILED 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculateFAILED 
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errorsFAILE
D 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registryFAILED 
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
gFAILED 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
FAILED 
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debugF
AILED 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocessFAILED 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_testsFAILED 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_envF
AILED 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
FAILED 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environmentFAILED 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ockerFAILED 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_taskFA
ILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthyFAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_userFAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_fileFAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissionsFAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_urlFAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpointFAILED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeedsFAI
LED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeedsFAILED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeedsFAILED 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeedsFAI
LED 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeedsFAILED 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memoryFAILED 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeedsFAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issueFAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_codeFAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotentFAILED 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scoresFAILED 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
dsFAILED 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeedsFAILED 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooksFAILED 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matchesFAILED 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_countsFAILED 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_roleFAILED 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primusFAILED 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeedsFAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeedsFAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeedsFAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeedsFAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeedsFAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeedsFAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeedsFAILED 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceedsFAILED 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeedsFAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceedsFAILED 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeedsFAILED 
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeedsFAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simpleFAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olutionFAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rityFAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
htedFAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tieFA
ILED tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote - 
Typ...FAILED 
tests/unit/general/test_api_health.py::test_health_endpoint_succeedsFAILED 
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeedsFAILED 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_errorFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeedsFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeedsFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeedsFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeedsFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeedsFAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeedsFAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeedsFAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failureFAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeedsFAILED 
tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled - Attri...FAILED 
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependencyFAILED 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ectedFAILED 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeedsFAILED 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeedsFAILED 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extractionFAILED 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initializationFAILED 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeedsFAILED 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success - A...FAILED 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure - A...FAILED 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeedsFAILED 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeedsFAILED 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeedsFAILED 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeedsFAILED 
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeedsFAILE
D 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeedsFAILED 
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeedsFAILED 
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeedsFAILED 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeedsFAILED 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeedsFAILED 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expectedFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeedsFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matchesFAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeedsFAILED 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_errorFAILED 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_errorFAILED 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeedsFAILED 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
edsFAILED 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeedsFAILED 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeedsFAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeedsFAILED 
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_errorFAILED 
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegatesFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributesFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cacheFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaultsFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_pathsFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_typesFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitizeFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_writeFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidanceFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channelsFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_outputFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_casesFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasksFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_writeFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hoursFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholdsFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branchFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True]FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False]FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False]FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpointsFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocationFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failureFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_errorFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_routerFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instanceFAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_runFAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_sessionFAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targetsFAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errorsFAILED 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchyFAILED 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariantsFAILED 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parentsFAILED 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_statusFAILED 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacksFAILED 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacksFAILED 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operationsFAILED 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycleFAILED 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_specFAILED 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guideFAILED 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stubFAILED 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_stateFAILED 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attributeFA
ILED 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loaderFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_errorFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceedsFAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
edsFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0]FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1]FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2]FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdownFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_contextFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_typesFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_typesFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_typesFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_typesFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_infoFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_writeFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headingsFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headingsFAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headingsFAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_onceFAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_etaFAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instanceFAILED 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidanceFAILED 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cachedFAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minuteFAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hourFAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutesFAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_statusFAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_timeFAILED 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritanceFAILED 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritanceFAILED 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritanceFAILED 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritanceFAILED 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependenciesFAILED 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_moduleFAILED 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
sesFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initializationFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeedsFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeedsFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_stepFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_nextFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_fileFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigationFAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persistFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_optionFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_errorFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_errorFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_errorFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_errorFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeedsFAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errorsFAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clockFAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completionFAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_errorFAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cacheFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidanceFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlitFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlitFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlitFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlitFAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formatsFAILED 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidanceFAILED 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritanceFAILED 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritanceFAILED 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritanceFAILED 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritanceFAILED 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependenciesFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detectionFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handlingFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrievalFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaultsFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_rangeFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokensFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handlingFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handlingFAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handlingFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_modelFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_rangeFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokensFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_setFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_headerFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handlingFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handlingFAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handlingFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_errorFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailableFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_rangeFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokensFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaultsFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_headerFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handlingFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handlingFAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handlingFAILED 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_onceFAILED 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggleFAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrixFAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_pathsFAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_pathsFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initializationFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_storesFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_storeFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_storeFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_foundFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commitFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exceptionFAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_typeFAILED 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_resultsFAILED 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failureFAILED 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_resultsFAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_completeFAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failureFAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterationsFAILED 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budgetFAILED 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phaseFAI
LED 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
FAILED 
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
FAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_completeFAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retriesFAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_calledFAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_stepFAILED 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offlineFAILED 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sortedFAILED 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentagesFAILED 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_successFAILED 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failureFAILED 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parityFAILED 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalidFAILED 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errorsF
AILED 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutationFAILED 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metricsFAILED 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_errorFAILED 
tests/unit/security/test_policy_audit.py::test_audit_detects_violationFAILED 
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_fileFAILED 
tests/unit/security/test_security_audit.py::test_run_requires_pre_deployFAILED 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validateFAILED 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ranFAILED 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_cachesFAILED 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_emptyFAILED 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_jsonFAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_changeFAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_changeFAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_changeFAILED 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_callFAILED 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refreshFAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiryFAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatchFAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_emptyFAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_idsFAILED 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_emptyFAILED 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_idsFAILED 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_namingFAILED 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflowF
AILED 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matchesFA
ILED 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizesFAILED 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeoutFAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exceptionFAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_errorFAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_markerFAILED 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_htmlFAILED 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_contextFAILED 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_successFAILED 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_cachesFAILED 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cacheFAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_pluginsFAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutatesFAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addoptsFAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flagsFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_markerFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tipsFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tipsFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_markerFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argumentFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matchesFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profileFAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabledFAILED 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailableFAILED 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_successFAILED 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_returnFAILED 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_successFAILED 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expressionFAILED 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expressionFAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_testsFAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egeneratesFAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_successFAILED 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flagsFAILED 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_messageFAILED 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dirFAILED 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_successFAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
acheFAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expiredFAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_missFA
ILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtimeFAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_markerFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rtFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filterFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_falseFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_messageFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ionFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failureFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
onFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segmentFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagationFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_allFAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categoriesFAILED 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttlFAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codesFAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudioFAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filtersFAILED 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_pathFAILED 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_returnFAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tipsFAILED 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertionsFAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_commandFAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_commandFAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_commandFAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_runFAILED 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_autoFAILED 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_autoFAILED 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeoutFAILED 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_onceFAILED 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dirFAILED 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_successFAILED 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tipsFAILED 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_onceFAILED 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfailFAILED 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collectionFAILED 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_successFAILED 
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_portsERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthroughERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environmentERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwardedERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_jsonERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_codeERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallelERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existingERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_messageERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_markerERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallelERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_runERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failuresERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_textERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runnerERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speedsERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanlyERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwardingERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_successERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_pathERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unsetERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existingERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warningERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallelERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifactsERROR 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_trueERROR 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_envERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_envERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabledERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclientERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumentedERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeedsERROR 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeedsERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeedsE
RROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
sERROR 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselectERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputsERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initializationERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structureERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patternsERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectoriesERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insightsERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_qualityERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initializationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_queryERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intentERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entitiesERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationshipsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hopsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entitiesERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversalERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversalERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initializationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_explorationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_qualityERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automataERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_queryERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initializationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_taskERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_taskERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individualERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_providerERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statisticsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initializationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_sessionERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalizationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_sessionERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insightsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvementsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_reportERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initializationERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_promptERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_promptERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directiveERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraintERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analyticsERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_promptERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unitERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integrationERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behaviorERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categoriesERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markersERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionalityERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cacheERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integrationERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_fileERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_codeERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_markerERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markersERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_resultsERROR 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeedsERRO
R 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modesERROR 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docsERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizesERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_infoERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_writeERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docsERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_typeERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_headerERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headingsERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key &amp;#x27;api&amp;#x27;-key_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-]ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
sERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expanderERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instanceERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_routerERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_errorERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_errorERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_etaERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flowERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instanceERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_routerERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_errorERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_errorERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdownERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errorsERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_informationERROR 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasksERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthroughERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: File not found: 
config.yaml-Make sure the file exists]ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Permission denied: 
secrets.env-necessary permissions]ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Invalid value: bad 
input-Please check your input]ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Missing key: 
&amp;#x27;api_key&amp;#x27;-Verify that the referenced key exists]ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Type error: wrong type-Check
that all inputs]ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exceptionERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpointsERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controlsERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidanceERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routingERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-]ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknownERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_onceERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_statusERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestionsERROR 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_htmlERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallbackERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviorsERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_etaERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instanceERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_routerERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errorsERROR 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layoutERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_routerERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpointsERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_renderERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_onceERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_outputERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasksERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selectionERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layoutERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_defaultERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choicesERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_valueERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docsERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_codeERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updatesERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instanceERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errorsERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router= 515 failed, 2172 passed, 73 skipped, 39182 warnings, 189 errors in 
281.91s (0:04:41) =Pytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_usage_hint 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_runtime_hin
t 
tests/unit/adapters/cli/test_typer_adapter.py::test_command_help_format_includes
_sections 
tests/unit/adapters/issues/test_github_adapter.py::test_fetch_github_issue 
tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_normalizes
_mapping 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_without_pa
rameters_returns_none 
tests/unit/adapters/llm/test_llm_adapter.py::test_default_factory_delegates_to_g
lobal_registry 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_uses_injected_
factory 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_emits_typed_er
ror_for_unknown_provider 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_maps_registere
d_message 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_propaga
tes_factory_rejection 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_success
tests/unit/adapters/llm/test_llm_adapter.py::test_unknown_llm_provider_error_pre
serves_cause 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_stream
_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_with_c
ontext_stream_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_chunk_response_
helper_respects_chunk_size 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_stream_chunks_y
ields_all_segments 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_mock_response_templa
te_serializes 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_round_trip_pr
eserves_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_matches_cus
tom_template 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_uses_defaul
t_when_no_template_matches 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
coerces_sequences 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
falls_back_to_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_adapter_initialises_
from_mapping 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_stream_prop
agates_generate_failure 
tests/unit/adapters/test_agent_adapter.py::test_factory_initializes_agent_with_c
onfig_payload 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_builds_consensus_p
ayload_from_solutions 
tests/unit/adapters/test_agent_adapter.py::test_process_task_without_agents_rais
es_validation_error 
tests/unit/adapters/test_agent_adapter.py::test_coerce_task_solutions_filters_in
valid_entries 
tests/unit/adapters/test_agent_adapter.py::test_import_agent_falls_back_on_error
tests/unit/adapters/test_agent_adapter.py::test_import_agent_rejects_non_class 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_caches_result
s 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_processing
_failures 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_critical_d
ecisions 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_requires_active_te
am 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_singl
e_agent_flow 
tests/unit/adapters/test_agent_adapter.py::test_agent_initialization_payload_han
dles_unknown_type 
tests/unit/adapters/test_agent_adapter.py::test_coerce_helpers_normalize_inputs 
tests/unit/adapters/test_agent_adapter.py::test_unified_agent_fallback_behaviour
tests/unit/adapters/test_agent_adapter.py::test_load_default_config_uses_yaml_lo
ader 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_uses_future_s
pecs 
tests/unit/adapters/test_agent_adapter.py::test_create_team_uses_collaborative_w
hen_memory_manager 
tests/unit/adapters/test_agent_adapter.py::test_add_agent_creates_default_team 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_multi
_agent_path 
tests/unit/adapters/test_backend_resource_gates.py::test_chromadb_adapter_import
s tests/unit/adapters/test_backend_resource_gates.py::test_kuzu_adapter_imports 
tests/unit/adapters/test_backend_resource_gates.py::test_faiss_store_imports_and
_minimal 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_transaction_commit_
and_delete 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_provider_fallback_u
ses_default_embedder 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_store_raises_after_
retries 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_search_handles_empt
y_results 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_commit_failure_mark
s_transaction 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_rollback_transactio
n_states 
tests/unit/adapters/test_fake_memory_store.py::test_fake_memory_store_store_retr
ieve_search_delete_and_txn 
tests/unit/adapters/test_fake_memory_store.py::test_fake_vector_store_similarity
_and_stats 
tests/unit/adapters/test_github_project_adapter.py::test_payload_serialization 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_request_payload
_and_helpers 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_creates_colu
mns_and_cards 
tests/unit/adapters/test_github_project_adapter.py::test_fetch_and_mutations_wit
h_stub_client 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_missing_data_ra
ises 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_skips_existi
ng_items 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_raises_on_gr
aphql_errors 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_error_formattin
g_handles_missing_messages 
tests/unit/adapters/test_jira_adapter.py::test_create_issue_payload_serializatio
n tests/unit/adapters/test_jira_adapter.py::test_transition_issue_missing_status
tests/unit/adapters/test_jira_adapter.py::test_create_issue_http_error_surfaced 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_i
nit_creates_empty_adapter 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_l
oad_model_sets_session 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_without_loaded_model_raises_error 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_with_loaded_model_calls_session_run 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_multiple_outputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_empty_inputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_propagates_onnx_exceptions 
tests/unit/adapters/test_provider_safe_defaults.py::test_default_safe_falls_back
_to_stub_without_keys_and_lmstudio 
tests/unit/adapters/test_provider_safe_defaults.py::test_openai_explicit_without
_key_raises 
tests/unit/adapters/test_provider_safe_defaults.py::test_anthropic_implicit_with
out_key_falls_back_safe_default_stub 
tests/unit/adapters/test_provider_safe_defaults.py::test_lmstudio_not_attempted_
without_availability_flag 
tests/unit/adapters/test_provider_safe_defaults.py::test_disable_providers_retur
ns_null 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_complete_and_embed
_are_deterministic 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_async_matches_sync
tests/unit/adapters/test_provider_stub.py::test_provider_system_reload_preserves
_settings_import 
tests/unit/adapters/test_provider_system.py::test_embed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_embed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_null_provider_complete_raises_
error 
tests/unit/adapters/test_provider_system.py::test_null_provider_acomplete_raises
_error 
tests/unit/adapters/test_provider_system.py::test_null_provider_embed_raises_err
or 
tests/unit/adapters/test_provider_system.py::test_null_provider_aembed_raises_er
ror 
tests/unit/adapters/test_provider_system.py::test_null_provider_initialization 
tests/unit/adapters/test_provider_system.py::test_provider_factory_create_provid
er_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_base_provider_methods_succeeds
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[OpenAIProvider-config0] 
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[LMStudioProvider-config1] 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_initializati
on_skips_health_check_when_network_guard_active 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_load_env_file_populates_config
tests/unit/adapters/test_provider_system.py::test_create_tls_config_has_expected
tests/unit/adapters/test_provider_system.py::test_get_env_or_default_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_config_has_expect
ed 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_has_e
xpected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_error
_raises_error 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_retry
_has_expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_acomplete_has_
expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_embed_has_expe
cted 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_complete_has
_expected 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_async_method
s_has_expected 
tests/unit/adapters/test_provider_system.py::test_provider_with_empty_inputs_has
_expected 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_selects_provider 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_survives_missing_settings 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_respects_ord
er 
tests/unit/adapters/test_provider_system.py::test_openai_provider_retries_after_
transient_failure 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_circuit_brea
ker_blocks_after_failure 
tests/unit/adapters/test_provider_system.py::test_complete_falls_back_to_next_pr
ovider 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_re
spects_disable_flag 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_falls_back_to_lmstudio_when_marked_available 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_req
uires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_lmstudio_provider_r
equires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_asy
nc_requires_httpx_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_defaults
_when_settings_missing 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_uses_exp
licit_settings 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_wir
ing 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_emi
ts_metrics_on_retry 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_n
o_valid_providers 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
ync_uses_circuit_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_failure_opens_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_respects_open_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_records_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
ll_failures_surface_last_error 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
hort_circuits_after_first_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
kips_providers_with_open_breakers 
tests/unit/adapters/test_provider_system_additional.py::test_complete_failure_in
crements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_embed_wraps_unexpec
ted_error 
tests/unit/adapters/test_provider_system_additional.py::test_acomplete_failure_i
ncrements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_aembed_wraps_unexpe
cted_error 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_uses_next_provider 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_raises_after_exhaustion 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_embed_wraps_une
xpected_exceptions 
tests/unit/adapters/test_provider_system_resilience.py::test_base_provider_retry
_harness_records_jitter 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_a
sync_breaker_failure_emits_metrics 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_s
ync_breaker_failure_emits_metrics 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_skips_by_def
ault 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_runs_when_en
abled 
tests/unit/adapters/test_storage_adapter_protocol.py::test_storage_adapter_proto
col_shape 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
turns_structure 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
gistered 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_returns_structure 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_registered 
tests/unit/agents/test_multi_agent_coordinator.py::test_reach_consensus_majority
_choice 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_returns_structure 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_registered 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_returns_
structure 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_register
ed 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
existing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
missing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
empty_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
with_whitespace 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_boundary_value
s_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_error_conditio
ns_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_with_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_without_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_mixed_availability 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_template_direc
tory_path_construction 
tests/unit/agents/test_tool_sandbox.py::test_file_access_restricted 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_blocked 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_allowed 
tests/unit/agents/test_tool_sandbox.py::test_sandbox_context_restores_hooks 
tests/unit/agents/test_tools.py::test_register_and_get_tool 
tests/unit/agents/test_tools.py::test_unknown_tool_returns_none 
tests/unit/agents/test_tools.py::test_export_for_openai_formats_tools 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_r
ecords_summary_and_flushes_memory 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_s
upports_primus_rotation_cycle 
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict 
tests/unit/api/test_public_api_contract.py::test_public_api_imports 
tests/unit/api/test_public_api_contract.py::test_deprecated_wrapper_emits_warnin
g 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_initializa
tion_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_process_ab
stract_method_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_create_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_update_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_get_role_p
rompt_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_error_raises_error 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_error_raises_error 
tests/unit/application/agents/test_test_agent_integration.py::test_process_scaff
olds_tests_from_context 
tests/unit/application/agents/test_validation_agent.py::test_process_affirmative
_is_valid_true 
tests/unit/application/agents/test_validation_agent.py::test_process_failure_tok
ens_set_invalid 
tests/unit/application/agents/test_validation_agent.py::test_process_neutral_tex
t_is_valid 
tests/unit/application/agents/test_validation_agent.py::test_is_valid_word_bound
ary_only 
tests/unit/application/agents/test_validation_agent.py::test_wsde_contains_agent
_and_role 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[All checks passed; no issues.-True] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[An error occurred in module A.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Exception occurred during run.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Some tests fail on CI.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Clean run; everything looks good.-True] 
tests/unit/application/agents/test_wsde_memory_integration_fast.py::test_store_a
nd_retrieve_dialectical_process 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_displays
_all_config 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_update_k
ey_value_saves_and_reports 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_list_mod
els_displays_models 
tests/unit/application/cli/commands/test_config_cmd.py::test_enable_feature_cmd_
updates_and_saves 
tests/unit/application/cli/commands/test_doctor_cmd_typed.py::test_doctor_cmd_ac
cepts_path_arguments 
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py::test_doctor_cm
d_does_not_import_streamlit_or_nicegui 
tests/unit/application/cli/commands/test_ingest_cli_command.py::test_ingest_cli_
command_uses_typed_options 
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py::test_
inspect_code_cmd_sanitizes_dynamic_output 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_preserves_alias_after_subtask_rename 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_rebinds_alias_on_multiple_description_updates 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_reports_eta_strings_when_progress_advances 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_records_failure_history_for_diagnostics 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_empty_list_returns_empty_dict 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_single_name_defaults_true 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_false_variants 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_true_variants 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
returns_mapping 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
invalid_json_returns_none 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
non_mapping_returns_none 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_allows_requests_
env_default_for_unit 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_smoke_mode_sets_
env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_feature_flag_map
ping_sets_env 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_marker_passthrou
gh 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_integration_targ
et_retains_cov_when_no_report 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_target_p
rints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_speed_pr
ints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inner_test_env_d
isables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_verbose_and_fast
_timeout_env_behavior 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_report_mode_prin
ts_report_path_message 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_target 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_speed 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_segmented_run_injects_plugins_and_emits_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate 
tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_check_requi
red_env_raises_when_missing_env 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_cmd_happy_path_with_skips 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_runs_when_not_skipped 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_run_secrets
_scan_detects_simple_pattern 
tests/unit/application/cli/commands/test_testing_cmd.py::testing_cmd 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_basic_functionality 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_shows_expected_content 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_uses_cli_bridge 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_logging_configuration 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_script_paths_checked 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_output_formatting 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_quick_actions_displayed 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_performance_achievements 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_phase_tasks_completed 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_group_cha
nges_categorizes_and_orders 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_generate_
message_includes_rationale_and_files 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_minimal_returns_text 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_simple_highlight_false_returns_str 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_standard_with_markup_returns_panel_passthrough 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_dict_and_list 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_unsupported_type_falls_back 
tests/unit/application/cli/test_command_output_formatter.py::test_format_list_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_code_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_help_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_display_does_n
ot_raise 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_defaults 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_reads_yaml 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_is_exported 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_protocol_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_aliases_listed_in_all 
tests/unit/application/cli/test_long_running_progress.py::test_update_adapts_int
erval_and_checkpoints 
tests/unit/application/cli/test_long_running_progress.py::test_status_history_tr
acks_unique_status_changes 
tests/unit/application/cli/test_long_running_progress.py::test_summary_reflects_
fake_timeline_and_sanitizes_descriptions 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_updates_r
emap_and_short_circuit 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_completio
n_rolls_up_and_freezes_summary 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_checkpoin
t_spacing_respects_minimum 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_produces_deterministic_transcript 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_tracks_history_and_alias_renames 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_remains_deterministic_after_reload 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_stays_exported 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_direct_import_succeeds 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_upd
ate_thresholds_with_deterministic_clock 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_sub
task_flow_preserves_mappings_and_progress 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_run
_with_progress_completes_after_exception 
tests/unit/application/cli/test_output.py::TestOutputType::test_output_type_valu
es 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
contains_all_types 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
values 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_in
fo 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_su
ccess 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_er
ror 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_info 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_succe
ss 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_error
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_is_concrete_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_available_at_import_time 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_protocol_exists 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_inherits_correctly 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_module_reload_preserves_base_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_import_from_module_works_after_reload 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_instantiation 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_has_expected_methods 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_deterministic_tests_can_import_base 
tests/unit/application/cli/test_requirements_commands.py::test_wizard_cmd_back_n
avigation_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_gather_requiremen
ts_cmd_yaml_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_initialize_servic
es_configures_singletons 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_handles_empty_repository 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_renders_rich_table 
tests/unit/application/cli/test_requirements_commands.py::test_create_requiremen
t_invokes_service 
tests/unit/application/cli/test_requirements_gathering.py::test_gather_cmd_loggi
ng_exc_info_succeeds 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs 
tests/unit/application/cli/test_setup_wizard_textual.py::test_textual_and_cli_pa
yloads_match 
tests/unit/application/cli/test_setup_wizard_textual.py::test_requirements_wizar
d_supports_shortcut_navigation 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_planning_cmd_re
turns_structured_plan 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_defaults_when_missing 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_handles_invalid_json 
tests/unit/application/code_analysis/test_analyzer.py::test_analyze_code_simple 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_add_docstring_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_complex_transformations_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_extract_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_remove_unused_imports_and_variables_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_identifier_no_change 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_parameter_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_variable_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_validate_syntax_is_valid 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_complexity_and_readability_metrics_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_differentiate_selects_best_option_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_expand_implementation_options_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_refine_implementation_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_retrospect_code_quality_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_initialization_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_index_files_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_detect_languages_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_categorize_file_assigns_lists 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_infer_architecture_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_identify_components_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_requirements_spec_alignment_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_generate_health_report_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_analyze_maps_dependencies_and_structure 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_initialization_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_architecture_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_unknown 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_layers_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_layer_dependencies_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_check_architecture_violations_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_code_quality_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_improvement_opportunities_succeeds 
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py::test_sel
f_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_transformer.py::TestAstTransformer::te
st_record_change_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestRedundantAssignmen
tRemover::test_remove_redundant_assignments_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestStringLiteralOptim
izer::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeStyleTransform
er::test_improve_code_style_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_code_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_file_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_directory_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_find_python_files_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestSymbolUsageCounter
::test_count_symbol_usage_succeeds 
tests/unit/application/code_analysis/test_transformer_basic.py::test_optimize_st
ring_literals_simple 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_apply_doc
string_spec_inserts_function_docstring 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_met
hod_from_function_respects_method_type 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_cla
ss_from_functions_wraps_functions 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_to_dict 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_accepts_string_payload 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_cr
eate_team_stores_in_memory 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_consensus_outcome_normalizes
_participants_and_metadata 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_peer_review_consensus_error_
embeds_serialized_outcome 
tests/unit/application/collaboration/test_memory_utils_conversion.py::test_task_
round_trip_to_memory_item 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_colla
boration_payload_protocol_support 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_messa
ge_filter_rejects_invalid_input 
tests/unit/application/collaboration/test_message_protocol.py::test_message_filt
er_invalid_timestamp_raises 
tests/unit/application/collaboration/test_peer_review_store.py::test_store_in_me
mory_persists_peer_review_record 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_returns_review_decisions 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_failure_yields_error_decision 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_wraps_consensus_error_with_serialized_outcome 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_summar
ize_voting_result_persists_summary 
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py::test_identify_conflicts_detects_opposing_opinions 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_tie 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_winner 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_consensus_result_methods 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_contradictions 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_different_approaches 
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py::tes
t_peer_review_solution_excludes_author 
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py::te
st_delegate_subtasks_assigns_best_agent 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_html_documentation_extracts_sections 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_markdown_documentation_respects_heading_levels 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_convert_docstrings_to_chunks_builds_expected_metadata 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_version_key_supports_numeric_sorting_and_literals 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_prefers_vector_results 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_falls_back_to_metadata_items 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_basic_creation 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_phase_context 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_details 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery 
tests/unit/application/edrr/test_coordinator.py::test_run_micro_cycles_stops_aft
er_threshold 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure 
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::test_enhanced_dec
ide_next_phase_respects_auto_phase 
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py::test_collect_ph
ase_metrics_uses_stubbed_helpers 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
missing_memory_manager 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flushes_
on_success 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
errors 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flush_fa
ilure_does_not_raise 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_norma
lizes_outputs 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_missi
ng_manager_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_witho
ut_support_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_stores_context 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_uses_deep_copy 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_ignores_empty 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_enforces_dependencies 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_updates_state 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_consumes_manual_override 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_requires_auto_transitions 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_returns_none_for_final_phase 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ne
xt_phase_rejects_final_phase 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error 
tests/unit/application/edrr/test_recursion_termination.py::test_micro_cycle_resp
ects_depth_bounds 
tests/unit/application/edrr/test_recursion_termination.py::test_complexity_thres
hold_triggers_termination 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_sp
rint_planning_phase_constant 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_basic 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_empty 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_partial 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_sprint_retrospective_phase_constant 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_basic 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_empty 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_none 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_partial 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_positive_in
t_handles_out_of_range 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_threshold_c
lamps_invalid_values 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_respects_config 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_returns_none_when_missing 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_micro_cycle_conf
ig_sanitizes_values 
tests/unit/application/ingestion/test_ingestion_pure.py::test_is_artifact_change
d_respects_metadata_differences 
tests/unit/application/ingestion/test_ingestion_pure.py::test_identify_improveme
nt_areas_flags_missing_manifest_information 
tests/unit/application/ingestion/test_ingestion_pure.py::test_generate_recommend
ations_reflects_project_context 
tests/unit/application/ingestion/test_phases.py::test_run_expand_phase_populates
_artifacts 
tests/unit/application/ingestion/test_phases.py::test_run_differentiate_phase_us
es_structure 
tests/unit/application/llm/test_import_without_openai.py::test_import_openai_pro
vider_without_openai_succeeds 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_in
valid_response_raises_model_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_lazy_import 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_caching 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_getattr_delegation 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_call 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_getattr 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_list_downloaded_models 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_configure_default_client 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_exists 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_has_expected_attributes 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_prefixes_with_offline 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_with_context_concatenates 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
et_embedding_is_deterministic 
tests/unit/application/llm/test_openai_env_key_mock.py::test_openai_provider_use
s_mocked_env_key_without_network 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_succ
ess_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_stre
am_yields_tokens_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_inva
lid_response_raises_model_error 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_not_selected_when_flag_false 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_selected_when_flag_true 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_offlin
e_killswitch_overrides_explicit_selection 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault 
tests/unit/application/memory/test_chromadb_store.py::test_store_and_retrieve_wi
th_fallback 
tests/unit/application/memory/test_chromadb_store_typed.py::test_search_normaliz
es_serialized_rows 
tests/unit/application/memory/test_chromadb_store_typed.py::test_fallback_retrie
ve_uses_serialization_helpers 
tests/unit/application/memory/test_circuit_breaker.py::test_circuit_breaker_open
s_after_failures 
tests/unit/application/memory/test_circuit_breaker.py::test_registry_returns_sam
e_instance 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_without_vector_extension_falls_back 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_configures_hnsw_when_enabled 
tests/unit/application/memory/test_error_logger.py::test_log_error_enforces_max_
errors 
tests/unit/application/memory/test_error_logger.py::test_log_error_accepts_neste
d_context 
tests/unit/application/memory/test_error_logger.py::test_persist_errors_respects
_toggle 
tests/unit/application/memory/test_error_logger.py::test_get_recent_errors_and_s
ummary 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_semantic_robustness_testing 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_get_learning_statistics 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_analyze_behavioral_intent 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior 
tests/unit/application/memory/test_faiss_store.py::test_store_and_retrieve_round
_trip_preserves_metadata 
tests/unit/application/memory/test_faiss_store.py::test_transaction_commit_persi
sts_changes 
tests/unit/application/memory/test_faiss_store.py::test_transaction_rollback_res
tores_snapshot 
tests/unit/application/memory/test_faiss_store.py::test_similarity_search_and_st
ats_ignore_deleted_vectors 
tests/unit/application/memory/test_fast_in_memory_components.py::test_graph_memo
ry_adapter_in_memory_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_enhanced_g
raph_memory_adapter_edrr_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_man
ager_sync_hooks_fire 
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
saction_context_commit_and_rollback 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sys
tem_adapter_in_memory_components 
tests/unit/application/memory/test_fast_in_memory_components.py::test_fallback_s
tore_falls_back_on_failure 
tests/unit/application/memory/test_fast_in_memory_components.py::test_json_file_
store_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sna
pshot_save_and_load 
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_traverse_graph_depth_and_missing_nodes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_begin_tran
saction_tracks_and_cleans_up 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_commit_tra
nsaction_persists_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_rollback_t
ransaction_discards_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_get_all_it
ems_returns_everything 
tests/unit/application/memory/test_memory_manager.py::TestRouteQuery::test_route
_query_normalizes_context_mapping 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_regist
er_and_notify_sync_hook_succeeds 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_sync_h
ook_errors_are_logged 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
disabled_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
enabled_uses_adapter_and_store 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_various_backends 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_kuzu_init
ialization_and_fallback 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_lmdb_miss
ing_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_branches_execution 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_cache_and
_transaction_workflow 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_transacti
on_wrappers_raise_without_support 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_round_trip_preserves_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_from_row_handles_stringified_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_build
_memory_record_coerces_legacy_mapping 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_analyze_and_segment_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_multi_hop_reasoning 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_hybrid_llm_processing 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_contextual_prompts 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_integrate_and_validate_results 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_get_system_status 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_benchmark_against_research 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_validate_system_integrity 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_system_performance 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_memory_graph_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execution_learning_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_automata_metacognitive_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt 
tests/unit/application/memory/test_query_router.py::test_direct_query_and_vector
_branch 
tests/unit/application/memory/test_query_router.py::test_cross_store_query_group
s_results 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
tests/unit/application/memory/test_query_router.py::test_context_aware_and_route
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_returns_existing_identifier 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_generates_uuid 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_transactio
n_methods_are_noops 
tests/unit/application/memory/test_search_memory_fallback.py::test_search_memory
_fallback_without_vector_adapter_returns_results 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record 
tests/unit/application/memory/test_sync_manager_transactions.py::test_transactio
n_rollback_uses_normalized_snapshots 
tests/unit/application/memory/test_tiered_cache_termination.py::test_eviction_lo
op_terminates 
tests/unit/application/memory/test_tiered_cache_termination.py::test_preserves_t
yped_values 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_default_
provider_registration 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_optional
_provider_guard 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_edrr_coo
rdinator_delegates_to_helper 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_returns_result 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_logs_consensus_failure 
tests/unit/application/promises/test_agent_create_promise.py::test_create_promis
e_sets_metadata_and_parent_relationship 
tests/unit/application/promises/test_interface_not_implemented.py::test_promise_
interface_id_not_implemented 
tests/unit/application/promises/test_interface_pure.py::test_basic_promise_metad
ata_round_trip 
tests/unit/application/promises/test_interface_pure.py::test_then_on_fulfilled_p
romise_invokes_callback_immediately 
tests/unit/application/promises/test_interface_pure.py::test_catch_on_rejected_p
romise_yields_handler_result 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_success_rate_and_a
verage_feedback_are_computed_from_state 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_performance_score_
combines_success_and_feedback 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_round_trip_seriali
sation_preserves_variant_fields 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_reaches_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_logs_consensus_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_failure_stores_retrospect 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_receives_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_runs_on_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_non_text_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_invalid_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_assess_im
pact_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_argument_parsing_consensus_failure_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_assess_impact_recommendations_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_requirements_collects_dependencies 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_components_merges_sources 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_asse
ss_risk_level_accounts_for_priority 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_esti
mate_effort_scales_with_affected_entities 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_writes_json 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_cancelled 
tests/unit/application/requirements/test_interactions.py::test_gather_requiremen
ts_supports_backtracking 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_updat
e_requirement_uses_typed_dto_and_dialectical_hooks 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_delet
e_requirement_emits_retrospect_phase 
tests/unit/application/requirements/test_wizard.py::test_priority_and_constraint
s_persist_after_navigation 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_each_step 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_exc_info 
tests/unit/application/sprint/test_planning.py::test_map_requirements_to_plan_ex
tracts_fields 
tests/unit/application/test_documentation_fetcher.py::test_download_success_retu
rns_manifest 
tests/unit/application/test_documentation_fetcher.py::test_download_failure_retu
rns_false_manifest 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization_with_memory_port 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_nonexistent 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_info 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_clear_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_operations 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_expiration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_file_corruption 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_as_dict 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_with_docstring 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_ttl_configuration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_unicode_decode_error 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_os_error_handling 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_memory_storage_failure 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_with_extra 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_without_extra 
tests/unit/application/utils/test_extras_helper.py::test_require_optional_packag
e_wraps_importerror 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors 
tests/unit/cli/test_cli_help.py::test_cli_help_exits_zero_and_shows_summary 
tests/unit/cli/test_command_module_loading.py::test_command_modules_register_com
mands_and_build_app 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry 
tests/unit/cli/test_command_registry.py::test_enable_feature_not_top_level 
tests/unit/cli/test_completion_progress.py::test_completion_cmd_outputs_script_a
nd_progress 
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation 
tests/unit/cli/test_entry_points_help.py::test_console_scripts_declared 
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module 
tests/unit/cli/test_help_examples.py::test_get_command_help_includes_examples 
tests/unit/cli/test_help_examples.py::test_get_command_help_unknown_command 
tests/unit/cli/test_import_gating.py::test_import_devsynth_does_not_import_heavy
_optionals 
tests/unit/cli/test_import_gating.py::test_cli_entrypoint_lazy_imports 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_list 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_json 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds 
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug 
tests/unit/cli/test_mvu_commands.py::test_mvu_help_lists_subcommands 
tests/unit/cli/test_mvu_commands.py::test_mvu_init_creates_config_and_matches_sc
hema 
tests/unit/cli/test_mvuu_command_registration.py::test_mvuu_dashboard_command_re
gistered 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_generat
es_signed_telemetry 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_uses_li
ve_connectors 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_falls_b
ack_on_connector_error 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_force_l
ocal_mode 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests 
tests/unit/cli/test_version.py::test_cli_version_option_prints_version_and_exits
_zero 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env 
tests/unit/config/test_exception_handling.py::test_is_devsynth_managed_project_i
nvalid_toml_returns_false 
tests/unit/config/test_exception_handling.py::test_unified_config_exists_returns
_false_on_invalid_toml 
tests/unit/config/test_exception_handling.py::test_load_config_malformed_toml_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_load_config_invalid_values_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_set_default_memory_dir_handle
s_configuration_error 
tests/unit/config/test_feature_flag_defaults.py::test_feature_flags_default_off 
tests/unit/config/test_feature_flag_defaults.py::test_can_enable_known_feature_f
lag 
tests/unit/config/test_provider_env.py::test_parse_bool_truthy_and_falsy_cases 
tests/unit/config/test_provider_env.py::test_from_env_defaults_and_with_test_def
aults_sets_stub_and_offline 
tests/unit/config/test_provider_env.py::test_apply_to_env_respects_existing_lmst
udio_flag 
tests/unit/config/test_provider_env.py::test_as_dict_roundtrip_and_types 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_sets_e
xpected_vars 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_does_n
ot_override_explicit_lmstudio_flag 
tests/unit/config/test_provider_env_apply_and_parse.py::test_from_env_reads_curr
ent_environment 
tests/unit/config/test_provider_env_behavior.py::test_from_env_defaults_when_uns
et 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_overrid
es_to_safe_when_unset 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_respect
s_explicit_provider 
tests/unit/config/test_provider_env_behavior.py::test_apply_to_env_and_as_dict_r
oundtrip 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_parses_
true_and_false_variants 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_unrecog
nized_values_fall_back_to_defaults 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_as_dict_reflects
_values_and_with_test_defaults_sets_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_sets_offline_stub_and_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_respects_explicit_provider 
tests/unit/config/test_unified_loader.py::test_loads_from_pyproject_succeeds 
tests/unit/core/mvu/test_api.py::test_get_by_trace_id 
tests/unit/core/mvu/test_api.py::test_get_by_affected_path 
tests/unit/core/mvu/test_atomic_rewrite.py::test_cluster_commits_by_file 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_valid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_block 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_bad_traceid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_issue 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_mvuu_false 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_mvuu 
tests/unit/core/mvu/test_mvuu_schema_validation.py::test_mvuu_example_conforms_t
o_schema tests/unit/core/mvu/test_report.py::test_generate_report_markdown 
tests/unit/core/mvu/test_report.py::test_generate_report_html 
tests/unit/core/mvu/test_storage.py::test_format_mvuu_footer_contains_json 
tests/unit/core/mvu/test_storage.py::test_append_mvuu_footer_appends_block 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_accepts_vali
d 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_rejects_bad_
header 
tests/unit/core/mvu/test_validator.py::test_validate_affected_files_reports_mism
atches 
tests/unit/core/test_config_loader.py::test_core_config_normalizes_mvuu_invalid_
entries 
tests/unit/core/test_config_loader_json_types.py::test_load_config_supports_nest
ed_json_resources 
tests/unit/core/test_config_loader_json_types.py::test_environment_override_pres
erves_resources 
tests/unit/core/test_config_loader_json_types.py::test_core_config_rejects_exces
sively_deep_resources 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
tests/unit/core/test_config_loader_optional_deps.py::test_load_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_dump_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_save_global_config_han
dles_missing_yaml 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[15-True] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[16-False] 
tests/unit/core/test_config_loader_validation.py::test_load_yaml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_load_toml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_merges_source
s_without_mutating_resources 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides 
tests/unit/core/test_deterministic_fixtures.py::test_deterministic_seed_sets_env
_and_random_sequence 
tests/unit/core/test_deterministic_fixtures.py::test_mock_datetime_fixture_freez
es_time 
tests/unit/core/test_deterministic_fixtures.py::test_mock_uuid_fixture_returns_f
ixed_uuid tests/unit/core/test_mvu.py::test_schema_has_required_fields 
tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task 
tests/unit/deployment/test_deployment_scripts.py::test_bootstrap_script_exists 
tests/unit/deployment/test_deployment_scripts.py::test_health_check_script_exist
s 
tests/unit/deployment/test_enforcement.py::test_shell_scripts_enforce_non_root_a
nd_env_validation 
tests/unit/deployment/test_enforcement.py::test_docker_compose_enforces_user_and
_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint 
tests/unit/deployment/test_scripts_dir.py::test_scripts_bootstrap_exists 
tests/unit/deployment/test_scripts_dir.py::test_scripts_health_check_exists 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_noo
p_without_flag 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_rai
ses_for_root 
tests/unit/deployment/test_security_hardening.py::test_check_required_env_vars 
tests/unit/deployment/test_security_hardening.py::test_apply_secure_umask 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_invokes_he
lpers 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_raises_whe
n_env_missing 
tests/unit/devsynth/test_consensus.py::test_build_consensus_majority 
tests/unit/devsynth/test_consensus.py::test_build_consensus_no_consensus 
tests/unit/devsynth/test_consensus.py::test_build_consensus_tracks_unique_dissen
ting_options 
tests/unit/devsynth/test_consensus.py::test_build_consensus_invalid_threshold 
tests/unit/devsynth/test_consensus.py::test_build_consensus_empty_votes 
tests/unit/devsynth/test_fallback_reliability.py::test_named_condition_callbacks
_record_metrics 
tests/unit/devsynth/test_fallback_reliability.py::test_circuit_breaker_open_hook
_and_metrics 
tests/unit/devsynth/test_logger.py::test_log_exception_object_normalized 
tests/unit/devsynth/test_logger.py::test_log_true_uses_current_exception 
tests/unit/devsynth/test_logger.py::test_log_invalid_exc_info_dropped 
tests/unit/devsynth/test_metrics.py::test_memory_metrics_increment_and_reset 
tests/unit/devsynth/test_metrics.py::test_provider_and_retry_metrics 
tests/unit/devsynth/test_metrics.py::test_dashboard_metrics 
tests/unit/devsynth/test_metrics.py::test_inc_memory_unhashable_raises_type_erro
r tests/unit/devsynth/test_simple_addition.py::test_add_returns_sum 
tests/unit/devsynth/test_simple_addition.py::test_add_raises_type_error_on_non_n
umeric 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_tests_but_
not_docs 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_docs_but_n
ot_tests 
tests/unit/domain/interfaces/test_interfaces.py::test_cli_interface_raises_not_i
mplemented 
tests/unit/domain/interfaces/test_interfaces.py::test_file_analysis_result_raise
s_not_implemented 
tests/unit/domain/interfaces/test_interfaces.py::test_onnx_runtime_raises_not_im
plemented 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_none_values 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_existing_values 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_initial
ization 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_seriali
zation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_creation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_content_has
h_generation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_serializati
on_roundtrip 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_link_manage
ment 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_salience_up
date 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_lifecycle_m
anagement 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_cognitive_t
ype_properties 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_creati
on 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_serial
ization 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_defa
ult_standard 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_mono
repo 
tests/unit/domain/models/test_project.py::test_artifact_metadata_defaults_to_sep
arate_dicts 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_init
ialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_str_
representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_repr
_representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_project_m
odel_initialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_structure_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_sta
ndard_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_mon
orepo_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
act_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
acts_by_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_relat
ed_artifacts_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_artifact_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_to_dict_s
ucceeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_add_agent_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_dialectical_hook_invok
ed_on_add_solution_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_rotate_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_empty_team_
succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_agent_by_role_succ
eeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ws
de_dataclass_initialises_timestamps 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_te
am_post_init_restores_missing_attributes 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_inserts_validation 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_noop_when_already_secure 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_error_hand
ling_wraps_body 
tests/unit/domain/models/test_wsde_decision_making.py::test_calculate_idea_simil
arity_overlap 
tests/unit/domain/models/test_wsde_decision_making.py::test_evaluate_options_ran
ks_by_weighted_score 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_similar_entries 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_handles_agent_failures 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_limits_count 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_duplicates_with_strict_threshold 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_antithe
sis_returns_typed_draft 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_categorize_criti
ques_by_domain_returns_tuples 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_synthes
is_returns_resolution_plan 
tests/unit/domain/models/test_wsde_dialectical_typing.py::test_dialectical_seque
nce_round_trip 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_dialectical_tas
k_serialization_round_trip 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_dynamic_role_reassignment_selects_expert_primus_succeeds 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_categorize_crit
iques_by_domain_groups_terms 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_identify_domain
_conflicts_finds_performance_security 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_generates_synthesis 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_requires_solution 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_combines_solutions 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_requires_solutions 
tests/unit/domain/models/test_wsde_knowledge.py::test_get_task_id_uses_existing_
id 
tests/unit/domain/models/test_wsde_knowledge.py::test_identify_relevant_knowledg
e_matches_keywords 
tests/unit/domain/models/test_wsde_knowledge.py::test_knowledge_graph_insights_p
arses_payload 
tests/unit/domain/models/test_wsde_knowledge.py::test_integrate_knowledge_builds
_summary 
tests/unit/domain/models/test_wsde_knowledge.py::test_generate_improvement_sugge
stions_deduplicates_entries 
tests/unit/domain/models/test_wsde_roles_personas.py::test_enumerate_research_pe
rsonas_includes_overlays 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Synthesizer] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Contrarian] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Fact Checker] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Planner] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Moderator] 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_s
cores_requirements 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_h
ighlights_gaps 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_identifies_best_solution 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_handles_empty 
tests/unit/domain/models/test_wsde_strategies.py::test_weighted_voting_prefers_d
omain_expertise 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores 
tests/unit/domain/models/test_wsde_strategies.py::test_multidisciplinary_analysi
s_structures_results 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_add_agent_succeed
s 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_rotate_primus_suc
ceeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_empty_
team_succeeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_assign_roles_succ
eeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds 
tests/unit/domain/models/test_wsde_utils.py::test_send_message_invokes_protocol 
tests/unit/domain/models/test_wsde_utils.py::test_broadcast_message_excludes_sen
der tests/unit/domain/models/test_wsde_utils.py::test_get_messages_uses_protocol
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_creates_cy
cle 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_collects_f
eedback 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_handles_mi
ssing_peer_review 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks 
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_logs_warni
ng_on_failure 
tests/unit/domain/models/test_wsde_voting_logic.py::test_deterministic_voting_wi
th_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_determi
nistic_with_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_tie_is_
fair 
tests/unit/domain/models/test_wsde_voting_logic.py::test_handle_tied_vote_produc
es_consensus_result 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_analyzer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_transformer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_simple_file_analysis 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches 
tests/unit/domain/test_wsde_facade.py::test_summarize_consensus_result_outputs_e
xpected_sections 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_peer_review_cross_stor
e_sync_succeeds 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_mvu_helpers_cover_modu
le 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_initial_selection_prefe
rs_unused_agent_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_assign_roles_for_phase_
rotates_after_all_primus_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_first_time_selection_prior
itizes_unused_agents_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_resets_after_all_
have_served_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_when_all_agents_u
sed_resets_flags_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_by_expertise_prefers_doc
umentation_agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotation_resets_after_all_have_served_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_prefers_doc_expertise_vi
a_config_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotate_primus_resets_usage_flags_and_r
ole_map_succeeds 
tests/unit/domain/test_wsde_team.py::test_multiple_task_cycles_reset_primus_flag
s_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds tests/unit/domain/test_wsde_team.py::test_force_wsde_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds 
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie 
tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_rounds 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_primus_t
ie 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_random 
tests/unit/fallback/test_retry_counts.py::test_retry_count_metrics 
tests/unit/fallback/test_retry_counts.py::test_retry_only_network_errors 
tests/unit/fallback/test_retry_predicates.py::test_retry_predicate_triggers_retr
y 
tests/unit/fallback/test_retry_predicates.py::test_integer_predicate_records_met
rics 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_add
_agent_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_agent_type_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_team_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_missing_parameters_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_no_agents_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_type_not_found_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_execution_error_raises_error 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_type_enum_s
ucceeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_init
ialization_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_with
_parameters_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_mvp_capabilities_
succeeds 
tests/unit/general/test_agent_system.py::test_agent_state_keys_has_expected 
tests/unit/general/test_agent_system.py::test_process_input_node_success_is_vali
d 
tests/unit/general/test_agent_system.py::test_process_input_node_empty_input_suc
ceeds 
tests/unit/general/test_agent_system.py::test_process_input_node_adds_tool_list 
tests/unit/general/test_agent_system.py::test_llm_call_node_success_succeeds 
tests/unit/general/test_agent_system.py::test_llm_call_node_llm_failure_fails 
tests/unit/general/test_agent_system.py::test_llm_call_node_skip_on_prior_error_
raises_error 
tests/unit/general/test_agent_system.py::test_llm_call_node_missing_processed_in
put_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_success_is_valid
tests/unit/general/test_agent_system.py::test_parse_output_node_missing_llm_resp
onse_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_skip_on_prior_er
ror_raises_error 
tests/unit/general/test_agent_system.py::test_base_agent_graph_compiles_raises_e
rror 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error 
tests/unit/general/test_api.py::test_verify_token_rejects_invalid_token 
tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token 
tests/unit/general/test_api_health.py::test_health_endpoint_succeeds 
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_help_shows_co
mmand 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_disabled_exit
s_with_guidance 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_enabled_dry_r
un_succeeds 
tests/unit/general/test_backend_resource_flags.py::test_backend_flag_mapping_res
pects_env_vars 
tests/unit/general/test_backend_resource_flags.py::test_rdflib_env_mapping_disab
les_rdflib 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
handles_partial_spec 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error 
tests/unit/general/test_base.py::test_dummy_adapter_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_initiali
zation_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_delete_succee
ds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_persistence_s
ucceeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_exact_
match_matches_expected 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_semant
ic_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_store_and_ret
rieve_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_token_usage_s
ucceeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_lists_comm
ands_succeeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_omits_depr
ecated_aliases_succeeds 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_provider_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_file_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_co
de_analysis_implementation_succeeds 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_fi
le_analysis_implementation_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_code_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_directo
ry_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_file_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_project_structu
re_metrics_succeeds 
tests/unit/general/test_config_loader.py::test_load_yaml_config_succeeds 
tests/unit/general/test_config_loader.py::test_load_pyproject_toml_succeeds 
tests/unit/general/test_config_loader.py::test_autocomplete_succeeds 
tests/unit/general/test_config_loader.py::test_save_persists_version_succeeds 
tests/unit/general/test_config_loader.py::test_version_mismatch_logs_warning_mat
ches_expected 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_default_values_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_from_environment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_llm_set
tings_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[True-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[TRUE-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[False-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[FALSE-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_file_not_found_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_with_dotenv_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_invalid_sec
urity_boolean_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_empty_opena
i_api_key_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_defaults_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_from_env_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_embedd
ed_attribute_lookup_succeeds 
tests/unit/general/test_core_config_loader.py::test_precedence_env_over_project_
over_global_succeeds 
tests/unit/general/test_core_config_loader.py::test_load_toml_project_succeeds 
tests/unit/general/test_core_config_loader.py::test_save_global_config_yaml_succ
eeds tests/unit/general/test_core_values.py::test_load_core_values_succeeds 
tests/unit/general/test_core_values.py::test_find_value_conflicts_succeeds 
tests/unit/general/test_core_values.py::test_check_report_for_value_conflicts_su
cceeds 
tests/unit/general/test_core_workflows.py::test_filter_args_removes_none_values_
succeeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds 
tests/unit/general/test_core_workflows.py::test_gather_requirements_creates_file
_succeeds 
tests/unit/general/test_core_workflows.py::test_workflow_manager_singleton_succe
eds 
tests/unit/general/test_delegate_task_disabled.py::test_delegate_task_collaborat
ion_disabled_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_c
reate_session_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_p
rocess_message_succeeds 
tests/unit/general/test_documentation_fetcher.py::test_fetcher_initialization_su
cceeds tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled 
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency 
tests/unit/general/test_dpg_flag.py::test_dpg_command_enabled 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_no_input_raises_e
rror 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_missing_
raises_error 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_success_
succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_prompt_success_su
cceeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manual_succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_custom_bridge_has
_expected 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_error_handling_ra
ises_error 
tests/unit/general/test_edrr_manifest_string.py::test_start_cycle_from_manifest_
string_succeeds 
tests/unit/general/test_exception_logging.py::test_log_exception_emits_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_message
_only_succeeds 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_error_c
ode_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_details
_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_to_dict_succeeds 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_validation_erro
r_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_configuration_e
rror_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_command_error_r
aises_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_internal_error_rai
ses_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_resource_exhausted
_error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_timeout_
error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_memory_adapter_er
ror_raises_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_agent_error_raises
_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_workflow_error_suc
ceeds 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_dialectical_reason
ing_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_error
_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_state
_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_ingestion_err
or_raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_memory_port_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_provider_port_error_
raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_agent_port_error_rai
ses_error 
tests/unit/general/test_fallback_utils.py::test_bulkhead_limits_concurrency 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_defau
lts_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_custo
m_manifest_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_dry_run_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_validate_o
nly_is_valid 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_verbose_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_forwards_a
uto_phase_flag 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_non_intera
ctive_flag_sets_env 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_env_var_en
ables_non_interactive 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_priority_u
pdates_config 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_manifest_e
rror_raises_error 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_ingestion_
error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_success_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_file_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_schema_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_validation_failed_fails 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_yaml
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_file
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_expand_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_differentiate_phase_has_
expected 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_refine_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected 
tests/unit/general/test_ingestion_edrr_integration.py::test_run_ingestion_invoke
s_edrr_phases_succeeds 
tests/unit/general/test_ingestion_type_hints.py::test_ingestion_type_hints_raise
s_error 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_update_succee
ds 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_prune_succeed
s 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_no_config_suc
ceeds 
tests/unit/general/test_inspect_config_cmd.py::test_analyze_project_structure_re
turns_directories 
tests/unit/general/test_inspect_config_cmd.py::test_compare_with_manifest_return
s_differences 
tests/unit/general/test_inspect_config_cmd.py::test_update_manifest_adds_directo
ry 
tests/unit/general/test_isolation.py::TestIsolation::test_devsynth_dir_isolation
_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_global_config_isolatio
n_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_memory_path_isolation_
succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_no_file_logging_preven
ts_directory_creation_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_path_redirection_in_te
st_environment_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_comprehensive_isolatio
n_succeeds 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_tmp_p
ath_fixture 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_netwo
rk_keyword 
tests/unit/general/test_kuzu_adapter.py::test_store_and_retrieve_vector_succeeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_succeeds 
tests/unit/general/test_kuzu_adapter.py::test_persistence_between_instances_succ
eeds 
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_without_numpy_su
cceeds 
tests/unit/general/test_kuzu_embedded_missing.py::test_ephemeral_kuzu_store_init
ialises_without_kuzu_embedded 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_creation_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_to_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_from_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_checkpoint_path_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_not_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_put_checkpoint_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
create_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
add_step_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
execute_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_save_and_get_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_list_workflows_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_registration 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_configuration_loading 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_initialization_with_defaults 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_environment_variable_handling 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_config_file_integration 
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url 
tests/unit/general/test_logger.py::test_configure_logging_creates_rotating_handl
er tests/unit/general/test_logger.py::test_dev_synth_logger_normalizes_exc_info 
tests/unit/general/test_logging_setup.py::test_log_records_include_request_conte
xt_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_passes_through_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_true_uses_current_except
ion 
tests/unit/general/test_logging_setup.py::test_extra_kwargs_and_reserved_keys_sa
fely_handled 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_idem
potent_no_duplicate_handlers 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_thre
ad_safe 
tests/unit/general/test_logging_setup_idempotent.py::test_no_file_logging_toggle
_prevents_file_handler 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_enu
m_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_ini
tialization_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_wit
h_metadata_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_ali
ases 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_typ
e_alias 
tests/unit/general/test_memory_store.py::test_memory_store_abstract_methods_succ
eeds 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_persistence_su
cceeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_token_usage_su
cceeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_add_and
_get_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_clear_c
ontext_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_get_ful
l_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_add
_and_get_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_cle
ar_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_full_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_relevant_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_per
sistence_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_tok
en_usage_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_file_bas
ed_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_in_memor
y_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_token_us
age_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_with_chromadb_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_without_vector_store_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_memory_and_vector_store_integration_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_context_manager_with_chromadb_succeeds 
tests/unit/general/test_methodology_logging.py::test_phase_timeout_logs_warning_
succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_bulk_add_agents_succeeds 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_success 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_failure 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_combines_streams 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_returns_exit_code 
tests/unit/general/test_mvu_init_cmd.py::test_mvu_init_cmd_creates_file 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure 
tests/unit/general/test_mvuu_dashboard_cli.py::test_mvuu_dashboard_help_succeeds
tests/unit/general/test_mypy_config.py::test_mypy_configuration_raises_error 
tests/unit/general/test_mypy_config.py::test_mypy_project_configuration_raises_e
rror 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_ensure_path_exists_respects_no_file_logging_succeeds 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_settings_respects_no_file_logging_succeeds 
tests/unit/general/test_onnx_port.py::test_onnx_port_load_and_run_succeeds 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds 
tests/unit/general/test_path_restrictions.py::test_configure_logging_within_proj
ect_dir_succeeds 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds 
tests/unit/general/test_primus_selection.py::test_highest_expertise_score_become
s_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_prioritizes_agents_who_have_no
t_served_as_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds 
tests/unit/general/test_primus_selection.py::test_rotation_resets_after_all_agen
ts_served_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_default_ma
nifest_returned_when_missing_returns_expected_result 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_in
itialization_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_di
rect_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_error_raises_error 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_agent_initializ
ation_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_regi
stration_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_and_fulfillment_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_unauthorized_ac
cess_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_not_
found_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_get_available_c
apabilities_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgentMixin::test_mixin_with
_custom_agent_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_initial_state_succe
eds 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_fulfilled_succ
eeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_rejected_succe
eds tests/unit/general/test_promise_system.py::TestPromise::test_catch_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_chaining_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_error_propagation_r
aises_error 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_value_stati
c_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_with_static_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_with_rejection_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_race_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_metadata_succeeds 
tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup 
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_chat_
models_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_diale
ctical_reasoning_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_impac
t_assessment_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_change_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_model_succeeds 
tests/unit/general/test_requirement_repository_interface.py::test_requirement_re
pository_interface_crud 
tests/unit/general/test_requirement_repository_port_interface.py::test_requireme
nt_repository_port_is_abstract 
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
uirement_port_methods_raise_not_implemented 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_app
rove_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_cre
ate_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_del
ete_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_rej
ect_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_upd
ate_requirement_succeeds 
tests/unit/general/test_resource_markers.py::test_is_lmstudio_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_codebase_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds 
tests/unit/general/test_resource_markers.py::test_is_resource_available_succeeds
tests/unit/general/test_resource_markers.py::test_with_resource_marker_succeeds 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds 
tests/unit/general/test_retry_failure_scenarios.py::test_named_retry_condition_a
borts_and_records_metrics 
tests/unit/general/test_retry_failure_scenarios.py::test_circuit_breaker_open_re
cords_abort_metrics 
tests/unit/general/test_speed_option.py::test_speed_option_recognized 
tests/unit/general/test_sync_manager_persistence.py::test_sync_manager_persists_
to_all_stores 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_templat
es_exist_in_temp_location_succeeds 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_can_use
_template_to_create_test_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_analyz
e_commit_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_calcul
ate_metrics_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_genera
te_metrics_report_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_get_co
mmit_history_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_main_s
ucceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_conversat
ion_tokens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_message_t
okens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_tokens_su
cceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_ensure_token_li
mit_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_fallback_tokeni
zer_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_prune_conversat
ion_succeeds 
tests/unit/general/test_unified_agent_code_prompt.py::test_process_code_task_inc
ludes_language_and_paradigm_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_yaml_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_pyproject_succe
eds 
tests/unit/general/test_unified_config_loader.py::test_save_and_exists_succeeds 
tests/unit/general/test_unified_config_loader.py::test_missing_files_succeeds 
tests/unit/general/test_unified_config_loader.py::test_version_mismatch_warning_
succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_yaml
_succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_pypr
oject_succeeds tests/unit/general/test_unit_cli_commands.py::test_cmd 
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds 
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_create_workflow_f
or_command_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_s
ucceeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_f
ailure_fails 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_h
uman_intervention_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
atus_enum_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
ep_initialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_in
itialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_wi
th_steps_succeeds 
tests/unit/general/test_wsde_dynamic_roles.py::test_assign_roles_for_phase_selec
ts_primus_by_expertise_has_expected 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_initialization_s
ucceeds 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_with_custom_valu
es_succeeds 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_wsde_team_init
ialization_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_add_agent_succ
eeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_rotate_primus_
succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_primus_suc
ceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_s
ucceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
by_expertise_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_autonomous_col
laboration_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds 
tests/unit/infrastructure/test_test_infrastructure_sanity.py::test_global_test_i
solation_sets_env_and_dirs 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_and_query_su
ccess 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_disabled_by_
flag 
tests/unit/integrations/test_autoresearch_client.py::test_query_failure_falls_ba
ck 
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_initialization 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_record_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_within_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_exceeds_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_prune_old_requests 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_multiple_clients 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_health_en
dpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_metrics_e
ndpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_init_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_gather_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_synthesiz
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_spec_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_code_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_doctor_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_edrr_cycl
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestRouter::test_router_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimitingIntegration::tes
t_rate_limiting_logic_integration 
tests/unit/interface/test_agentapi_enhanced.py::TestErrorHandling::test_error_re
sponse_structure 
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
quest_models_validation 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_answers_a
nd_defaults 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_confirm_c
hoice_coerces_booleans 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_progress_tr
acks_subtasks 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_rate_limit_
blocks_abusive_clients 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_allow
s_after_window 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_raise
s_when_exceeded 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_records_subtasks 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_normalizes_string_advances 
tests/unit/interface/test_api_endpoints.py::test_enhanced_rate_limit_state_track
s_buckets 
tests/unit/interface/test_api_endpoints.py::test_enhanced_metrics_snapshot_typed
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error 
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_sanitize_
output_succeeds 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_ini
t_with_bad_description_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_upd
ate_with_bad_inputs_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_sub
tasks_with_bad_inputs_use_fallbacks 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_ask_question_us
es_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_confirm_choice_
uses_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_prompt_fallback
_to_rich 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_noninteractive_re
turns_defaults_and_logs 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches 
tests/unit/interface/test_command_output.py::test_format_and_display_message 
tests/unit/interface/test_command_output.py::test_format_error_suggestions 
tests/unit/interface/test_command_output.py::test_list_and_structured_outputs 
tests/unit/interface/test_command_output.py::test_set_console 
tests/unit/interface/test_dpg_ui.py::test_all_buttons_trigger_callbacks_and_prog
ress tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog 
tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog_error 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_actionable_error_suggestion_str_includes_details 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_format_error_wraps_with_footer 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_defaul
t_file 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_specif
ied_file 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_invokes_strea
mlit tests/unit/interface/test_mvuu_dashboard.py::test_require_streamlit_raises 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_snaps
hot 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_witho
ut_optional_sections 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_with_overlays
_loads_telemetry 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_pointer_legacy_env 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_secret_falls_back_to
_legacy 
tests/unit/interface/test_mvuu_dashboard.py::test_resolve_telemetry_path_prefers
_legacy 
tests/unit/interface/test_nicegui_bridge.py::test_session_storage_roundtrip 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_notifies_and_re
cords 
tests/unit/interface/test_nicegui_bridge.py::test_progress_indicator_updates_and
_completes 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_falls_back_with
out_nicegui 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_update 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_complete 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_create_progress 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_main_function
_exists 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_json_yaml_with_and_without_console 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_table_fallback_and_empty_list 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_rich_renderables 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_sanitize_outp
ut_delegates_and_handles_edge_cases 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[ERROR: Disk failure-error] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Task completed successfully-success] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[INFO: FYI-info] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Routine update-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_format_messag
e_applies_status_styles 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_highl
ight_branch_emits_panel 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_witho
ut_console_raises_value_error 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_format_
message_error_styles_and_escapes_markup 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_markdow
n_branch_sanitizes_hyperlinks 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_table_b
ranch_sanitizes_script_links 
tests/unit/interface/test_output_formatter_fallbacks.py::test_table_format_falls
_back_to_text_for_nontabular_inputs 
tests/unit/interface/test_output_formatter_fallbacks.py::test_rich_format_select
s_renderables_for_data_shapes 
tests/unit/interface/test_output_formatter_fallbacks.py::test_list_of_dicts_tabl
e_renders_missing_and_complex_values 
tests/unit/interface/test_output_formatter_fallbacks.py::test_set_format_options
_and_command_output_overrides 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_mark
down_handles_nested_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_to_mark
down_handles_mixed_items 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_tabl
e_serializes_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_of_dict
s_to_table_handles_missing_keys 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_table
_and_list_preserve_sanitized_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_command_outp
ut_unknown_extension_and_highlight_panel 
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds 
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
ed_protocol 
tests/unit/interface/test_progress_helpers.py::test_subtask_snapshot_typed_struc
ture 
tests/unit/interface/test_progress_utils.py::test_progress_manager_create_get_co
mplete_and_context_manager 
tests/unit/interface/test_progress_utils.py::test_progress_manager_track_updates
_on_item_and_slice 
tests/unit/interface/test_progress_utils.py::test_progress_indicator_context_man
ager_completes 
tests/unit/interface/test_progress_utils.py::test_step_progress_sequencing_and_c
omplete 
tests/unit/interface/test_progress_utils.py::test_create_and_track_progress_help
ers_use_manager 
tests/unit/interface/test_progress_utils.py::test_progress_tracker_forced_update
_and_complete 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_prefers_di
alog_selection 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_validates_
input 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_multi_select_re
turns_checkbox_choices 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_produces_timeline_snapshot 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_merges_extended_metadata 
tests/unit/interface/test_research_telemetry.py::test_merge_extended_metadata_in
to_payload_appends_sections 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_invokes_connectors 
tests/unit/interface/test_research_telemetry.py::test_signature_roundtrip_valida
tes 
tests/unit/interface/test_research_telemetry.py::test_signature_failure_with_wro
ng_secret 
tests/unit/interface/test_textual_ux_bridge.py::test_question_and_display_intera
ctions_are_recorded 
tests/unit/interface/test_textual_ux_bridge.py::test_confirm_choice_falls_back_t
o_default 
tests/unit/interface/test_textual_ux_bridge.py::test_progress_updates_capture_ne
sted_subtasks 
tests/unit/interface/test_textual_ux_bridge.py::test_capabilities_reflect_textua
l_availability 
tests/unit/interface/test_textual_ux_bridge.py::test_require_textual_guard 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_enabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_disabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_backward_compatib
ility_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_handle_error_defa
ult_implementation 
tests/unit/interface/test_ux_bridge_coverage.py::test_progress_indicator_context
_manager 
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_fallback_i
mport tests/unit/interface/test_uxbridge_aliases.py::test_function 
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants 
tests/unit/interface/test_webui_bridge_cli_parity.py::test_webui_bridge_matches_
cli_prompt_defaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_helpers_normal
ize_mixed_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_prompt_helpers_echo_d
efaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_append
s_documentation_links 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_require_streamlit_cac
hes_and_guides 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_clamps_handle_
invalid_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_and_pr
ogress_use_formatter 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_highli
ght_falls_back_to_write 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_uses
_cached_stub 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_impo
rts_when_missing 
tests/unit/interface/test_webui_bridge_handshake.py::test_adjust_wizard_step_han
dles_invalid_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_routes_
messages_and_sanitizes 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_error_b
ranch_records_message 
tests/unit/interface/test_webui_bridge_handshake.py::test_bridge_prompt_helpers_
return_defaults 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
invalid_total_defaults_to_zero 
tests/unit/interface/test_webui_bridge_normalize.py::test_progress_indicator_rej
ects_missing_parent 
tests/unit/interface/test_webui_bridge_normalize.py::test_display_result_routes_
messages_to_streamlit 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_upda
te_paths 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations 
tests/unit/interface/test_webui_bridge_progress.py::test_require_streamlit_failu
re 
tests/unit/interface/test_webui_bridge_progress.py::test_adjust_wizard_step_edge
s 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_display_re
sult_routes_and_sanitizes 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_session_ac
cess_wrappers 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_prompt_ali
ases_and_progress 
tests/unit/interface/test_webui_bridge_progress.py::test_normalize_wizard_step_v
aried_inputs 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_require_stream
lit_raises 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_progress_indic
ator_status_transitions 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_handshake
_routes_to_streamlit 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_error_rou
te_sanitizes_output 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_respects_
sanitization_flag 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_highlight
_routes_to_info 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_success_r
outes_to_success 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_require_streamlit
_missing_dependency_surfaces_install_guidance 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_navigation
_normalization_matches_state_invariants 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_prompt_defaults_a
lign_with_uxbridge_contract 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_display_result_ch
annels_respect_output_formatter_contract 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_get_wiza
rd_manager_uses_session_state 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_session_
helpers_delegate 
tests/unit/interface/test_webui_bridge_targeted.py::test_adjust_wizard_step_inva
lid_direction_keeps_bounds 
tests/unit/interface/test_webui_bridge_targeted.py::test_normalize_wizard_step_h
andles_strings 
tests/unit/interface/test_webui_bridge_targeted.py::test_question_and_confirmati
on_defaults 
tests/unit/interface/test_webui_bridge_targeted.py::test_display_result_highligh
t_routes_to_info 
tests/unit/interface/test_webui_bridge_targeted.py::test_create_progress_cycles_
statuses 
tests/unit/interface/test_webui_bridge_targeted.py::test_session_helpers_delegat
e_to_state_access 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_requ
ires_session_state 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_nested_completion_and_sanitization 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_wizard_na
vigation_and_display_fallback 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_default_s
tatus_thresholds 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_updates_and_completion 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_nested_su
btask_lifecycle 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_display_r
esult_routes_by_type 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_get_wizar
d_manager_and_create 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute 
tests/unit/interface/test_webui_commands.py::test_cli_returns_none_when_missing 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_pass_thr
ough 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-File not found] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Permission denied] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Invalid value] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Missing key] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[&amp;lt;lambda&amp;gt;-Type error] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_generic_
exception 
tests/unit/interface/test_webui_commands.py::test_cli_uses_cli_module_when_avail
able 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_reraises
_devsynth_error 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3] 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key &amp;#x27;api&amp;#x27;-key_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-] 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: File not found: 
config.yaml-Make sure the file exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Permission denied: 
secrets.env-necessary permissions] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Invalid value: bad 
input-Please check your input] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Missing key: 
&amp;#x27;api_key&amp;#x27;-Verify that the referenced key exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[&amp;lt;lambda&amp;gt;-ERROR: Type error: wrong type-Check
that all inputs] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_normalize_ste
p_logs_warning_on_invalid_value 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_adjust_step_w
arns_on_invalid_direction 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors 
tests/unit/interface/test_webui_progress_time.py::test_update_records_time 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/interface/test_webui_rendering_module.py::test_validate_requirements_
step_requires_fields 
tests/unit/interface/test_webui_rendering_module.py::test_handle_requirements_na
vigation_cancel_clears_state 
tests/unit/interface/test_webui_rendering_module.py::test_save_requirements_clea
rs_temporary_keys 
tests/unit/interface/test_webui_rendering_progress.py::test_gather_wizard_render
s_cli_summary 
tests/unit/interface/test_webui_rendering_progress.py::test_render_progress_summ
ary_prefers_checkpoint_eta_strings 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist 
tests/unit/interface/test_webui_routing.py::test_router_uses_session_state 
tests/unit/interface/test_webui_routing.py::test_router_resets_invalid_selection
tests/unit/interface/test_webui_routing.py::test_router_handles_sidebar_exceptio
n 
tests/unit/interface/test_webui_routing.py::test_router_surfaces_page_exception 
tests/unit/interface/test_webui_routing.py::test_router_requires_pages 
tests/unit/interface/test_webui_routing.py::test_router_honors_explicit_default 
tests/unit/interface/test_webui_routing.py::test_router_reports_missing_page_han
dler 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_main_block_succeeds 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_simulatio
n_sanitises_nested_tasks 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_require_s
treamlit_guidance 
tests/unit/interface/test_webui_state_errors.py::test_clear_reraises_after_loggi
ng 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_require_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_progre
ss_indicator_nested_lifecycle_and_statuses 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[0-0-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[10-100-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[25-100-Processing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[50-100-Halfway there...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[80-100-Almost done...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[99-100-Finalizing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[100-100-Complete] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_wizard_helpe
rs_clamp_malformed_inputs 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_valid_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_default_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_auto_model_selection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_custom_port 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_lmstudio_unavailable 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_precedence 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_counting_integration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_limit_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_c
ircuit_breaker_initialization 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_r
etry_logic_configuration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_valid_config 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_environment_variable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_without_api_key_raises_error 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_custom_base_url 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_openai_client_unavailable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_with_defaults 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_precedence 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_counting_integration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_limit_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_circu
it_breaker_initialization 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_logic_configuration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_metrics_
collection_setup 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_telemetr
y_emission 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_message_args_
and_mappings 
tests/unit/logging/test_logging_setup.py::test_redact_filter_property_loop_prese
rves_inputs 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_attaches_c
ontext 
tests/unit/logging/test_logging_setup.py::test_json_formatter_serializes_request
_context 
tests/unit/logging/test_logging_setup.py::test_redaction_in_message_and_payload 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_injects_fi
elds_and_clears 
tests/unit/logging/test_logging_setup.py::test_jsonformatter_includes_exception_
block 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_no_file_l
ogging 
tests/unit/logging/test_logging_setup.py::test_get_log_dir_and_file_use_env_over
rides 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_uses_project_dir_f
or_relative_path 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_under_te
st_project_dir 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_absolute
_outside_home 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_project_d
ir_when_file_logging_disabled 
tests/unit/logging/test_logging_setup.py::test_configure_logging_redirects_home_
and_disables_file_handler 
tests/unit/logging/test_logging_setup.py::test_short_secret_not_redacted 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_merges_and_fi
lters_kwargs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_table_normali
zation 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_does_not_muta
te_extra_inputs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_normalizes_tr
uthy_exc_info 
tests/unit/logging/test_logging_setup.py::test_configure_logging_console_only_us
es_caplog 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_secret_tokens
_via_caplog 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_handles_missing_
log_file_path 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_emits_structured
_extras_with_context 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_secrets_f
ilter_masks_values 
tests/unit/logging/test_logging_setup_additional_paths.py::test_json_formatter_i
ncludes_context_and_extras 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_respects_project_dir 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_skips_creation_when_disabled 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_warns_when_creation_fails 
tests/unit/logging/test_logging_setup_additional_paths.py::test_devsynth_logger_
filters_reserved_extra_keys 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_filter_ma
sks_args_and_payload 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_provis
ions_json_file_handler 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_consol
e_only_mode 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_idempo
tent_with_identical_configuration 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_e
xplicit_level_overrides_env 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_j
son_handler_writes_structured_output 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_r
econfigures_console_only_toggle 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_idempotent_with_identical_settings 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_preserves_filters_on_reconfigure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_falls_back_to_console_on_file_handler_failure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_create_dir_guard_preserves_console_only_mode 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_cli_context_wires_consol
e_and_json_file_handlers 
tests/unit/logging/test_logging_setup_contexts.py::test_test_context_redirects_a
nd_supports_console_only_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_create_dir_toggle_disabl
es_json_file_handler 
tests/unit/logging/test_logging_setup_contexts.py::test_console_and_json_handler
s_report_consistent_payloads 
tests/unit/logging/test_logging_setup_invariants.py::test_configure_logging_is_i
dempotent_for_handlers 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
masks_known_tokens 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
redacts_payload_and_details 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
survives_mapping_errors 
tests/unit/logging/test_logging_setup_invariants.py::test_cli_to_test_context_sw
itch_updates_log_destination 
tests/unit/logging/test_logging_setup_invariants.py::test_json_formatter_include
s_structured_extras 
tests/unit/logging/test_logging_setup_levels.py::test_configure_logging_honors_e
nv_log_level 
tests/unit/logging/test_logging_setup_levels.py::test_json_formatter_captures_re
quest_context 
tests/unit/logging/test_logging_setup_levels.py::test_dev_logger_attaches_filter
s_and_handlers 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths 
tests/unit/memory/test_issue3_regression_guard.py::test_issue3_findings_persist 
tests/unit/memory/test_layered_cache.py::test_promotes_value_to_higher_layer 
tests/unit/memory/test_layered_cache.py::test_hit_ratio_tracking 
tests/unit/memory/test_layered_cache.py::test_read_and_write_alias_methods 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_rel
oad_exposes_runtime_protocol 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_protocol_runtime_
checks_accept_custom_layers 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_pro
tocol_remains_runtime_checkable 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_missing_required_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_m
emory_store_protocol_runtime_check 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_impor
t_and_construction_succeeds 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_accep
ts_optional_backends 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_still
_requires_primary_backend 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_rejec
ts_unknown_backend_names 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_stub_store_matches
_protocol_runtime 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_memory_store_param
eters_are_runtime_typevars 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_parameterised_memo
ry_store_runtime_is_safe 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_snapshot_alias_pre
serves_runtime_origin 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_value_typevar_iden
tity_is_preserved 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_and_s
napshot_share_runtime_typevar 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_transaction_rolls_
back_typed_stores 
tests/unit/memory/test_sync_manager_transaction_failure.py::test_transaction_rol
ls_back_all_stores 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_commit_unknown_tr
ansaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_rollback_unknown_
transaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_double_commit_fai
ls_and_state_persists 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_complete
s_with_deterministic_seed 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_tr
ansitions_and_memory_integration 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_seeds_random_and_numpy_modules 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_logs_backoff_and_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_coordinator_records_each_phase 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_exits_when_total_budget_elapsed 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_accepts_dialectical_sequence_payload 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_records_unknown_phase_and_next_phase_fallbacks 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_default_path_executes 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_dia
lectical_sequence_records_with_coordinator_fallback 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_tolerates_seed_failures 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_trace_complete 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_configures_seed_providers 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_budget_precheck 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_retries_then_succeeds 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_exhaustion_sets_stop 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_copies_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_handles_dialectical_sequence 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_raises_for_non_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_halts_when_result_missing 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_matrix 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_exhausts_retry_budget_and_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_retries_clamp_sleep_to_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_stops_retry_when_total_budget_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_coordinator_records_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_records_dialectical_sequences_for_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_fallbacks_for_invalid_phase_and_next_phase 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_honors_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_seeds_random_sources 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_preserves_nonstandard_phase_without_hints 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_handles_extended_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_enforces_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retries_until_success 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_fallback_transitions_and_propagation 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_respects_max_iterations_limit 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retry_backoff_respects_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_honors_phase_and_next_phase_fields 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_clamps_retry_when_budget_consumed 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_logs_retry_exhaustion_telemetry 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_exits_when_budget_elapsed_before_iteration 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_retry_sequence_updates_phase_and_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_records_results_before_consensus_failure 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
tries_on_transient 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_emits_debug_and_clamps_sleep 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_without_budget_uses_base_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_clamps_backoff_and_respects_budget 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_remaining_budget_spent 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
cords_consensus_failure_via_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_consensus_failure_without_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_budget_already_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_invalid_next
_phase_falls_back_to_transition_map 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_missing_stat
us_relies_on_max_iterations 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_raises_for_non_mapping_results 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_handles_seed_failures_gracefully 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_logs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_import_he
lper_exposes_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_immediate_timeout_skips_apply_invocation 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_respects_total_budget_and_emits_debug 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_uses_fallback_after_invalid_phase 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_stops_after_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_seeds_random_and_numpy 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_applies_synthesis_to_task 
tests/unit/methodology/test_adhoc_adapter.py::test_should_start_cycle_true 
tests/unit/methodology/test_adhoc_adapter.py::test_should_progress_to_next_phase
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_evaluation
_terminates_with_many_hooks 
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_hooks_cont
inue_after_exception 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations 
tests/unit/methodology/test_edrr_coordinator.py::test_automate_retrospective_rev
iew_summarizes_results 
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs tests/unit/methodology/test_kanban_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_kanban_adapter.py::test_progress_respects_wip_limit 
tests/unit/methodology/test_milestone_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_milestone_adapter.py::test_progress_requires_approva
l_when_configured 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget 
tests/unit/methodology/test_sprint_adapter.py::test_calculate_phase_end_time 
tests/unit/methodology/test_sprint_adapter.py::test_is_phase_time_exceeded_false
tests/unit/methodology/test_sprint_adapter.py::test_should_progress_when_time_ex
ceeded 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase 
tests/unit/methodology/test_sprint_adapter.py::test_before_cycle_provides_contex
t 
tests/unit/methodology/test_sprint_adapter.py::test_before_expand_sets_phase_sta
rt_time 
tests/unit/methodology/test_sprint_adapter.py::test_after_retrospect_captures_sp
rint_plan 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_failure_br
anch_sets_failed 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step 
tests/unit/policies/test_verify_security_policy.py::test_passes_when_all_variabl
es_set 
tests/unit/policies/test_verify_security_policy.py::test_fails_when_variable_mis
sing 
tests/unit/providers/test_provider_contract.py::test_stub_provider_offline_defau
lts_to_stub 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_uses_
safe_provider 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_null_
provider 
tests/unit/providers/test_provider_system_additional.py::test_unknown_provider_f
alls_back 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_us
es_provider_config 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_re
spects_track_metrics_flag 
tests/unit/providers/test_provider_system_additional.py::test_stub_provider_dete
rministic_embeddings 
tests/unit/providers/test_provider_system_additional.py::test_create_tls_config_
uses_settings 
tests/unit/providers/test_provider_system_additional.py::test_provider_factory_p
refers_explicit_tls_config 
tests/unit/providers/test_provider_system_additional.py::test_fallback_async_ski
ps_open_circuit 
tests/unit/providers/test_provider_system_branches.py::test_factory_honors_disab
le_flag 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_s
tub_safe_default 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_n
ull_when_requested 
tests/unit/providers/test_provider_system_branches.py::test_explicit_openai_with
out_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_availabilit
y_guard_returns_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_fallback_fa
ilure_promotes_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_explicit_anthropic_w
ithout_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_anthropic_unsupporte
d_error 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_us
es_next_provider_on_failure 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_pr
opagates_failure_when_all_fail 
tests/unit/providers/test_provider_system_branches.py::test_fallback_disabled_tr
ies_only_first_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_initializat
ion_orders_providers_and_records_circuit_results 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_skips
_open_circuit_breaker 
tests/unit/providers/test_provider_system_branches.py::test_openai_async_retry_e
mits_telemetry 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_circu
it_breaker_recovery 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations 
tests/unit/providers/test_provider_system_branches.py::test_factory_applies_tls_
and_retry_settings 
tests/unit/providers/test_provider_system_branches.py::test_emit_retry_telemetry
_logs_and_counts 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_comp
lete_builds_payload 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_reje
cts_invalid_temperature 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_embe
d_returns_embeddings 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_co
mplete_uses_custom_messages 
tests/unit/providers/test_provider_system_branches.py::test_fallback_embed_moves
_to_next_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_aembed_reco
vers_from_failure 
tests/unit/providers/test_provider_system_branches.py::test_get_provider_config_
reads_env_file 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ope
nai_success_path 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ant
hropic_requires_key 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_unk
nown_provider_uses_null 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_lms
tudio_fallback_when_openai_missing 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_asyn
c_paths 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_rea
l_module_branches 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_as
ync_paths 
tests/unit/providers/test_provider_system_branches.py::test_complete_helper_incr
ements_metrics_and_propagates_error 
tests/unit/providers/test_provider_system_branches.py::test_embed_helper_wraps_n
on_provider_errors 
tests/unit/providers/test_provider_system_branches.py::test_acomplete_helper_inc
rements_metrics 
tests/unit/providers/test_provider_system_branches.py::test_aembed_helper_promot
es_unexpected_errors 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_components_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_requirements_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_edrr_phas
e_mapping_on_persist 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_evaluatio
n_hook_invoked_on_consensus_true 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_file_operations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_network_calls 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_global_state 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_fixture_usage 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_simple_test_file 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_file_with_isolation_marker 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_handles_syntax_errors 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_generates_recommendations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages 
tests/unit/scripts/test_analyze_test_dependencies.py::TestIntegration::test_end_
to_end_analysis 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_help 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_missing
_test_dir 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_test_execution_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_coverage_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_validation_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_shell_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_syntax_errors 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_finds_
testing_scripts 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_analyz
es_overlaps 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_git_us
age_frequency 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_genera
tes_consolidation_recommendations 
tests/unit/scripts/test_audit_testing_scripts.py::TestMarkdownGeneration::test_g
enerates_markdown_report 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_help 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_missing_scr
ipts_dir 
tests/unit/scripts/test_audit_testing_scripts.py::test_integration_audit_workflo
w 
tests/unit/scripts/test_auto_issue_comment.py::test_parse_issue_numbers_extracts
_ids 
tests/unit/scripts/test_auto_issue_comment.py::test_dry_run_when_env_missing 
tests/unit/scripts/test_auto_issue_comment.py::test_posts_comment_when_env_prese
nt 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_initialization 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_timeout 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_empty 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_with_data 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_generates_recommendations 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_help 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_invalid_
workers 
tests/unit/scripts/test_benchmark_test_execution.py::test_integration_benchmark_
workflow 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
valid_anchor 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
missing_anchor 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_missing_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_unit 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity 
tests/unit/scripts/test_examples_smoke_script.py::test_main_default_examples_suc
ceeds 
tests/unit/scripts/test_examples_smoke_script.py::test_main_reports_failure_when
_analyze_raises 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors 
tests/unit/scripts/test_gen_ref_pages.py::test_gen_ref_pages_matches_examples 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_with_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_without_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_property_test_metrics 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_calculate_overall_quality_score 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_generate_quality_recommendations 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics 
tests/unit/scripts/test_generate_quality_report.py::test_html_generation 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures 
tests/unit/scripts/test_security_ops.py::test_collect_logs_missing_directory 
tests/unit/scripts/test_security_ops.py::test_run_audit_calls_security_audit 
tests/unit/scripts/test_security_ops.py::test_list_outdated_runs_poetry 
tests/unit/scripts/test_security_ops.py::test_apply_updates_runs_poetry 
tests/unit/scripts/test_security_scan_script.py::test_main_non_strict_no_tools_r
eturns_ok 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_passes_when_above 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_fails_when_below 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_valid 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_issue 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_mvuu 
tests/unit/scripts/test_verify_release_state.py::test_draft_status_missing_tag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_without_t
ag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_with_tag 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_returns
_fields 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_without
_header 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_missing 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_present 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_when_log_mi
ssing 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_unreso
lved_questions 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_only_r
esolved 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_invali
d_json 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache_i
nvalidation 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_path_fi
lter 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_f
lags_missing_docs 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_p
asses_when_documented 
tests/unit/scripts/test_verify_test_markers_cli.py::test_argparser_includes_chan
ged_flag 
tests/unit/scripts/test_verify_test_markers_cli.py::test_verify_files_with_temp_
test 
tests/unit/scripts/test_verify_test_markers_cross_check.py::test_argparser_inclu
des_cross_check_flag 
tests/unit/scripts/test_wsde_edrr_simulation.py::test_simulation_converges 
tests/unit/security/test_api_authentication.py::test_verify_token_valid_is_valid
tests/unit/security/test_api_authentication.py::test_verify_token_invalid_is_val
id 
tests/unit/security/test_api_authentication.py::test_verify_token_missing_succee
ds 
tests/unit/security/test_api_authentication.py::test_verify_token_wrong_format_s
ucceeds 
tests/unit/security/test_api_authentication.py::test_verify_token_access_control
_disabled_succeeds 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_password_hasher_parameters_safe_defaults 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_hash_and_verify_roundtrip 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_generate_key_validates_and_encrypts 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_invalid_key_rejected 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_missing_key_env_raises 
tests/unit/security/test_authentication_optional_dependency.py::test_authenticat
ion_handles_missing_argon2 
tests/unit/security/test_authorization_checks.py::test_require_authorization_all
ows_authorized_action 
tests/unit/security/test_authorization_checks.py::test_require_authorization_rai
ses_forbidden 
tests/unit/security/test_deployment_coverage.py::test_require_non_root_user_when
_not_required 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_missing_vars 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_all_present 
tests/unit/security/test_deployment_coverage.py::test_apply_secure_umask 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_with_requir
ed_env 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_without_req
uired_env 
tests/unit/security/test_encryption.py::test_generate_key_returns_expected_resul
t 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_roundtrip_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_string_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_bytes_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_env_var_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_no_key_raises_error 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_with_env_var_succee
ds 
tests/unit/security/test_encryption.py::test_decrypt_invalid_token_raises_error 
tests/unit/security/test_encryption.py::test_decrypt_with_wrong_key_raises_error
tests/unit/security/test_logging_redaction.py::test_logging_redacts_openai_api_k
ey 
tests/unit/security/test_logging_redaction.py::test_logging_redacts_in_extra_det
ails 
tests/unit/security/test_memory_encryption.py::test_json_file_store_encryption_s
ucceeds 
tests/unit/security/test_memory_encryption.py::test_lmdb_store_encryption_succee
ds 
tests/unit/security/test_memory_encryption.py::test_tinydb_store_encryption_succ
eeds tests/unit/security/test_policy_audit.py::test_audit_detects_violation 
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file 
tests/unit/security/test_review.py::test_review_due_when_interval_elapsed 
tests/unit/security/test_review.py::test_review_not_due_before_interval 
tests/unit/security/test_review.py::test_next_review_date_calculation 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_script_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_control_ch
ars_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_both_succe
eds 
tests/unit/security/test_sanitization.py::test_sanitize_input_strips_whitespace_
succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_script_tags_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_control_chars_s
ucceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_complex_script_tag
s_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_multiple_script_ta
gs_succeeds 
tests/unit/security/test_sanitization.py::test_validate_safe_input_with_safe_inp
ut_returns_expected_result 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_s
cript_raises_error 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_c
ontrol_chars_raises_error 
tests/unit/security/test_security_audit.py::test_run_executes_checks 
tests/unit/security/test_security_audit.py::test_run_raises_on_policy_failure 
tests/unit/security/test_security_audit.py::test_report_writes_results 
tests/unit/security/test_security_audit.py::test_report_records_failure 
tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_runs_che
cks 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_respects
_skip_flags 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_register
ed 
tests/unit/security/test_security_flags_env.py::test_authentication_disabled_all
ows_any_credentials 
tests/unit/security/test_security_flags_env.py::test_authentication_enabled_enfo
rces 
tests/unit/security/test_security_flags_env.py::test_authorization_disabled_allo
ws 
tests/unit/security/test_security_flags_env.py::test_authorization_enabled_enfor
ces 
tests/unit/security/test_security_flags_env.py::test_sanitization_disabled_no_er
ror 
tests/unit/security/test_security_flags_env.py::test_sanitization_enabled_raises
tests/unit/security/test_tls_config.py::test_tls_config_timeout_env_override 
tests/unit/security/test_tls_config.py::test_tls_config_timeout_explicit_overrid
e 
tests/unit/security/test_tls_config.py::test_tls_config_validation_raises_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_partial_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_key_only_succ
eeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_cert_only_suc
ceeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_missing_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_defau
lt_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_verif
y_false_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
ca_file_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_and_key_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_only_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_ca_fi
le_precedence_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_all_p
arams_succeeds 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_valid_string_
is_valid 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[   ] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[None] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_non_string_va
lue_succeeds 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[-5--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[0-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[5-1-10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[1-1-10-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[10-1-10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[-5--10-0--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[0--10-10-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[1.5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[None] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[0-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[-5-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[5-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[0--1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[100-99] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[1-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[True-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[None-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[5-choices5] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[4-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[None-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[20-choices4] 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_option 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_with_no_path 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_run_tests_command 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_standard_cli_fallback 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_missing_run_tests_m
odule 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_cli_import_errors 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_runtime_errors 
tests/unit/test_sentinel_speed_markers.py::test_sentinel_fast_bucket_present 
tests/unit/test_simple_addition.py::test_add_returns_sum_for_integers 
tests/unit/test_simple_addition.py::test_add_accepts_floats_and_mixed_numeric_ty
pes 
tests/unit/test_simple_addition.py::test_add_raises_type_error_for_non_numeric_i
nputs 
tests/unit/test_verify_test_organization_sentinel.py::test_verify_test_organizat
ion_returns_zero 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran 
tests/unit/testing/test_collect_cache_sanitize.py::test_sanitize_node_ids_strips
_line_numbers_only_when_no_function_delimiter 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_union_
reaches_threshold_with_overlap 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_thresh
old_detection_matches_cli_expectations 
tests/unit/testing/test_deterministic_seed_fixture.py::test_deterministic_seed_f
ixture_sets_env_vars 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_bad_ttl_env_falls_back_to_
default 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_sanitize_node_ids_preserve
s_function_qualifier_and_strips_line_numbers 
tests/unit/testing/test_failure_tips.py::test_failure_tips_contains_core_guidanc
e 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_can_mutate_addition 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_mutates_addition_to_subtraction 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_cannot_mutate_non_arithmetic 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_can_mutate_equality 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_mutates_equality_to_inequality 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_ca
n_mutate_and_operation 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_mu
tates_and_to_or 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_can_
mutate_not_operation 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_muta
tes_not_by_removal 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_can_mutat
e_boolean_constant 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_t
rue_to_false 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_n
umber_to_zero_and_one 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_mutations_for_simple_code 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_handles
_syntax_errors 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_different_mutation_types 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_initializa
tion 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_killed 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_survived 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
esult_dataclass 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
eport_dataclass 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow 
tests/unit/testing/test_run_tests.py::test_sanitize_node_ids_strips_line_numbers
_without_function_delimiter 
tests/unit/testing/test_run_tests.py::test_failure_tips_contains_key_guidance_li
nes 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches 
tests/unit/testing/test_run_tests.py::test_run_tests_segment_batches 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes 
tests/unit/testing/test_run_tests_additional_coverage.py::test_failure_tips_ment
ions_core_troubleshooting_flags 
tests/unit/testing/test_run_tests_additional_coverage.py::test_ensure_pytest_cov
_plugin_env_injects_and_skips 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_missing_json 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_errors 
tests/unit/testing/test_run_tests_additional_coverage.py::test_sanitize_node_ids
_removes_line_numbers 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker 
tests/unit/testing/test_run_tests_artifacts.py::test_reset_coverage_artifacts_re
moves_stale_files 
tests/unit/testing/test_run_tests_artifacts.py::test_ensure_coverage_artifacts_g
enerates_reports 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_fails_when_pytest
_cov_missing 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_successful_single
_batch 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_h
andles_missing_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_r
ejects_invalid_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_missing_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_s
uccess 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_failure_tips_con
tains_suggestions 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_i
nject_plugins_and_emit_tips 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_failure_tips_surfac
e_cli_remediations 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_batches_fo
llow_segment_size 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_han
dles_resource_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_co
verage_totals 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_skips_placeh
older_artifacts 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_env_passthrough_an
d_coverage_lifecycle 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled 
tests/unit/testing/test_run_tests_collection_cache.py::test_sanitize_node_ids_st
rips_trailing_line_without_function_delimiter 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_reset_coverage_art
ifacts_removes_files_and_directories 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_data_missing 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_no_measured_files 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_generates_reports_and_syncs_legacy 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_html_failure_still_writes_json 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_run_tests_writes_m
anifest_with_coverage_reference 
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py::test_ensure_c
overage_artifacts_combines_fragment_files 
tests/unit/testing/test_run_tests_coverage_short_circuit.py::test_ensure_coverag
e_artifacts_short_circuits_without_measured_files 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_repor
ts_missing_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_flags
_invalid_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_totals 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_html_index 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_rejec
ts_empty_html 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_succe
ss_path 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_failure_tips_formats_
return_code_and_cmd 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_reset_coverage_artifa
cts_handles_oserror 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_ensure_coverage_artif
acts_handles_unreadable_html 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags 
tests/unit/testing/test_run_tests_keyword_exec.py::test_keyword_marker_executes_
matching_node_ids 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success 
tests/unit/testing/test_run_tests_logic.py::test_sanitize_node_ids_strips_line_n
umbers_without_function_delimiter 
tests/unit/testing/test_run_tests_logic.py::test_failure_tips_contains_key_guida
nce_lines 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_success 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_failure 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_segmented_exe
cution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_marker_expres
sion_building 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_env_defaults 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exception_han
dling 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exit_code_5_s
uccess 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_dry_run_skips
_execution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_keyword_filte
r 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_maxfail_optio
n 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_smoke_mode_pl
ugin_injection 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_s
uccess 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_f
rom_existing_cache 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_c
ollection_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_maxfail 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_custom_env 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_execution_failur
e_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_no_target_p
ath_raises_error 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories 
tests/unit/testing/test_run_tests_marker_fallback.py::test_run_tests_marker_fall
back_skips_segmentation 
tests/unit/testing/test_run_tests_marker_fallback.py::test_build_segment_metadat
a_uses_typed_sequences 
tests/unit/testing/test_run_tests_marker_merge.py::test_speed_marker_merged_with
_lmstudio_keyword_filter 
tests/unit/testing/test_run_tests_marker_merge.py::test_global_marker_with_lmstu
dio_keyword_filter 
tests/unit/testing/test_run_tests_module.py::test_sanitize_node_ids_dedup_and_st
rip_line_numbers 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return 
tests/unit/testing/test_run_tests_module.py::test_failure_tips_includes_segmenta
tion_guidance 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_missing_file 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_invalid_json 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions 
tests/unit/testing/test_run_tests_option_parsing.py::test_parse_pytest_addopts_h
andles_balanced_and_unbalanced_quotes 
tests/unit/testing/test_run_tests_option_parsing.py::test_addopts_has_plugin_det
ects_split_and_concatenated_forms 
tests/unit/testing/test_run_tests_option_parsing.py::test_coverage_plugin_disabl
ed_detects_common_overrides 
tests/unit/testing/test_run_tests_orchestration.py::test_verbose_flag_adds_v_to_
pytest_command 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run 
tests/unit/testing/test_run_tests_orchestration.py::test_pytest_addopts_are_pres
erved 
tests/unit/testing/test_run_tests_orchestration.py::test_extra_marker_adds_m_fla
g_to_command 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_honors_env_timeout 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_respects_explicit_disables 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_detects_inline_plugin_token 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[--no-cov -s-False---no-cov -s] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[-k smoke-True--k smoke -p pytest_cov] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_pytest_cov_support_
status_missing_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_run_tests_aborts_wh
en_pytest_cov_missing 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_detects_existing_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_respects_explicit_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-p no:pytest_bdd -s-False--p no:pytest_bdd 
-s] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-k feature-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_strips_tra
iling_line_numbers_without_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_keeps_ids_
with_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_deduplicat
es_preserving_order 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_single_speed 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_multiple_speeds 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_no_tests_found 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_failure_with_maxfail 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_reuses_cache_and_preserves_environment 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_dry_run_batches_use_typed_requests 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_command_building 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_multiple_node_ids 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_smoke_mode_env 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_no_parallel 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_batches
_execute 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_honors_
keyword_filter 
tests/unit/testing/test_run_tests_segmented.py::test_run_segmented_tests_stop_af
ter_maxfail 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_failure_appends_tips 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_segmente
d_failure_surfaces_remediation 
tests/unit/testing/test_run_tests_segmented_failures.py::test__run_segmented_tes
ts_aggregates_outputs 
tests/unit/testing/test_run_tests_segmented_failures.py::test_segmented_runs_rei
nject_plugins_without_clobbering_addopts 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_single_b
atch_uses_request_object 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_success_invokes_publish 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_failure_skips_graph 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_reports_append_graph 
tests/unit/testing/test_run_tests_segmented_report_flag.py::test_run_segmented_t
ests_reports_only_last_segment 
tests/unit/testing/test_run_tests_speed_keyword_loop.py::test_speed_loop_uses_ke
yword_filter_and_executes_node_ids 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_merges_fast
_and_medium_collections 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_defaults_to
_fast_and_medium_when_unspecified 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_excludes_gu
i_by_default 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_allows_gui_
when_requested 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_strips_line
_numbers_without_function_selector 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_preserves_o
rder 
tests/unit/testing/test_sanitize_node_ids_minimal.py::test_sanitize_node_ids_str
ips_line_when_no_function 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_tuple_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_invalid
_exc_info 
tests/unit/utils/test_logging_coverage.py::test_get_logger_returns_correct_insta
nce 
tests/unit/utils/test_logging_coverage.py::test_setup_logging_with_different_log
_levels 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_false_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_none_ex
c_info 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
baseexception_direct 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
true_with_active_exception 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
none_and_false_explicit 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_invalid_e
xc_info_to_hit_line_48 
tests/unit/utils/test_logging_final_coverage.py::test_get_logger_function_direct
_call 
tests/unit/utils/test_logging_final_coverage.py::test_setup_logging_function_dir
ect_calls 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_with_kwar
gs 
tests/unit/utils/test_logging_utils.py::test_dev_synth_logger_normalizes_exc_inf
o_tuple_and_exception 
tests/unit/utils/test_logging_utils.py::test_setup_logging_calls_configure_loggi
ng 
tests/unit/utils/test_logging_utils.py::test_get_logger_returns_dev_synth_logger
_instance 
tests/unit/utils/test_serialization.py::test_dumps_deterministic_round_trip_simp
le tests/unit/utils/test_serialization.py::test_dump_and_load_file_round_trip 
tests/unit/utils/test_serialization.py::test_provider_env_as_dict_deterministic_
serialization 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_with_s
tring_already_having_newline 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_ensure
s_single_newline 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_no_trailing_new
line 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_trailing_newlin
e 
tests/unit/utils/test_serialization_coverage.py::test_dump_to_file_complete_cove
rage 
tests/unit/utils/test_serialization_coverage.py::test_load_from_file_complete_co
verage 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_multiple_traili
ng_newlines 
tests/unit/utils/test_serialization_coverage.py::test_serialization_with_unicode
_characters 
tests/unit/utils/test_serialization_coverage.py::test_serialization_edge_cases 
tests/unit/utils/test_serialization_coverage.py::test_file_operations_with_speci
al_paths 
tests/unit/utils/test_serialization_edges.py::test_loads_tolerates_missing_and_s
ingle_trailing_newline 
tests/unit/utils/test_serialization_edges.py::test_dump_to_file_overwrites_and_k
eeps_single_newline 
tests/unit/utils/test_serialization_extra.py::test_dumps_and_loads_deterministic
_round_trip_unicode_and_newline 
tests/unit/utils/test_serialization_extra.py::test_dump_and_load_file_round_trip
_handles_utf8 
tests/unit/utils/test_serialization_extra.py::test_loads_accepts_without_trailin
g_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
direct_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
with_string_that_might_have_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_direct_line_co
verage 
tests/unit/utils/test_serialization_final_coverage.py::test_dump_to_file_direct_
line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_load_from_file_direc
t_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_serialization_functi
ons_with_mock_to_ensure_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_with_various_n
ewline_scenarios 
tests/unit/utils/test_serialization_final_coverage.py::test_file_operations_with
_explicit_paths 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
return_path 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_return_path 
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_pass 
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_failure 
tests/integration/agents/test_generation/test_scaffold_generation.py::test_scaff
old_hook_creates_placeholder 
tests/integration/agents/test_generation/test_scaffold_generation.py::test_proce
ss_generates_tests_and_scaffolds 
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_ports 
tests/integration/api/test_api_startup.py::test_agent_openapi_documents_workflow
_models 
tests/integration/deployment/test_compose_workflow.py::test_setup_env_refuses_ro
ot 
tests/integration/deployment/test_compose_workflow.py::test_check_health_env_per
missions 
tests/integration/deployment/test_compose_workflow.py::test_rollback_requires_ta
g 
tests/integration/deployment/test_deployment_scripts.py::test_bootstrap_env_refu
ses_root 
tests/integration/deployment/test_deployment_scripts.py::test_health_check_valid
ates_url 
tests/integration/deployment/test_deployment_scripts.py::test_prometheus_exporte
r_refuses_root 
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions 
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions tests/integration/general/test_complex_workflow.py::test_cmd 
tests/integration/general/test_end_to_end_workflow.py::test_cmd 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_registration 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_configuration_loading 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_settings_extraction 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_initialization_with_defaults 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_mock_initialization 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_environment_variable_handling 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_config_file_integration 
tests/integration/generated/test_generated_module.py::test_generated_module_work
flow 
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_success 
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_failure 
tests/integration/llm/test_lmstudio_timing_baseline.py::test_timeout_configurati
on_sanity 
tests/integration/mvu/test_command_execution.py::test_mvu_exec_runs_command 
tests/integration/mvu/test_command_execution.py::test_mvu_exec_propagates_error 
tests/integration/utils/test_logging_integration.py::test_setup_logging_returns_
project_logger 
tests/integration/utils/test_logging_integration.py::test_log_normalizes_excepti
on -m not memory_intensive and fast and not gui --cov=src/devsynth 
--cov-report=json:test_reports/coverage.json --cov-report=html:htmlcov 
--cov-appendTroubleshooting tips:- Smoke mode: reduce third-party plugin surface
to isolate issues:  poetry run devsynth run-tests --smoke --speed=fast 
--no-parallel --maxfail=1- Marker discipline: default is &amp;#x27;-m not 
memory_intensive&amp;#x27;.  Ensure exactly ONE of @pytest.mark.fast|medium|slow
per test.- Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using 
--smoke; plugin options may fail otherwise.- Diagnostics: run &amp;#x27;poetry 
run devsynth doctor&amp;#x27; for a quick environment check.- Narrow scope: use 
&amp;#x27;-k &amp;lt;expr&amp;gt;&amp;#x27; and &amp;#x27;-vv&amp;#x27; to focus
a failure.- Segment large suites to localize failures and flakes:  devsynth 
run-tests --target unit-tests --speed=fast --segment --segment-size=50- Limit 
failures early to speed iteration:  poetry run devsynth run-tests --target 
unit-tests --speed=fast --maxfail=1- Disable parallelism if xdist interaction is
suspected:  devsynth run-tests --target unit-tests --speed=fast --no-parallel- 
Generate an HTML report for context (saved under test_reports/):  devsynth 
run-tests --target unit-tests --speed=fast --report
Tests failed
