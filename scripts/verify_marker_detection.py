#!/usr/bin/env python
"""
Script to verify marker detection accuracy after standardization.

This script compares the marker counts reported by apply_speed_markers.py and
common_test_collector.py to ensure they're consistent. It helps identify any
remaining discrepancies in marker detection.

Usage:
    python scripts/verify_marker_detection.py [options]

Options:
    --verbose             Show verbose output
    --fix                 Attempt to fix any discrepancies found
    --report-file FILE    Save the report to the specified file (default: marker_detection_report.json)
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# Import common test collector for marker detection
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from common_test_collector import (
        clear_cache,
        get_marker_counts,
        get_tests_with_markers,
    )
except ImportError:
    print(
        "Error: common_test_collector.py not found. This script requires common_test_collector.py."
    )
    sys.exit(1)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Verify marker detection accuracy after standardization."
    )
    parser.add_argument("--verbose", action="store_true", help="Show verbose output")
    parser.add_argument(
        "--fix", action="store_true", help="Attempt to fix any discrepancies found"
    )
    parser.add_argument(
        "--report-file",
        type=str,
        default="marker_detection_report.json",
        help="Save the report to the specified file",
    )
    return parser.parse_args()


def load_apply_speed_markers_report() -> dict[str, Any]:
    """
    Load the report generated by apply_speed_markers.py.

    Returns:
        Dictionary containing the report data
    """
    report_path = "test_markers_report.json"
    if not os.path.exists(report_path):
        print(
            f"Warning: {report_path} not found. Running apply_speed_markers.py to generate it..."
        )
        try:
            subprocess.run(
                [sys.executable, "scripts/apply_speed_markers.py", "--dry-run"],
                check=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"Error running apply_speed_markers.py: {e}")
            return {}

    try:
        with open(report_path) as f:
            return json.load(f)
    except (json.JSONDecodeError, OSError) as e:
        print(f"Error loading {report_path}: {e}")
        return {}


def get_collector_marker_counts() -> dict[str, int]:
    """
    Get marker counts from common_test_collector.py.

    Returns:
        Dictionary mapping marker types to counts
    """
    # Clear the cache first to ensure we get fresh data
    clear_cache()

    # Get marker counts with no cache
    marker_counts = get_marker_counts(use_cache=False)

    # Extract the "total" category, which contains the overall counts
    if "total" in marker_counts:
        return marker_counts["total"]

    # If "total" is not available, sum up counts across all categories
    result = {}
    for category in marker_counts:
        if category != "total":
            for marker_type, count in marker_counts[category].items():
                if marker_type not in result:
                    result[marker_type] = 0
                result[marker_type] += count

    return result


def get_collector_marked_tests() -> dict[str, dict[str, list[str]]]:
    """
    Get tests with markers from common_test_collector.py.

    Returns:
        Dictionary mapping categories to marker types to lists of test paths
    """
    # Clear the cache first to ensure we get fresh data
    clear_cache()

    # Get tests with markers with no cache
    return get_tests_with_markers(use_cache=False)


def find_discrepancies(
    apply_report: dict[str, Any], collector_counts: dict[str, int]
) -> dict[str, Any]:
    """
    Find discrepancies between apply_speed_markers.py and common_test_collector.py reports.

    Args:
        apply_report: Report from apply_speed_markers.py
        collector_counts: Marker counts from common_test_collector.py

    Returns:
        Dictionary containing discrepancy information
    """
    discrepancies = {
        "total_markers": {
            "apply_speed_markers": apply_report.get("fast_tests", 0)
            + apply_report.get("medium_tests", 0)
            + apply_report.get("slow_tests", 0),
            "common_test_collector": sum(collector_counts.values()),
        },
        "by_type": {
            "fast": {
                "apply_speed_markers": apply_report.get("fast_tests", 0),
                "common_test_collector": collector_counts.get("fast", 0),
            },
            "medium": {
                "apply_speed_markers": apply_report.get("medium_tests", 0),
                "common_test_collector": collector_counts.get("medium", 0),
            },
            "slow": {
                "apply_speed_markers": apply_report.get("slow_tests", 0),
                "common_test_collector": collector_counts.get("slow", 0),
            },
        },
    }

    # Calculate differences
    discrepancies["total_difference"] = (
        discrepancies["total_markers"]["apply_speed_markers"]
        - discrepancies["total_markers"]["common_test_collector"]
    )
    discrepancies["by_type"]["fast"]["difference"] = (
        discrepancies["by_type"]["fast"]["apply_speed_markers"]
        - discrepancies["by_type"]["fast"]["common_test_collector"]
    )
    discrepancies["by_type"]["medium"]["difference"] = (
        discrepancies["by_type"]["medium"]["apply_speed_markers"]
        - discrepancies["by_type"]["medium"]["common_test_collector"]
    )
    discrepancies["by_type"]["slow"]["difference"] = (
        discrepancies["by_type"]["slow"]["apply_speed_markers"]
        - discrepancies["by_type"]["slow"]["common_test_collector"]
    )

    return discrepancies


def find_problematic_tests(marked_tests: dict[str, dict[str, list[str]]]) -> list[str]:
    """
    Find tests that might have problematic marker placement.

    Args:
        marked_tests: Tests with markers from common_test_collector.py

    Returns:
        List of test files that might have problematic marker placement
    """
    problematic_files = set()

    # Check for tests with multiple speed markers
    for category, markers in marked_tests.items():
        fast_tests = set(markers.get("fast", []))
        medium_tests = set(markers.get("medium", []))
        slow_tests = set(markers.get("slow", []))

        # Find tests with multiple speed markers
        fast_and_medium = fast_tests.intersection(medium_tests)
        fast_and_slow = fast_tests.intersection(slow_tests)
        medium_and_slow = medium_tests.intersection(slow_tests)

        # Add problematic files
        for test in fast_and_medium.union(fast_and_slow).union(medium_and_slow):
            if "::" in test:
                problematic_files.add(test.split("::")[0])
            else:
                problematic_files.add(test)

    return sorted(list(problematic_files))


def fix_discrepancies(problematic_files: list[str], verbose: bool = False) -> bool:
    """
    Attempt to fix discrepancies by standardizing marker placement in problematic files.

    Args:
        problematic_files: List of files with problematic marker placement
        verbose: Whether to show verbose output

    Returns:
        Whether the fix was successful
    """
    if not problematic_files:
        print("No problematic files to fix.")
        return True

    print(f"Attempting to fix {len(problematic_files)} problematic files...")

    try:
        # Run standardize_marker_placement.py on the problematic files
        cmd = [sys.executable, "scripts/standardize_marker_placement.py"]
        if verbose:
            cmd.append("--verbose")

        # Run the script
        result = subprocess.run(cmd, check=True)

        # Clear the cache to ensure we get fresh data
        clear_cache()

        return result.returncode == 0
    except subprocess.CalledProcessError as e:
        print(f"Error running standardize_marker_placement.py: {e}")
        return False


def main():
    """Main function."""
    args = parse_args()

    print("Verifying marker detection accuracy...")

    # Load the report from apply_speed_markers.py
    apply_report = load_apply_speed_markers_report()
    if not apply_report:
        print("Error: Could not load apply_speed_markers.py report.")
        return

    # Get marker counts from common_test_collector.py
    collector_counts = get_collector_marker_counts()
    if not collector_counts:
        print("Error: Could not get marker counts from common_test_collector.py.")
        return

    # Find discrepancies
    discrepancies = find_discrepancies(apply_report, collector_counts)

    # Print discrepancy report
    print("\nMarker Count Comparison:")
    print(
        f"Total markers (apply_speed_markers.py): {discrepancies['total_markers']['apply_speed_markers']}"
    )
    print(
        f"Total markers (common_test_collector.py): {discrepancies['total_markers']['common_test_collector']}"
    )
    print(f"Difference: {discrepancies['total_difference']}")

    print("\nBy marker type:")
    for marker_type in ["fast", "medium", "slow"]:
        print(
            f"  {marker_type.capitalize()} markers (apply_speed_markers.py): {discrepancies['by_type'][marker_type]['apply_speed_markers']}"
        )
        print(
            f"  {marker_type.capitalize()} markers (common_test_collector.py): {discrepancies['by_type'][marker_type]['common_test_collector']}"
        )
        print(f"  Difference: {discrepancies['by_type'][marker_type]['difference']}")

    # If there are discrepancies, try to find problematic tests
    if discrepancies["total_difference"] != 0:
        print("\nInvestigating discrepancies...")

        # Get tests with markers from common_test_collector.py
        marked_tests = get_collector_marked_tests()

        # Find problematic tests
        problematic_files = find_problematic_tests(marked_tests)

        if problematic_files:
            print(f"\nFound {len(problematic_files)} potentially problematic files:")
            for file in problematic_files[:10]:  # Show only the first 10
                print(f"  {file}")
            if len(problematic_files) > 10:
                print(f"  ... and {len(problematic_files) - 10} more")

            # Try to fix discrepancies if requested
            if args.fix:
                print("\nAttempting to fix discrepancies...")
                if fix_discrepancies(problematic_files, args.verbose):
                    print("Fix completed. Please run this script again to verify.")
                else:
                    print("Fix failed. Please check the errors and try again.")
        else:
            print(
                "No problematic files found. The discrepancy might be due to other issues."
            )
    else:
        print("\nNo discrepancies found! Marker detection is consistent.")

    # Save the report
    report = {
        "timestamp": (
            str(Path(args.report_file).stat().st_mtime)
            if Path(args.report_file).exists()
            else None
        ),
        "apply_speed_markers": apply_report,
        "common_test_collector": {"marker_counts": collector_counts},
        "discrepancies": discrepancies,
        "problematic_files": (
            problematic_files if "problematic_files" in locals() else []
        ),
    }

    with open(args.report_file, "w") as f:
        json.dump(report, f, indent=2)

    print(f"\nReport saved to {args.report_file}")


if __name__ == "__main__":
    main()
