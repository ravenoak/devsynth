2025-10-29 10:47:27,538 - devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:47:27,539 - devsynth.application.cli.commands.run_tests_cmd - INFO - CLI appended -p pytest_asyncio.plugin to PYTEST_ADDOPTS to preserve async test support
-p pytest_asyncio.plugin appended to PYTEST_ADDOPTS because plugin autoloading 
is disabled
2025-10-29 10:47:27,594 - devsynth.testing.run_tests - INFO - Injected -p pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:47:27,619 - devsynth.testing.run_tests - INFO - test collection cache hit for target=all-tests (fast)
2025-10-29 10:49:14,831 - devsynth.testing.run_tests - INFO - Coverage data file detected at .coverage (196608 bytes)
2025-10-29 10:49:23,653 - devsynth.testing.run_tests - INFO - Coverage HTML report generated
2025-10-29 10:49:26,618 - devsynth.testing.run_tests - INFO - Coverage JSON report generated
============================= test session starts ==============================
platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
rootdir: /Users/caitlyn/Projects/github.com/ravenoak/devsynth
configfile: pytest.ini
plugins: cov-7.0.0
asyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=function
collected 2969 items / 1 deselected / 2968 selected

tests/unit/adapters/cli/test_typer_adapter.py ....                       [  0%]
tests/unit/adapters/issues/test_github_adapter.py .                      [  0%]
tests/unit/adapters/issues/test_jira_adapter.py .                        [  0%]
tests/unit/adapters/llm/test_llm_adapter.py .........                    [  0%]
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py ....          [  0%]
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py ........           [  0%]
tests/unit/adapters/test_agent_adapter.py ...................            [  1%]
tests/unit/adapters/test_backend_resource_gates.py sss                   [  1%]
tests/unit/adapters/test_chromadb_memory_store_unit.py ......            [  1%]
tests/unit/adapters/test_fake_memory_store.py ..                         [  1%]
tests/unit/adapters/test_github_project_adapter.py ........              [  2%]
tests/unit/adapters/test_jira_adapter.py ...                             [  2%]
tests/unit/adapters/test_onnx_runtime_adapter.py sssssss                 [  2%]
tests/unit/adapters/test_provider_safe_defaults.py .....                 [  2%]
tests/unit/adapters/test_provider_stub.py ...                            [  2%]
tests/unit/adapters/test_provider_system.py ............................ [  3%]
..........                                                               [  4%]
tests/unit/adapters/test_provider_system_additional.py .FFF.FFFF........ [  4%]
...........                                                              [  5%]
tests/unit/adapters/test_provider_system_fallbacks_fast.py ...           [  5%]
tests/unit/adapters/test_provider_system_resilience.py ...               [  5%]
tests/unit/adapters/test_resource_gating_seams.py s.                     [  5%]
tests/unit/adapters/test_storage_adapter_protocol.py .                   [  5%]
tests/unit/agents/test_alignment_metrics_tool.py ..                      [  5%]
tests/unit/agents/test_doctor_tool.py ..                                 [  5%]
tests/unit/agents/test_multi_agent_coordinator.py .                      [  5%]
tests/unit/agents/test_run_tests_tool.py ..                              [  5%]
tests/unit/agents/test_security_audit_tool.py ..                         [  5%]
tests/unit/agents/test_test_generator.py ..........                      [  5%]
tests/unit/agents/test_tool_sandbox.py ....                              [  6%]
tests/unit/agents/test_tools.py ...                                      [  6%]
tests/unit/agents/test_wsde_team_coordinator_strict.py ..                [  6%]
tests/unit/api/test_fastapi_testclient_import.py F                       [  6%]
tests/unit/api/test_public_api_contract.py ..                            [  6%]
tests/unit/application/agents/test_base_agent.py ...........             [  6%]
tests/unit/application/agents/test_test_agent_integration.py .           [  6%]
tests/unit/application/agents/test_validation_agent.py .....             [  6%]
tests/unit/application/agents/test_validation_agent_decision.py .....    [  7%]
tests/unit/application/agents/test_wsde_memory_integration_fast.py .     [  7%]
tests/unit/application/cli/commands/test_config_cmd.py ....              [  7%]
tests/unit/application/cli/commands/test_doctor_cmd_typed.py .           [  7%]
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py .       [  7%]
tests/unit/application/cli/commands/test_ingest_cli_command.py .         [  7%]
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py . [  
7%]
                                                                         [  7%]
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y . [  7%]
...                                                                      [  7%]
tests/unit/application/cli/commands/test_module_imports.py ............. [  7%]
.....................................                                    [  9%]
tests/unit/application/cli/commands/test_parse_feature_options_unit.py . [  9%]
...                                                                      [  9%]
tests/unit/application/cli/commands/test_run_pipeline_cmd.py ...         [  9%]
tests/unit/application/cli/commands/test_run_tests_cmd.py ....F......FFF [  9%]
                                                                         [  9%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py EEEE [ 10%]
E                                                                        [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py . [ 10%]
.FFFF                                                                    [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py . [ 
10%]
FFFFFF                                                                   [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py 
F [ 10%]
FF                                                                       [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py F [
10%]
FFFFFF                                                                   [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py EE   [ 10%]
tests/unit/application/cli/commands/test_run_tests_cmd_features.py EE    [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py E   [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py FF   [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py E [ 11%]
EEE                                                                      [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py EE     [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_more.py EEE       [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py E [ 
11%]
E                                                                        [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py F [ 
11%]
F                                                                        [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py EE [ 11%]
EE                                                                       [ 11%]
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py F [ 11%]
FF                                                                       [ 11%]
tests/unit/application/cli/commands/test_run_tests_dummy.py .            [ 11%]
tests/unit/application/cli/commands/test_run_tests_features.py F         [ 11%]
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py F [ 11%]
                                                                         [ 11%]
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py E [ 11%]
E                                                                        [ 11%]
tests/unit/application/cli/commands/test_run_tests_subprocess.py F       [ 11%]
tests/unit/application/cli/commands/test_run_tests_validation.py FF      [ 12%]
tests/unit/application/cli/commands/test_security_audit_cmd.py ....      [ 12%]
tests/unit/application/cli/commands/test_testing_cmd.py ..........       [ 12%]
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py ..      [ 12%]
tests/unit/application/cli/test_command_output_formatter.py .........    [ 12%]
tests/unit/application/cli/test_ingest_cmd.py ..                         [ 12%]
tests/unit/application/cli/test_long_running_progress.py .............   [ 13%]
tests/unit/application/cli/test_long_running_progress_deterministic.py . [ 13%]
....                                                                     [ 13%]
tests/unit/application/cli/test_output.py .........                      [ 13%]
tests/unit/application/cli/test_progress.py F                            [ 13%]
tests/unit/application/cli/test_progress_aliasing.py .........           [ 14%]
tests/unit/application/cli/test_requirements_commands.py ......          [ 14%]
tests/unit/application/cli/test_requirements_gathering.py .              [ 14%]
tests/unit/application/cli/test_run_tests_cmd.py FFFFF                   [ 14%]
tests/unit/application/cli/test_run_tests_cmd_options.py FFF             [ 14%]
tests/unit/application/cli/test_run_tests_cmd_smoke.py EEEE              [ 14%]
tests/unit/application/cli/test_setup_wizard.py EEEEEE                   [ 15%]
tests/unit/application/cli/test_setup_wizard_textual.py ..               [ 15%]
tests/unit/application/cli/test_sprint_cmd_types.py ...                  [ 15%]
tests/unit/application/code_analysis/test_analyzer.py .                  [ 15%]
tests/unit/application/code_analysis/test_ast_transformer.py ..........  [ 15%]
tests/unit/application/code_analysis/test_ast_workflow_integration.py .. [ 15%]
...                                                                      [ 15%]
tests/unit/application/code_analysis/test_project_state_analyzer.py .... [ 15%]
.....                                                                    [ 16%]
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py 
F [ 16%]
                                                                         [ 16%]
tests/unit/application/code_analysis/test_repo_analyzer.py ..            [ 16%]
tests/unit/application/code_analysis/test_self_analyzer.py ...........   [ 16%]
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py . [ 16%]
                                                                         [ 16%]
tests/unit/application/code_analysis/test_transformer.py ...........     [ 16%]
tests/unit/application/code_analysis/test_transformer_basic.py .         [ 16%]
tests/unit/application/code_analysis/test_transformer_helpers.py ...     [ 17%]
tests/unit/application/collaboration/test_agent_collaboration_system.py . [ 17%]
..                                                                       [ 17%]
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py . [ 17%]
.                                                                        [ 17%]
tests/unit/application/collaboration/test_memory_utils_conversion.py .   [ 17%]
tests/unit/application/collaboration/test_message_protocol.py ...        [ 17%]
tests/unit/application/collaboration/test_peer_review_store.py ....      [ 17%]
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py F.   [ 17%]
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py . [ 17%]
                                                                         [ 17%]
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py . [ 
17%]
..F                                                                      [ 17%]
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py . [ 17%]
.                                                                        [ 17%]
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py . [ 
17%]
                                                                         [ 17%]
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py . [
17%]
                                                                         [ 17%]
tests/unit/application/documentation/test_documentation_fetcher_parsing.py . [ 
17%]
...                                                                      [ 18%]
tests/unit/application/documentation/test_ingestion_search_variance.py . [ 18%]
.                                                                        [ 18%]
tests/unit/application/edrr/coordinator/test_core.py ...FFFFFFFFFFFFFFFF [ 18%]
FFFF                                                                     [ 18%]
tests/unit/application/edrr/test_coordinator.py .                        [ 18%]
tests/unit/application/edrr/test_coordinator_core.py F                   [ 18%]
tests/unit/application/edrr/test_coordinator_reasoning.py FF             [ 19%]
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py .          [ 19%]
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py .        [ 19%]
tests/unit/application/edrr/test_persistence_module.py ..........        [ 19%]
tests/unit/application/edrr/test_phase_management_module.py ..FF....     [ 19%]
tests/unit/application/edrr/test_reasoning_loop_retries.py F             [ 19%]
tests/unit/application/edrr/test_recursion_termination.py ss             [ 19%]
tests/unit/application/edrr/test_sprint_planning.py ....                 [ 19%]
tests/unit/application/edrr/test_sprint_retrospective.py .....           [ 20%]
tests/unit/application/edrr/test_threshold_helpers.py .....              [ 20%]
tests/unit/application/ingestion/test_ingestion_pure.py ...              [ 20%]
tests/unit/application/ingestion/test_phases.py ..                       [ 20%]
tests/unit/application/llm/test_import_without_openai.py .F              [ 20%]
tests/unit/application/llm/test_lmstudio_health_check.py FF              [ 20%]
tests/unit/application/llm/test_lmstudio_offline_resilience.py ..        [ 20%]
tests/unit/application/llm/test_lmstudio_provider.py ...........FFFFFFFF [ 21%]
FF......                                                                 [ 21%]
tests/unit/application/llm/test_offline_provider.py ...                  [ 21%]
tests/unit/application/llm/test_openai_env_key_mock.py .                 [ 21%]
tests/unit/application/llm/test_openai_offline_resilience.py ....        [ 21%]
tests/unit/application/llm/test_provider_factory.py FF                   [ 21%]
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py ...  [ 21%]
tests/unit/application/llm/test_provider_selection.py FF                 [ 22%]
tests/unit/application/memory/test_chromadb_store.py s                   [ 22%]
tests/unit/application/memory/test_chromadb_store_typed.py ss            [ 22%]
tests/unit/application/memory/test_circuit_breaker.py ..                 [ 22%]
tests/unit/application/memory/test_duckdb_store_schema_flags.py ss       [ 22%]
tests/unit/application/memory/test_error_logger.py ....                  [ 22%]
tests/unit/application/memory/test_execution_learning_integration.py .FF [ 22%]
..FFEEEEEEF.FF                                                           [ 22%]
tests/unit/application/memory/test_faiss_store.py ssss                   [ 23%]
tests/unit/application/memory/test_fast_in_memory_components.py ........ [ 23%]
                                                                         [ 23%]
tests/unit/application/memory/test_graph_memory_adapter.py .             [ 23%]
tests/unit/application/memory/test_lmdb_store.py ....                    [ 23%]
tests/unit/application/memory/test_memory_manager.py ...                 [ 23%]
tests/unit/application/memory/test_memory_system_adapter_unit.py ....... [ 23%]
.                                                                        [ 23%]
tests/unit/application/memory/test_metadata_serialization_helpers.py ..F [ 24%]
.                                                                        [ 24%]
tests/unit/application/memory/test_phase3_integration_system.py .F...F.. [ 24%]
..F.....EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE                              [ 25%]
tests/unit/application/memory/test_query_router.py ..F.                  [ 25%]
tests/unit/application/memory/test_rdflib_store_transactions.py ...      [ 26%]
tests/unit/application/memory/test_search_memory_fallback.py .           [ 26%]
tests/unit/application/memory/test_sync_manager_transactions.py F.       [ 26%]
tests/unit/application/memory/test_tiered_cache_termination.py ..        [ 26%]
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py F       [ 26%]
tests/unit/application/memory/test_vector_memory_adapter_extra.py ss     [ 26%]
tests/unit/application/orchestration/test_dialectical_reasoner.py ...    [ 26%]
tests/unit/application/promises/test_agent_create_promise.py .           [ 26%]
tests/unit/application/promises/test_interface_not_implemented.py .      [ 26%]
tests/unit/application/promises/test_interface_pure.py ...               [ 26%]
tests/unit/application/prompts/test_auto_tuning_pure.py ...              [ 26%]
tests/unit/application/requirements/test_dialectical_reasoner.py ..F.... [ 26%]
..FFFF                                                                   [ 27%]
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y . [ 27%]
.                                                                        [ 27%]
tests/unit/application/requirements/test_dialectical_reasoner_pure.py .. [ 27%]
..                                                                       [ 27%]
tests/unit/application/requirements/test_interactions.py ...             [ 27%]
tests/unit/application/requirements/test_requirement_service_dtos.py ..  [ 27%]
tests/unit/application/requirements/test_wizard.py ...                   [ 27%]
tests/unit/application/sprint/test_planning.py .                         [ 27%]
tests/unit/application/test_documentation_fetcher.py ..                  [ 27%]
tests/unit/application/testing/test_enhanced_test_collector.py ..EEE.EEE [ 27%]
E..EEEEE..EF.....F....                                                   [ 28%]
tests/unit/application/utils/test_extras_helper.py ...                   [ 28%]
tests/unit/behavior/test_alignment_metrics_steps_unit.py F               [ 28%]
tests/unit/behavior/test_analyze_commands_steps_unit.py ..               [ 28%]
tests/unit/cli/test_cli_entry.py .                                       [ 28%]
tests/unit/cli/test_cli_error_handling.py F                              [ 28%]
tests/unit/cli/test_cli_help.py .                                        [ 29%]
tests/unit/cli/test_command_module_loading.py .                          [ 29%]
tests/unit/cli/test_command_registry.py F.                               [ 29%]
tests/unit/cli/test_completion_progress.py .                             [ 29%]
tests/unit/cli/test_entry_points_help.py ...                             [ 29%]
tests/unit/cli/test_help_examples.py ..                                  [ 29%]
tests/unit/cli/test_import_gating.py ..                                  [ 29%]
tests/unit/cli/test_init_features_option.py ..                           [ 29%]
tests/unit/cli/test_key_commands_help.py .......                         [ 29%]
tests/unit/cli/test_logging_flags.py FFF                                 [ 29%]
tests/unit/cli/test_mvu_commands.py ..                                   [ 29%]
tests/unit/cli/test_mvuu_command_registration.py .                       [ 29%]
tests/unit/cli/test_mvuu_dashboard_smoke.py F                            [ 29%]
tests/unit/cli/test_mvuu_dashboard_telemetry.py ....                     [ 30%]
tests/unit/cli/test_run_tests_regression.py F                            [ 30%]
tests/unit/cli/test_version.py .                                         [ 30%]
tests/unit/config/test_config_llm_env.py F                               [ 30%]
tests/unit/config/test_exception_handling.py .....                       [ 30%]
tests/unit/config/test_feature_flag_defaults.py ..                       [ 30%]
tests/unit/config/test_provider_env.py ....                              [ 30%]
tests/unit/config/test_provider_env_apply_and_parse.py ...               [ 30%]
tests/unit/config/test_provider_env_behavior.py ....                     [ 30%]
tests/unit/config/test_provider_env_bool_parsing_edges.py ...            [ 30%]
tests/unit/config/test_provider_env_with_test_defaults.py ..             [ 30%]
tests/unit/config/test_unified_loader.py .                               [ 30%]
tests/unit/core/mvu/test_api.py ..                                       [ 31%]
tests/unit/core/mvu/test_atomic_rewrite.py .                             [ 31%]
tests/unit/core/mvu/test_linter.py ......                                [ 31%]
tests/unit/core/mvu/test_mvuu_schema_validation.py .                     [ 31%]
tests/unit/core/mvu/test_report.py ..                                    [ 31%]
tests/unit/core/mvu/test_storage.py ..                                   [ 31%]
tests/unit/core/mvu/test_validator.py ...                                [ 31%]
tests/unit/core/test_config_loader.py .                                  [ 31%]
tests/unit/core/test_config_loader_json_types.py ...                     [ 31%]
tests/unit/core/test_config_loader_mvu.py F                              [ 31%]
tests/unit/core/test_config_loader_optional_deps.py ...                  [ 31%]
tests/unit/core/test_config_loader_validation.py ....................... [ 32%]
....                                                                     [ 32%]
tests/unit/core/test_deterministic_fixtures.py ...                       [ 32%]
tests/unit/core/test_mvu.py ..                                           [ 32%]
tests/unit/deployment/test_bootstrap_script.py FFF                       [ 32%]
tests/unit/deployment/test_deployment_scripts.py ..                      [ 33%]
tests/unit/deployment/test_enforcement.py ..                             [ 33%]
tests/unit/deployment/test_health_check_smoke.py FFFFFF                  [ 33%]
tests/unit/deployment/test_scripts_dir.py ..                             [ 33%]
tests/unit/deployment/test_security_hardening.py ......                  [ 33%]
tests/unit/devsynth/test_consensus.py .....                              [ 33%]
tests/unit/devsynth/test_fallback_reliability.py ..                      [ 33%]
tests/unit/devsynth/test_logger.py ...                                   [ 33%]
tests/unit/devsynth/test_metrics.py ....                                 [ 34%]
tests/unit/devsynth/test_simple_addition.py ..                           [ 34%]
tests/unit/docs/test_dialectical_audit.py ..                             [ 34%]
tests/unit/domain/interfaces/test_interfaces.py ...                      [ 34%]
tests/unit/domain/models/test_agent_coverage.py ..                       [ 34%]
tests/unit/domain/models/test_memetic_unit.py ...........                [ 34%]
tests/unit/domain/models/test_project.py ...                             [ 34%]
tests/unit/domain/models/test_project_model.py ............              [ 35%]
tests/unit/domain/models/test_wsde.py ...F...FFFF                        [ 35%]
tests/unit/domain/models/test_wsde_base_methods.py ..                    [ 35%]
tests/unit/domain/models/test_wsde_code_improvements.py ...              [ 35%]
tests/unit/domain/models/test_wsde_decision_making.py ......             [ 35%]
tests/unit/domain/models/test_wsde_dialectical_helpers.py ...            [ 36%]
tests/unit/domain/models/test_wsde_dialectical_typing.py .               [ 36%]
tests/unit/domain/models/test_wsde_dialectical_workflow.py F.            [ 36%]
tests/unit/domain/models/test_wsde_dynamic_workflows.py .F               [ 36%]
tests/unit/domain/models/test_wsde_enhanced_dialectical.py ......        [ 36%]
tests/unit/domain/models/test_wsde_knowledge.py .....                    [ 36%]
tests/unit/domain/models/test_wsde_roles_personas.py ......              [ 36%]
tests/unit/domain/models/test_wsde_security_checks.py FFF                [ 36%]
tests/unit/domain/models/test_wsde_solution_analysis.py ....             [ 37%]
tests/unit/domain/models/test_wsde_strategies.py .F.                     [ 37%]
tests/unit/domain/models/test_wsde_team.py ..F..F                        [ 37%]
tests/unit/domain/models/test_wsde_utils.py ......F.                     [ 37%]
tests/unit/domain/models/test_wsde_voting_logic.py ....                  [ 37%]
tests/unit/domain/test_code_analysis_interfaces.py ...                   [ 37%]
tests/unit/domain/test_wsde_expertise_score.py F                         [ 37%]
tests/unit/domain/test_wsde_facade.py .F                                 [ 37%]
tests/unit/domain/test_wsde_facade_roles.py FF                           [ 38%]
tests/unit/domain/test_wsde_peer_review_workflow.py ..                   [ 38%]
tests/unit/domain/test_wsde_phase_role_rotation.py .F.                   [ 38%]
tests/unit/domain/test_wsde_primus_selection.py ..FFF.F                  [ 38%]
tests/unit/domain/test_wsde_team.py .FFFF....F.FF                        [ 38%]
tests/unit/domain/test_wsde_voting_logic.py FFFFFFF....                  [ 39%]
tests/unit/fallback/test_retry_counts.py ..                              [ 39%]
tests/unit/fallback/test_retry_predicates.py ..                          [ 39%]
tests/unit/general/test_agent_coordinator.py .......                     [ 39%]
tests/unit/general/test_agent_models.py ....                             [ 39%]
tests/unit/general/test_agent_system.py ............                     [ 40%]
tests/unit/general/test_anthropic_provider_unit.py FFFFF                 [ 40%]
tests/unit/general/test_api.py .F                                        [ 40%]
tests/unit/general/test_api_health.py FF                                 [ 40%]
tests/unit/general/test_atomic_rewrite_cli.py ...                        [ 40%]
tests/unit/general/test_backend_resource_flags.py ...F                   [ 40%]
tests/unit/general/test_base.py .                                        [ 40%]
tests/unit/general/test_chroma_db_adapter.py .FFFFFF                     [ 40%]
tests/unit/general/test_chromadb_store.py ssssss                         [ 41%]
tests/unit/general/test_cli_commands.py ..                               [ 41%]
tests/unit/general/test_code_analysis_interface.py ...                   [ 41%]
tests/unit/general/test_code_analysis_models.py ..                       [ 41%]
tests/unit/general/test_code_analyzer.py ....                            [ 41%]
tests/unit/general/test_config_loader.py .....                           [ 41%]
tests/unit/general/test_config_settings.py .................             [ 42%]
tests/unit/general/test_core_config_loader.py ...                        [ 42%]
tests/unit/general/test_core_values.py ...                               [ 42%]
tests/unit/general/test_core_workflows.py ...........                    [ 42%]
tests/unit/general/test_delegate_task_disabled.py .                      [ 42%]
tests/unit/general/test_dialectical_reasoner.py F.FF.                    [ 43%]
tests/unit/general/test_documentation_fetcher.py .                       [ 43%]
tests/unit/general/test_dpg_flag.py FFs                                  [ 43%]
tests/unit/general/test_edrr_cycle_cmd.py .......                        [ 43%]
tests/unit/general/test_edrr_manifest_string.py .                        [ 43%]
tests/unit/general/test_exception_logging.py .                           [ 43%]
tests/unit/general/test_exceptions.py .....................              [ 44%]
tests/unit/general/test_fallback_utils.py .                              [ 44%]
tests/unit/general/test_ingest_cmd.py ...............F.....F             [ 44%]
tests/unit/general/test_ingestion_edrr_integration.py .                  [ 45%]
tests/unit/general/test_ingestion_type_hints.py s                        [ 45%]
tests/unit/general/test_inspect_config_cmd.py ......                     [ 45%]
tests/unit/general/test_isolation.py ......                              [ 45%]
tests/unit/general/test_isolation_auto_marking.py ..                     [ 45%]
tests/unit/general/test_kuzu_adapter.py ssss                             [ 45%]
tests/unit/general/test_kuzu_embedded_missing.py .                       [ 45%]
tests/unit/general/test_langgraph_adapter.py ............                [ 46%]
tests/unit/general/test_llm_provider_selection.py FF                     [ 46%]
tests/unit/general/test_lmstudio_integration_regression.py ..F.F..       [ 46%]
tests/unit/general/test_lmstudio_service.py s                            [ 46%]
tests/unit/general/test_logger.py ..                                     [ 46%]
tests/unit/general/test_logging_setup.py ....                            [ 46%]
tests/unit/general/test_logging_setup_idempotent.py ...                  [ 46%]
tests/unit/general/test_memory_models.py .....                           [ 46%]
tests/unit/general/test_memory_store.py .                                [ 46%]
tests/unit/general/test_memory_system.py ....................            [ 47%]
tests/unit/general/test_memory_system_with_chromadb.py ssss              [ 47%]
tests/unit/general/test_methodology_logging.py .                         [ 47%]
tests/unit/general/test_multi_agent_adapter_workflow.py F.               [ 47%]
tests/unit/general/test_mvu_exec_cli.py ..                               [ 47%]
tests/unit/general/test_mvu_exec_cmd.py ..                               [ 47%]
tests/unit/general/test_mvu_init_cmd.py .                                [ 48%]
tests/unit/general/test_mvu_lint_cli.py FF                               [ 48%]
tests/unit/general/test_mvuu_dashboard_cli.py .                          [ 48%]
tests/unit/general/test_mypy_config.py ..                                [ 48%]
tests/unit/general/test_no_devsynth_dir_creation.py ..                   [ 48%]
tests/unit/general/test_onnx_port.py .                                   [ 48%]
tests/unit/general/test_path_restrictions.py F.                          [ 48%]
tests/unit/general/test_ports_with_fixtures.py E                         [ 48%]
tests/unit/general/test_primus_selection.py ..FF.F                       [ 48%]
tests/unit/general/test_project_yaml.py FFFF.                            [ 48%]
tests/unit/general/test_promise_agent.py ...........                     [ 49%]
tests/unit/general/test_promise_system.py ..............                 [ 49%]
tests/unit/general/test_provider_logging.py ss                           [ 49%]
tests/unit/general/test_requirement_models.py .....                      [ 49%]
tests/unit/general/test_requirement_repository_interface.py .            [ 49%]
tests/unit/general/test_requirement_repository_port_interface.py ..      [ 49%]
tests/unit/general/test_requirement_service.py .....                     [ 50%]
tests/unit/general/test_resource_markers.py ..F.sF                       [ 50%]
tests/unit/general/test_retry_failure_scenarios.py ..                    [ 50%]
tests/unit/general/test_speed_option.py .                                [ 50%]
tests/unit/general/test_sync_manager_persistence.py .                    [ 50%]
tests/unit/general/test_template_location.py ..                          [ 50%]
tests/unit/general/test_test_first_metrics.py .....                      [ 50%]
tests/unit/general/test_token_tracker.py ......                          [ 50%]
tests/unit/general/test_unified_agent_code_prompt.py .                   [ 50%]
tests/unit/general/test_unified_config_loader.py .......                 [ 51%]
tests/unit/general/test_unit_cli_commands.py .                           [ 51%]
tests/unit/general/test_ux_bridge.py FF                                  [ 51%]
tests/unit/general/test_workflow.py F.F...                               [ 51%]
tests/unit/general/test_workflow_models.py ....                          [ 51%]
tests/unit/general/test_wsde_dynamic_roles.py .                          [ 51%]
tests/unit/general/test_wsde_model.py ..                                 [ 51%]
tests/unit/general/test_wsde_role_mapping.py F                           [ 51%]
tests/unit/general/test_wsde_team_extended.py .....F.F.FFFFFFFFF.FFFF    [ 52%]
tests/unit/general/test_wsde_team_voting_invalid.py FF                   [ 52%]
tests/unit/general/test_wsde_voting.py FFFF                              [ 52%]
tests/unit/general/test_wsde_voting_mechanisms.py FFFFFF                 [ 52%]
tests/unit/infrastructure/test_test_infrastructure_sanity.py .           [ 52%]
tests/unit/integrations/test_autoresearch_client.py ...                  [ 53%]
tests/unit/interface/test_agent_api_fastapi_guard.py F                   [ 53%]
tests/unit/interface/test_agentapi_enhanced.py ...................       [ 53%]
tests/unit/interface/test_agentapi_enhanced_bridge.py ssss               [ 53%]
tests/unit/interface/test_agentapi_rate_limit_progress.py ssss           [ 53%]
tests/unit/interface/test_api_endpoints.py ..F                           [ 54%]
tests/unit/interface/test_cli_components.py .                            [ 54%]
tests/unit/interface/test_cli_progress_indicator.py ...                  [ 54%]
tests/unit/interface/test_cli_prompt_toolkit_bridge.py ...               [ 54%]
tests/unit/interface/test_cli_uxbridge_noninteractive.py .F              [ 54%]
tests/unit/interface/test_command_output.py ....                         [ 54%]
tests/unit/interface/test_dpg_ui.py ...                                  [ 54%]
tests/unit/interface/test_enhanced_error_handler.py ..                   [ 54%]
tests/unit/interface/test_mvuu_dashboard.py ..........                   [ 55%]
tests/unit/interface/test_nicegui_bridge.py ....                         [ 55%]
tests/unit/interface/test_nicegui_webui.py ......                        [ 55%]
tests/unit/interface/test_output_formatter_command_options_fast.py ...   [ 55%]
tests/unit/interface/test_output_formatter_core_behaviors.py ........... [ 55%]
                                                                         [ 55%]
tests/unit/interface/test_output_formatter_error_rendering_fast.py ...   [ 55%]
tests/unit/interface/test_output_formatter_fallbacks.py ....             [ 56%]
tests/unit/interface/test_output_formatter_structured_fast.py .......... [ 56%]
.................                                                        [ 56%]
tests/unit/interface/test_output_sanitization.py F                       [ 57%]
tests/unit/interface/test_progress_helpers.py ..                         [ 57%]
tests/unit/interface/test_progress_utils.py ......                       [ 57%]
tests/unit/interface/test_prompt_toolkit_adapter.py ...                  [ 57%]
tests/unit/interface/test_research_telemetry.py ......                   [ 57%]
tests/unit/interface/test_textual_ux_bridge.py ....s                     [ 57%]
tests/unit/interface/test_ux_bridge_coverage.py .......                  [ 57%]
tests/unit/interface/test_uxbridge_aliases.py .F                         [ 58%]
tests/unit/interface/test_webui_behavior_checklist_fast.py FFFFFFFFFFFFF [ 58%]
FFFFFFFFFFFFF                                                            [ 58%]
tests/unit/interface/test_webui_bootstrap_fast.py FFF                    [ 59%]
tests/unit/interface/test_webui_bridge_aa_coverage.py FF                 [ 59%]
tests/unit/interface/test_webui_bridge_cli_parity.py .                   [ 59%]
tests/unit/interface/test_webui_bridge_fast_suite.py FF.......           [ 59%]
tests/unit/interface/test_webui_bridge_handshake.py ....FF...            [ 59%]
tests/unit/interface/test_webui_bridge_normalize.py ....                 [ 59%]
tests/unit/interface/test_webui_bridge_progress.py .F..F....             [ 60%]
tests/unit/interface/test_webui_bridge_require_streamlit.py ..           [ 60%]
tests/unit/interface/test_webui_bridge_routing.py .....                  [ 60%]
tests/unit/interface/test_webui_bridge_spec_alignment.py .F.F..          [ 60%]
tests/unit/interface/test_webui_bridge_state_fast.py .F.                 [ 60%]
tests/unit/interface/test_webui_bridge_targeted.py ......F.              [ 60%]
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py ....... [ 61%]
                                                                         [ 61%]
tests/unit/interface/test_webui_commands.py F..........                  [ 61%]
tests/unit/interface/test_webui_dashboard_toggles_fast.py EE             [ 61%]
tests/unit/interface/test_webui_display_and_layout.py FEEEEEEEEEEEEEEEEE [ 62%]
EEEEEEEEEEEEEEEEEEEE                                                     [ 62%]
tests/unit/interface/test_webui_display_guidance.py EEEE                 [ 63%]
tests/unit/interface/test_webui_enhanced.py FFFFFFFFF                    [ 63%]
tests/unit/interface/test_webui_handle_command_errors.py EEEEEEE         [ 63%]
tests/unit/interface/test_webui_layout_and_display_branching.py FFFFFFFF [ 63%]
FFFFFF                                                                   [ 64%]
tests/unit/interface/test_webui_layout_and_messaging.py EEEEEEEEEEEEEEEE [ 64%]
EEE                                                                      [ 64%]
tests/unit/interface/test_webui_lazy_loader_fast.py FFF                  [ 64%]
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py EFEE   [ 64%]
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py FE..        [ 65%]
tests/unit/interface/test_webui_progress.py FFFFF                        [ 65%]
tests/unit/interface/test_webui_progress_cascade_fast.py EEEEEE          [ 65%]
tests/unit/interface/test_webui_progress_time.py s                       [ 65%]
tests/unit/interface/test_webui_rendering.py ....F..F..F..F.......F..... [ 66%]
.                                                                        [ 66%]
tests/unit/interface/test_webui_rendering_module.py sss                  [ 66%]
tests/unit/interface/test_webui_rendering_progress.py ..                 [ 66%]
tests/unit/interface/test_webui_require_streamlit.py FF                  [ 66%]
tests/unit/interface/test_webui_requirements_wizard.py FFFFFFFF          [ 66%]
tests/unit/interface/test_webui_routing.py .......                       [ 67%]
tests/unit/interface/test_webui_run_edge_cases.py FFFFFFF.               [ 67%]
tests/unit/interface/test_webui_run_fast.py E                            [ 67%]
tests/unit/interface/test_webui_simulations_fast.py FFFF.F.              [ 67%]
tests/unit/interface/test_webui_state_errors.py .                        [ 67%]
tests/unit/interface/test_webui_streamlit_free_progress_fast.py EEE      [ 67%]
tests/unit/interface/test_webui_streamlit_free_regressions.py F.FFFF.... [ 68%]
........F.                                                               [ 68%]
tests/unit/interface/test_webui_streamlit_stub.py EFEEEE                 [ 68%]
tests/unit/interface/test_webui_targeted_branches.py EEEEEEEEE           [ 69%]
tests/unit/interface/webui/test_rendering.py ....F..F..F..F.......F..... [ 69%]
.                                                                        [ 69%]
tests/unit/llm/test_lmstudio_provider.py .....FFF.F.....FFFFF            [ 70%]
tests/unit/llm/test_openai_provider.py ..sF..FF.........FFFFF            [ 71%]
tests/unit/llm/test_openrouter_provider.py FFFFFFFFFFFFFFFFFFFFFF        [ 72%]
tests/unit/logging/test_logging_setup.py .......................         [ 72%]
tests/unit/logging/test_logging_setup_additional_paths.py .......        [ 73%]
tests/unit/logging/test_logging_setup_branches.py .....                  [ 73%]
tests/unit/logging/test_logging_setup_configuration.py ...               [ 73%]
tests/unit/logging/test_logging_setup_configure_logging.py .....F...F    [ 73%]
tests/unit/logging/test_logging_setup_contexts.py ....                   [ 73%]
tests/unit/logging/test_logging_setup_invariants.py ......               [ 74%]
tests/unit/logging/test_logging_setup_levels.py ...                      [ 74%]
tests/unit/logging/test_logging_setup_retention.py F...FF                [ 74%]
tests/unit/memory/test_issue3_regression_guard.py .                      [ 74%]
tests/unit/memory/test_layered_cache.py ...                              [ 74%]
tests/unit/memory/test_layered_cache_runtime_protocol.py ...             [ 74%]
tests/unit/memory/test_sync_manager_protocol.py F.FFFFFF.F               [ 74%]
tests/unit/memory/test_sync_manager_protocol_runtime.py ...........      [ 75%]
tests/unit/memory/test_sync_manager_transaction_failure.py .             [ 75%]
tests/unit/memory/test_transaction_lifecycle_failures.py ...             [ 75%]
tests/unit/methodology/edrr/test_reasoning_loop.py .........             [ 75%]
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py . [ 75%]
......                                                                   [ 76%]
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py . [ 76%]
.............                                                            [ 76%]
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py ........ [ 76%]
                                                                         [ 76%]
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py ..    [ 76%]
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py .........  [ 77%]
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py ...       [ 77%]
tests/unit/methodology/edrr/test_reasoning_loop_retry.py .........       [ 77%]
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py ....       [ 77%]
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py ..     [ 77%]
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py ....... [ 77%]
                                                                         [ 77%]
tests/unit/methodology/test_adhoc_adapter.py ..                          [ 78%]
tests/unit/methodology/test_dialectical_reasoner_termination.py ..       [ 78%]
tests/unit/methodology/test_dialectical_reasoning.py EEE                 [ 78%]
tests/unit/methodology/test_dialectical_reasoning_loop.py FFF            [ 78%]
tests/unit/methodology/test_edrr_coordinator.py .E                       [ 78%]
tests/unit/methodology/test_kanban_adapter.py ..                         [ 78%]
tests/unit/methodology/test_milestone_adapter.py ..                      [ 78%]
tests/unit/methodology/test_reasoning_loop_time_budget.py F              [ 78%]
tests/unit/methodology/test_sprint_adapter.py ...F...                    [ 78%]
tests/unit/methodology/test_sprint_hooks.py FF                           [ 78%]
tests/unit/orchestration/test_graph_transitions_and_controls.py F.FFF    [ 79%]
tests/unit/policies/test_verify_security_policy.py ..                    [ 79%]
tests/unit/providers/test_provider_contract.py .                         [ 79%]
tests/unit/providers/test_provider_stub_offline.py F                     [ 79%]
tests/unit/providers/test_provider_system_additional.py .........        [ 79%]
tests/unit/providers/test_provider_system_branches.py .................. [ 80%]
....................                                                     [ 80%]
tests/unit/providers/test_resource_gating_meta.py ..                     [ 80%]
tests/unit/requirements/test_dialectical_reasoner_determinism.py ..F..   [ 80%]
tests/unit/retrieval/test_backend_gating_smoke.py sss.                   [ 81%]
tests/unit/scripts/test_analyze_test_dependencies.py ........F...        [ 81%]
tests/unit/scripts/test_audit_testing_scripts.py .............           [ 81%]
tests/unit/scripts/test_auto_issue_comment.py ...                        [ 82%]
tests/unit/scripts/test_benchmark_test_execution.py .F.F......           [ 82%]
tests/unit/scripts/test_check_internal_links.py ..                       [ 82%]
tests/unit/scripts/test_enhanced_test_parser.py ...                      [ 82%]
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py F          [ 82%]
tests/unit/scripts/test_examples_smoke_script.py ..                      [ 82%]
tests/unit/scripts/test_find_syntax_errors.py FF                         [ 82%]
tests/unit/scripts/test_gen_ref_pages.py .                               [ 82%]
tests/unit/scripts/test_generate_quality_report.py .....FF.              [ 83%]
tests/unit/scripts/test_run_all_tests_wrapper.py ...                     [ 83%]
tests/unit/scripts/test_security_ops.py ....                             [ 83%]
tests/unit/scripts/test_security_scan_script.py .                        [ 83%]
tests/unit/scripts/test_verify_coverage_threshold.py ..                  [ 83%]
tests/unit/scripts/test_verify_mvuu_references.py ....                   [ 83%]
tests/unit/scripts/test_verify_release_state.py ...........              [ 83%]
tests/unit/scripts/test_verify_test_markers.py .F....                    [ 84%]
tests/unit/scripts/test_verify_test_markers_cli.py ..                    [ 84%]
tests/unit/scripts/test_verify_test_markers_cross_check.py .             [ 84%]
tests/unit/scripts/test_wsde_edrr_simulation.py .                        [ 84%]
tests/unit/security/test_api_authentication.py .....                     [ 84%]
tests/unit/security/test_auth_and_encryption_defaults.py .....           [ 84%]
tests/unit/security/test_authentication_optional_dependency.py .         [ 84%]
tests/unit/security/test_authorization_checks.py ..                      [ 84%]
tests/unit/security/test_deployment_coverage.py ......                   [ 84%]
tests/unit/security/test_encryption.py ..........                        [ 85%]
tests/unit/security/test_logging_redaction.py ..                         [ 85%]
tests/unit/security/test_memory_encryption.py ...                        [ 85%]
tests/unit/security/test_policy_audit.py FF                              [ 85%]
tests/unit/security/test_review.py ...                                   [ 85%]
tests/unit/security/test_sanitization.py ...........                     [ 85%]
tests/unit/security/test_security_audit.py ....F                         [ 86%]
tests/unit/security/test_security_audit_cmd.py ...                       [ 86%]
tests/unit/security/test_security_flags_env.py ......                    [ 86%]
tests/unit/security/test_tls_config.py ..............                    [ 86%]
tests/unit/security/test_validation.py ................................. [ 87%]
.....                                                                    [ 88%]
tests/unit/specifications/test_mvuu_config_schema_validation.py F        [ 88%]
tests/unit/test_cli.py ..s.ss.                                           [ 88%]
tests/unit/test_sentinel_speed_markers.py .                              [ 88%]
tests/unit/test_simple_addition.py ...                                   [ 88%]
tests/unit/test_verify_test_organization_sentinel.py .                   [ 88%]
tests/unit/testing/test_collect_behavior_fallback.py F                   [ 88%]
tests/unit/testing/test_collect_cache_sanitize.py .F                     [ 88%]
tests/unit/testing/test_collect_synthesize_on_empty.py F                 [ 88%]
tests/unit/testing/test_collect_tests_cache_bad_json.py F                [ 88%]
tests/unit/testing/test_collect_tests_cache_invalidation.py FFF          [ 88%]
tests/unit/testing/test_collect_tests_cache_ttl.py FF                    [ 88%]
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py FFF [ 88%]
F                                                                        [ 89%]
tests/unit/testing/test_collect_tests_with_cache_fallback.py FF          [ 89%]
tests/unit/testing/test_coverage_segmentation_simulation.py ..           [ 89%]
tests/unit/testing/test_deterministic_seed_fixture.py .                  [ 89%]
tests/unit/testing/test_env_ttl_and_sanitize.py ..                       [ 89%]
tests/unit/testing/test_failure_tips.py .                                [ 89%]
tests/unit/testing/test_html_report_artifacts.py F                       [ 89%]
tests/unit/testing/test_mutation_testing.py ....................F        [ 90%]
tests/unit/testing/test_run_tests.py ..F.F                               [ 90%]
tests/unit/testing/test_run_tests_additional_coverage.py .......F        [ 90%]
tests/unit/testing/test_run_tests_additional_error_paths.py FFF          [ 90%]
tests/unit/testing/test_run_tests_artifacts.py .......F.F                [ 90%]
tests/unit/testing/test_run_tests_benchmark_warning.py F                 [ 90%]
tests/unit/testing/test_run_tests_cache_prune_and_tips.py .F             [ 91%]
tests/unit/testing/test_run_tests_cache_pruning.py F                     [ 91%]
tests/unit/testing/test_run_tests_cli_helpers_focus.py .FFFF.            [ 91%]
tests/unit/testing/test_run_tests_cli_invocation.py FF.F.FF...FFF        [ 91%]
tests/unit/testing/test_run_tests_collection_cache.py .                  [ 91%]
tests/unit/testing/test_run_tests_coverage_artifacts.py ....F..          [ 91%]
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py .      [ 91%]
tests/unit/testing/test_run_tests_coverage_short_circuit.py .            [ 92%]
tests/unit/testing/test_run_tests_coverage_status.py ......              [ 92%]
tests/unit/testing/test_run_tests_coverage_uplift.py ...                 [ 92%]
tests/unit/testing/test_run_tests_extra.py FF                            [ 92%]
tests/unit/testing/test_run_tests_extra_marker.py FF                     [ 92%]
tests/unit/testing/test_run_tests_extra_marker_passthrough.py F          [ 92%]
tests/unit/testing/test_run_tests_extra_paths.py FFF                     [ 92%]
tests/unit/testing/test_run_tests_failure_tips.py F                      [ 92%]
tests/unit/testing/test_run_tests_keyword_exec.py .                      [ 92%]
tests/unit/testing/test_run_tests_keyword_filter.py FF                   [ 92%]
tests/unit/testing/test_run_tests_keyword_filter_empty.py F              [ 92%]
tests/unit/testing/test_run_tests_logic.py ..FFFFF                       [ 92%]
tests/unit/testing/test_run_tests_main_function.py ...........           [ 93%]
tests/unit/testing/test_run_tests_main_logic.py ...F.F..FF.FFFFF.FF      [ 94%]
tests/unit/testing/test_run_tests_marker_fallback.py ..                  [ 94%]
tests/unit/testing/test_run_tests_marker_merge.py ..                     [ 94%]
tests/unit/testing/test_run_tests_module.py .FFFFFF.F..                  [ 94%]
tests/unit/testing/test_run_tests_no_xdist_assertions.py F               [ 94%]
tests/unit/testing/test_run_tests_option_parsing.py ...                  [ 94%]
tests/unit/testing/test_run_tests_orchestration.py .FFFF..               [ 94%]
tests/unit/testing/test_run_tests_parallel_flags.py F                    [ 94%]
tests/unit/testing/test_run_tests_parallel_no_cov.py F                   [ 94%]
tests/unit/testing/test_run_tests_plugin_env.py ....                     [ 95%]
tests/unit/testing/test_run_tests_plugin_timeouts.py F.                  [ 95%]
tests/unit/testing/test_run_tests_pytest_cov_plugin.py ........          [ 95%]
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py ......F          [ 95%]
tests/unit/testing/test_run_tests_report.py F                            [ 95%]
tests/unit/testing/test_run_tests_returncode5_success.py F               [ 95%]
tests/unit/testing/test_run_tests_sanitize_node_ids.py ...               [ 95%]
tests/unit/testing/test_run_tests_segmentation.py F                      [ 95%]
tests/unit/testing/test_run_tests_segmentation_helpers.py ....FF......   [ 96%]
tests/unit/testing/test_run_tests_segmented.py ...                       [ 96%]
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py F [ 96%]
                                                                         [ 96%]
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py F       [ 96%]
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py F          [ 96%]
tests/unit/testing/test_run_tests_segmented_failure_paths.py .F          [ 96%]
tests/unit/testing/test_run_tests_segmented_failures.py ....             [ 96%]
tests/unit/testing/test_run_tests_segmented_orchestration.py ...         [ 96%]
tests/unit/testing/test_run_tests_segmented_report_flag.py .             [ 96%]
tests/unit/testing/test_run_tests_speed_keyword_loop.py .                [ 96%]
tests/unit/testing/test_run_tests_speed_selection.py ....                [ 96%]
tests/unit/testing/test_sanitize_node_ids.py ..                          [ 97%]
tests/unit/testing/test_sanitize_node_ids_minimal.py .                   [ 97%]
tests/unit/utils/test_logging_coverage.py ......                         [ 97%]
tests/unit/utils/test_logging_final_coverage.py .......                  [ 97%]
tests/unit/utils/test_logging_utils.py ...                               [ 97%]
tests/unit/utils/test_serialization.py ...                               [ 97%]
tests/unit/utils/test_serialization_coverage.py ..........               [ 98%]
tests/unit/utils/test_serialization_edges.py ..                          [ 98%]
tests/unit/utils/test_serialization_extra.py ...                         [ 98%]
tests/unit/utils/test_serialization_final_coverage.py ..........         [ 98%]
tests/integration/agents/test_generation/test_run_generated_tests.py ..  [ 98%]
tests/integration/agents/test_generation/test_scaffold_generation.py ..  [ 98%]
tests/integration/api/test_api_startup.py FF                             [ 98%]
tests/integration/deployment/test_compose_workflow.py ...                [ 98%]
tests/integration/deployment/test_deployment_scripts.py .....            [ 99%]
tests/integration/general/test_complex_workflow.py .                     [ 99%]
tests/integration/general/test_end_to_end_workflow.py .                  [ 99%]
tests/integration/general/test_lmstudio_integration_regression.py ssssss [ 99%]
s                                                                        [ 99%]
tests/integration/generated/test_generated_module.py s                   [ 99%]
tests/integration/generated/test_run_generated_tests.py ..               [ 99%]
tests/integration/llm/test_lmstudio_timing_baseline.py s                 [ 99%]
tests/integration/mvu/test_command_execution.py ..                       [ 99%]
tests/integration/utils/test_logging_integration.py ..                   [ 99%]
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py ssss     [ 99%]
tests/behavior/steps/test_webui_synthesis_steps.py ss                    [ 99%]
tests/behavior/test_documentation_generation.py s                        [ 99%]
tests/behavior/test_marker_auto_injection_guardrail.py s                 [ 99%]
tests/behavior/test_progress_failover_and_recursion.py ssss              [100%]

==================================== ERRORS ====================================
________________ ERROR at setup of test_cli_marker_passthrough _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c405640>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage 
helpers."""
    
        app, module = build_minimal_cli_app(monkeypatch)
    
        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
___________ ERROR at setup of test_cli_feature_flags_set_environment ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c4041d0>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage 
helpers."""
    
        app, module = build_minimal_cli_app(monkeypatch)
    
        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
_________ ERROR at setup of test_cli_segmentation_arguments_forwarded __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c4051f0>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage 
helpers."""
    
        app, module = build_minimal_cli_app(monkeypatch)
    
        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
____________ ERROR at setup of test_cli_inventory_mode_exports_json ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c404920>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage 
helpers."""
    
        app, module = build_minimal_cli_app(monkeypatch)
    
        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
___________ ERROR at setup of test_cli_failure_propagates_exit_code ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c404ec0>

    @pytest.fixture
    def cli_app(
        monkeypatch: pytest.MonkeyPatch,
    ) -> tuple[ModuleType, Typer, RecordingBridge]:
        """Load the CLI command with a stub bridge and patched coverage 
helpers."""
    
        app, module = build_minimal_cli_app(monkeypatch)
    
        bridge = RecordingBridge()
>       monkeypatch.setattr(module, "bridge", bridge)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'bridge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_focus.py:56: AttributeError
_____ ERROR at setup of test_inner_test_env_tightening_forces_no_parallel ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c5d96d0>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        # Ensure a clean slate for env vars we mutate
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_env_paths.py:31: AttributeError
_ ERROR at setup of 
test_unit_tests_sets_allow_requests_by_default_and_respects_existing _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c105a30>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        # Ensure a clean slate for env vars we mutate
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_env_paths.py:31: AttributeError
_______ ERROR at setup of test_feature_flags_set_env_and_success_message _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c639220>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_features.py:11: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_marker_option_is_passed_as_extra_marker ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c101070>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_features.py:11: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____ ERROR at setup of test_inner_test_mode_disables_plugins_and_parallel _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc7ad0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inner_test.py:21: AttributeError
_______ ERROR at setup of test_inventory_mode_exports_json_and_skips_run _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc7080>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
______ ERROR at setup of test_inventory_mode_handles_collection_failures _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc6ae0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
__________ ERROR at setup of test_invalid_target_exits_with_help_text __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc6cc0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
_________ ERROR at setup of test_marker_option_is_forwarded_to_runner __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc7530>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory_and_validation.py:28: AttributeError
_______ ERROR at setup of test_marker_anding_passthrough_multiple_speeds _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc7980>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_markers.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_invalid_marker_expression_exits_cleanly ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc4e60>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_markers.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ ERROR at setup of test_speed_and_marker_forwarding ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e810>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_________ ERROR at setup of test_report_true_prints_output_and_success _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e150>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_____________ ERROR at setup of test_observability_and_error_path ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72cb30>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "PYTEST_DISABLE_PLUGIN_AUTOLOAD",
            "PYTEST_ADDOPTS",
            "DEVSYNTH_TEST_TIMEOUT_SECONDS",
            "DEVSYNTH_INNER_TEST",
            "DEVSYNTH_TEST_ALLOW_REQUESTS",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
            "DEVSYNTH_FEATURE_EXPA",
            "DEVSYNTH_FEATURE_FEATURE_B",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_more.py:31: AttributeError
_______ ERROR at setup of test_provider_defaults_are_applied_when_unset ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72dfa0>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_provider_defaults.py:26: AttributeError
______ ERROR at setup of test_provider_defaults_do_not_override_existing _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f200>

    @pytest.fixture(autouse=True)
    def _clean_env(monkeypatch: pytest.MonkeyPatch):
        keys = [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]
        for k in keys:
            monkeypatch.delenv(k, raising=False)
>       monkeypatch.setattr(rtc, "enforce_coverage_threshold", lambda *a, **k: 
100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_provider_defaults.py:26: AttributeError
___ ERROR at setup of test_report_flag_with_missing_directory_prints_warning ___

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f890>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_smoke_mode_sets_env_and_disables_parallel _______

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72ec90>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________________ ERROR at setup of test_no_parallel_maps_to_n0 _________________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bfa8140>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_emit_coverage_messages_reports_artifacts ________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b7620>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.enforce_coverage_th
reshold",
            lambda *a, **k: 100.0,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_path.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'enforce_coverage_threshold'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______ ERROR at setup of test_run_tests_cli_report_option_forwards_true _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b6360>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_reporting_and_env.py:21: AttributeError
_____ ERROR at setup of test_run_tests_cmd_respects_explicit_provider_env ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b7800>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_reporting_and_env.py:21: AttributeError
__ ERROR at setup of test_smoke_mode_sets_pytest_disable_plugin_autoload_env ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6862a0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:18: AttributeError
___ ERROR at setup of test_smoke_mode_skips_coverage_gate_when_cov_disabled ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6849e0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:18: AttributeError
_______ ERROR at setup of test_smoke_mode_cli_imports_fastapi_testclient _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c684860>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:18: AttributeError
___ ERROR at setup of test_smoke_mode_skips_coverage_gate_when_instrumented ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c686fc0>

    @pytest.fixture(autouse=True)
    def _patch_coverage_helper(monkeypatch: pytest.MonkeyPatch) -> None:
>       monkeypatch.setattr(module, "enforce_coverage_threshold", lambda *a, 
**k: 100.0)
E       AttributeError: <function run_tests_cmd at 0x10aba4720> has no attribute
'enforce_coverage_threshold'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_smoke.py:18: AttributeError
__________ ERROR at setup of test_setup_wizard_instantiation_succeeds __________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6879b0>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
________ ERROR at setup of test_wizard_prompts_via_cli_bridge_succeeds _________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c686f30>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ ERROR at setup of test_setup_wizard_run_succeeds _______________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c60f200>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ ERROR at setup of test_setup_wizard_abort_succeeds ______________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c60ce60>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____ ERROR at setup of test_prompt_features_uses_prompt_toolkit_multiselect ____

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c60d700>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________ ERROR at setup of test_setup_wizard_accepts_typed_inputs ___________

name = 'setup_wizard'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c60e510>

    @pytest.fixture(autouse=True)
    def disable_prompt_toolkit(monkeypatch):
        """Disable prompt-toolkit integration for legacy wizard tests."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.setup_wizard.get_prompt_toolkit_adapter", 
lambda: None
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_setup_wizard.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'setup_wizard', ann = 'devsynth.application.cli.setup_wizard'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.setup_wizard has no attribute 'setup_wizard'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____ ERROR at setup of TestExecutionTrajectoryCollector.test_initialization ____

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e13500>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
_ ERROR at setup of TestExecutionTrajectoryCollector.test_analyze_code_structure
_

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e13590>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
_ ERROR at setup of 
TestExecutionTrajectoryCollector.test_extract_execution_patterns _

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e13aa0>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
_ ERROR at setup of 
TestExecutionTrajectoryCollector.test_create_memetic_units_from_trajectories _

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e28500>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
_ ERROR at setup of TestExecutionTrajectoryCollector.test_get_execution_insights
_

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e28a10>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
_ ERROR at setup of 
TestExecutionTrajectoryCollector.test_validate_trajectory_quality _

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
TrajectoryCollector object at 0x118e28f20>

    def setup_method(self):
        """Set up test fixtures."""
>       self.collector = ExecutionTrajectoryCollector(sandbox_enabled=False)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ExecutionTrajectoryCollector' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:255: NameError
____ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_initialization _____

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1190949b0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,944 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,944 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_process_complex_query _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119094e30>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,952 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,952 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
__ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_parse_query_intent ___

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119083950>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,961 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,961 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
___ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_extract_entities ____

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119094f50>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,970 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,970 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_extract_relationships _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119095bb0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,979 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,979 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
_ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_calculate_required_hops
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1190960c0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,988 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,988 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
___ ERROR at setup of TestEnhancedGraphRAGQueryEngine.test_resolve_entities ____

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x1190965d0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,997 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,997 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_plan_multi_hop_traversal _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119096ae0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:30,006 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:30,006 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
_ ERROR at setup of 
TestEnhancedGraphRAGQueryEngine.test_execute_semantic_traversal _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestEnhancedGraphR
AGQueryEngine object at 0x119096ff0>

    def setup_method(self):
        """Set up test fixtures."""
        self.enhanced_graph = EnhancedKnowledgeGraph()
        self.execution_learning = Mock()
>       self.query_engine = EnhancedGraphRAGQueryEngine(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            self.enhanced_graph, self.execution_learning
        )
E       NameError: name 'EnhancedGraphRAGQueryEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:396: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:30,015 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:30,015 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
______ ERROR at setup of TestAutomataSynthesisEngine.test_initialization _______

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x119097200>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_ ERROR at setup of 
TestAutomataSynthesisEngine.test_synthesize_automata_from_exploration _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x119097680>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_ ERROR at setup of TestAutomataSynthesisEngine.test_generate_task_segmentation 
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x11909c0e0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_ ERROR at setup of TestAutomataSynthesisEngine.test_validate_automata_quality _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x11909c5f0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_ ERROR at setup of 
TestAutomataSynthesisEngine.test_create_memetic_units_from_automata _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x11909cb00>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_ ERROR at setup of 
TestAutomataSynthesisEngine.test_get_task_segmentation_for_query _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestAutomataSynthe
sisEngine object at 0x11909d010>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.automata_engine = AutomataSynthesisEngine(self.execution_learning)
                               ^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'AutomataSynthesisEngine' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:564: NameError
_______ ERROR at setup of TestHybridLLMArchitecture.test_initialization ________

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x11909d250>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
_ ERROR at setup of 
TestHybridLLMArchitecture.test_process_complex_reasoning_task _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x119082f00>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
_ ERROR at setup of TestHybridLLMArchitecture.test_get_optimal_provider_for_task
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x11909d220>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
_ ERROR at setup of 
TestHybridLLMArchitecture.test_benchmark_hybrid_vs_individual _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x11909d9a0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
________ ERROR at setup of TestHybridLLMArchitecture.test_add_provider _________

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x11909deb0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
_ ERROR at setup of TestHybridLLMArchitecture.test_get_architecture_statistics _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestHybridLLMArchi
tecture object at 0x11909e3c0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.hybrid_llm = HybridLLMArchitecture(self.execution_learning)
                          ^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'HybridLLMArchitecture' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:686: NameError
____ ERROR at setup of TestMetacognitiveTrainingSystem.test_initialization _____

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x11909eb10>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_start_think_aloud_session _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x11909ef90>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of TestMetacognitiveTrainingSystem.test_record_verbalization __

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x11909f4a0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of TestMetacognitiveTrainingSystem.test_end_think_aloud_session
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x11909f9b0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_get_metacognitive_insights _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1190a8410>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_apply_metacognitive_improvements _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1190a8920>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_ ERROR at setup of 
TestMetacognitiveTrainingSystem.test_generate_self_monitoring_report _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestMetacognitiveT
rainingSystem object at 0x1190a8e30>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.metacognitive_system = 
MetacognitiveTrainingSystem(self.execution_learning)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'MetacognitiveTrainingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:804: NameError
_____ ERROR at setup of TestContextualPromptingSystem.test_initialization ______

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190a9010>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of TestContextualPromptingSystem.test_create_contextual_prompt 
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190a9490>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of 
TestContextualPromptingSystem.test_engineer_contextual_prompt _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x11909d1f0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of TestContextualPromptingSystem.test_add_behavioral_directive 
_

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190a8fe0>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of 
TestContextualPromptingSystem.test_add_environmental_constraint _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190a9c70>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of 
TestContextualPromptingSystem.test_get_prompt_performance_analytics _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190aa180>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of 
TestContextualPromptingSystem.test_create_agent_specific_prompt _

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestContextualProm
ptingSystem object at 0x1190aa690>

    def setup_method(self):
        """Set up test fixtures."""
        self.execution_learning = Mock()
>       self.prompting_system = 
ContextualPromptingSystem(self.execution_learning)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
E       NameError: name 'ContextualPromptingSystem' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:933: NameError
_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_unit _

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category0/test_project/tests/behavior/feat
ures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_by_category0/test_project/tests/behavior/features/examp
le.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_integration _

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category1')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category1/test_project/tests/behavior/feat
ures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_by_category1/test_project/tests/behavior/features/examp
le.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of 
TestEnhancedTestCollector.test_collect_tests_by_category_behavior _

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category2')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_by_category2/test_project/tests/behavior/feat
ures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_by_category2/test_project/tests/behavior/features/examp
le.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_ ERROR at setup of TestEnhancedTestCollector.test_collect_tests_all_categories 
_

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_all_categor0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_all_categor0/test_project/tests/behavior/feat
ures/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_all_categor0/test_project/tests/behavior/features/examp
le.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
___ ERROR at setup of TestEnhancedTestCollector.test_get_tests_with_markers ____

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_get_tests_with_markers0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_get_tests_with_markers0/test_project/tests/behavior/feature
s/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_get_tests_with_markers0/test_project/tests/behavior/features/example.
feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
____ ERROR at setup of TestEnhancedTestCollector.test_caching_functionality ____

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_caching_functionality0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_caching_functionality0/test_project/tests/behavior/features
/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_caching_functionality0/test_project/tests/behavior/features/example.f
eature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_force_refresh_cache _____

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_force_refresh_cache0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_force_refresh_cache0/test_project/tests/behavior/features/e
xample.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_force_refresh_cache0/test_project/tests/behavior/features/example.fea
ture'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_memory_integration ______

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_memory_integration0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_memory_integration0/test_project/tests/behavior/features/ex
ample.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_memory_integration0/test_project/tests/behavior/features/example.feat
ure'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_is_valid_test_file ______

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_is_valid_test_file0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_is_valid_test_file0/test_project/tests/behavior/features/ex
ample.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_is_valid_test_file0/test_project/tests/behavior/features/example.feat
ure'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_____ ERROR at setup of TestEnhancedTestCollector.test_contains_test_code ______

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_contains_test_code0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_contains_test_code0/test_project/tests/behavior/features/ex
ample.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_contains_test_code0/test_project/tests/behavior/features/example.feat
ure'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_______ ERROR at setup of TestEnhancedTestCollector.test_test_has_marker _______

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_test_has_marker0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_test_has_marker0/test_project/tests/behavior/features/examp
le.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_test_has_marker0/test_project/tests/behavior/features/example.feature
'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
_______ ERROR at setup of TestEnhancedTestCollector.test_analyze_markers _______

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_analyze_markers0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_analyze_markers0/test_project/tests/behavior/features/examp
le.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_analyze_markers0/test_project/tests/behavior/features/example.feature
'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
__ ERROR at setup of TestEnhancedTestCollector.test_store_collection_results ___

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_store_collection_results0')

    @pytest.fixture
    def temp_project(tmp_path):
        """Create a temporary test project."""
        project_dir = tmp_path / "test_project"
    
        # Create test directories
        (project_dir / "tests" / "unit").mkdir(parents=True)
        (project_dir / "tests" / "integration").mkdir(parents=True)
        (project_dir / "tests" / "behavior").mkdir(parents=True)
    
        # Create unit tests
        (project_dir / "tests" / "unit" / "test_example.py").write_text(
            'import pytest\n\n@pytest.mark.fast\ndef test_example():\n    
"""Test example function."""\n    assert 1 + 1 == 2\n\n@pytest.mark.medium\ndef 
test_another_example():\n    """Test another function."""\n    assert True is 
True'
        )
    
        # Create integration tests
        (project_dir / "tests" / "integration" / 
"test_integration.py").write_text(
            'import pytest\n\n@pytest.mark.slow\ndef test_integration():\n    
"""Test integration functionality."""\n    assert "integration" in "integration 
test"'
        )
    
        # Create behavior tests
>       (project_dir / "tests" / "behavior" / "features" / 
"example.feature").write_text(
            "Feature: Example Feature\n  Scenario: Example scenario\n    Given 
something\n    When I do something\n    Then something should happen"
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1047: in write_text
    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) 
as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_store_collection_results0/test_project/tests/behavior/featu
res/example.feature')
mode = 'w', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed to by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return io.open(self, mode, buffering, encoding, errors, newline)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_store_collection_results0/test_project/tests/behavior/features/exampl
e.feature'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1013: FileNotFoundError
________________ ERROR at setup of test_ports_fixtures_succeeds ________________

fixturedef = <FixtureDef argname='llm_port' scope='function' baseid=''>
request = <SubRequest 'llm_port' for <Function test_ports_fixtures_succeeds>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict 
mode
                # This applies to pytest_trio fixtures, for example
                return (yield)
            if not _is_coroutine_or_asyncgen(fixturedef.func):
>               return (yield)
                        ^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/pytest_asyncio/plugin.py:735: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @pytest.fixture
    def llm_port():
        """Provide an LLMPort using the MockLLMAdapter."""
>       port = LLMPort(_MockProviderFactory())
                       ^^^^^^^^^^^^^^^^^^^^
E       NameError: name '_MockProviderFactory' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/ports.py:92:
NameError
_____ ERROR at setup of test_webui_layout_breakpoints_toggle_between_modes _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36fc80>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) -> 
Iterator[tuple[object, object]]:
        """Reload ``devsynth.interface.webui`` with a deterministic Streamlit 
stub."""
    
        st = _mock_streamlit()
        st.markdown.reset_mock()
        st.error.reset_mock()
    
        stub_argon2 = ModuleType("argon2")
        stub_argon2.PasswordHasher = MagicMock()
        stub_exceptions = ModuleType("argon2.exceptions")
    
        class _VerifyMismatchError(Exception):
            pass
    
        stub_exceptions.VerifyMismatchError = _VerifyMismatchError
        stub_argon2.exceptions = stub_exceptions
        monkeypatch.setitem(sys.modules, "argon2", stub_argon2)
        monkeypatch.setitem(sys.modules, "argon2.exceptions", stub_exceptions)
    
        stub_config = ModuleType("devsynth.config")
        stub_config.load_project_config = MagicMock(return_value={})
        stub_config.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", stub_config)
    
        stub_yaml = ModuleType("yaml")
        stub_yaml.safe_dump = MagicMock(return_value="{}")
        monkeypatch.setitem(sys.modules, "yaml", stub_yaml)
    
        stub_output_formatter = 
ModuleType("devsynth.interface.output_formatter")
    
        class _Formatter:
            def __init__(self, *_args, **_kwargs) -> None:
                pass
    
            def format_message(
                self,
                message: str,
                *,
                message_type: str | None = None,
                highlight: bool = False,
            ) -> str:
                return message
    
        stub_output_formatter.OutputFormatter = _Formatter
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.output_formatter", 
stub_output_formatter
        )
    
        stub_shared_bridge = ModuleType("devsynth.interface.shared_bridge")
    
        class _SharedBridgeMixin:
            def __init__(self, *args, **kwargs) -> None:
                super().__init__(*args, **kwargs)  # type: ignore[misc]
                self.formatter = _Formatter(None)
    
            def _format_for_output(
                self,
                message: str,
                *,
                highlight: bool = False,
                message_type: str | None = None,
            ) -> str:
                return message
    
        stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.shared_bridge", stub_shared_bridge
        )
    
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_dashboard_toggles_fast.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_webui_error_guidance_surfaces_suggestions_and_docs ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1b0d40>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch) -> 
Iterator[tuple[object, object]]:
        """Reload ``devsynth.interface.webui`` with a deterministic Streamlit 
stub."""
    
        st = _mock_streamlit()
        st.markdown.reset_mock()
        st.error.reset_mock()
    
        stub_argon2 = ModuleType("argon2")
        stub_argon2.PasswordHasher = MagicMock()
        stub_exceptions = ModuleType("argon2.exceptions")
    
        class _VerifyMismatchError(Exception):
            pass
    
        stub_exceptions.VerifyMismatchError = _VerifyMismatchError
        stub_argon2.exceptions = stub_exceptions
        monkeypatch.setitem(sys.modules, "argon2", stub_argon2)
        monkeypatch.setitem(sys.modules, "argon2.exceptions", stub_exceptions)
    
        stub_config = ModuleType("devsynth.config")
        stub_config.load_project_config = MagicMock(return_value={})
        stub_config.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", stub_config)
    
        stub_yaml = ModuleType("yaml")
        stub_yaml.safe_dump = MagicMock(return_value="{}")
        monkeypatch.setitem(sys.modules, "yaml", stub_yaml)
    
        stub_output_formatter = 
ModuleType("devsynth.interface.output_formatter")
    
        class _Formatter:
            def __init__(self, *_args, **_kwargs) -> None:
                pass
    
            def format_message(
                self,
                message: str,
                *,
                message_type: str | None = None,
                highlight: bool = False,
            ) -> str:
                return message
    
        stub_output_formatter.OutputFormatter = _Formatter
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.output_formatter", 
stub_output_formatter
        )
    
        stub_shared_bridge = ModuleType("devsynth.interface.shared_bridge")
    
        class _SharedBridgeMixin:
            def __init__(self, *args, **kwargs) -> None:
                super().__init__(*args, **kwargs)  # type: ignore[misc]
                self.formatter = _Formatter(None)
    
            def _format_for_output(
                self,
                message: str,
                *,
                highlight: bool = False,
                message_type: str | None = None,
            ) -> str:
                return message
    
        stub_shared_bridge.SharedBridgeMixin = _SharedBridgeMixin
        monkeypatch.setitem(
            sys.modules, "devsynth.interface.shared_bridge", stub_shared_bridge
        )
    
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_dashboard_toggles_fast.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[500-expected0] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e6f90>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[800-expected1] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e52e0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[1200-expected2] _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e4470>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_get_layout_config_breakpoints[None-expected3] _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e4710>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______ ERROR at setup of test_display_result_renders_markup_and_sanitizes ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e4d40>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_display_result_highlight_uses_info ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c352c60>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_display_result_routes_message_types_and_plain_write __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c167cb0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______ ERROR at setup of test_display_result_error_suggestions_and_docs _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c166150>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_display_result_error_prefix_without_message_type ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c165970>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_display_result_heading_routes_to_header ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c306a50>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_display_result_additional_headings ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1129f0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[File not found: 
missing.yaml-file_not_found] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36fc80>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Permission denied when 
opening-permission_denied] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36d160>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Invalid parameter 
--foo-invalid_parameter] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36caa0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Invalid format 
provided-invalid_format] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36fdd0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Missing key 'api'-key_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c113c20>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Type error while 
casting-type_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bafa9c0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Configuration error 
detected-config_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13baf9670>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Connection error 
occurred-connection_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bafb1d0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_get_error_type_mappings[API error status-api_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13baf8410>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Validation error 
raised-validation_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13baf9100>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Syntax error unexpected 
token-syntax_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3abec0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_mappings[Import error for 
module-import_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c351310>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______ ERROR at setup of test_get_error_type_mappings[Unrelated message-] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e4110>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ ERROR at setup of test_error_helper_defaults _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1b02c0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_render_traceback_uses_expander _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3fbd40>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ ERROR at setup of test_format_error_message __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c18da60>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ensure_router_caches_instance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c18d910>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_run_configures_streamlit_and_router __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c18d4f0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_handles_page_config_error _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c18e6c0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_handles_components_error ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c165460>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ui_progress_updates_emit_eta ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bac5010>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ ERROR at setup of test_ui_progress_subtask_flow ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13baf8ad0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_webui_ensure_router_caches_instance __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c165580>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_webui_run_configures_layout_and_router _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bafaae0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ ERROR at setup of test_webui_run_handles_page_config_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bafafc0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_webui_run_handles_component_error ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1221e0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a lightweight Streamlit substitute."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_display_result_translates_markup_to_markdown ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36fb00>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation", 
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication", 
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")
    
        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str, 
object]]] = []
    
            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))
    
        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")
    
        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs
    
        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")
    
        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs
    
        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")
    
        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
    
        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")
    
        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs
    
        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")
    
        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] = 
[]
    
            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))
    
            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))
    
        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")
    
        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  # 
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj
    
        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_display_result_surfaces_guidance_for_file_errors ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c36f9e0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation", 
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication", 
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")
    
        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str, 
object]]] = []
    
            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))
    
        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")
    
        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs
    
        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")
    
        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs
    
        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")
    
        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
    
        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")
    
        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs
    
        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")
    
        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] = 
[]
    
            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))
    
            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))
    
        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")
    
        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  # 
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj
    
        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ ERROR at setup of test_display_result_highlights_information _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3fae70>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation", 
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication", 
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")
    
        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str, 
object]]] = []
    
            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))
    
        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")
    
        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs
    
        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")
    
        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs
    
        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")
    
        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
    
        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")
    
        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs
    
        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")
    
        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] = 
[]
    
            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))
    
            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))
    
        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")
    
        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  # 
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj
    
        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_ui_progress_tracks_status_and_subtasks _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3afb90>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch):
        streamlit_mod, state = _make_streamlit_stub()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_mod)
        monkeypatch.setitem(sys.modules, "chromadb", MagicMock())
        monkeypatch.setitem(sys.modules, "uvicorn", MagicMock())
        security_pkg = ModuleType("devsynth.security")
        security_pkg.__path__ = []  # type: ignore[attr-defined]
        validation_stub = ModuleType("devsynth.security.validation")
        validation_stub.parse_bool_env = lambda _name, default=True: default
        sanitization_stub = ModuleType("devsynth.security.sanitization")
        sanitization_stub.sanitize_input = lambda text: text
        auth_stub = ModuleType("devsynth.security.authentication")
        auth_stub.authenticate = MagicMock(return_value=True)
        auth_stub.hash_password = MagicMock(return_value="hash")
        auth_stub.verify_password = MagicMock(return_value=True)
        monkeypatch.setitem(sys.modules, "devsynth.security", security_pkg)
        monkeypatch.setitem(sys.modules, "devsynth.security.validation", 
validation_stub)
        monkeypatch.setitem(
            sys.modules, "devsynth.security.sanitization", sanitization_stub
        )
        monkeypatch.setitem(sys.modules, "devsynth.security.authentication", 
auth_stub)
        security_pkg.validation = validation_stub
        security_pkg.sanitization = sanitization_stub
        security_pkg.authentication = auth_stub
        config_stub = ModuleType("devsynth.config")
        config_stub.load_project_config = MagicMock(return_value={})
        config_stub.save_config = MagicMock()
        monkeypatch.setitem(sys.modules, "devsynth.config", config_stub)
        monkeypatch.setitem(sys.modules, "yaml", MagicMock())
        rich_module = ModuleType("rich")
        rich_box = ModuleType("rich.box")
        rich_box.ROUNDED = MagicMock()
        rich_box.Box = MagicMock()
        rich_console = ModuleType("rich.console")
    
        class _Console:
            def __init__(self, *args, **kwargs):
                self.print_calls: list[tuple[tuple[object, ...], dict[str, 
object]]] = []
    
            def print(self, *args, **kwargs):
                self.print_calls.append((args, kwargs))
    
        rich_console.Console = _Console
        rich_markdown = ModuleType("rich.markdown")
    
        class _Markdown:
            def __init__(self, text: str, **kwargs: object) -> None:
                self.text = text
                self.kwargs = kwargs
    
        rich_markdown.Markdown = _Markdown
        rich_panel = ModuleType("rich.panel")
    
        class _Panel:
            def __init__(self, renderable: object, **kwargs: object) -> None:
                self.renderable = renderable
                self.kwargs = kwargs
    
        rich_panel.Panel = _Panel
        rich_style = ModuleType("rich.style")
    
        class _Style:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
    
        rich_style.Style = _Style
        rich_syntax = ModuleType("rich.syntax")
    
        class _Syntax:
            def __init__(
                self, code: str, lexer: str | None = None, **kwargs: object
            ) -> None:
                self.code = code
                self.lexer = lexer
                self.kwargs = kwargs
    
        rich_syntax.Syntax = _Syntax
        rich_table = ModuleType("rich.table")
    
        class _Table:
            def __init__(self, *args: object, **kwargs: object) -> None:
                self.args = args
                self.kwargs = kwargs
                self.columns: list[tuple[str, dict[str, object]]] = []
                self.rows: list[tuple[tuple[object, ...], dict[str, object]]] = 
[]
    
            def add_column(self, name: str, **kwargs: object) -> None:
                self.columns.append((name, kwargs))
    
            def add_row(self, *cells: object, **kwargs: object) -> None:
                self.rows.append((cells, kwargs))
    
        rich_table.Table = _Table
        rich_text = ModuleType("rich.text")
    
        class _Text(str):
            def __new__(cls, text: str, *args: object, **kwargs: object):  # 
type: ignore[override]
                obj = str.__new__(cls, text)
                obj._args = args  # type: ignore[attr-defined]
                obj._kwargs = kwargs  # type: ignore[attr-defined]
                return obj
    
        rich_text.Text = _Text
        rich_module.box = rich_box
        rich_module.console = rich_console
        rich_module.markdown = rich_markdown
        rich_module.panel = rich_panel
        rich_module.style = rich_style
        rich_module.syntax = rich_syntax
        rich_module.table = rich_table
        rich_module.text = rich_text
        monkeypatch.setitem(sys.modules, "rich", rich_module)
        monkeypatch.setitem(sys.modules, "rich.box", rich_box)
        monkeypatch.setitem(sys.modules, "rich.console", rich_console)
        monkeypatch.setitem(sys.modules, "rich.markdown", rich_markdown)
        monkeypatch.setitem(sys.modules, "rich.panel", rich_panel)
        monkeypatch.setitem(sys.modules, "rich.style", rich_style)
        monkeypatch.setitem(sys.modules, "rich.syntax", rich_syntax)
        monkeypatch.setitem(sys.modules, "rich.table", rich_table)
        monkeypatch.setitem(sys.modules, "rich.text", rich_text)
        sys.modules.pop("devsynth.interface.webui", None)
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_guidance.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_handle_command_errors_passthrough ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c58b2f0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR: 
File not found: config.yaml-Make sure the file exists] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c742330>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR: 
Permission denied: secrets.env-necessary permissions] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c807fe0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR: 
Invalid value: bad input-Please check your input] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86f170>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR: 
Missing key: 'api_key'-Verify that the referenced key exists] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86e390>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_handle_command_errors_known_exceptions[<lambda>-ERROR: 
Type error: wrong type-Check that all inputs] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86f440>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_handle_command_errors_generic_exception ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86f3e0>

    @pytest.fixture
    def webui_module(monkeypatch):
        """Reload the WebUI module with a mocked Streamlit dependency."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_handle_command_errors.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_get_layout_config_respects_breakpoints _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86c2c0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_ask_question_and_confirm_choice_use_streamlit_controls 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c830620>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____ ERROR at setup of test_display_result_message_types_provide_guidance _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c92b980>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______ ERROR at setup of test_display_result_markup_and_keyword_routing _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c929430>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[File not 
found-file_not_found] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c929af0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Permission 
denied-permission_denied] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c350410>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Invalid 
parameter-invalid_parameter] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b472210>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Invalid 
format-invalid_format] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c329d90>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Missing key-key_error] 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7c31d0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Type error-type_error] 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7c3740>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[TypeError-type_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c251df0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Configuration 
error-config_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ae9af60>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Connection 
error-connection_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ae99790>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[API error-api_error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9db410>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Validation 
error-validation_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9d9e50>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Syntax 
error-syntax_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2a98b0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Import 
error-import_error] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b170d40>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of test_get_error_type_matches_keywords[Completely different-] 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ba4b3e0>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_error_suggestions_and_docs_cover_known_and_unknown ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b171b50>

    @pytest.fixture
    def webui_module(monkeypatch: pytest.MonkeyPatch) -> tuple[object, object]:
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        st = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_messaging.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________ ERROR at setup of test_lazy_streamlit_proxy_imports_once ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c07c500>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) -> 
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the 
duration."""
    
        original_import = importlib.import_module
        stub = _HarnessStreamlit()
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
___ ERROR at setup of test_progress_indicator_emits_eta_and_sanitized_status ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1d32f0>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) -> 
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the 
duration."""
    
        original_import = importlib.import_module
        stub = _HarnessStreamlit()
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
______ ERROR at setup of test_permission_denied_error_renders_suggestions ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1d02f0>

    @pytest.fixture()
    def harness_streamlit(monkeypatch: pytest.MonkeyPatch) -> 
Iterator[_HarnessStreamlit]:
        """Install the harness as the cached Streamlit module for the 
duration."""
    
        original_import = importlib.import_module
        stub = _HarnessStreamlit()
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:116: AttributeError
_______ ERROR at setup of test_display_result_translates_markup_to_html ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1baff0>

    @pytest.fixture
    def reloaded_webui(monkeypatch: pytest.MonkeyPatch):
        """Reload ``devsynth.interface.webui`` with a fresh Streamlit stub."""
    
        streamlit_stub = make_streamlit_mock()
        monkeypatch.setitem(sys.modules, "streamlit", streamlit_stub)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_streamlit_and_wizard.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_progress_complete_cascades_with_sanitized_fallback ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bac9f10>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
__________ ERROR at setup of test_webui_layout_and_display_behaviors ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c85fb60>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
________ ERROR at setup of test_ui_progress_status_transitions_and_eta _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c85f0e0>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
_____________ ERROR at setup of test_ensure_router_caches_instance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9af530>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
________ ERROR at setup of test_webui_run_configures_layout_and_router _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9bfc50>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
__________ ERROR at setup of test_webui_run_handles_streamlit_errors ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9bf770>

    @pytest.fixture()
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> _StubStreamlit:
        stub = _StubStreamlit()
>       monkeypatch.setattr(webui, "st", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress_cascade_fast.py:249: AttributeError
_ ERROR at setup of test_webui_run_injects_resize_script_and_configures_layout _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c533e00>

    @pytest.fixture
    def streamlit_router_stub(monkeypatch: pytest.MonkeyPatch):
        """Provide a stubbed Streamlit module and Router replacement."""
    
        from devsynth.interface import webui as webui_module
    
        previous_streamlit = sys.modules.get("streamlit")
        # Import Router from the webui package submodule
>       from devsynth.interface.webui import Router

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_fast.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___ ERROR at setup of test_webui_run_configures_dashboard_and_invokes_router ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c7c7ef0>

    @pytest.fixture
    def streamlit_free_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[tuple[ModuleType, ModuleType, dict[str, MagicMock]]]:
        """Reload :mod:`devsynth.interface.webui` with a deterministic Streamlit
stub."""
    
        st = _mock_streamlit()
        st.sidebar.radio = MagicMock(return_value="Summary")
        st.session_state.screen_width = 860
        st.session_state.screen_height = 600
    
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_ ERROR at setup of 
test_progress_updates_emit_telemetry_and_sanitize_checkpoints _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c596300>

    @pytest.fixture
    def progress_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[tuple[ModuleType, ModuleType, MagicMock, MagicMock, 
MagicMock]]:
        """Provide a reloaded WebUI module with deterministic progress 
containers."""
    
        st = _mock_streamlit()
        status_container = MagicMock(name="status_container")
        time_container = MagicMock(name="time_container")
        bar_container = MagicMock(name="bar_container")
    
        st.empty = MagicMock(side_effect=[status_container, time_container])
        st.progress = MagicMock(return_value=bar_container)
    
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____ ERROR at setup of test_display_result_sanitizes_message_before_render _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c533650>

    @pytest.fixture
    def sanitized_webui(
        monkeypatch: pytest.MonkeyPatch,
    ) -> Iterator[tuple[ModuleType, ModuleType]]:
        """Yield a reloaded WebUI module for sanitization assertions."""
    
        st = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", st)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_progress_fast.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_lazy_loader_imports_streamlit_stub_once ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6da630>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""
    
        stub = StreamlitStub()
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:228: AttributeError
_________ ERROR at setup of test_display_result_sanitizes_error_output _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c4bb380>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""
    
        stub = StreamlitStub()
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:228: AttributeError
________ ERROR at setup of test_ui_progress_tracks_status_and_subtasks _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c4baea0>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""
    
        stub = StreamlitStub()
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:228: AttributeError
____ ERROR at setup of test_router_run_uses_default_and_persists_selection _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6d9cd0>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""
    
        stub = StreamlitStub()
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:228: AttributeError
________ ERROR at setup of test_webui_run_configures_router_and_layout _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c71ecc0>

    @pytest.fixture
    def streamlit_stub(monkeypatch: pytest.MonkeyPatch) -> StreamlitStub:
        """Install a deterministic Streamlit stub for the duration of a test."""
    
        stub = StreamlitStub()
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            if name == "streamlit":
                stub.import_requests += 1
                return stub
            return original_import(name, package)
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:228: AttributeError
________ ERROR at setup of test_ask_question_selectbox_indexes_default _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c76adb0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_ask_question_text_input_when_no_choices ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c71ed50>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ ERROR at setup of test_confirm_choice_returns_checkbox_value _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c4bb200>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__ ERROR at setup of test_display_result_error_surfaces_suggestions_and_docs ___

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9ba660>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________ ERROR at setup of test_render_traceback_expander_renders_code _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9ba300>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_ui_progress_sanitizes_updates _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c2e1700>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_ensure_router_memoizes_instance ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc48290>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_run_handles_page_config_errors _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc4b680>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ ERROR at setup of test_run_renders_layout_and_router _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ae99df0>

    @pytest.fixture
    def webui_under_test(monkeypatch: pytest.MonkeyPatch) -> SimpleNamespace:
        """Reload ``devsynth.interface.webui`` with a rich Streamlit double."""
    
        from tests.unit.interface.test_webui_enhanced import _mock_streamlit
    
        fake_streamlit = _mock_streamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_targeted_branches.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ ERROR at setup of test_reasoning_loop_records_results _____________
file 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py, line 12
  @pytest.mark.fast
  def test_reasoning_loop_records_results(mocker) -> None:
E       fixture 'mocker' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example, 
_restore_env_and_cwd_between_tests, _session_scoped_runner, _sys_snapshot, 
cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, 
chromadb_client, chromadb_temp_path, cov, coverage_stub_factory, 
deterministic_seed, disable_network, doctest_namespace, duckdb_connection, 
duckdb_path, enforce_test_timeout, ephemeral_kuzu_store, event_loop_policy, 
faiss_index, gather_wizard_state, gather_wizard_state_manager, 
global_test_isolation, kuzu_db_path, linecomp, llm_port, lmdb_env, 
lmstudio_mock, lmstudio_service, memory_port, mock_datetime, 
mock_lm_studio_provider, mock_openai_provider, mock_session_state, 
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui, 
mock_wizard_state, monkeypatch, no_cover, normalize_subsystem_stubs, onnx_port, 
patch_settings_paths, pytestbdd_stepdef_given_trace, 
pytestbdd_stepdef_then_trace, pytestbdd_stepdef_when_trace, pytestconfig, 
pytester, rdflib_graph, record_property, record_testsuite_property, 
record_xml_attribute, recwarn, require_modules, reset_coverage, 
reset_global_state, streamlit_bridge_stub, stub_devsynth_config, 
stub_optional_dependencies, temp_log_dir, temp_memory_path, 
temporary_kuzu_config, test_environment, testdir, tinydb_path, tmp_path, 
tmp_path_factory, tmp_project_dir, tmpdir, tmpdir_factory, unused_tcp_port, 
unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, 
webui_context, wizard_state, wizard_state_manager
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:12
_________ ERROR at setup of test_reasoning_loop_logs_consensus_failure _________
file 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py, line 33
  @pytest.mark.fast
  def test_reasoning_loop_logs_consensus_failure(mocker) -> None:
E       fixture 'mocker' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example, 
_restore_env_and_cwd_between_tests, _session_scoped_runner, _sys_snapshot, 
cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, 
chromadb_client, chromadb_temp_path, cov, coverage_stub_factory, 
deterministic_seed, disable_network, doctest_namespace, duckdb_connection, 
duckdb_path, enforce_test_timeout, ephemeral_kuzu_store, event_loop_policy, 
faiss_index, gather_wizard_state, gather_wizard_state_manager, 
global_test_isolation, kuzu_db_path, linecomp, llm_port, lmdb_env, 
lmstudio_mock, lmstudio_service, memory_port, mock_datetime, 
mock_lm_studio_provider, mock_openai_provider, mock_session_state, 
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui, 
mock_wizard_state, monkeypatch, no_cover, normalize_subsystem_stubs, onnx_port, 
patch_settings_paths, pytestbdd_stepdef_given_trace, 
pytestbdd_stepdef_then_trace, pytestbdd_stepdef_when_trace, pytestconfig, 
pytester, rdflib_graph, record_property, record_testsuite_property, 
record_xml_attribute, recwarn, require_modules, reset_coverage, 
reset_global_state, streamlit_bridge_stub, stub_devsynth_config, 
stub_optional_dependencies, temp_log_dir, temp_memory_path, 
temporary_kuzu_config, test_environment, testdir, tinydb_path, tmp_path, 
tmp_path_factory, tmp_project_dir, tmpdir, tmpdir_factory, unused_tcp_port, 
unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, 
webui_context, wizard_state, wizard_state_manager
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:33
_________ ERROR at setup of test_reasoning_loop_persists_phase_results _________
file 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py, line 57
  @pytest.mark.fast
  def test_reasoning_loop_persists_phase_results(mocker) -> None:
E       fixture 'mocker' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example, 
_restore_env_and_cwd_between_tests, _session_scoped_runner, _sys_snapshot, 
cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, 
chromadb_client, chromadb_temp_path, cov, coverage_stub_factory, 
deterministic_seed, disable_network, doctest_namespace, duckdb_connection, 
duckdb_path, enforce_test_timeout, ephemeral_kuzu_store, event_loop_policy, 
faiss_index, gather_wizard_state, gather_wizard_state_manager, 
global_test_isolation, kuzu_db_path, linecomp, llm_port, lmdb_env, 
lmstudio_mock, lmstudio_service, memory_port, mock_datetime, 
mock_lm_studio_provider, mock_openai_provider, mock_session_state, 
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui, 
mock_wizard_state, monkeypatch, no_cover, normalize_subsystem_stubs, onnx_port, 
patch_settings_paths, pytestbdd_stepdef_given_trace, 
pytestbdd_stepdef_then_trace, pytestbdd_stepdef_when_trace, pytestconfig, 
pytester, rdflib_graph, record_property, record_testsuite_property, 
record_xml_attribute, recwarn, require_modules, reset_coverage, 
reset_global_state, streamlit_bridge_stub, stub_devsynth_config, 
stub_optional_dependencies, temp_log_dir, temp_memory_path, 
temporary_kuzu_config, test_environment, testdir, tinydb_path, tmp_path, 
tmp_path_factory, tmp_project_dir, tmpdir, tmpdir_factory, unused_tcp_port, 
unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, 
webui_context, wizard_state, wizard_state_manager
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning.py:57
_____________ ERROR at setup of test_record_consensus_failure_logs _____________
file 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_edrr_coordinator.py, line 24
  def test_record_consensus_failure_logs(mocker) -> None:
E       fixture 'mocker' not found
>       available fixtures: LineMatcher, _class_scoped_runner, _config_for_test,
_default_timeout_by_speed, _devsynth_test_env_defaults, _function_scoped_runner,
_module_scoped_runner, _package_scoped_runner, _pytest, _pytest_bdd_example, 
_restore_env_and_cwd_between_tests, _session_scoped_runner, _sys_snapshot, 
cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, 
chromadb_client, chromadb_temp_path, cov, coverage_stub_factory, 
deterministic_seed, disable_network, doctest_namespace, duckdb_connection, 
duckdb_path, enforce_test_timeout, ephemeral_kuzu_store, event_loop_policy, 
faiss_index, gather_wizard_state, gather_wizard_state_manager, 
global_test_isolation, kuzu_db_path, linecomp, llm_port, lmdb_env, 
lmstudio_mock, lmstudio_service, memory_port, mock_datetime, 
mock_lm_studio_provider, mock_openai_provider, mock_session_state, 
mock_streamlit, mock_streamlit_for_state, mock_uuid, mock_webui, 
mock_wizard_state, monkeypatch, no_cover, normalize_subsystem_stubs, onnx_port, 
patch_settings_paths, pytestbdd_stepdef_given_trace, 
pytestbdd_stepdef_then_trace, pytestbdd_stepdef_when_trace, pytestconfig, 
pytester, rdflib_graph, record_property, record_testsuite_property, 
record_xml_attribute, recwarn, require_modules, reset_coverage, 
reset_global_state, streamlit_bridge_stub, stub_devsynth_config, 
stub_optional_dependencies, temp_log_dir, temp_memory_path, 
temporary_kuzu_config, test_environment, testdir, tinydb_path, tmp_path, 
tmp_path_factory, tmp_project_dir, tmpdir, tmpdir_factory, unused_tcp_port, 
unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, 
webui_context, wizard_state, wizard_state_manager
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_edrr_coordinator.py:24
=================================== FAILURES ===================================
_____________ test_provider_factory_offline_uses_stub_safe_default _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bde9ee0>

    @pytest.mark.fast
    def test_provider_factory_offline_uses_stub_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return the stub provider when offline guard is enabled.
    
        ReqID: N/A
        """
    
        config = _make_provider_config(openai_key="token")
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "stub")
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", 
raising=False)
    
        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, StubProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.StubProvider object at 
0x10b0a2ae0>, StubProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:251: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,243 - devsynth.adapters.provider_system - INFO - Falling 
back to Stub provider: DEVSYNTH_OFFLINE active; using safe provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 250, in 
test_provider_factory_offline_uses_stub_safe_default
    provider = ProviderFactory.create_provider("openai")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 311, in create_provider
    return _safe_provider("DEVSYNTH_OFFLINE active; using safe provider")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 300, in _safe_provider
    logger.info("Falling back to Stub provider: %s", reason)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Stub provider: %s'
Arguments: ('DEVSYNTH_OFFLINE active; using safe provider',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to 
Stub provider: DEVSYNTH_OFFLINE active; using safe provider
_____________ test_provider_factory_offline_uses_null_safe_default _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bde9be0>

    @pytest.mark.fast
    def test_provider_factory_offline_uses_null_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return the null provider when offline guard favors null safe 
defaults.
    
        ReqID: N/A
        """
    
        config = _make_provider_config(openai_key="token")
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "1")
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", 
raising=False)
    
        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.NullProvider object at 
0x10c5ef8f0>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:272: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,330 - devsynth.adapters.provider_system - INFO - Falling 
back to Null provider: DEVSYNTH_OFFLINE active; using safe provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 271, in 
test_provider_factory_offline_uses_null_safe_default
    provider = ProviderFactory.create_provider("openai")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 311, in create_provider
    return _safe_provider("DEVSYNTH_OFFLINE active; using safe provider")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('DEVSYNTH_OFFLINE active; using safe provider',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to 
Null provider: DEVSYNTH_OFFLINE active; using safe provider
_ 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdebec0>

    @pytest.mark.fast
    def 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Use the safe default provider when LM Studio fallback is not 
permitted.
    
        ReqID: N/A
        """
    
        config = _make_provider_config(openai_key=None)
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "false")
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
    
        provider = ProviderFactory.create_provider()
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.NullProvider object at 
0x10c5e5b20>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:294: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,340 - devsynth.adapters.provider_system - INFO - Falling 
back to Null provider: No OPENAI_API_KEY; LM Studio not marked available. Hint: 
export OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set 
DEVSYNTH_PROVIDER=stub for offline runs.
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 293, in 
test_provider_factory_missing_openai_key_defaults_to_safe_provider_when_lmstudio
_unavailable
    provider = ProviderFactory.create_provider()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 349, in create_provider
    return _safe_provider(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('No OPENAI_API_KEY; LM Studio not marked available. Hint: export 
OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set 
DEVSYNTH_PROVIDER=stub for offline runs.',)
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to 
Null provider: No OPENAI_API_KEY; LM Studio not marked available. Hint: export 
OPENAI_API_KEY, export DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE=true, or set 
DEVSYNTH_PROVIDER=stub for offline runs.
_ test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6f7a70>

    @pytest.mark.fast
    def 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Return a null provider when LM Studio instantiation raises an 
exception.
    
        ReqID: N/A
        """
    
        config = _make_provider_config()
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.setenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", "null")
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", 
raising=False)
    
        monkeypatch.setattr(
            provider_system,
            "LMStudioProvider",
            MagicMock(side_effect=RuntimeError("kaboom")),
        )
    
        provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.NullProvider object at 
0x10c751970>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:351: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,353 - devsynth.adapters.provider_system - WARNING - Unknown 
provider type 'ProviderType.LMSTUDIO', falling back to safe default
2025-10-29 10:47:38,353 - devsynth.adapters.provider_system - INFO - Falling 
back to Null provider: Unknown provider type
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 350, in 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default
    provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 432, in create_provider
    logger.warning(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 627, in warning
    self._log(logging.WARNING, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: "Unknown provider type '%s', falling back to safe default"
Arguments: (<ProviderType.LMSTUDIO: 'lmstudio'>,)
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 350, in 
test_provider_factory_lmstudio_instantiation_failure_uses_null_safe_default
    provider = ProviderFactory.create_provider(ProviderType.LMSTUDIO)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 436, in create_provider
    return _safe_provider("Unknown provider type")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 305, in _safe_provider
    logger.info("Falling back to Null provider: %s", reason)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Null provider: %s'
Arguments: ('Unknown provider type',)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider
type 'ProviderType.LMSTUDIO', falling back to safe default
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to 
Null provider: Unknown provider type
_______ test_provider_factory_openai_explicit_missing_key_surfaces_error _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6f7140>

    @pytest.mark.fast
    def test_provider_factory_openai_explicit_missing_key_surfaces_error(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Surface explicit OpenAI credential errors via null provider.
    
        ReqID: N/A
        """
    
        config = _make_provider_config(openai_key=None)
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", raising=False)
    
        provider = ProviderFactory.create_provider("openai")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.NullProvider object at 
0x10c753440>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:372: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,363 - devsynth.adapters.provider_system - ERROR - OpenAI API
key is missing for explicitly requested OpenAI provider. Set OPENAI_API_KEY or 
choose a safe provider via DEVSYNTH_PROVIDER=stub.
2025-10-29 10:47:38,363 - devsynth.adapters.provider_system - ERROR - Failed to 
create provider openai: Missing OPENAI_API_KEY for OpenAI provider. Set 
OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 371, in 
test_provider_factory_openai_explicit_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("openai")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 318, in create_provider
    logger.error(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'OpenAI API key is missing for explicitly requested OpenAI provider. 
Set OPENAI_API_KEY or choose a safe provider via DEVSYNTH_PROVIDER=stub.'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 324, in create_provider
    raise ProviderError(
devsynth.exceptions.ProviderError: Missing OPENAI_API_KEY for OpenAI provider. 
Set OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 371, in 
test_provider_factory_openai_explicit_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("openai")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 438, in create_provider
    logger.error(f"Failed to create provider {provider_type}: {e}")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Failed to create provider openai: Missing OPENAI_API_KEY for OpenAI 
provider. Set OPENAI_API_KEY or use DEVSYNTH_PROVIDER=stub.'
Arguments: ()
------------------------------ Captured log call -------------------------------
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 OpenAI API key 
is missing for explicitly requested OpenAI provider. Set OPENAI_API_KEY or 
choose a safe provider via DEVSYNTH_PROVIDER=stub.
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Failed to create
provider openai: Missing OPENAI_API_KEY for OpenAI provider. Set OPENAI_API_KEY 
or use DEVSYNTH_PROVIDER=stub.
__________ test_provider_factory_anthropic_missing_key_surfaces_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6f74d0>

    @pytest.mark.fast
    def test_provider_factory_anthropic_missing_key_surfaces_error(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Surface explicit Anthropic credential errors via null provider.
    
        ReqID: N/A
        """
    
        config = _make_provider_config()
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
    
        provider = ProviderFactory.create_provider("anthropic")
>       assert isinstance(provider, NullProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.NullProvider object at 
0x10c681f10>, NullProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:393: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,373 - devsynth.adapters.provider_system - ERROR - Anthropic 
API key is missing for explicitly requested Anthropic provider
2025-10-29 10:47:38,373 - devsynth.adapters.provider_system - ERROR - Failed to 
create provider anthropic: Anthropic API key is required for Anthropic provider
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 392, in 
test_provider_factory_anthropic_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("anthropic")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 387, in create_provider
    logger.error(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Anthropic API key is missing for explicitly requested Anthropic 
provider'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 391, in create_provider
    raise ProviderError(
devsynth.exceptions.ProviderError: Anthropic API key is required for Anthropic 
provider

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 392, in 
test_provider_factory_anthropic_missing_key_surfaces_error
    provider = ProviderFactory.create_provider("anthropic")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 438, in create_provider
    logger.error(f"Failed to create provider {provider_type}: {e}")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 631, in error
    self._log(logging.ERROR, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Failed to create provider anthropic: Anthropic API key is required for
Anthropic provider'
Arguments: ()
------------------------------ Captured log call -------------------------------
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Anthropic API 
key is missing for explicitly requested Anthropic provider
ERROR    devsynth.adapters.provider_system:logging_setup.py:615 Failed to create
provider anthropic: Anthropic API key is required for Anthropic provider
_______________ test_provider_factory_accepts_provider_type_enum _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6f7080>

    @pytest.mark.fast
    def test_provider_factory_accepts_provider_type_enum(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Interpret ``ProviderType`` enum values when selecting providers.
    
        ReqID: N/A
        """
    
        config = _make_provider_config(default_provider="stub")
        _install_factory_config(monkeypatch, config)
    
        monkeypatch.delenv("DEVSYNTH_OFFLINE", raising=False)
        monkeypatch.delenv("DEVSYNTH_DISABLE_PROVIDERS", raising=False)
        monkeypatch.delenv("DEVSYNTH_SAFE_DEFAULT_PROVIDER", raising=False)
    
        provider = ProviderFactory.create_provider(ProviderType.STUB)
>       assert isinstance(provider, StubProvider)
E       assert False
E        +  where False = 
isinstance(<devsynth.adapters.provider_system.StubProvider object at 
0x10c3949b0>, StubProvider)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_pr
ovider_system_additional.py:414: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:47:38,383 - devsynth.adapters.provider_system - WARNING - Unknown 
provider type 'ProviderType.STUB', falling back to safe default
2025-10-29 10:47:38,383 - devsynth.adapters.provider_system - INFO - Falling 
back to Stub provider: Unknown provider type
----------------------------- Captured stderr call -----------------------------
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 413, in 
test_provider_factory_accepts_provider_type_enum
    provider = ProviderFactory.create_provider(ProviderType.STUB)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 432, in create_provider
    logger.warning(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 627, in warning
    self._log(logging.WARNING, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: "Unknown provider type '%s', falling back to safe default"
Arguments: (<ProviderType.STUB: 'stub'>,)
--- Logging error ---
Traceback (most recent call last):
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/handlers.py", line 191, in shouldRollover
    self.stream = self._open()
                  ^^^^^^^^^^^^
  File 
"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/logging/__init__.py", line 1263, in _open
    return open_func(self.baseFilename, self.mode,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/logs/devsynth.log'
Call stack:
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pytest/__main__.py", line 9, in <module>
    raise SystemExit(pytest.console_main())
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 201, in console_main
    code = main()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/config/__init__.py", line 175, in main
    ret: ExitCode | int = config.hook.pytest_cmdline_main(config=config)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 336, in pytest_cmdline_main
    return wrap_session(config, _main)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 289, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 343, in _main
    config.hook.pytest_runtestloop(session=session)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/main.py", line 367, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 136, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 245, in call_and_report
    call = CallInfo.from_call(
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 344, in from_call
    result: TResult | None = func()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 246, in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/runner.py", line 178, in pytest_runtest_call
    item.runtest()
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 1671, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_hooks.py", line 512, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, 
firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/pluggy/_callers.py", line 121, in _multicall
    res = hook_impl.function(*args)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-
packages/_pytest/python.py", line 157, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/adapters/test_p
rovider_system_additional.py", line 413, in 
test_provider_factory_accepts_provider_type_enum
    provider = ProviderFactory.create_provider(ProviderType.STUB)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 436, in create_provider
    return _safe_provider("Unknown provider type")
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/prov
ider_system.py", line 300, in _safe_provider
    logger.info("Falling back to Stub provider: %s", reason)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 623, in info
    self._log(logging.INFO, msg, *args, **kwargs)
  File 
"/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py", line 615, in _log
    self.logger.log(level, msg, *args, **log_kwargs)
Message: 'Falling back to Stub provider: %s'
Arguments: ('Unknown provider type',)
------------------------------ Captured log call -------------------------------
WARNING  devsynth.adapters.provider_system:logging_setup.py:615 Unknown provider
type 'ProviderType.STUB', falling back to safe default
INFO     devsynth.adapters.provider_system:logging_setup.py:615 Falling back to 
Stub provider: Unknown provider type
_________________ test_testclient_imports_without_mro_conflict _________________

    def test_testclient_imports_without_mro_conflict() -> None:
        """ReqID: FASTAPI-STARLETTE-0001
        Ensure TestClient import avoids the Starlette MRO regression."""
    
>       from fastapi.testclient import TestClient

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/api/test_fastapi
_testclient_import.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/fastapi/testclient.py:1: in <module>
    from starlette.testclient import TestClient as TestClient  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError
_________________________ test_inventory_exports_file __________________________

obj = <function run_tests_cmd at 0x10aba4720>, name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute 
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc7ef0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_inventory_exports_file0')
tmp_path_factory = TempPathFactory(_given_basetemp=None, 
_trace=<pluggy._tracing.TagTracerSub object at 0x10b5a42f0>, 
_basetemp=PosixPath...folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-cait
lyn/pytest-57'), _retention_count=3, _retention_policy='all')

    @pytest.mark.fast
    def test_inventory_exports_file(monkeypatch, tmp_path, tmp_path_factory) -> 
None:
        """ReqID: CLI-RT-08  inventory mode exports JSON and skips run."""
    
        # Return deterministic collections
        def fake_collect(tgt: str, spd: str | None = None) -> list[str]:
            return ["tests/unit/test_example.py::test_ok"]
    
        def fake_run_tests(*args, **kwargs) -> tuple[bool, str]:
            pytest.fail("run_tests should not be called in inventory mode")
    
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_
cache",
            fake_collect,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <function run_tests_cmd at 0x10aba4720>, name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'function' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
__________________ test_failed_run_surfaces_maxfail_guidance ___________________

obj = <function run_tests_cmd at 0x10aba4720>, name = 'run_tests'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c663ad0>

    @pytest.mark.fast
    def test_failed_run_surfaces_maxfail_guidance(monkeypatch) -> None:
        """ReqID: CLI-RT-17  Failed runs surface maxfail troubleshooting 
tips."""
    
        failure_output = (
            "Pytest exited with code 1. Command: python -m pytest --maxfail=2 
tests/unit\n"
            "Troubleshooting tips:\n"
            "- Segment large suites to localize failures.\n"
        )
    
        def fake_run_tests(*args, **kwargs) -> tuple[bool, str]:  # noqa: ANN001
            return False, failure_output
    
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.run_tests",
            fake_run_tests,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:403: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <function run_tests_cmd at 0x10aba4720>, name = 'run_tests'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'function' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ test_run_tests_cmd_exits_when_pytest_cov_missing _______________

obj = <function run_tests_cmd at 0x10aba4720>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute 
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdf0bf0>

    @pytest.mark.fast
    def test_run_tests_cmd_exits_when_pytest_cov_missing(monkeypatch) -> None:
        """Missing pytest-cov triggers an actionable remediation banner."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_
status",
            lambda env=None: (
                False,
                run_tests_cmd_module.PYTEST_COV_PLUGIN_MISSING_MESSAGE,
            ),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:437: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <function run_tests_cmd at 0x10aba4720>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'function' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________ test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov ___________

obj = <function run_tests_cmd at 0x10aba4720>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: 'function' object has no attribute 
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c405eb0>

    @pytest.mark.fast
    def test_run_tests_cmd_exits_when_autoload_blocks_pytest_cov(monkeypatch) ->
None:
        """Autoload blocking pytest-cov halts execution for standard runs."""
    
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.pytest_cov_support_
status",
            lambda env=None: (
                False,
                run_tests_cmd_module.PYTEST_COV_AUTOLOAD_DISABLED_MESSAGE,
            ),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd.py:483: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <function run_tests_cmd at 0x10aba4720>
name = 'pytest_cov_support_status'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'function' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'pytest_cov_support_status'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________ test_cli_runner_inventory_handles_collection_errors ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c4045c0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_runner_inventory_hand0')

    @pytest.mark.fast
    def test_cli_runner_inventory_handles_collection_errors(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Inventory exports tolerate collection errors and still succeed."""
    
        monkeypatch.chdir(tmp_path)
    
        calls: list[tuple[str, str | None]] = []
    
        def fake_collect(target: str, speed: str | None) -> list[str]:
            calls.append((target, speed))
            if target == "integration-tests" and speed == "medium":
                raise RuntimeError("collection failed")
            suffix = speed or "all"
            return [f"{target}::{suffix}::test_case"]
    
        app, cli_module = _build_minimal_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache", 
fake_collect)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:311: AttributeError
_____________ test_cli_runner_failed_run_surfaces_maxfail_guidance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c717380>

    @pytest.mark.fast
    def test_cli_runner_failed_run_surfaces_maxfail_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Failed Typer invocation should surface the maxfail troubleshooting 
tip."""
    
        app, cli_module = _build_minimal_app(monkeypatch)
    
        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            cmd = ["python", "-m", "pytest", "--maxfail", "2"]
            tips = run_tests_module._failure_tips(1, cmd)
            return False, "segment error\n" + tips
    
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:340: AttributeError
____________ test_cli_runner_inventory_write_failure_exits_nonzero _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f140>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_runner_inventory_writ0')

    @pytest.mark.fast
    def test_cli_runner_inventory_write_failure_exits_nonzero(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """Disk errors while exporting inventory should exit with code 1."""
    
        monkeypatch.chdir(tmp_path)
    
        def fake_collect(target: str, speed: str | None) -> list[str]:
            suffix = speed or "all"
            return [f"{target}::{suffix}::case"]
    
        app, cli_module = _build_minimal_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache", 
fake_collect)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:365: AttributeError
_____________ test_cli_runner_maxfail_option_propagates_to_runner ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e330>

    @pytest.mark.fast
    def test_cli_runner_maxfail_option_propagates_to_runner(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The Typer surface must forward --maxfail to run_tests."""
    
        app, cli_module = _build_minimal_app(monkeypatch)
    
        received: dict[str, object] = {}
    
        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            received["args"] = args
            received["kwargs"] = dict(kwargs)
            return True, "pytest ok"
    
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_invalid_inputs.py:395: AttributeError
________________ test_cli_inventory_mode_exports_json_via_typer ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72de80>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_inventory_mode_export1')

    @pytest.mark.fast
    def test_cli_inventory_mode_exports_json_via_typer(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Inventory-only mode collects all targets and writes JSON via the 
CLI."""
    
        monkeypatch.chdir(tmp_path)
    
        calls: list[tuple[str, str | None]] = []
    
        def fake_collect(target: str, speed: str | None) -> list[str]:
            calls.append((target, speed))
            # Encode target/speed in the node id so assertions are deterministic
            suffix = speed or "all"
            return [f"{target}::{suffix}::test_case"]
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache", 
fake_collect)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:112: AttributeError
____________________ test_cli_smoke_dry_run_invokes_preview ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f230>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_smoke_dry_run_invokes0')

    @pytest.mark.fast
    def test_cli_smoke_dry_run_invokes_preview(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Smoke dry-run previews pytest invocation without executing suites."""
    
        monkeypatch.chdir(tmp_path)
        sys.modules.pop("devsynth.config", None)
        sys.modules.pop("devsynth.config.settings", None)
        sys.modules.pop("devsynth.config.provider_env", None)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("DEVSYNTH_TEST_TIMEOUT_SECONDS", raising=False)
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        calls: list[dict[str, Any]] = []
    
        def fake_run_tests(*args: Any, **kwargs: Any) -> tuple[bool, str]:
            calls.append({"args": args, "kwargs": kwargs})
            return (
                True,
                "Dry run: pytest invocation prepared but not executed.\n"
                "Command: python -m pytest 
tests/unit/test_example.py::test_case\n",
            )
    
        monkeypatch.setattr(run_tests_module, "run_tests", fake_run_tests)
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:159: AttributeError
_____________ test_cli_enforces_coverage_threshold_via_cli_runner ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e180>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_enforces_coverage_thr0')

    @pytest.mark.fast
    def test_cli_enforces_coverage_threshold_via_cli_runner(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """Successful Typer invocation enforces coverage thresholds and emits 
tips."""
    
        monkeypatch.chdir(tmp_path)
    
        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader", 
core_stub)
        monkeypatch.setitem(sys.modules, "tinydb", ModuleType("tinydb"))
    
        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd", 
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            ("devsynth.application.cli.commands.generate_docs_cmd", 
"generate_docs_cmd"),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        runner = CliRunner()
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *args, **kwargs: 
(True, "ok"))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:239: AttributeError
___________ test_cli_smoke_mode_reports_coverage_skip_and_artifacts ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f7a0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_smoke_mode_reports_co0')

    @pytest.mark.fast
    def test_cli_smoke_mode_reports_coverage_skip_and_artifacts(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Smoke mode prints diagnostic skip notice while still surfacing 
artifacts."""
    
        monkeypatch.chdir(tmp_path)
    
        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader", 
core_stub)
        monkeypatch.setitem(sys.modules, "tinydb", ModuleType("tinydb"))
    
        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd", 
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            ("devsynth.application.cli.commands.generate_docs_cmd", 
"generate_docs_cmd"),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        recorded_args: list[tuple[tuple[object, ...], dict[str, object]]] = []
    
        def fake_run_tests(*args: object, **kwargs: object) -> tuple[bool, str]:
            recorded_args.append((args, kwargs))
            return True, "pytest output"
    
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:340: AttributeError
_______________ test_cli_exits_when_autoload_disables_pytest_cov _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72daf0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_exits_when_autoload_d0')

    @pytest.mark.fast
    def test_cli_exits_when_autoload_disables_pytest_cov(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Fail fast when plugin autoloading disables pytest-cov 
instrumentation."""
    
        monkeypatch.chdir(tmp_path)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True, 
""))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:397: AttributeError
_____________ test_cli_exits_when_pytest_cov_disabled_via_autoload _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72f8f0>

    @pytest.mark.fast
    def test_cli_exits_when_pytest_cov_disabled_via_autoload(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Typer run surfaces remediation tips when pytest-cov is disabled."""
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        runner = CliRunner()
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *args, **kwargs: 
(True, ""))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_paths.py:444: AttributeError
_________________ test_cli_reports_coverage_artifacts_success __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72dc40>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_reports_coverage_arti0')

    @pytest.mark.fast
    def test_cli_reports_coverage_artifacts_success(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Successful CLI run emits artifact locations after enforcing 
thresholds."""
    
        monkeypatch.chdir(tmp_path)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        html_dir = tmp_path / "htmlcov"
        html_dir.mkdir()
        html_index = html_dir / "index.html"
        html_index.write_text("<html>coverage</html>")
    
        json_path = tmp_path / "test_reports" / "coverage.json"
        json_path.parent.mkdir()
        json_path.write_text("{}")
    
        monkeypatch.setattr(cli_module, "COVERAGE_HTML_DIR", html_dir)
        monkeypatch.setattr(cli_module, "COVERAGE_JSON_PATH", json_path)
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True, 
"pytest ok"))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:35: AttributeError
________________ test_cli_exits_when_coverage_artifacts_missing ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e2a0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_exits_when_coverage_a0')

    @pytest.mark.fast
    def test_cli_exits_when_coverage_artifacts_missing(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Missing coverage artifacts trigger an exit with remediation 
guidance."""
    
        monkeypatch.chdir(tmp_path)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True, 
""))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:82: AttributeError
__________________ test_cli_surfaces_threshold_runtime_errors __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72dbb0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_surfaces_threshold_ru0')

    @pytest.mark.fast
    def test_cli_surfaces_threshold_runtime_errors(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Runtime errors from threshold enforcement bubble up through the 
CLI."""
    
        monkeypatch.chdir(tmp_path)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True, 
""))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_cli_runner_thresholds.py:125: AttributeError
_______________ test_smoke_command_generates_coverage_artifacts ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72d2e0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_smoke_command_generates_c0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c18c6b0>

    @pytest.mark.fast
    def test_smoke_command_generates_coverage_artifacts(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """Smoke profile writes coverage artifacts even with plugin autoload 
disabled."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
    
        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_________________ test_smoke_command_injects_pytest_bdd_plugin _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c103800>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_smoke_command_injects_pyt0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c101e50>

    @pytest.mark.fast
    def test_smoke_command_injects_pytest_bdd_plugin(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """Smoke profile explicitly loads pytest-bdd when plugin autoload is 
disabled."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
        monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
    
        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_ test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c103080>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_fast_medium_command_gener0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c1566f0>

    @pytest.mark.fast
    def 
test_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """Fast+medium aggregate run preserves coverage artifacts when autoload 
is disabled."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
    
        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_fast_medium_preserves_existing_cov_fail_under ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c103020>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_fast_medium_preserves_exi0')

    @pytest.mark.fast
    def test_fast_medium_preserves_existing_cov_fail_under(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Custom fail-under thresholds survive coverage plugin injection."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "--cov-fail-under=95")
    
        popen_envs, _ = _install_pytest_stubs(monkeypatch)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_fast_medium_command_handles_empty_collection _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c1036b0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_fast_medium_command_handl0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x10c5d7aa0>

    @pytest.mark.fast
    def test_fast_medium_command_handles_empty_collection(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """Fallback to marker execution when collection returns no node 
identifiers."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
    
        popen_envs, combine_calls = _install_pytest_stubs(
            monkeypatch, collect_stdout_sequence=["", ""]
        )
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:323: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
_________ test_fast_profile_generates_coverage_and_exits_successfully __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c5db200>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_fast_profile_generates_co0')

    @pytest.mark.fast
    def test_fast_profile_generates_coverage_and_exits_successfully(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Default fast profile produces coverage artifacts and a zero exit 
code."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.delenv("PYTEST_ADDOPTS", raising=False)
    
        popen_envs, combine_calls = _install_pytest_stubs(monkeypatch)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______ test_fast_profile_missing_coverage_artifacts_returns_exit_code_one ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c5db7d0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_fast_profile_missing_cove0')

    @pytest.mark.fast
    def test_fast_profile_missing_coverage_artifacts_returns_exit_code_one(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Missing coverage artifacts in fast profile should exit with code 
1."""
    
        monkeypatch.chdir(tmp_path)
    
        runner = CliRunner()
>       app = _build_minimal_app(monkeypatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:81: in _build_minimal_app
    cli_module = _load_cli_module(monkeypatch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_coverage_artifacts.py:75: in _load_cli_module
    return 
importlib.import_module("devsynth.application.cli.commands.run_tests_cmd")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """CLI command to run DevSynth tests.
    
    Wraps :func:`devsynth.testing.run_tests` to provide a `devsynth run-tests`
    command. This command mirrors the options exposed by the underlying helper.
    
    Example:
        `devsynth run-tests --target unit-tests --speed fast`
    """
    
    from __future__ import annotations
    
    import os
    import shlex
    from typing import cast
    
    import typer
    
    # Import run_tests module so monkeypatching 
``devsynth.testing.run_tests.run_tests``
    # still affects the symbol used by this CLI command. Accessing attributes 
through
    # the module avoids stale references when tests replace ``run_tests``.
    import devsynth.testing.run_tests as run_tests_module
    
    # Ensure sitecustomize is loaded for Python 3.12+ compatibility patches
    import sitecustomize  # noqa: F401
>   from devsynth.config.provider_env import ProviderEnv
E   ModuleNotFoundError: No module named 'devsynth.config.provider_env'; 
'devsynth.config' is not a package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/commands/run_tests_cmd.py:25: ModuleNotFoundError
______________ test_inventory_mode_writes_file_and_prints_message ______________

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 
'devsynth.application.cli.commands.run_tests_cmd' has no attribute 
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc4f80>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_inventory_mode_writes_fil0')
capsys = <_pytest.capture.CaptureFixture object at 0x10c5d2d50>

    @pytest.mark.fast
    def test_inventory_mode_writes_file_and_prints_message(monkeypatch, 
tmp_path, capsys):
        """ReqID: TR-CLI-03  Inventory mode writes JSON and prints message.
    
        Validates that running with --inventory writes 
test_reports/test_inventory.json
        and prints a user-facing message with the path.
        """
        # run in temporary cwd
        monkeypatch.chdir(tmp_path)
    
        # Return deterministic collections
        def fake_collect(target: str, speed: str):  # noqa: ARG001
            return [f"{target}::{speed}::test_x"]
    
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("DEVSYNTH_INNER_TEST", "1")
        monkeypatch.setenv("DEVSYNTH_TEST_ALLOW_REQUESTS", "true")
    
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.run_tests_cmd.collect_tests_with_
cache",
            fake_collect,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'>
name = 'collect_tests_with_cache'
ann = 'devsynth.application.cli.commands.run_tests_cmd'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.application.cli.commands.run_tests_cmd has no attribute 
'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
___________________ test_inventory_handles_collection_errors ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10bdc4d40>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_inventory_handles_collect0')

    @pytest.mark.fast
    def test_inventory_handles_collection_errors(monkeypatch, tmp_path):
        """ReqID: TR-CLI-04  Inventory mode handles collection errors.
    
        When collection raises, ensure JSON still includes empty lists for all
            targets and speeds.
        """
        monkeypatch.chdir(tmp_path)
    
        def flaky_collect(target: str, speed: str):  # noqa: ARG001
            raise RuntimeError("boom")
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
>       monkeypatch.setattr(cli_module, "collect_tests_with_cache", 
flaky_collect)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'collect_tests_with_cache'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_inventory.py:60: AttributeError
______________ test_cli_report_flag_warns_when_directory_missing _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72ed50>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_report_flag_warns_whe0')

    @pytest.mark.fast
    def test_cli_report_flag_warns_when_directory_missing(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Successful runs with --report mention missing directories."""
    
        monkeypatch.chdir(tmp_path)
    
        # Provide minimal config loader stubs so CLI bootstraps cleanly.
        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader", 
core_stub)
    
        tinydb_stub = ModuleType("tinydb")
        tinydb_stub.Query = object  # type: ignore[attr-defined]
        tinydb_stub.TinyDB = object  # type: ignore[attr-defined]
        monkeypatch.setitem(sys.modules, "tinydb", tinydb_stub)
    
        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd", 
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.generate_docs_cmd",
                "generate_docs_cmd",
            ),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        runner = CliRunner()
    
>       monkeypatch.setattr(cli_module, "run_tests", lambda *_, **__: (True, 
"pytest ok"))
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_guidance.py:68: AttributeError
____________ test_cli_segment_option_failure_surfaces_failure_tips _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c72e240>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_segment_option_failur0')

    @pytest.mark.fast
    def test_cli_segment_option_failure_surfaces_failure_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Segmentation failures bubble remediation guidance and exit 
non-zero."""
    
        monkeypatch.chdir(tmp_path)
    
        core_stub = ModuleType("devsynth.core.config_loader")
        core_stub.ConfigSearchResult = object  # type: ignore[attr-defined]
        core_stub.load_config = lambda *args, **kwargs: object()
        core_stub._find_project_config = lambda *args, **kwargs: None
        monkeypatch.setitem(sys.modules, "devsynth.core.config_loader", 
core_stub)
        tinydb_stub = ModuleType("tinydb")
        tinydb_stub.Query = object  # type: ignore[attr-defined]
        tinydb_stub.TinyDB = object  # type: ignore[attr-defined]
        monkeypatch.setitem(sys.modules, "tinydb", tinydb_stub)
    
        for module_name, attr in [
            ("devsynth.application.cli.commands.edrr_cycle_cmd", 
"edrr_cycle_cmd"),
            ("devsynth.application.cli.commands.align_cmd", "align_cmd"),
            (
                "devsynth.application.cli.commands.analyze_manifest_cmd",
                "analyze_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.generate_docs_cmd",
                "generate_docs_cmd",
            ),
            ("devsynth.application.cli.commands.ingest_cmd", "ingest_cmd"),
            ("devsynth.application.cli.commands.doctor_cmd", "doctor_cmd"),
            (
                "devsynth.application.cli.commands.validate_manifest_cmd",
                "validate_manifest_cmd",
            ),
            (
                "devsynth.application.cli.commands.validate_metadata_cmd",
                "validate_metadata_cmd",
            ),
        ]:
            stub_module = ModuleType(module_name)
            setattr(stub_module, attr, lambda *args, **kwargs: None)
            monkeypatch.setitem(sys.modules, module_name, stub_module)
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        captured_args: tuple[object, ...] = ()
        captured_kwargs: dict[str, object] = {}
    
        def failing_run_tests(*args: object, **kwargs: object) -> tuple[bool, 
str]:
            nonlocal captured_args
            captured_args = args
            captured_kwargs.update(kwargs)
            return False, "pytest failure output\n" + SEGMENTATION_FAILURE_TIPS
    
>       monkeypatch.setattr(cli_module, "run_tests", failing_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_report_guidance.py:152: AttributeError
____________ test_segmented_cli_failure_emits_tips_and_reinjection _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b70b0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_cli_failure_emi0')

    @pytest.mark.fast
    def test_segmented_cli_failure_emits_tips_and_reinjection(
        monkeypatch, tmp_path
    ) -> None:
        """Segmented runs surface remediation tips and reinjection notices 
once."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        cov_calls: list[dict[str, str]] = []
        bdd_calls: list[dict[str, str]] = []
    
        def cov_wrapper(env: dict[str, str]) -> bool:
            cov_calls.append(env.copy())
            return run_tests_module.ensure_pytest_cov_plugin_env(env)
    
        def bdd_wrapper(env: dict[str, str]) -> bool:
            bdd_calls.append(env.copy())
            return run_tests_module.ensure_pytest_bdd_plugin_env(env)
    
>       monkeypatch.setattr(cli_module, "ensure_pytest_cov_plugin_env", 
cov_wrapper)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'ensure_pytest_cov_plugin_env'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:47: AttributeError
_ 
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-batch] 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b67e0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_cli_failure_rep0')
failed_batches = ['one']

    @pytest.mark.fast
    @pytest.mark.parametrize(
        "failed_batches",
        [
            pytest.param(["one"], id="single-batch"),
            pytest.param(["one", "two", "three"], id="multiple-batches"),
        ],
    )
    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        failed_batches: list[str],
    ) -> None:
        """Remediation banners surface once per failed segment and aggregate."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        observed_batches: list[str] = []
    
        def raising_segment(label: str) -> None:
            observed_batches.append(label)
            raise RuntimeError(f"segment {label} crashed")
    
        monkeypatch.setattr(
            run_tests_module,
            "_segment_batches",
            raising_segment,
            raising=False,
        )
    
        received_kwargs: dict[str, object] = {}
    
        def fake_run_tests(*_: object, **kwargs: object) -> tuple[bool, str]:
            received_kwargs.update(kwargs)
            batch_outputs: list[str] = []
            for index, batch in enumerate(failed_batches, start=1):
                try:
                    run_tests_module._segment_batches(batch)  # type: 
ignore[attr-defined]
                except RuntimeError as exc:  # pragma: no cover - exercised via 
test logic
                    batch_outputs.append(_build_batch_output(str(index), exc))
            batch_outputs.append(
                "Aggregate segmentation failure\n" + SEGMENTATION_FAILURE_TIPS
            )
            return False, "\n".join(batch_outputs)
    
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:139: AttributeError
_ 
test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-batch
es] _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b6a20>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_cli_failure_rep1')
failed_batches = ['one', 'two', 'three']

    @pytest.mark.fast
    @pytest.mark.parametrize(
        "failed_batches",
        [
            pytest.param(["one"], id="single-batch"),
            pytest.param(["one", "two", "three"], id="multiple-batches"),
        ],
    )
    def test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        failed_batches: list[str],
    ) -> None:
        """Remediation banners surface once per failed segment and aggregate."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "")
    
        app, cli_module = build_minimal_cli_app(monkeypatch)
    
        observed_batches: list[str] = []
    
        def raising_segment(label: str) -> None:
            observed_batches.append(label)
            raise RuntimeError(f"segment {label} crashed")
    
        monkeypatch.setattr(
            run_tests_module,
            "_segment_batches",
            raising_segment,
            raising=False,
        )
    
        received_kwargs: dict[str, object] = {}
    
        def fake_run_tests(*_: object, **kwargs: object) -> tuple[bool, str]:
            received_kwargs.update(kwargs)
            batch_outputs: list[str] = []
            for index, batch in enumerate(failed_batches, start=1):
                try:
                    run_tests_module._segment_batches(batch)  # type: 
ignore[attr-defined]
                except RuntimeError as exc:  # pragma: no cover - exercised via 
test logic
                    batch_outputs.append(_build_batch_output(str(index), exc))
            batch_outputs.append(
                "Aggregate segmentation failure\n" + SEGMENTATION_FAILURE_TIPS
            )
            return False, "\n".join(batch_outputs)
    
>       monkeypatch.setattr(cli_module, "run_tests", fake_run_tests)
E       AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> has no attribute 'run_tests'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_cmd_segmentation_regressions.py:139: AttributeError
___________________ test_run_tests_cli_feature_flags_set_env ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b6210>

    @pytest.mark.fast
    def test_run_tests_cli_feature_flags_set_env(monkeypatch):
        # Ensure a clean env for the feature vars
        monkeypatch.delenv("DEVSYNTH_FEATURE_EXPERIMENTAL", raising=False)
        monkeypatch.delenv("DEVSYNTH_FEATURE_LOGGING", raising=False)
    
        runner = CliRunner()
>       with patch(
            "devsynth.application.cli.commands.run_tests_cmd.run_tests",
            return_value=(True, ""),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_features.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x11f98fd40>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 
'devsynth.application.cli.commands.run_tests_cmd' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/commands/run_tests_cmd.py'> does not have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________ test_run_tests_cmd_applies_stub_offline_defaults_when_unset __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11f9b76b0>

    @pytest.mark.fast
    def test_run_tests_cmd_applies_stub_offline_defaults_when_unset(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        # Ensure variables are unset
        for key in [
            "DEVSYNTH_PROVIDER",
            "DEVSYNTH_OFFLINE",
            "DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE",
        ]:
            monkeypatch.delenv(key, raising=False)
    
>       with patch.object(module, "run_tests", return_value=(True, "")):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_provider_defaults.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x11f9ccbc0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________ test_run_tests_command_succeeds_without_optional_providers __________

    @pytest.mark.fast
    def test_run_tests_command_succeeds_without_optional_providers() -> None:
        """``devsynth run-tests`` should exit 0 without external providers.
    
        ReqID: FR-22
        """
    
        if os.environ.get("DEVSYNTH_INNER_TEST") == "1":
            pytest.skip("inner run")
    
        env = os.environ.copy()
        env["DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE"] = "false"
        env["DEVSYNTH_INNER_TEST"] = "1"
        env["PYTEST_ADDOPTS"] = "-k test_dummy"
>       result = subprocess.run(
            [
                "devsynth",
                "run-tests",
                "--speed",
                "fast",
            ],
            capture_output=True,
            text=True,
            env=env,
            cwd=Path(__file__).resolve().parents[5],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_subprocess.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:550: in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:1209: in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:2115: in _communicate
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/selectors.py:415: in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

signum = 14
frame = <frame at 0x10bf54d40, file 
'/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3
.12/lib/python3.12/selectors.py', line 416, code select>

    def _handler(signum, frame):  # noqa: ARG001 - signature required by signal
>       raise RuntimeError(
            f"Test timed out after {timeout} seconds 
(DEVSYNTH_TEST_TIMEOUT_SECONDS)"
        )
E       RuntimeError: Test timed out after 30 seconds 
(DEVSYNTH_TEST_TIMEOUT_SECONDS)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/fixtures/determinism.
py:113: RuntimeError
________________ test_invalid_target_exits_with_helpful_message ________________

    @pytest.mark.fast
    def test_invalid_target_exits_with_helpful_message() -> None:
        runner = CliRunner()
        app = build_app()
>       result = runner.invoke(app, ["run-tests", "--target", "weird-tests"])  #
nosec
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_validation.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke
    use_cli = _get_command(app)
              ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:352: in get_command
    click_command: click.Command = get_group(typer_instance)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:334: in get_group
    group = get_group_from_info(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:479: in get_group_from_info
    sub_group = get_group_from_info(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

group_info = <typer.models.TyperInfo object at 0x10c72ca40>

    def get_group_from_info(
        group_info: TyperInfo,
        *,
        pretty_exceptions_short: bool,
        rich_markup_mode: MarkupMode,
    ) -> TyperGroup:
>       assert group_info.typer_instance, (
               ^^^^^^^^^^^^^^^^^^^^^^^^^
            "A Typer instance is needed to generate a Click Group"
        )
E       AssertionError: A Typer instance is needed to generate a Click Group

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:466: AssertionError
________________ test_invalid_speed_exits_with_helpful_message _________________

    @pytest.mark.fast
    def test_invalid_speed_exits_with_helpful_message() -> None:
        runner = CliRunner()
        app = build_app()
        # Include one valid and one invalid to ensure detection
>       result = runner.invoke(
            app,
            [
                "run-tests",
                "--target",
                "unit-tests",
                "--speed",
                "fast",
                "--speed",
                "warp",
            ],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
commands/test_run_tests_validation.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/testing.py:20: in invoke
    use_cli = _get_command(app)
              ^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:352: in get_command
    click_command: click.Command = get_group(typer_instance)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:334: in get_group
    group = get_group_from_info(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:479: in get_group_from_info
    sub_group = get_group_from_info(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

group_info = <typer.models.TyperInfo object at 0x11f9b6990>

    def get_group_from_info(
        group_info: TyperInfo,
        *,
        pretty_exceptions_short: bool,
        rich_markup_mode: MarkupMode,
    ) -> TyperGroup:
>       assert group_info.typer_instance, (
               ^^^^^^^^^^^^^^^^^^^^^^^^^
            "A Typer instance is needed to generate a Click Group"
        )
E       AssertionError: A Typer instance is needed to generate a Click Group

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/typer/main.py:466: AssertionError
___________________ test_progress_manager_handles_lifecycle ____________________

    def test_progress_manager_handles_lifecycle():
        bridge = DummyBridge()
        manager = ProgressManager(bridge)  # type: ignore[arg-type]
    
        indicator = manager.create_progress("task", "Task", total=2)
        manager.update_progress("task", description="step one")
>       bridge.indicator.update.assert_called_once_with(advance=1, 
description="step one")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_progress.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.update' id='4503822672'>, args = ()
kwargs = {'advance': 1, 'description': 'step one'}
expected = call(advance=1, description='step one')
actual = call(advance=1, description='step one', status=None)
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x10bf163e0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: update(advance=1, description='step one')
E             Actual: update(advance=1, description='step one', status=None)

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
__________________________ test_parse_feature_options __________________________

    @pytest.mark.fast
    def test_parse_feature_options() -> None:
        """`_parse_feature_options` converts option values to booleans."""
    
>       result = module._parse_feature_options(["a", "b=false", "c=1"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'function' object has no attribute 
'_parse_feature_options'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:16: AttributeError
________________________ test_cli_accepts_feature_flags ________________________

    @pytest.mark.fast
    def test_cli_accepts_feature_flags() -> None:
        """CLI invocation with ``--feature`` delegates to `run_tests`."""
    
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as 
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:25: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c698740>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_cli_reports_coverage_percent _______________________

    @pytest.mark.fast
    def test_cli_reports_coverage_percent() -> None:
        """Successful runs print the measured coverage percentage."""
    
        runner = CliRunner()
        app = build_app()
    
        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers", 
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages", 
return_value=None),
            patch.object(
                module, "enforce_coverage_threshold", return_value=92.5
            ) as mock_enforce,
            patch.object(
                module, "_coverage_instrumentation_status", return_value=(True, 
None)
            ),
            patch.object(module, "coverage_artifacts_status", 
return_value=(True, None)),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c621a00>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_cli_errors_when_plugins_disabled _____________________

    @pytest.mark.fast
    def test_cli_errors_when_plugins_disabled() -> None:
        """CLI fails fast when pytest-cov was disabled by plugin autoload 
settings."""
    
        runner = CliRunner()
        app = build_app()
    
        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers", 
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages", 
return_value=None),
            patch.object(module, "enforce_coverage_threshold", 
return_value=95.0),
            patch.object(
                module,
                "_coverage_instrumentation_status",
                return_value=(
                    False,
                    "pytest plugin autoload disabled without -p pytest_cov",
                ),
            ),
            patch.object(
                module,
                "coverage_artifacts_status",
                return_value=(False, "Coverage JSON missing at 
test_reports/coverage.json"),
            ),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c63ec90>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_cli_errors_when_artifacts_missing ____________________

    @pytest.mark.fast
    def test_cli_errors_when_artifacts_missing() -> None:
        """CLI reports actionable remediation when coverage artifacts are 
absent."""
    
        runner = CliRunner()
        app = build_app()
    
        with (
>           patch.object(module, "run_tests", return_value=(True, "")),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            patch.object(module, "_configure_optional_providers", 
return_value=None),
            patch.object(module, "_emit_coverage_artifact_messages", 
return_value=None),
            patch.object(
                module, "enforce_coverage_threshold", return_value=95.0
            ) as mock_enforce,
            patch.object(
                module, "_coverage_instrumentation_status", return_value=(True, 
None)
            ),
            patch.object(
                module,
                "coverage_artifacts_status",
                return_value=(False, "Coverage JSON missing at 
test_reports/coverage.json"),
            ),
            patch.object(module, "increment_counter", return_value=None),
        ):

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c6c6420>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_feature_flags_set_environment ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c69b770>

    @pytest.mark.fast
    def test_feature_flags_set_environment(monkeypatch) -> None:
        """--feature name[=bool] should set DEVSYNTH_FEATURE_<NAME> env vars."""
        # Ensure clean env
        monkeypatch.delenv("DEVSYNTH_FEATURE_ALPHA", raising=False)
        monkeypatch.delenv("DEVSYNTH_FEATURE_BETA", raising=False)
    
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as 
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c686510>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
__________________ test_no_parallel_flag_is_passed_to_runner ___________________

    @pytest.mark.fast
    def test_no_parallel_flag_is_passed_to_runner() -> None:
        """--no-parallel should result in parallel=False in run_tests call."""
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as 
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c60ca40>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_____________________ test_segment_options_are_propagated ______________________

    @pytest.mark.fast
    def test_segment_options_are_propagated() -> None:
        """--segment and --segment-size should be passed through to 
run_tests."""
        runner = CliRunner()
>       with patch.object(module, "run_tests", return_value=(True, "")) as 
mock_run:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/cli/
test_run_tests_cmd_options.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x10c6c5d30>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <function run_tests_cmd at 0x10aba4720> does not 
have the attribute 'run_tests'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ test_project_state_analyzer_analyze_graceful_fallback _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10c6b1fa0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_project_state_analyzer_an0')

    @pytest.mark.fast
    def test_project_state_analyzer_analyze_graceful_fallback(monkeypatch, 
tmp_path: Path):
        """ReqID: T-ANALYZER-ERR-002
        When any internal step raises, ProjectStateAnalyzer.analyze should not 
raise
        and must return a safe dictionary shape with default values.
        It must also avoid creating files outside the provided environment.
        """
    
        # Force an early method to raise to cover the error path
        def boom(self, *args: Any, **kwargs: Any) -> None:  # noqa: ARG002
            raise RuntimeError("boom")
    
        monkeypatch.setattr(ProjectStateAnalyzer, "_index_files", boom)
    
        before = set(os.listdir(tmp_path))
    
        analyzer = ProjectStateAnalyzer(str(tmp_path))
    
        # Act
        result: dict[str, Any] = analyzer.analyze()
    
        after = set(os.listdir(tmp_path))
    
        # Assert: safe shape
        assert set(result.keys()) == {
            "files",
            "languages",
            "architecture",
            "components",
            "health_report",
        }
    
        assert result["files"] == {}
        assert result["languages"] == {}
        assert isinstance(result["architecture"], dict)
        assert result["architecture"].get("components", []) == []
        assert result["components"] == []
    
        hr = result["health_report"]
        assert isinstance(hr, dict)
>       assert hr.get("status") == "unknown"
E       AssertionError: assert None == 'unknown'
E        +  where None = <built-in method get of dict object at 
0x11f771d40>('status')
E        +    where <built-in method get of dict object at 0x11f771d40> = 
{'architecture': {'components': [], 'confidence': 0.0, 'type': 'unknown'}, 
'code_count': 0, 'file_count': 0, 'health_score': 0.0, ...}.get

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/code
_analysis/test_project_state_analyzer_error_paths.py:52: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:10,330 - 
devsynth.application.code_analysis.project_state_analyzer - INFO - Starting 
project analysis for 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_project_state_analyzer_an0
2025-10-29 10:48:10,330 - 
devsynth.application.code_analysis.project_state_analyzer - ERROR - 
ProjectStateAnalyzer.analyze failed: boom
------------------------------ Captured log call -------------------------------
INFO     
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615 
Starting project analysis for 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_project_state_analyzer_an0
ERROR    
devsynth.application.code_analysis.project_state_analyzer:logging_setup.py:615 
ProjectStateAnalyzer.analyze failed: boom
_______________ test_build_consensus_stores_decision_and_summary _______________

memory_manager = <devsynth.application.memory.memory_manager.MemoryManager 
object at 0x13ad3f4d0>

    @pytest.mark.fast
    def test_build_consensus_stores_decision_and_summary(
        memory_manager: MemoryManager,
    ) -> None:
        team = DummyTeam(memory_manager)
        task = {"id": "t1", "title": "Test"}
>       team.build_consensus(task)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_wsde_memory_sync_hooks.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:61: in build_consensus
    return self._build_consensus_inner(task, phase)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<tests.unit.application.collaboration.test_wsde_memory_sync_hooks.DummyTeam 
object at 0x13ad3f470>
task = {'id': 't1', 'title': 'Test'}, phase = None

    def _build_consensus_inner(
        self, task: dict[str, Any], phase: Phase | None = None
    ) -> ConsensusOutcome:
        """Internal implementation of consensus building."""
        if "id" not in task:
            task["id"] = str(uuid.uuid4())
    
        self.logger.info(
            f"Building consensus for task {task['id']}: {task.get('title', 
'Untitled')}"
        )
    
        task_text = (
            (task.get("description", "") or "") + " " + (task.get("title", "") 
or "")
        )
        keywords = set(re.findall(r"\b\w+\b", task_text.lower()))
    
        agent_opinions = self._collect_agent_opinion_records(task, 
keywords=keywords)
        if not agent_opinions:
            self._generate_agent_opinions(task)
            agent_opinions = self._collect_agent_opinion_records(
                task, keywords=keywords
            )
    
        consensus_id = str(uuid.uuid4())
>       conflicts = self._identify_conflicts(task, agent_opinions)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: DummyTeam._identify_conflicts() takes 2 positional arguments 
but 3 were given

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/wsde_team_consensus.py:87: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:12,208 - devsynth.application.memory.memory_manager - INFO - 
Memory Manager initialized with adapters: tinydb, graph
2025-10-29 10:48:12,208 - devsynth.application.memory.tiered_cache - INFO - 
Tiered cache initialized with max size 50
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory 
Manager initialized with adapters: tinydb, graph
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered 
cache initialized with max size 50
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,208 - dummy - INFO - Building consensus for task t1: Test
------------------------------ Captured log call -------------------------------
INFO     dummy:wsde_team_consensus.py:70 Building consensus for task t1: Test
______________ test_consensus_outcome_round_trip_orders_conflicts ______________

    @pytest.mark.fast
    def test_consensus_outcome_round_trip_orders_conflicts() -> None:
        mixin = DummyTeam()
        payload = {
            "dto_type": "ConsensusOutcome",
            "consensus_id": "c3",
            "task_id": "t1",
            "method": "conflict_resolution_synthesis",
            "agent_opinions": [
                {
                    "dto_type": "AgentOpinionRecord",
                    "agent_id": "beta",
                    "opinion": "no",
                    "timestamp": "2025-01-02T00:00:00",
                },
                {
                    "dto_type": "AgentOpinionRecord",
                    "agent_id": "alpha",
                    "opinion": "yes",
                    "timestamp": "2025-01-01T00:00:00",
                },
            ],
            "conflicts": [
                {
                    "dto_type": "ConflictRecord",
                    "conflict_id": "c2",
                    "task_id": "t1",
                    "agent_a": "beta",
                    "agent_b": "alpha",
                    "opinion_a": "no",
                    "opinion_b": "yes",
                },
                {
                    "dto_type": "ConflictRecord",
                    "conflict_id": "c1",
                    "task_id": "t1",
                    "agent_a": "alpha",
                    "agent_b": "beta",
                    "opinion_a": "yes",
                    "opinion_b": "no",
                },
            ],
            "conflicts_identified": 0,
            "synthesis": {
                "dto_type": "SynthesisArtifact",
                "text": "resolved",
                "key_points": ["compromise"],
                "expertise_weights": {"alpha": 0.6, "beta": 0.4},
                "readability_score": {"flesch_reading_ease": 65.0},
            },
            "timestamp": "2025-01-01T00:00:00",
        }
    
>       outcome = ConsensusOutcome.from_dict(payload)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/coll
aboration/test_wsde_team_consensus_summary.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:263: in from_dict
    return cls(**kwargs)
           ^^^^^^^^^^^^^
<string>:18: in __init__
    ???
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:333: in __post_init__
    sorted(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

opinion = {'agent_id': 'beta', 'dto_type': 'AgentOpinionRecord', 'opinion': 
'no', 'timestamp': '2025-01-02T00:00:00'}

>           key=lambda opinion: (opinion.agent_id or "", opinion.timestamp or 
""),
                                 ^^^^^^^^^^^^^^^^
        )
    )
E   AttributeError: 'dict' object has no attribute 'agent_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/dto.py:335: AttributeError
__ TestEDRRCoordinatorInitialization.test_coordinator_initialization_defaults __

self = <test_core.TestEDRRCoordinatorInitialization object at 0x10f733830>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpi6uywevk')

    @pytest.mark.fast
    def test_coordinator_initialization_defaults(self, tmp_project_dir):
        """Test coordinator initialization with default values."""
>       with patch(
            "devsynth.application.edrr.coordinator.core.get_llm_settings"
        ) as mock_get_settings:

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13ad56db0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.edrr.coordinator.core'
from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/e
drr/coordinator/core.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_ 
TestEDRRCoordinatorInitialization.test_coordinator_initialization_custom_config 
_

self = <test_core.TestEDRRCoordinatorInitialization object at 0x10f75c0e0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpfi254ltf')

    @pytest.mark.fast
    def test_coordinator_initialization_custom_config(self, tmp_project_dir):
        """Test coordinator initialization with custom configuration."""
        custom_config = {
            "max_recursion_depth": 5,
            "granularity_threshold": 0.3,
            "cost_benefit_ratio": 0.6,
        }
    
>       coordinator = EDRRCoordinator(custom_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 5 required positional 
arguments: 'wsde_team', 'code_analyzer', 'ast_transformer', 'prompt_manager', 
and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:89: TypeError
_ TestEDRRCoordinatorInitialization.test_coordinator_dependencies_initialization
_

self = <test_core.TestEDRRCoordinatorInitialization object at 0x10f75c230>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp0ahdq938')

    @pytest.mark.fast
    def test_coordinator_dependencies_initialization(self, tmp_project_dir):
        """Test that coordinator initializes all dependencies correctly."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:98: TypeError
_______ TestEDRRCoordinatorPhaseExecution.test_start_cycle_from_manifest _______

self = <test_core.TestEDRRCoordinatorPhaseExecution object at 0x10f75ce90>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpnl0p22hj')

    @pytest.mark.fast
    def test_start_cycle_from_manifest(self, tmp_project_dir):
        """Test cycle execution from manifest file."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:165: TypeError
___ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_depth_limit ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x10f75d400>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpp5jhqdv3')

    @pytest.mark.fast
    def test_should_terminate_recursion_depth_limit(self, tmp_project_dir):
        """Test recursion termination based on depth limit."""
>       coordinator = EDRRCoordinator({"max_recursion_depth": 2})
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 5 required positional 
arguments: 'wsde_team', 'code_analyzer', 'ast_transformer', 'prompt_manager', 
and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:198: TypeError
___ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_granularity ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x10f75d820>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp7wwfhthd')

    @pytest.mark.fast
    def test_should_terminate_recursion_granularity(self, tmp_project_dir):
        """Test recursion termination based on granularity threshold."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:211: TypeError
__ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_cost_benefit ___

self = <test_core.TestEDRRCoordinatorRecursion object at 0x10f75dcd0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmphidk15qk')

    @pytest.mark.fast
    def test_should_terminate_recursion_cost_benefit(self, tmp_project_dir):
        """Test recursion termination based on cost-benefit ratio."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:227: TypeError
_ TestEDRRCoordinatorRecursion.test_should_terminate_recursion_resource_limit __

self = <test_core.TestEDRRCoordinatorRecursion object at 0x10f75e180>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpfce5wnum')

    @pytest.mark.fast
    def test_should_terminate_recursion_resource_limit(self, tmp_project_dir):
        """Test recursion termination based on resource limits."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:243: TypeError
_ TestEDRRCoordinatorRecursion.test_should_not_terminate_recursion_good_metrics 
_

self = <test_core.TestEDRRCoordinatorRecursion object at 0x10f75e630>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpo_xzojcc')

    @pytest.mark.fast
    def test_should_not_terminate_recursion_good_metrics(self, tmp_project_dir):
        """Test that recursion continues when metrics are good."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:259: TypeError
________ TestEDRRCoordinatorMicroCycles.test_register_micro_cycle_hook _________

self = <test_core.TestEDRRCoordinatorMicroCycles object at 0x10f75e150>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp7i7o_9q2')

    @pytest.mark.fast
    def test_register_micro_cycle_hook(self, tmp_project_dir):
        """Test micro-cycle hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:301: TypeError
_________ TestEDRRCoordinatorMicroCycles.test_invoke_micro_cycle_hooks _________

self = <test_core.TestEDRRCoordinatorMicroCycles object at 0x10f75ed80>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmplh706gag')

    @pytest.mark.fast
    def test_invoke_micro_cycle_hooks(self, tmp_project_dir):
        """Test micro-cycle hook invocation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:314: TypeError
_______________ TestEDRRCoordinatorHooks.test_register_sync_hook _______________

self = <test_core.TestEDRRCoordinatorHooks object at 0x10f75f320>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpiwv6vynh')

    @pytest.mark.fast
    def test_register_sync_hook(self, tmp_project_dir):
        """Test sync hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:345: TypeError
_______________ TestEDRRCoordinatorHooks.test_invoke_sync_hooks ________________

self = <test_core.TestEDRRCoordinatorHooks object at 0x10f75f740>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmppv321wlq')

    @pytest.mark.fast
    def test_invoke_sync_hooks(self, tmp_project_dir):
        """Test sync hook invocation."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:358: TypeError
_____________ TestEDRRCoordinatorHooks.test_register_recovery_hook _____________

self = <test_core.TestEDRRCoordinatorHooks object at 0x10f7680e0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpktf9xxv0')

    @pytest.mark.fast
    def test_register_recovery_hook(self, tmp_project_dir):
        """Test recovery hook registration."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:380: TypeError
_____________ TestEDRRCoordinatorHooks.test_execute_recovery_hooks _____________

self = <test_core.TestEDRRCoordinatorHooks object at 0x10f768590>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp1qa357i6')

    @pytest.mark.fast
    def test_execute_recovery_hooks(self, tmp_project_dir):
        """Test recovery hook execution."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:393: TypeError
______ TestEDRRCoordinatorPhaseManagement.test_set_manual_phase_override _______

self = <test_core.TestEDRRCoordinatorPhaseManagement object at 0x10f7686b0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpys9m1vi4')

    @pytest.mark.fast
    def test_set_manual_phase_override(self, tmp_project_dir):
        """Test manual phase override setting."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:425: TypeError
_____ TestEDRRCoordinatorPhaseManagement.test_get_phase_quality_threshold ______

self = <test_core.TestEDRRCoordinatorPhaseManagement object at 0x10f768ad0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpf2zklsbv')

    @pytest.mark.fast
    def test_get_phase_quality_threshold(self, tmp_project_dir):
        """Test phase quality threshold retrieval."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:438: TypeError
_________ TestEDRRCoordinatorUtilityMethods.test_sanitize_positive_int _________

self = <test_core.TestEDRRCoordinatorUtilityMethods object at 0x10f769010>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpb_7hqh3i')

    @pytest.mark.fast
    def test_sanitize_positive_int(self, tmp_project_dir):
        """Test positive integer sanitization."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:456: TypeError
__________ TestEDRRCoordinatorUtilityMethods.test_sanitize_threshold ___________

self = <test_core.TestEDRRCoordinatorUtilityMethods object at 0x10f769460>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpbt7ssew0')

    @pytest.mark.fast
    def test_sanitize_threshold(self, tmp_project_dir):
        """Test threshold sanitization."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:481: TypeError
________ TestEDRRCoordinatorIntegration.test_edrr_cycle_error_recovery _________

self = <test_core.TestEDRRCoordinatorIntegration object at 0x10f76a2a0>
tmp_project_dir = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpri7rjlun')

    @pytest.mark.fast
    def test_edrr_cycle_error_recovery(self, tmp_project_dir):
        """Test EDRR cycle with error recovery."""
>       coordinator = EDRRCoordinator()
                      ^^^^^^^^^^^^^^^^^
E       TypeError: EDRRCoordinator.__init__() missing 6 required positional 
arguments: 'memory_manager', 'wsde_team', 'code_analyzer', 'ast_transformer', 
'prompt_manager', and 'documentation_manager'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/coordinator/test_core.py:574: TypeError
____________________ test_maybe_auto_progress_respects_flag ____________________

    @pytest.mark.fast
    def test_maybe_auto_progress_respects_flag():
        """Auto progress runs only when enabled.
    
        ReqID: N/A"""
        core = EDRRCoordinatorCore(
            memory_manager=MagicMock(spec=MemoryManager),
            wsde_team=MagicMock(spec=WSDETeam),
            code_analyzer=MagicMock(spec=CodeAnalyzer),
            ast_transformer=MagicMock(spec=AstTransformer),
            prompt_manager=MagicMock(spec=PromptManager),
            documentation_manager=MagicMock(spec=DocumentationManager),
        )
    
        core.current_phase = Phase.EXPAND
        core.task = {"name": "task"}
    
        core.config = {"auto_progress": True}
        with patch.object(
            core, "_decide_next_phase", return_value=Phase.DIFFERENTIATE
        ) as decide:
            with patch.object(core, "progress_to_phase") as progress:
                core._maybe_auto_progress()
>       decide.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_core.py:575: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='_decide_next_phase' id='5282516288'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected '_decide_next_phase' to have been called 
once. Called 10 times.
E           Calls: [call(), call(), call(), call(), call(), call(), call(), 
call(), call(), call()].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Initialized EDRR Coordinator (cycle_id: 3f36b9d2-d141-4f37-a339-2cb8453e5aae)
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
2025-10-29 10:48:12,456 - devsynth.application.edrr.coordinator_core - INFO - 
Auto-progressing to next phase
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Initialized EDRR Coordinator (cycle_id: 3f36b9d2-d141-4f37-a339-2cb8453e5aae)
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
INFO     devsynth.application.edrr.coordinator_core:logging_setup.py:615 
Auto-progressing to next phase
___________________ test_apply_dialectical_reasoning_success ___________________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x13ad5d460>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_success(coordinator: EDRRCoordinator) 
-> None:
        """ReqID: N/A"""
    
        final = {"status": "completed", "synthesis": "done"}
        with patch(
            "devsynth.application.edrr.coordinator.core.reasoning_loop",
            return_value=[{"synthesis": "next"}, final],
        ) as rl:
            result = coordinator.apply_dialectical_reasoning(
                {"solution": "initial"}, MagicMock()
            )
        rl.assert_called_once()
        coordinator.memory_manager.flush_updates.assert_called_once()
>       assert isinstance(result, DialecticalSequence)
E       AssertionError: assert False
E        +  where False = isinstance({'status': 'completed', 'synthesis': 
'done'}, DialecticalSequence)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_reasoning.py:51: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:12,481 - devsynth.application.edrr.templates - INFO - 
Registered expand_phase template
2025-10-29 10:48:12,481 - devsynth.application.edrr.templates - INFO - 
Registered differentiate_phase template
2025-10-29 10:48:12,481 - devsynth.application.edrr.templates - INFO - 
Registered refine_phase template
2025-10-29 10:48:12,481 - devsynth.application.edrr.templates - INFO - 
Registered retrospect_phase template
2025-10-29 10:48:12,481 - devsynth.application.edrr.manifest_parser - INFO - 
Manifest Parser initialized with enhanced traceability
2025-10-29 10:48:12,481 - devsynth.application.edrr.coordinator.core - INFO - 
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR 
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,481 - devsynth.application.edrr.coordinator.core - INFO - 
EDRRCoordinator invoking dialectical reasoning
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 
EDRRCoordinator invoking dialectical reasoning
______________ test_apply_dialectical_reasoning_consensus_failure ______________

coordinator = <devsynth.application.edrr.coordinator.core.EDRRCoordinator object
at 0x13ad4b1d0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13ad45250>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_consensus_failure(
        coordinator: EDRRCoordinator, caplog: pytest.LogCaptureFixture
    ) -> None:
        """ReqID: N/A"""
    
        with patch(
            "devsynth.application.edrr.coordinator.core.reasoning_loop",
            return_value=[],
        ):
            with caplog.at_level(logging.WARNING):
                result = coordinator.apply_dialectical_reasoning(
                    {"solution": "initial"}, MagicMock()
                )
>       assert isinstance(result, DialecticalSequence)
E       assert False
E        +  where False = isinstance({}, DialecticalSequence)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_coordinator_reasoning.py:70: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:12,486 - devsynth.application.edrr.templates - INFO - 
Registered expand_phase template
2025-10-29 10:48:12,486 - devsynth.application.edrr.templates - INFO - 
Registered differentiate_phase template
2025-10-29 10:48:12,486 - devsynth.application.edrr.templates - INFO - 
Registered refine_phase template
2025-10-29 10:48:12,486 - devsynth.application.edrr.templates - INFO - 
Registered retrospect_phase template
2025-10-29 10:48:12,487 - devsynth.application.edrr.manifest_parser - INFO - 
Manifest Parser initialized with enhanced traceability
2025-10-29 10:48:12,487 - devsynth.application.edrr.coordinator.core - INFO - 
EDRR coordinator initialized (recursion depth: 0)
------------------------------ Captured log setup ------------------------------
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
expand_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
differentiate_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
refine_phase template
INFO     devsynth.application.edrr.templates:logging_setup.py:615 Registered 
retrospect_phase template
INFO     devsynth.application.edrr.manifest_parser:logging_setup.py:615 Manifest
Parser initialized with enhanced traceability
INFO     devsynth.application.edrr.coordinator.core:logging_setup.py:615 EDRR 
coordinator initialized (recursion depth: 0)
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,487 - devsynth.application.edrr.coordinator.core - WARNING -
Consensus failure during dialectical reasoning
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.edrr.coordinator.core:logging_setup.py:615 
Consensus failure during dialectical reasoning
______________ test_decide_next_phase_respects_quality_threshold _______________

coordinator = 
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x13b0fc620>

    def test_decide_next_phase_respects_quality_threshold(
        coordinator: StubCoordinator,
    ) -> None:
        """ReqID: N/A"""
    
        coordinator.current_phase = Phase.EXPAND
        coordinator.results = {"EXPAND": {"quality_score": 0.2}}
        coordinator._quality_thresholds = {Phase.EXPAND: 0.5}
        assert coordinator._decide_next_phase() is None
    
        coordinator.results["EXPAND"]["quality_score"] = 0.8
        coordinator._phase_start_times[Phase.EXPAND] = datetime.now() - 
timedelta(
            seconds=10
        )
        coordinator.phase_transition_timeout = 1
>       assert coordinator._decide_next_phase() == Phase.DIFFERENTIATE
E       AssertionError: assert None == <Phase.DIFFERENTIATE: 'differentiate'>
E        +  where None = _decide_next_phase()
E        +    where _decide_next_phase = 
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x13b0fc620>._decide_next_phase
E        +  and   <Phase.DIFFERENTIATE: 'differentiate'> = Phase.DIFFERENTIATE

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_management_module.py:108: AssertionError
_________________ test_maybe_auto_progress_invokes_progression _________________

coordinator = 
<tests.unit.application.edrr.test_phase_management_module.StubCoordinator object
at 0x13ad54860>

    def test_maybe_auto_progress_invokes_progression(coordinator: 
StubCoordinator) -> None:
        """ReqID: N/A"""
    
        coordinator.auto_phase_transitions = True
        coordinator.wsde_team.elaborate_details = MagicMock()
        coordinator._decide_next_phase = MagicMock(side_effect=[Phase.REFINE, 
None])
        coordinator.progress_to_phase = MagicMock()
    
        coordinator._maybe_auto_progress()
    
>       coordinator.progress_to_phase.assert_called_once_with(Phase.REFINE)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_phase_management_module.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock id='5281966720'>, args = (<Phase.REFINE: 'refine'>,)
kwargs = {}, msg = "Expected 'mock' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'mock' to be called once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:960: AssertionError
________________ test_reasoning_loop_retries_on_transient_error ________________

obj = <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'>
name = '_apply_dialectical_reasoning'
ann = 'devsynth.methodology.edrr.reasoning_loop'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.methodology.edrr.reasoning_loop' 
has no attribute '_apply_dialectical_reasoning'. Did you mean: 
'ApplyDialecticalReasoning'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0f9640>

    @pytest.mark.fast
    @pytest.mark.unit
    def test_reasoning_loop_retries_on_transient_error(monkeypatch):
        calls = {"count": 0}
    
        def flaky_apply(team, task, critic, memory):  # signature mirrors 
underlying call
            calls["count"] += 1
            # First call fails with a transient error; second returns success
            if calls["count"] == 1:
                raise RuntimeError("transient")
            return {
                "status": "completed",
                "phase": "refine",
                "synthesis": task.get("solution"),
            }
    
        # Patch the internal alias used by the reasoning loop
>       monkeypatch.setattr(
            "devsynth.methodology.edrr.reasoning_loop._apply_dialectical_reasoni
ng",
            flaky_apply,
            raising=True,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/edrr
/test_reasoning_loop_retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'>
name = '_apply_dialectical_reasoning'
ann = 'devsynth.methodology.edrr.reasoning_loop'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at 
devsynth.methodology.edrr.reasoning_loop has no attribute 
'_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
____________________ test_openai_provider_requires_api_key _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13adaa330>

    @pytest.mark.fast
    def test_openai_provider_requires_api_key(monkeypatch):
        """Missing API key should raise a clear error."""
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)
    
        # Import the module (it should handle missing OpenAI gracefully)
        module = 
importlib.import_module("devsynth.application.llm.openai_provider")
    
        # Test that instantiation fails with a clear error when no API key is 
provided
        with pytest.raises(module.OpenAIConnectionError) as exc:
>           module.OpenAIProvider({})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_import_without_openai.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x13b061d60>
config = {}

    def __init__(self, config: dict[str, Any] = None):
        """Initialize the OpenAI provider.
    
        Args:
            config: Configuration dictionary with the following keys:
                - api_key: OpenAI API key (default: from config)
                - model: Model name to use (default: from config)
                - max_tokens: Maximum tokens for responses (default: from 
config)
                - temperature: Temperature for generation (default: from config)
                - api_base: Base URL for the OpenAI API (default: from config)
    
        Raises:
            OpenAIConnectionError: If no API key is provided or available in 
environment
        """
        # Get default settings from configuration
        from ...config.settings import get_llm_settings
    
        default_settings = get_llm_settings()
    
        # Initialize with default settings, overridden by provided config
        self.config = {**default_settings, **(config or {})}
    
        # Set instance variables from config using standardized parameter names
        self.api_key = self.config.get("api_key") or self.config.get(
            "openai_api_key"
        )  # Support both old and new names
        self.model = (
            self.config.get("model")
            or self.config.get("openai_model")
            or "gpt-3.5-turbo"
        )
        self.max_tokens = self.config.get("max_tokens") or 1024
        self.temperature = self.config.get("temperature") or 0.7
        self.api_base = self.config.get("base_url") or self.config.get(
            "api_base"
        )  # Support both old and new names
        self.timeout = self.config.get("timeout") or 60
        self.max_retries = self.config.get("max_retries", 3)
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=self.config.get("failure_threshold", 3),
            recovery_timeout=self.config.get("recovery_timeout", 60),
        )
        # Deterministic per-call timeout (seconds)
        # Env var precedence: OPENAI_HTTP_TIMEOUT (docs/tasks.md Task 70)
        try:
            timeout_env = os.environ.get("OPENAI_HTTP_TIMEOUT")
            self.call_timeout = (
                float(timeout_env)
                if timeout_env is not None
                else float(self.config.get("call_timeout", 15))
            )
        except (TypeError, ValueError):
            self.call_timeout = 15.0
    
        # Check for API key in config or environment
        if not self.api_key and "OPENAI_API_KEY" in os.environ:
            self.api_key = os.environ["OPENAI_API_KEY"]
    
        # Initialize token tracker
        self.token_tracker = TokenTracker()
    
        # Require API key explicitly for this provider; tests enforce clear 
error
        if not self.api_key:
>           raise OpenAIAuthenticationError(
                "OpenAI API key is required. Set OPENAI_API_KEY or provide 
'api_key' in config."
            )
E           devsynth.application.llm.openai_provider.OpenAIAuthenticationError: 
OpenAI API key is required. Set OPENAI_API_KEY or provide 'api_key' in config.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:132: OpenAIAuthenticationError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,658 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
____________ test_health_check_succeeds_when_sync_api_lists_models _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b060050>

    @pytest.mark.fast
    def test_health_check_succeeds_when_sync_api_lists_models(monkeypatch):
        """ReqID: LMSTUDIO-HC-1
        When sync_api.list_downloaded_models returns, health_check should be 
True.
        """
        # Ensure resource flag is enabled so health_check runs
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "true")
    
        from devsynth.application.llm.lmstudio_provider import LMStudioProvider
    
        provider = LMStudioProvider({"auto_select_model": False})
    
        # Patch list_downloaded_models to return non-empty list quickly
        with patch(
            "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.list_d
ownloaded_models",
            return_value=[type("M", (), {"model_key": "m", "display_name": 
"M"})()],
        ):
>           assert provider.health_check() is True
E           assert False is True
E            +  where False = health_check()
E            +    where health_check = 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13b063680>.health_check

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_health_check.py:25: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:12,664 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: module 'lmstudio' has no 
attribute 'sync_api'
2025-10-29 10:48:12,664 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:12,665 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:12,665 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:12,665 - devsynth.application.llm.lmstudio_provider - INFO - 
Using default model: qwen/qwen3-4b-2507
2025-10-29 10:48:13,171 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio health_check failed within budget: Network access disabled during tests
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio default client configuration failed: module 'lmstudio' has no attribute 
'sync_api'
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using 
default model: qwen/qwen3-4b-2507
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio health_check failed within budget: Network access disabled during tests
_________ test_health_check_bounded_retry_and_returns_false_on_failure _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b063d70>

    @pytest.mark.fast
    def 
test_health_check_bounded_retry_and_returns_false_on_failure(monkeypatch):
        """ReqID: LMSTUDIO-HC-2
        If sync_api.list_downloaded_models keeps failing, health_check returns 
False within ~5s budget.
        """
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "true")
        # Keep retries small and timeout small to speed up
        monkeypatch.setenv("DEVSYNTH_LMSTUDIO_RETRIES", "3")
        monkeypatch.setenv("DEVSYNTH_LMSTUDIO_TIMEOUT_SECONDS", "0.4")
    
        from devsynth.application.llm.lmstudio_provider import LMStudioProvider
    
        provider = LMStudioProvider({"auto_select_model": False})
    
        call_count = {"n": 0}
    
        def _boom(kind: str):  # noqa: ARG001
            call_count["n"] += 1
            raise RuntimeError("unreachable")
    
        with (
            patch(
                "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.li
st_downloaded_models",
                side_effect=_boom,
            ),
            patch(
                "devsynth.application.llm.lmstudio_provider.lmstudio.sync_api.co
nfigure_default_client",
                return_value=None,
            ),
        ):
            t0 = time.perf_counter()
            ok = provider.health_check()
            duration = time.perf_counter() - t0
            assert ok is False
            assert duration <= 5.5  # bounded by implementation budget
>           assert call_count["n"] >= 1
E           assert 0 >= 1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_health_check.py:63: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:13,188 - devsynth.application.llm.lmstudio_provider - WARNING -
LM Studio default client configuration failed: module 'lmstudio' has no 
attribute 'sync_api'
2025-10-29 10:48:13,189 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:13,190 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:13,190 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:13,191 - devsynth.application.llm.lmstudio_provider - INFO - 
Using default model: qwen/qwen3-4b-2507
2025-10-29 10:48:16,711 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio health_check failed within budget: Network access disabled during tests
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio default client configuration failed: module 'lmstudio' has no attribute 
'sync_api'
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Using 
default model: qwen/qwen3-4b-2507
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio health_check failed within budget: Network access disabled during tests
____________ TestRequireLMStudio.test_require_lmstudio_import_error ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestRequireLMStudio 
object at 0x118d61340>

    @pytest.mark.fast
    def test_require_lmstudio_import_error(self):
        """Test error handling when lmstudio module is not available."""
        with patch.dict("sys.modules", {}, clear=True):
            with patch(
                "builtins.__import__", side_effect=ImportError("Module not 
found")
            ):
                with pytest.raises(DevSynthError) as exc_info:
>                   _require_lmstudio()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:43: in _require_lmstudio
    import lmstudio as _lmstudio  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/__init__.py:16: in <module>
    from .sdk_api import *
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/lmstudio/sdk_api.py:3: in <module>
    from contextlib import AsyncContextDecorator, ContextDecorator
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:3: in <module>
    import os
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap>:1176: in exec_module
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   AttributeError: module 'sys' has no attribute 'builtin_module_names'

<frozen os>:33: AttributeError
_______ TestLMStudioProvider.test_provider_initialization_default_config _______

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118b70500>

    @pytest.mark.fast
    def test_provider_initialization_default_config(self):
        """Test provider initialization with default configuration."""
>       with patch(
            "devsynth.application.llm.lmstudio_provider.get_llm_settings"
        ) as mock_get_settings:

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:226: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13ad48740>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'devsynth.application.llm.lmstudio_provider'
from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/lmstudio_provider.py'> does not have the attribute 'get_llm_settings'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______ TestLMStudioProvider.test_provider_initialization_custom_config ________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118b701a0>

    @pytest.mark.fast
    def test_provider_initialization_custom_config(self):
        """Test provider initialization with custom configuration."""
        custom_config = {
            "api_base": "http://custom:8080",
            "model": "custom-model",
            "max_tokens": 2000,
            "temperature": 0.8,
        }
    
        provider = LMStudioProvider(custom_config)
    
        assert provider.api_base == "http://custom:8080"
>       assert provider.model == "custom-model"
E       AssertionError: assert 'qwen/qwen3-4b-2507' == 'custom-model'
E         
E         - custom-model
E         + qwen/qwen3-4b-2507

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:262: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,739 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,739 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,744 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): 
Max retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad536e0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)"))
2025-10-29 10:48:23,745 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'custom-model': Failed to connect to LM Studio:
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url: 
/api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad536e0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)")), falling back to auto-selection
2025-10-29 10:48:23,747 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): 
Max retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad50aa0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)"))
2025-10-29 10:48:23,747 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: 
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url: 
/api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad50aa0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)")). Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad536e0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)"))
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'custom-model': Failed to connect to LM Studio: 
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url: 
/api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad536e0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)")), falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: HTTPConnectionPool(host='custom', port=8080): Max 
retries exceeded with url: /api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad50aa0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)"))
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: 
HTTPConnectionPool(host='custom', port=8080): Max retries exceeded with url: 
/api/v0/models (Caused by 
NameResolutionError("<urllib3.connection.HTTPConnection object at 0x13ad50aa0>: 
Failed to resolve 'custom' ([Errno 8] nodename nor servname provided, or not 
known)")). Using fallback: qwen/qwen3-4b-2507
______________ TestLMStudioProvider.test_provider_complete_method ______________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d61e50>

    @pytest.mark.fast
    def test_provider_complete_method(self):
        """Test the complete method functionality."""
        provider = LMStudioProvider()
    
>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13aded370>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13adee450> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,755 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,755 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,755 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,755 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,756 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,756 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_______________ TestLMStudioProvider.test_provider_embed_method ________________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d622d0>

    @pytest.mark.fast
    def test_provider_embed_method(self):
        """Test the embed method functionality."""
        provider = LMStudioProvider()
    
>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13ad4bbf0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13ad4b6e0> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,793 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,793 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,794 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,794 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,794 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,794 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________ TestLMStudioProvider.test_provider_health_check_success ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d62720>

    @pytest.mark.fast
    def test_provider_health_check_success(self):
        """Test successful health check."""
        provider = LMStudioProvider()
    
>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13ad47200>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13ad47ce0> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,827 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,827 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,828 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,828 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,829 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,829 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________ TestLMStudioProvider.test_provider_health_check_failure ____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d62ba0>

    @pytest.mark.fast
    def test_provider_health_check_failure(self):
        """Test failed health check."""
        provider = LMStudioProvider()
    
>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:321: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13adaf4d0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13adad5e0> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,860 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,860 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,861 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,861 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,862 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,862 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_____________ TestLMStudioProvider.test_provider_get_client_method _____________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d63020>

    @pytest.mark.fast
    def test_provider_get_client_method(self):
        """Test the _get_client method."""
        provider = LMStudioProvider()
    
        with patch(
            "devsynth.application.llm.lmstudio_provider.lmstudio"
        ) as mock_lmstudio:
            mock_client = MagicMock()
            mock_lmstudio.llm.return_value = mock_client
    
>           result = provider._get_client()
                     ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute 
'_get_client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:339: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,893 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,893 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,894 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,894 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,894 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,894 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______________ TestLMStudioProvider.test_provider_model_property _______________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d634a0>

    @pytest.mark.fast
    def test_provider_model_property(self):
        """Test the model property getter and setter."""
        provider = LMStudioProvider()
    
        # Test getter
>       assert provider.model == "default-model"
E       AssertionError: assert 'qwen/qwen3-4b-2507' == 'default-model'
E         
E         - default-model
E         + qwen/qwen3-4b-2507

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:350: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,902 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,902 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,903 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,903 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,903 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,903 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_________ TestLMStudioProvider.test_provider_available_models_property _________

self = <tests.unit.application.llm.test_lmstudio_provider.TestLMStudioProvider 
object at 0x118d63920>

    @pytest.mark.fast
    def test_provider_available_models_property(self):
        """Test the available_models property."""
        provider = LMStudioProvider()
    
>       with patch.object(provider, "_get_client") as mock_get_client:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_lmstudio_provider.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x13adb9130>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13adb9eb0> does not have the attribute '_get_client'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:23,909 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:23,909 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:23,910 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,910 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:23,910 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:23,910 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___________________ test_default_selection_is_deterministic ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad4ccb0>

    def test_default_selection_is_deterministic(monkeypatch):
        """Factory should fall back to the next provider in order."""
    
        class DummyOpenAI:
            def __init__(self, config=None):
                self.config = config
    
        class DummyAnthropic:
            def __init__(self, config=None):
                self.config = config
    
        monkeypatch.setattr(
            factory, "provider_types", {"openai": DummyOpenAI, "anthropic": 
DummyAnthropic}
        )
        monkeypatch.delitem(factory.provider_types, "openai", raising=False)
>       provider = factory.create_provider()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_factory.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:61: in create_provider
    return super().create_provider("offline", config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.provider_factory.ProviderFactory object at 
0x118bfd6a0>
provider_type = 'offline', config = None

    def create_provider(
        self, provider_type: str, config: dict[str, Any] = None
    ) -> LLMProvider:
        """Create an LLM provider of the specified type."""
        if provider_type not in self.provider_types:
            if provider_type == "lmstudio":
                raise ValidationError(
                    "LMStudio provider is unavailable. Install the 'lmstudio' 
package to enable this provider."
                )
>           raise ValidationError(f"Unknown provider type: {provider_type}")
E           devsynth.application.llm.providers.ValidationError: Unknown provider
type: offline

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:339: ValidationError
_______________________ test_case_insensitive_selection ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0a8620>

    def test_case_insensitive_selection(monkeypatch):
        class DummyOpenAI:
            def __init__(self, config=None):
                self.config = config
    
        monkeypatch.setattr(factory, "provider_types", {"openai": DummyOpenAI})
>       provider = factory.create_provider("OPENAI")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_factory.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/provider_factory.py:50: in create_provider
    return super().create_provider("offline", config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.provider_factory.ProviderFactory object at 
0x118bfd6a0>
provider_type = 'offline', config = None

    def create_provider(
        self, provider_type: str, config: dict[str, Any] = None
    ) -> LLMProvider:
        """Create an LLM provider of the specified type."""
        if provider_type not in self.provider_types:
            if provider_type == "lmstudio":
                raise ValidationError(
                    "LMStudio provider is unavailable. Install the 'lmstudio' 
package to enable this provider."
                )
>           raise ValidationError(f"Unknown provider type: {provider_type}")
E           devsynth.application.llm.providers.ValidationError: Unknown provider
type: offline

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/providers.py:339: ValidationError
________________________ test_get_llm_provider_offline _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0a8200>

    @pytest.mark.fast
    def test_get_llm_provider_offline(monkeypatch):
        """Selects offline provider when offline mode is enabled.
    
        ReqID: FR-85"""
        dummy = DummyFactory()
        monkeypatch.setattr(providers, "factory", dummy)
        monkeypatch.setattr(
            providers,
            "load_config",
            lambda: types.SimpleNamespace(
                as_dict=lambda: {"offline_mode": True, "offline_provider": 
"local"}
            ),
        )
>       monkeypatch.setattr(providers, "get_llm_settings", lambda: {"provider": 
"openai"})
E       AttributeError: <module 'devsynth.application.llm.providers' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_selection.py:31: AttributeError
________________________ test_get_llm_provider_default _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0a9070>

    @pytest.mark.fast
    def test_get_llm_provider_default(monkeypatch):
        """Uses configured provider when offline mode is disabled.
    
        ReqID: FR-85"""
        dummy = DummyFactory()
        monkeypatch.setattr(providers, "factory", dummy)
        monkeypatch.setattr(
            providers, "load_config", lambda: 
types.SimpleNamespace(as_dict=lambda: {})
        )
>       monkeypatch.setattr(providers, "get_llm_settings", lambda: {"provider": 
"local"})
E       AttributeError: <module 'devsynth.application.llm.providers' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/providers.py'> has no attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/llm/
test_provider_selection.py:49: AttributeError
_______ TestExecutionLearningIntegration.test_learn_from_code_execution ________

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x118e113d0>

    def test_learn_from_code_execution(self):
        """Test learning from code execution."""
        code_snippets = [
            "def add(a, b): return a + b",
            "def multiply(a, b): return a * b",
        ]
    
        # Mock the learning process
        with patch.object(
            self.integration.trajectory_collector, "collect_python_trajectories"
        ) as mock_collect:
            mock_collect.return_value = [
                ExecutionTrace(
                    code=code_snippets[0],
                    execution_steps=[
                        ExecutionStep(
                            step_number=1, line_number=1, code_line="def add(a, 
b):"
                        )
                    ],
                    execution_outcome="success",
                )
            ]
    
            with patch.object(
                self.integration.learning_algorithm, "train_on_trajectories"
            ) as mock_train:
                mock_train.return_value = {
                    "trajectories_processed": 1,
                    "patterns_extracted": 2,
                    "validation_score": 0.85,
                    "patterns": {"pattern1": Mock(), "pattern2": Mock()},
                    "understandings": {"understanding1": Mock()},
                }
    
>               result = 
self.integration.learn_from_code_execution(code_snippets)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:68: in learn_from_code_execution
    semantic_understandings = 
self.learning_algorithm._build_semantic_understanding(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_algorithm.py:306: in _build_semantic_understanding
    relevant_patterns = self._find_relevant_patterns(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.execution_learning_algorithm.ExecutionLearningAlgor
ithm object at 0x13b0a6300>
patterns = {'pattern1': <Mock id='5285225216'>, 'pattern2': <Mock 
id='5285217776'>}
components = {'avg_execution_time': 0.0, 'error_types': [], 'function_calls': 
{}, 'success_rate': 1.0, ...}

    def _find_relevant_patterns(
        self, patterns: list[ExecutionPattern], components: dict[str, Any]
    ) -> list[ExecutionPattern]:
        """Find patterns relevant to the given semantic components."""
        relevant_patterns = []
    
        # Match patterns based on function calls
        if "function_calls" in components:
            for pattern in patterns:
>               if pattern.pattern_type == "function_behavior":
                   ^^^^^^^^^^^^^^^^^^^^
E               AttributeError: 'str' object has no attribute 'pattern_type'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_algorithm.py:405: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,585 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,585 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,585 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,585 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
_______ TestExecutionLearningIntegration.test_enhance_code_understanding _______

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x118e119a0>

    def test_enhance_code_understanding(self):
        """Test code understanding enhancement."""
        code = (
            "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + 
fibonacci(n-2)"
        )
    
        # Mock pattern library
        with patch.object(
            self.integration.pattern_library, "find_matches"
        ) as mock_find:
            mock_pattern = Mock()
            mock_pattern.pattern_id = "fib_pattern"
            mock_pattern.pattern_type = "recursive_algorithm"
            mock_pattern.confidence = 0.9
            mock_find.return_value = [mock_pattern]
    
            with patch.object(
                self.integration.understanding_engine, 
"predict_execution_behavior"
            ) as mock_predict:
                mock_predict.return_value = {
                    "prediction": "recursive_execution",
                    "confidence": 0.85,
                    "predicted_success_rate": 0.9,
                }
    
                with patch.object(
                    self.integration.understanding_engine, 
"analyze_behavioral_intent"
                ) as mock_intent:
                    mock_intent.return_value = Mock(
                        primary_purpose="fibonacci_calculation",
                        complexity_level="moderate",
                        intent_confidence=0.8,
                    )
    
>                   result = self.integration.enhance_code_understanding(code)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:125: in enhance_code_understanding
    components = self.understanding_engine.extract_semantic_components(code)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:91: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x13b02adb0>
code = 'def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + 
fibonacci(n-2)'

    def _analyze_ast_structure(self, code: str) -> dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)
    
            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:275: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,605 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,606 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,606 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,606 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
__ TestExecutionLearningIntegration.test_validate_against_research_benchmarks __

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x118e128d0>

    def test_validate_against_research_benchmarks(self):
        """Test validation against research benchmarks."""
        validation_suite = {
            "semantic_robustness": Mock(
                overall_score=0.91, benchmark_compliance={"mutation_resistance":
True}
            ),
            "execution_prediction": Mock(
                overall_score=0.83, benchmark_compliance={"prediction_accuracy":
True}
            ),
            "multi_hop_reasoning": Mock(
                overall_score=0.87, benchmark_compliance={"multi_hop_accuracy": 
True}
            ),
        }
    
>       result = 
self.integration.validate_against_research_benchmarks(validation_suite)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.execution_learning_integration.ExecutionLearningInt
egration object at 0x13b0c68a0>
test_results = {'execution_prediction': <Mock id='5285634288'>, 
'multi_hop_reasoning': <Mock id='5285626080'>, 'semantic_robustness': <Mock 
id='5285626128'>}

    def validate_against_research_benchmarks(
        self, test_results: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate learning results against research benchmarks."""
        benchmarks = {
            "semantic_understanding": 0.8,  # 80% semantic understanding target
            "mutation_resistance": 0.9,  # 90% resistance to semantic mutations
            "pattern_accuracy": 0.85,  # 85% pattern prediction accuracy
            "execution_prediction": 0.8,  # 80% execution outcome prediction
        }
    
        validation_report = {
            "benchmark_comparison": {},
            "research_alignment": True,
            "improvement_areas": [],
            "validation_method": "research_benchmark_comparison",
        }
    
        # Compare against benchmarks
        for metric, benchmark_value in benchmarks.items():
            if metric in test_results:
                achieved_value = test_results[metric]
>               meets_benchmark = achieved_value >= benchmark_value
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E               TypeError: '>=' not supported between instances of 'Mock' and 
'float'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:424: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,631 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,631 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,631 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,631 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
______ TestExecutionLearningIntegration.test_export_import_learning_state ______

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestExecution
LearningIntegration object at 0x118e12de0>

    def test_export_import_learning_state(self):
        """Test learning state export and import."""
        # Set up learning state
        self.integration.learning_history = [{"test": "session"}]
        self.integration.understanding_cache = {"test": "cache"}
    
        # Mock pattern library export
        with patch.object(
            self.integration.pattern_library, "export_patterns"
        ) as mock_export:
            mock_export.return_value = {"patterns": {}, "total_patterns": 0}
    
            # Export state
>           exported_state = self.integration.export_learning_state()
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:231: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:457: in export_learning_state
    "statistics": self.get_learning_statistics(),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:370: in get_learning_statistics
    total_patterns = sum(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <list_iterator object at 0x13b0fa230>

    total_patterns = sum(
>       session["patterns_learned"] for session in self.learning_history
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
E   KeyError: 'patterns_learned'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:371: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,641 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,641 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,641 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,641 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
_______ TestSemanticUnderstandingEngine.test_extract_semantic_components _______

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x118e29160>

        def test_extract_semantic_components(self):
            """Test semantic component extraction."""
            code = """
    def calculate_fibonacci(n):
        if n <= 1:
            return n
        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
    """
    
>           components = self.engine.extract_semantic_components(code)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:408: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:91: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x13b02b710>
code = '\ndef calculate_fibonacci(n):\n    if n <= 1:\n        return n\n    
return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n'

    def _analyze_ast_structure(self, code: str) -> dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)
    
            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:275: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,690 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,690 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
_______ TestSemanticUnderstandingEngine.test_detect_semantic_equivalence _______

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x118e29af0>

    def test_detect_semantic_equivalence(self):
        """Test semantic equivalence detection."""
        code1 = (
            "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + 
fibonacci(n-2)"
        )
        code2 = "def fib_calc(num): return num if num <= 1 else fib_calc(num-1) 
+ fib_calc(num-2)"
    
>       equivalence = self.engine.detect_semantic_equivalence(code1, code2)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:442: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:180: in detect_semantic_equivalence
    components1 = self.extract_semantic_components(code1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:91: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x13b0200e0>
code = 'def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + 
fibonacci(n-2)'

    def _analyze_ast_structure(self, code: str) -> dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)
    
            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:275: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,708 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,708 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
_______ TestSemanticUnderstandingEngine.test_predict_execution_behavior ________

self = 
<tests.unit.application.memory.test_execution_learning_integration.TestSemanticU
nderstandingEngine object at 0x118e2a000>

    def test_predict_execution_behavior(self):
        """Test execution behavior prediction."""
        code = "def divide(a, b): return a / b"
    
        # Mock pattern library to return relevant patterns
        mock_pattern = Mock()
        mock_pattern.pattern_id = "division_pattern"
        mock_pattern.pattern_type = "mathematical_operation"
        mock_pattern.confidence = 0.8
        mock_pattern.expected_outcomes = {"success_rate": 0.7}
    
        with patch.object(self.engine.pattern_library, "find_matches") as 
mock_find:
            mock_find.return_value = [mock_pattern]
    
>           prediction = self.engine.predict_execution_behavior(code)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_execution_learning_integration.py:463: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:220: in predict_execution_behavior
    components = self.extract_semantic_components(code)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:91: in extract_semantic_components
    ast_analysis = self._analyze_ast_structure(code)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.memory.semantic_understanding_engine.SemanticUnderstanding
Engine object at 0x13b060ce0>
code = 'def divide(a, b): return a / b'

    def _analyze_ast_structure(self, code: str) -> dict[str, Any]:
        """Analyze AST structure for semantic understanding."""
        try:
            tree = ast.parse(code)
    
            # Count different AST node types
>           node_counts = defaultdict(int)
                          ^^^^^^^^^^^
E           NameError: name 'defaultdict' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/semantic_understanding_engine.py:275: NameError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,724 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,724 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
_________________ test_query_results_from_rows_shapes_records __________________

    @pytest.mark.fast
    def test_query_results_from_rows_shapes_records() -> None:
        """Row helpers should produce query results with normalized metadata."""
    
        rows = [
            {
                "id": "a",
                "content": "alpha",
                "memory_type": "context",
                "metadata": to_serializable({"score": 1}),
            },
            {
                "id": "b",
                "content": "beta",
                "memory_type": "knowledge",
                "metadata": to_serializable({"score": 2}),
                "source": "secondary",
                "similarity": 0.33,
            },
        ]
    
        results = query_results_from_rows(
            "primary",
            rows,
            total="2",
            latency_ms="3.5",
            metadata=to_serializable(
                {"batch": 1, "started_at": datetime(2024, 4, 5, 6, 7)}
            ),
        )
    
        assert results["store"] == "primary"
        assert results["total"] == 2
        assert results["latency_ms"] == pytest.approx(3.5)
        assert results["metadata"]["batch"] == 1
        assert results["metadata"]["started_at"] == datetime(2024, 4, 5, 6, 7)
    
        primary_record, secondary_record = results["records"]
        assert primary_record.source == "primary"
        assert primary_record.memory_type is MemoryType.CONTEXT
        assert secondary_record.source == "secondary"
        assert secondary_record.similarity == pytest.approx(0.33)
>       assert secondary_record.memory_type is MemoryType.CONTEXT
E       AssertionError: assert <MemoryType.KNOWLEDGE: 'knowledge'> is 
<MemoryType.CONTEXT: 'context'>
E        +  where <MemoryType.KNOWLEDGE: 'knowledge'> = 
MemoryRecord(item=MemoryItem(id='b', content='beta', 
memory_type=<MemoryType.KNOWLEDGE: 'knowledge'>, 
metadata={'score...ed_at=datetime.datetime(2025, 10, 29, 10, 48, 29, 855923)), 
similarity=0.33, source='secondary', metadata={'score': 2}).memory_type
E        +  and   <MemoryType.CONTEXT: 'context'> = MemoryType.CONTEXT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_metadata_serialization_helpers.py:132: AssertionError
_______ TestPhase3IntegrationSystem.test_process_advanced_reasoning_task _______

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x119080350>

    def test_process_advanced_reasoning_task(self):
        """Test processing of complex reasoning tasks."""
        task = {
            "task_id": "test_task_123",
            "description": "Analyze user authentication system",
            "type": "analysis",
            "complexity": "medium",
        }
    
        # Mock all the component methods
        with patch.object(
            self.integration_system, "_analyze_and_segment_task"
        ) as mock_segment:
            mock_segment.return_value = [
                {"segment_id": "seg1", "description": "Analyze requirements"},
                {"segment_id": "seg2", "description": "Review implementation"},
            ]
    
            with patch.object(
                self.integration_system, "_execute_multi_hop_reasoning"
            ) as mock_reasoning:
                mock_reasoning.return_value = {
                    "success": True,
                    "total_hops": 3,
                    "confidence": 0.85,
                }
    
                with patch.object(
                    self.integration_system, "_execute_hybrid_llm_processing"
                ) as mock_hybrid:
                    mock_hybrid.return_value = {
                        "success": True,
                        "result": {"confidence": 0.9, "execution_time": 2.5},
                    }
    
                    with patch.object(
                        self.integration_system, 
"_apply_metacognitive_enhancement"
                    ) as mock_meta:
                        mock_meta.return_value = {
                            "success": True,
                            "insights": ["Strategy improvement", "Efficiency 
gain"],
                        }
    
                        with patch.object(
                            self.integration_system, 
"_optimize_contextual_prompts"
                        ) as mock_prompts:
                            mock_prompts.return_value = {
                                "success": True,
                                "engineered_prompts": ["Prompt 1", "Prompt 2"],
                            }
    
                            result = (
                                self.integration_system.process_advanced_reasoni
ng_task(
                                    task
                                )
                            )
    
        assert result["success"] is True
        assert result["task_id"] == "test_task_123"
        assert "processing_summary" in result
        assert result["processing_summary"]["task_segments"] == 2
        assert result["processing_summary"]["reasoning_hops"] == 3
>       assert result["processing_summary"]["confidence_score"] > 0.8
E       assert 0.6125 > 0.8

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:113: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,867 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,867 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
2025-10-29 10:48:29,867 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-29 10:48:29,867 - devsynth.application.memory.automata_synthesis_engine 
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-29 10:48:29,867 - devsynth.application.memory.hybrid_llm_architecture - 
INFO - Hybrid LLM architecture initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-29 10:48:29,867 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized
2025-10-29 10:48:29,867 - devsynth.application.memory.phase3_integration_system 
- INFO - Phase 3 Integration System initialized with all advanced reasoning 
components
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initialized
INFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)
INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initialized
INFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initialized
INFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initialized
INFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:29,868 - devsynth.application.memory.phase3_integration_system 
- INFO - Processing advanced reasoning task: test_task_123
------------------------------ Captured log call -------------------------------
INFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 
Processing advanced reasoning task: test_task_123
_______ TestPhase3IntegrationSystem.test_apply_metacognitive_enhancement _______

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x119080980>

    def test_apply_metacognitive_enhancement(self):
        """Test metacognitive enhancement application."""
        task = {"description": "Test metacognitive enhancement"}
        hybrid_results = {"result": {"confidence": 0.85}}
    
        # Mock metacognitive training
        with patch.object(
            self.integration_system.metacognitive_training, 
"start_think_aloud_session"
        ) as mock_start:
            mock_start.return_value = "session_123"
    
            with patch.object(
                self.integration_system.metacognitive_training, 
"record_verbalization"
            ) as mock_record:
                with patch.object(
                    self.integration_system.metacognitive_training,
                    "end_think_aloud_session",
                ) as mock_end:
                    mock_end.return_value = {
                        "session_id": "session_123",
                        "insights": ["Strategy improvement", "Error pattern"],
                        "verbalizations_count": 5,
                    }
    
                    result = 
self.integration_system._apply_metacognitive_enhancement(
                        task, hybrid_results
                    )
    
        assert result["success"] is True
>       assert len(result["insights"]) == 2
E       assert 0 == 2
E        +  where 0 = len([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:221: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,887 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,887 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,887 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,887 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,887 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,887 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,887 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,887 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,888 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
2025-10-29 10:48:29,888 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-29 10:48:29,888 - devsynth.application.memory.automata_synthesis_engine 
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-29 10:48:29,888 - devsynth.application.memory.hybrid_llm_architecture - 
INFO - Hybrid LLM architecture initialized
2025-10-29 10:48:29,888 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-29 10:48:29,888 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized
2025-10-29 10:48:29,888 - devsynth.application.memory.phase3_integration_system 
- INFO - Phase 3 Integration System initialized with all advanced reasoning 
components
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initialized
INFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)
INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initialized
INFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initialized
INFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initialized
INFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
_________ TestPhase3IntegrationSystem.test_export_import_system_state __________

self = 
<tests.unit.application.memory.test_phase3_integration_system.TestPhase3Integrat
ionSystem object at 0x119082e10>

    def test_export_import_system_state(self):
        """Test system state export and import."""
        # Set up some state
        self.integration_system.execution_learning.learning_history = [
            {"test": "session"}
        ]
        self.integration_system.enhanced_graphrag.query_cache = {"query1": 
"result1"}
    
        # Export state
>       exported_state = self.integration_system.export_system_state()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_phase3_integration_system.py:334: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/phase3_integration_system.py:547: in export_system_state
    "execution_learning": self.execution_learning.export_learning_state(),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:457: in export_learning_state
    "statistics": self.get_learning_statistics(),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:370: in get_learning_statistics
    total_patterns = sum(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <list_iterator object at 0x13b0377c0>

    total_patterns = sum(
>       session["patterns_learned"] for session in self.learning_history
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
E   KeyError: 'patterns_learned'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/me
mory/execution_learning_integration.py:371: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:29,910 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Intent discovery engine initialized with threshold 0.7
2025-10-29 10:48:29,910 - devsynth.application.memory.enhanced_knowledge_graph -
INFO - Enhanced knowledge graph initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_trajectory_collector - INFO - Execution 
trajectory collector initialized (sandbox: True, timeout: 30.0s)
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Execution 
learning algorithm initialized (min_freq: 3, threshold: 0.7)
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.semantic_understanding_engine - INFO - Semantic 
understanding engine initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_learning_algorithm - INFO - Pattern 
library initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.execution_learning_integration - INFO - Execution 
learning integration initialized (max_trajectories: 1000)
2025-10-29 10:48:29,910 - devsynth.application.memory.enhanced_graphrag_engine -
INFO - Enhanced GraphRAG query engine initialized
2025-10-29 10:48:29,910 - devsynth.application.memory.automata_synthesis_engine 
- INFO - Automata synthesis engine initialized (min_samples: 10)
2025-10-29 10:48:29,910 - devsynth.application.memory.hybrid_llm_architecture - 
INFO - Hybrid LLM architecture initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.metacognitive_training_system - INFO - Metacognitive
training system initialized
2025-10-29 10:48:29,910 - 
devsynth.application.memory.contextual_prompting_system - INFO - Contextual 
prompting system initialized
2025-10-29 10:48:29,910 - devsynth.application.memory.phase3_integration_system 
- INFO - Phase 3 Integration System initialized with all advanced reasoning 
components
------------------------------ Captured log setup ------------------------------
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 Intent
discovery engine initialized with threshold 0.7
INFO     
devsynth.application.memory.enhanced_knowledge_graph:logging_setup.py:615 
Enhanced knowledge graph initialized
INFO     
devsynth.application.memory.execution_trajectory_collector:logging_setup.py:615 
Execution trajectory collector initialized (sandbox: True, timeout: 30.0s)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Execution learning algorithm initialized (min_freq: 3, threshold: 0.7)
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.semantic_understanding_engine:logging_setup.py:615 
Semantic understanding engine initialized
INFO     
devsynth.application.memory.execution_learning_algorithm:logging_setup.py:615 
Pattern library initialized
INFO     
devsynth.application.memory.execution_learning_integration:logging_setup.py:615 
Execution learning integration initialized (max_trajectories: 1000)
INFO     
devsynth.application.memory.enhanced_graphrag_engine:logging_setup.py:615 
Enhanced GraphRAG query engine initialized
INFO     
devsynth.application.memory.automata_synthesis_engine:logging_setup.py:615 
Automata synthesis engine initialized (min_samples: 10)
INFO     
devsynth.application.memory.hybrid_llm_architecture:logging_setup.py:615 Hybrid 
LLM architecture initialized
INFO     
devsynth.application.memory.metacognitive_training_system:logging_setup.py:615 
Metacognitive training system initialized
INFO     
devsynth.application.memory.contextual_prompting_system:logging_setup.py:615 
Contextual prompting system initialized
INFO     
devsynth.application.memory.phase3_integration_system:logging_setup.py:615 Phase
3 Integration System initialized with all advanced reasoning components
_________________________ test_cascading_and_federated _________________________

router = <devsynth.application.memory.query_router.QueryRouter object at 
0x13afd6cc0>

    @pytest.mark.fast
    def test_cascading_and_federated(router: QueryRouter) -> None:
        """Cascading and federated strategies yield MemoryRecord sequences."""
    
        cascading = router.cascading_query("topic")
>       assert {record.source for record in cascading} == {"vector", "graph"}
E       AssertionError: assert {'graph', 'tinydb', 'vector'} == {'graph', 
'vector'}
E         
E         Extra items in the left set:
E         'tinydb'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_query_router.py:128: AssertionError
___________________ test_queue_update_enqueues_memory_record ___________________

    @pytest.mark.fast
    def test_queue_update_enqueues_memory_record() -> None:
        manager = _manager()
        sync_manager: SyncManager = manager.sync_manager
        item = MemoryItem(
            id="queued-1",
            content="queue-test",
            memory_type=MemoryType.SHORT_TERM,
            metadata={"origin": "alpha"},
            created_at=datetime.now(),
        )
    
        sync_manager.queue_update("alpha", item)
    
        with sync_manager._queue_lock:  # noqa: SLF001 - internal verification 
for test
>           queued_store, record = sync_manager._queue[-1]
            ^^^^^^^^^^^^^^^^^^^^
E           ValueError: too many values to unpack (expected 2)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_sync_manager_transactions.py:61: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:30,277 - devsynth.application.memory.memory_manager - INFO - 
Memory Manager initialized with adapters: alpha, beta
2025-10-29 10:48:30,277 - devsynth.application.memory.tiered_cache - INFO - 
Tiered cache initialized with max size 50
2025-10-29 10:48:30,277 - devsynth.application.memory.tiered_cache - INFO - 
Cache cleared
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.memory.memory_manager:logging_setup.py:615 Memory 
Manager initialized with adapters: alpha, beta
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Tiered 
cache initialized with max size 50
INFO     devsynth.application.memory.tiered_cache:logging_setup.py:615 Cache 
cleared
________________ test_tinydb_adapter_serializes_bytes_and_tuple ________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_tinydb_adapter_serializes0')

    @pytest.mark.fast
    def test_tinydb_adapter_serializes_bytes_and_tuple(tmp_path):
        """Ensure TinyDBMemoryAdapter serializes non-JSON types safely.
    
        This guards against the TypeError observed during `task release:prep` 
when
        metadata/content contain bytes or tuples.
        """
        adapter = TinyDBMemoryAdapter(db_path=str(tmp_path / "db.json"))
    
        item = MemoryItem(
            id="bytes_tuple",
            content={
                "payload": b"hello",
                "coords": (1, 2, 3),
                "nested": {"t": ("a", "b")},
            },
            memory_type=MemoryType.KNOWLEDGE,
            metadata={
                "tags": ("x", "y"),
                "raw": b"world",
                "timestamp": datetime(2024, 1, 1),
            },
        )
    
        stored_id = adapter.store(item)
        assert stored_id == item.id
    
        retrieved = adapter.retrieve(stored_id)
        assert retrieved is not None
    
        # bytes should become a string representation (utf-8 or base64); at 
least be str
        assert isinstance(retrieved.content["payload"], str)
        assert isinstance(retrieved.metadata["raw"], str)
    
        # tuples should become lists
        assert retrieved.content["coords"] == [1, 2, 3]
        assert retrieved.content["nested"]["t"] == ["a", "b"]
        assert retrieved.metadata["tags"] == ["x", "y"]
    
        # datetime should round-trip as ISO string (already covered elsewhere 
but asserted here for completeness)
>       assert isinstance(retrieved.metadata["timestamp"], str)
E       assert False
E        +  where False = isinstance(datetime.datetime(2024, 1, 1, 0, 0), str)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/memo
ry/test_tinydb_adapter_bytes_tuple.py:54: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:30,290 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB 
Memory Adapter initialized
2025-10-29 10:48:30,290 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - Stored 
memory item with ID bytes_tuple in TinyDB Memory Adapter
------------------------------ Captured log call -------------------------------
INFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
TinyDB Memory Adapter initialized
INFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
Stored memory item with ID bytes_tuple in TinyDB Memory Adapter
____________________ test_evaluate_change_stores_with_phase ____________________

    def test_evaluate_change_stores_with_phase():
        memory = DummyMemoryManager()
        service = _build_service("yes", memory_manager=memory)
        change = RequirementChange(requirement_id=uuid4(), created_by="carol")
    
        service.evaluate_change(change, edrr_phase=EDRRPhase.EXPAND)
    
        assert memory.calls
>       assert memory.calls[0][1] == MemoryType.DIALECTICAL_REASONING
E       AssertionError: assert <MemoryType.RELATIONSHIP: 'relationship'> == 
<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>
E        +  where <MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'> = 
MemoryType.DIALECTICAL_REASONING

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:124: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:30,337 - devsynth.application.requirements.dialectical_reasoner
- INFO - Evaluating change
2025-10-29 10:48:30,337 - devsynth.application.requirements.dialectical_reasoner
- INFO - Consensus reached for change
------------------------------ Captured log call -------------------------------
INFO     
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615 
Evaluating change
INFO     
devsynth.application.requirements.dialectical_reasoner:logging_setup.py:615 
Consensus reached for change
_______________ test_generate_arguments_parses_counterarguments ________________

    def test_generate_arguments_parses_counterarguments():
        response = (
            "Argument 1:\n"
            "Position: Thesis\n"
            "Content: Improve UX\n"
            "Counterargument: Increases complexity\n\n"
            "Argument 2:\n"
            "Position: Antithesis\n"
            "Content: Maintain simplicity\n"
            "Counterargument: Misses UX gains"
        )
        service = _build_service_for_arguments(response)
        change = RequirementChange(requirement_id=uuid4(), created_by="mallory")
>       args = service._generate_arguments(change, "thesis", "antithesis")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:220: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:576: in _generate_arguments
    prompt = self._create_arguments_prompt(change, thesis, antithesis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.requirements.dialectical_reasoner.DialecticalReasonerServi
ce object at 0x13afd2bd0>
change = RequirementChange(id=UUID('d10922a1-715d-491c-94bf-c34b63d1db9d'), 
requirement_id=UUID('7793a25d-f5c0-40a2-a024-0b6dac... 10, 48, 30, 360375), 
created_by='mallory', reason='', approved=False, approved_at=None, 
approved_by=None, comments=[])
thesis = 'thesis', antithesis = 'antithesis'

    def _create_arguments_prompt(
        self, change: RequirementChange, thesis: str, antithesis: str
    ) -> str:
        """
        Create a prompt for generating arguments.
    
        Args:
            change: The requirement change.
            thesis: The thesis statement.
            antithesis: The antithesis statement.
    
        Returns:
            The prompt.
        """
        prompt = (
            "You are a requirements analyst evaluating a proposed change to a 
requirement. "
            "Please generate a list of arguments for and against the proposed 
change. "
            "For each argument, specify whether it supports the thesis or 
antithesis, provide a clear explanation, "
            "and then offer a counterargument that challenges the original 
point. "
            "\n\nProposed change:\n"
        )
    
        if change.change_type.value == "add":
            prompt += f"Add a new requirement: {change.new_state.title}\n"
            prompt += f"Description: {change.new_state.description}\n"
        elif change.change_type.value == "remove":
            prompt += f"Remove requirement: {change.previous_state.title}\n"
            prompt += f"Description: {change.previous_state.description}\n"
        elif change.change_type.value == "modify":
            prompt += f"Modify requirement from:\n"
>           prompt += f"Title: {change.previous_state.title}\n"
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'title'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:1069: AttributeError
___________ test_generate_arguments_handles_missing_counterargument ____________

    def test_generate_arguments_handles_missing_counterargument():
        response = (
            "Argument 1:\n"
            "Position: Thesis\n"
            "Content: Example argument\n\n"
            "Argument 2:\n"
            "Position: Antithesis\n"
            "Content: Another argument"
        )
        service = _build_service_for_arguments(response)
        change = RequirementChange(requirement_id=uuid4(), created_by="nina")
>       args = service._generate_arguments(change, "thesis", "antithesis")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:247: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:576: in _generate_arguments
    prompt = self._create_arguments_prompt(change, thesis, antithesis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
<devsynth.application.requirements.dialectical_reasoner.DialecticalReasonerServi
ce object at 0x13afee9c0>
change = RequirementChange(id=UUID('5742e2e5-0c02-4ba0-84b2-abe61909ae60'), 
requirement_id=UUID('130c213f-5a0a-49c8-8279-9066ae...29, 10, 48, 30, 375083), 
created_by='nina', reason='', approved=False, approved_at=None, 
approved_by=None, comments=[])
thesis = 'thesis', antithesis = 'antithesis'

    def _create_arguments_prompt(
        self, change: RequirementChange, thesis: str, antithesis: str
    ) -> str:
        """
        Create a prompt for generating arguments.
    
        Args:
            change: The requirement change.
            thesis: The thesis statement.
            antithesis: The antithesis statement.
    
        Returns:
            The prompt.
        """
        prompt = (
            "You are a requirements analyst evaluating a proposed change to a 
requirement. "
            "Please generate a list of arguments for and against the proposed 
change. "
            "For each argument, specify whether it supports the thesis or 
antithesis, provide a clear explanation, "
            "and then offer a counterargument that challenges the original 
point. "
            "\n\nProposed change:\n"
        )
    
        if change.change_type.value == "add":
            prompt += f"Add a new requirement: {change.new_state.title}\n"
            prompt += f"Description: {change.new_state.description}\n"
        elif change.change_type.value == "remove":
            prompt += f"Remove requirement: {change.previous_state.title}\n"
            prompt += f"Description: {change.previous_state.description}\n"
        elif change.change_type.value == "modify":
            prompt += f"Modify requirement from:\n"
>           prompt += f"Title: {change.previous_state.title}\n"
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'title'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/re
quirements/dialectical_reasoner.py:1069: AttributeError
______________________ test_wsde_team_hook_positive_path _______________________

    def test_wsde_team_hook_positive_path():
        team = WSDETeam("test")
        service = _build_service("yes")
>       service.register_evaluation_hook(team.requirement_evaluation_hook)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 
'requirement_evaluation_hook'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:266: AttributeError
______________________ test_wsde_team_hook_negative_path _______________________

    def test_wsde_team_hook_negative_path():
        team = WSDETeam("test")
        service = _build_service("no")
>       service.register_evaluation_hook(team.requirement_evaluation_hook)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 
'requirement_evaluation_hook'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/requ
irements/test_dialectical_reasoner.py:279: AttributeError
_____________ TestEnhancedTestCollector.test_nonexistent_directory _____________

self = <test_enhanced_test_collector.TestEnhancedTestCollector object at 
0x1192e2f00>

    def test_nonexistent_directory(self):
        """Test handling of nonexistent directories."""
        collector = EnhancedTestCollector()
    
        tests = collector.collect_tests_by_category("nonexistent")
        assert len(tests) == 0
    
>       isolation_report = collector._isolation_analyzer.analyze_test_isolation(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "nonexistent"
        )
E       AttributeError: 'EnhancedTestCollector' object has no attribute 
'_isolation_analyzer'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:320: AttributeError
______________ TestCacheOperations.test_cache_directory_creation _______________

self = <test_enhanced_test_collector.TestCacheOperations object at 0x1192ec7a0>

    def test_cache_directory_creation(self):
        """Test that cache directory is created."""
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = Path(temp_dir) / "cache"
            collector = EnhancedTestCollector()
            collector.cache_dir = cache_dir
    
            # Cache directory should be created
>           assert cache_dir.exists()
E           AssertionError: assert False
E            +  where False = exists()
E            +    where exists = 
PosixPath('/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp1tac1e1q/cache').
exists

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/application/test
ing/test_enhanced_test_collector.py:420: AssertionError
_____________________ test_metrics_fail_patches_calculate ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0fe930>

    @pytest.mark.fast
    def test_metrics_fail_patches_calculate(monkeypatch):
>       metrics_fail(monkeypatch)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/behavior/test_al
ignment_metrics_steps_unit.py:17: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

command_context = <_pytest.monkeypatch.MonkeyPatch object at 0x13b0fe930>

    @given("alignment metrics calculation fails")
    def metrics_fail(command_context: MutableMapping[str, object]) -> None:
        """Force the simulated CLI command to raise an error."""
    
>       command_context["force_error"] = True
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'MonkeyPatch' object does not support item assignment

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/behavior/steps/test_a
lignment_metrics_steps.py:47: TypeError
_______________________ test_main_handles_run_cli_errors _______________________

name = 'errors'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'errors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13adb6b70>

    @pytest.mark.fast
    def test_main_handles_run_cli_errors(monkeypatch):
        def failing_run_cli():
            raise RuntimeError("boom")
    
        handled = {}
    
        def fake_handle_error(_bridge, err):
            handled["error"] = err
    
        monkeypatch.setattr("devsynth.adapters.cli.typer_adapter.run_cli", 
failing_run_cli)
>       monkeypatch.setattr(
            "devsynth.application.cli.errors.handle_error", fake_handle_error
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_cli_err
or_handling.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'errors', ann = 'devsynth.application.cli.errors'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.errors 
has no attribute 'errors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_______________ test_build_app_registers_commands_from_registry ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13adcf050>

    @pytest.mark.fast
    def test_build_app_registers_commands_from_registry(monkeypatch):
        """Commands in COMMAND_REGISTRY should be registered with the CLI."""
        called = {}
    
        def sample_cmd():
            called["ran"] = True
    
        monkeypatch.setattr(adapter, "COMMAND_REGISTRY", {"sample": sample_cmd})
>       monkeypatch.setattr(adapter, "config_app", typer.Typer())
E       AttributeError: <module 'devsynth.adapters.cli.typer_adapter' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/adapters/cli/
typer_adapter.py'> has no attribute 'config_app'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_command
_registry.py:17: AttributeError
_________________ test_global_debug_flag_sets_log_level_debug __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad6e9f0>

    @pytest.mark.fast
    def test_global_debug_flag_sets_log_level_debug(monkeypatch):
        runner = CliRunner()
        # Ensure env does not force level
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)
        monkeypatch.delenv("DEVSYNTH_DEBUG", raising=False)
    
        app = build_app()
        result = runner.invoke(app, ["--debug", "--version"])  # triggers 
callback and exit
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.DEBUG
E       assert 20 == 10
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root 
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at 
0x104d52980>()
E        +        where <function getLogger at 0x104d52980> = logging.getLogger
E        +  and   10 = logging.DEBUG

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:29: AssertionError
__________________ test_env_debug_sets_log_level_when_no_flag __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad38350>

    @pytest.mark.fast
    def test_env_debug_sets_log_level_when_no_flag(monkeypatch):
        runner = CliRunner()
        monkeypatch.setenv("DEVSYNTH_DEBUG", "1")
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)
    
        app = build_app()
        result = runner.invoke(app, ["--version"])  # triggers callback
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.DEBUG
E       assert 20 == 10
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root 
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at 
0x104d52980>()
E        +        where <function getLogger at 0x104d52980> = logging.getLogger
E        +  and   10 = logging.DEBUG

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:42: AssertionError
__________________ test_log_level_option_overrides_env_debug ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad6c5f0>

    @pytest.mark.fast
    def test_log_level_option_overrides_env_debug(monkeypatch):
        runner = CliRunner()
        monkeypatch.setenv("DEVSYNTH_DEBUG", "true")
        monkeypatch.delenv("DEVSYNTH_LOG_LEVEL", raising=False)
    
        app = build_app()
        result = runner.invoke(app, ["--log-level", "WARNING", "--version"])  # 
eager
        assert result.exit_code == 0
>       assert logging.getLogger().getEffectiveLevel() == logging.WARNING
E       assert 20 == 30
E        +  where 20 = getEffectiveLevel()
E        +    where getEffectiveLevel = <RootLogger root 
(INFO)>.getEffectiveLevel
E        +      where <RootLogger root (INFO)> = <function getLogger at 
0x104d52980>()
E        +        where <function getLogger at 0x104d52980> = logging.getLogger
E        +  and   30 = logging.WARNING

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_logging
_flags.py:55: AssertionError
_____________ test_mvuu_dashboard_module_no_run_avoids_subprocess ______________

    @pytest.mark.fast
    @pytest.mark.smoke
    def test_mvuu_dashboard_module_no_run_avoids_subprocess():
        # Ensure running the module with --no-run exits cleanly and does not 
spawn subprocesses
        with mock.patch.object(subprocess, "run") as mocked_run:
            proc = subprocess.run(
                [
                    sys.executable,
                    "-m",
                    "devsynth.application.cli.commands.mvuu_dashboard_cmd",
                    "--no-run",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=15,
            )
        # The outer subprocess.run executed our Python process; it should have 
succeeded
>       assert proc.returncode == 0
E       AssertionError: assert <MagicMock name='run().returncode' 
id='5283528880'> == 0
E        +  where <MagicMock name='run().returncode' id='5283528880'> = 
<MagicMock name='run()' id='5281968112'>.returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_mvuu_da
shboard_smoke.py:26: AssertionError
__________ test_cli_run_tests_unit_fast_completes_with_non_zero_tests __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad43530>

    @pytest.mark.fast
    def test_cli_run_tests_unit_fast_completes_with_non_zero_tests(monkeypatch):
        """
        Regression test: ensure the shared test runner completes in fast mode 
and
        executes a non-zero number of tests, without hanging on optional 
providers.
        """
        # Disable optional external providers to avoid network/UI stalls
        monkeypatch.setenv("DEVSYNTH_RESOURCE_LMSTUDIO_AVAILABLE", "false")
    
        success, output = run_tests(
            target="unit-tests",
            speed_categories=("fast",),
            verbose=False,
            report=False,
            parallel=False,  # run in-process to minimize flakiness on CI
            segment=False,
            segment_size=50,
            maxfail=1,
        )
    
        # Must succeed overall
>       assert success, f"Runner did not succeed. Output:\n{output}"
E       AssertionError: Runner did not succeed. Output:
E         ERROR: file or directory not found: 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s
E         
E         ============================= test session starts 
==============================
E         platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
E         rootdir: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_cli_run_tests_unit_fast_c0/project
E         plugins: cov-7.0.0
E         asyncio: mode=Mode.AUTO, debug=False, 
asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=function
E         collected 0 items
E         
E         ============================ no tests ran in 0.00s 
=============================
E         
E         Pytest exited with code 4. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_usage_hint 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_runtime_hin
t 
tests/unit/adapters/cli/test_typer_adapter.py::test_command_help_format_includes
_sections 
tests/unit/adapters/issues/test_github_adapter.py::test_fetch_github_issue 
tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_normalizes
_mapping 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_without_pa
rameters_returns_none 
tests/unit/adapters/llm/test_llm_adapter.py::test_default_factory_delegates_to_g
lobal_registry 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_uses_injected_
factory 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_emits_typed_er
ror_for_unknown_provider 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_maps_registere
d_message 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_propaga
tes_factory_rejection 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_success
tests/unit/adapters/llm/test_llm_adapter.py::test_unknown_llm_provider_error_pre
serves_cause 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_stream
_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_with_c
ontext_stream_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_chunk_response_
helper_respects_chunk_size 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_stream_chunks_y
ields_all_segments 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_mock_response_templa
te_serializes 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_round_trip_pr
eserves_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_matches_cus
tom_template 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_uses_defaul
t_when_no_template_matches 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
coerces_sequences 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
falls_back_to_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_adapter_initialises_
from_mapping 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_stream_prop
agates_generate_failure 
tests/unit/adapters/test_agent_adapter.py::test_factory_initializes_agent_with_c
onfig_payload 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_builds_consensus_p
ayload_from_solutions 
tests/unit/adapters/test_agent_adapter.py::test_process_task_without_agents_rais
es_validation_error 
tests/unit/adapters/test_agent_adapter.py::test_coerce_task_solutions_filters_in
valid_entries 
tests/unit/adapters/test_agent_adapter.py::test_import_agent_falls_back_on_error
tests/unit/adapters/test_agent_adapter.py::test_import_agent_rejects_non_class 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_caches_result
s 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_processing
_failures 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_critical_d
ecisions 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_requires_active_te
am 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_singl
e_agent_flow 
tests/unit/adapters/test_agent_adapter.py::test_agent_initialization_payload_han
dles_unknown_type 
tests/unit/adapters/test_agent_adapter.py::test_coerce_helpers_normalize_inputs 
tests/unit/adapters/test_agent_adapter.py::test_unified_agent_fallback_behaviour
tests/unit/adapters/test_agent_adapter.py::test_load_default_config_uses_yaml_lo
ader 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_uses_future_s
pecs 
tests/unit/adapters/test_agent_adapter.py::test_create_team_uses_collaborative_w
hen_memory_manager 
tests/unit/adapters/test_agent_adapter.py::test_add_agent_creates_default_team 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_multi
_agent_path 
tests/unit/adapters/test_backend_resource_gates.py::test_chromadb_adapter_import
s tests/unit/adapters/test_backend_resource_gates.py::test_kuzu_adapter_imports 
tests/unit/adapters/test_backend_resource_gates.py::test_faiss_store_imports_and
_minimal 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_transaction_commit_
and_delete 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_provider_fallback_u
ses_default_embedder 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_store_raises_after_
retries 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_search_handles_empt
y_results 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_commit_failure_mark
s_transaction 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_rollback_transactio
n_states 
tests/unit/adapters/test_fake_memory_store.py::test_fake_memory_store_store_retr
ieve_search_delete_and_txn 
tests/unit/adapters/test_fake_memory_store.py::test_fake_vector_store_similarity
_and_stats 
tests/unit/adapters/test_github_project_adapter.py::test_payload_serialization 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_request_payload
_and_helpers 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_creates_colu
mns_and_cards 
tests/unit/adapters/test_github_project_adapter.py::test_fetch_and_mutations_wit
h_stub_client 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_missing_data_ra
ises 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_skips_existi
ng_items 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_raises_on_gr
aphql_errors 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_error_formattin
g_handles_missing_messages 
tests/unit/adapters/test_jira_adapter.py::test_create_issue_payload_serializatio
n tests/unit/adapters/test_jira_adapter.py::test_transition_issue_missing_status
tests/unit/adapters/test_jira_adapter.py::test_create_issue_http_error_surfaced 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_i
nit_creates_empty_adapter 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_l
oad_model_sets_session 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_without_loaded_model_raises_error 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_with_loaded_model_calls_session_run 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_multiple_outputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_empty_inputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_propagates_onnx_exceptions 
tests/unit/adapters/test_provider_safe_defaults.py::test_default_safe_falls_back
_to_stub_without_keys_and_lmstudio 
tests/unit/adapters/test_provider_safe_defaults.py::test_openai_explicit_without
_key_raises 
tests/unit/adapters/test_provider_safe_defaults.py::test_anthropic_implicit_with
out_key_falls_back_safe_default_stub 
tests/unit/adapters/test_provider_safe_defaults.py::test_lmstudio_not_attempted_
without_availability_flag 
tests/unit/adapters/test_provider_safe_defaults.py::test_disable_providers_retur
ns_null 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_complete_and_embed
_are_deterministic 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_async_matches_sync
tests/unit/adapters/test_provider_stub.py::test_provider_system_reload_preserves
_settings_import 
tests/unit/adapters/test_provider_system.py::test_embed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_embed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_null_provider_complete_raises_
error 
tests/unit/adapters/test_provider_system.py::test_null_provider_acomplete_raises
_error 
tests/unit/adapters/test_provider_system.py::test_null_provider_embed_raises_err
or 
tests/unit/adapters/test_provider_system.py::test_null_provider_aembed_raises_er
ror 
tests/unit/adapters/test_provider_system.py::test_null_provider_initialization 
tests/unit/adapters/test_provider_system.py::test_provider_factory_create_provid
er_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_base_provider_methods_succeeds
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[OpenAIProvider-config0] 
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[LMStudioProvider-config1] 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_initializati
on_skips_health_check_when_network_guard_active 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_load_env_file_populates_config
tests/unit/adapters/test_provider_system.py::test_create_tls_config_has_expected
tests/unit/adapters/test_provider_system.py::test_get_env_or_default_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_config_has_expect
ed 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_has_e
xpected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_error
_raises_error 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_retry
_has_expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_acomplete_has_
expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_embed_has_expe
cted 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_complete_has
_expected 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_async_method
s_has_expected 
tests/unit/adapters/test_provider_system.py::test_provider_with_empty_inputs_has
_expected 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_selects_provider 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_survives_missing_settings 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_respects_ord
er 
tests/unit/adapters/test_provider_system.py::test_openai_provider_retries_after_
transient_failure 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_circuit_brea
ker_blocks_after_failure 
tests/unit/adapters/test_provider_system.py::test_complete_falls_back_to_next_pr
ovider 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_re
spects_disable_flag 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_falls_back_to_lmstudio_when_marked_available 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_req
uires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_lmstudio_provider_r
equires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_asy
nc_requires_httpx_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_defaults
_when_settings_missing 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_uses_exp
licit_settings 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_wir
ing 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_emi
ts_metrics_on_retry 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_n
o_valid_providers 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
ync_uses_circuit_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_failure_opens_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_respects_open_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_records_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
ll_failures_surface_last_error 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
hort_circuits_after_first_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
kips_providers_with_open_breakers 
tests/unit/adapters/test_provider_system_additional.py::test_complete_failure_in
crements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_embed_wraps_unexpec
ted_error 
tests/unit/adapters/test_provider_system_additional.py::test_acomplete_failure_i
ncrements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_aembed_wraps_unexpe
cted_error 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_uses_next_provider 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_raises_after_exhaustion 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_embed_wraps_une
xpected_exceptions 
tests/unit/adapters/test_provider_system_resilience.py::test_base_provider_retry
_harness_records_jitter 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_a
sync_breaker_failure_emits_metrics 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_s
ync_breaker_failure_emits_metrics 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_skips_by_def
ault 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_runs_when_en
abled 
tests/unit/adapters/test_storage_adapter_protocol.py::test_storage_adapter_proto
col_shape 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
turns_structure 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
gistered 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_returns_structure 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_registered 
tests/unit/agents/test_multi_agent_coordinator.py::test_reach_consensus_majority
_choice 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_returns_structure 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_registered 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_returns_
structure 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_register
ed 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
existing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
missing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
empty_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
with_whitespace 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_boundary_value
s_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_error_conditio
ns_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_with_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_without_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_mixed_availability 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_template_direc
tory_path_construction 
tests/unit/agents/test_tool_sandbox.py::test_file_access_restricted 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_blocked 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_allowed 
tests/unit/agents/test_tool_sandbox.py::test_sandbox_context_restores_hooks 
tests/unit/agents/test_tools.py::test_register_and_get_tool 
tests/unit/agents/test_tools.py::test_unknown_tool_returns_none 
tests/unit/agents/test_tools.py::test_export_for_openai_formats_tools 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_r
ecords_summary_and_flushes_memory 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_s
upports_primus_rotation_cycle 
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict 
tests/unit/api/test_public_api_contract.py::test_public_api_imports 
tests/unit/api/test_public_api_contract.py::test_deprecated_wrapper_emits_warnin
g 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_initializa
tion_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_process_ab
stract_method_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_create_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_update_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_get_role_p
rompt_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_error_raises_error 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_error_raises_error 
tests/unit/application/agents/test_test_agent_integration.py::test_process_scaff
olds_tests_from_context 
tests/unit/application/agents/test_validation_agent.py::test_process_affirmative
_is_valid_true 
tests/unit/application/agents/test_validation_agent.py::test_process_failure_tok
ens_set_invalid 
tests/unit/application/agents/test_validation_agent.py::test_process_neutral_tex
t_is_valid 
tests/unit/application/agents/test_validation_agent.py::test_is_valid_word_bound
ary_only 
tests/unit/application/agents/test_validation_agent.py::test_wsde_contains_agent
_and_role 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[All checks passed; no issues.-True] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[An error occurred in module A.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Exception occurred during run.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Some tests fail on CI.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Clean run; everything looks good.-True] 
tests/unit/application/agents/test_wsde_memory_integration_fast.py::test_store_a
nd_retrieve_dialectical_process 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_displays
_all_config 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_update_k
ey_value_saves_and_reports 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_list_mod
els_displays_models 
tests/unit/application/cli/commands/test_config_cmd.py::test_enable_feature_cmd_
updates_and_saves 
tests/unit/application/cli/commands/test_doctor_cmd_typed.py::test_doctor_cmd_ac
cepts_path_arguments 
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py::test_doctor_cm
d_does_not_import_streamlit_or_nicegui 
tests/unit/application/cli/commands/test_ingest_cli_command.py::test_ingest_cli_
command_uses_typed_options 
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py::test_
inspect_code_cmd_sanitizes_dynamic_output 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_preserves_alias_after_subtask_rename 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_rebinds_alias_on_multiple_description_updates 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_reports_eta_strings_when_progress_advances 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_records_failure_history_for_diagnostics 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.align_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.alignment_metrics_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.analyze_manifest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.atomic_rewrite_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.code_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.completion_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dbschema_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.diagnostics_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.doctor_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.documentation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dpg_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.edrr_cycle_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.enhanced_analysis_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.extra_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.gather_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generate_docs_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.ingest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.init_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_code_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_config_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.interface_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.metrics_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_exec_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_init_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_lint_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_report_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_rewrite_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvuu_dashboard_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.pipeline_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.refactor_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.reprioritize_issues_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_pipeline_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_tests_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.security_audit_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.serve_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.spec_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_metrics_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.testing_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_manifest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_metadata_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_chunk_commit_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_fix_rebase_pr_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webapp_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webui_cmd] 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_empty_list_returns_empty_dict 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_single_name_defaults_true 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_false_variants 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_true_variants 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
returns_mapping 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
invalid_json_returns_none 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
non_mapping_returns_none 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_allows_requests_
env_default_for_unit 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_smoke_mode_sets_
env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_feature_flag_map
ping_sets_env 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_marker_passthrou
gh 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_integration_targ
et_retains_cov_when_no_report 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_target_p
rints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_speed_pr
ints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inner_test_env_d
isables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_verbose_and_fast
_timeout_env_behavior 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_report_mode_prin
ts_report_path_message 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_target 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_speed 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_segmented_run_injects_plugins_and_emits_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-bat
ch] 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-b
atches] tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_check_requi
red_env_raises_when_missing_env 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_cmd_happy_path_with_skips 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_runs_when_not_skipped 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_run_secrets
_scan_detects_simple_pattern 
tests/unit/application/cli/commands/test_testing_cmd.py::testing_cmd 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_basic_functionality 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_shows_expected_content 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_uses_cli_bridge 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_logging_configuration 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_script_paths_checked 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_output_formatting 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_quick_actions_displayed 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_performance_achievements 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_phase_tasks_completed 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_group_cha
nges_categorizes_and_orders 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_generate_
message_includes_rationale_and_files 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_minimal_returns_text 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_simple_highlight_false_returns_str 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_standard_with_markup_returns_panel_passthrough 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_dict_and_list 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_unsupported_type_falls_back 
tests/unit/application/cli/test_command_output_formatter.py::test_format_list_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_code_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_help_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_display_does_n
ot_raise 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_defaults 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_reads_yaml 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_is_exported 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_protocol_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_aliases_listed_in_all 
tests/unit/application/cli/test_long_running_progress.py::test_update_adapts_int
erval_and_checkpoints 
tests/unit/application/cli/test_long_running_progress.py::test_status_history_tr
acks_unique_status_changes 
tests/unit/application/cli/test_long_running_progress.py::test_summary_reflects_
fake_timeline_and_sanitizes_descriptions 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_updates_r
emap_and_short_circuit 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_completio
n_rolls_up_and_freezes_summary 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_checkpoin
t_spacing_respects_minimum 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_produces_deterministic_transcript 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_tracks_history_and_alias_renames 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_remains_deterministic_after_reload 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_stays_exported 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_direct_import_succeeds 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_upd
ate_thresholds_with_deterministic_clock 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_sub
task_flow_preserves_mappings_and_progress 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_run
_with_progress_completes_after_exception 
tests/unit/application/cli/test_output.py::TestOutputType::test_output_type_valu
es 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
contains_all_types 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
values 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_in
fo 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_su
ccess 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_er
ror 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_info 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_succe
ss 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_error
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_is_concrete_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_available_at_import_time 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_protocol_exists 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_inherits_correctly 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_module_reload_preserves_base_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_import_from_module_works_after_reload 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_instantiation 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_has_expected_methods 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_deterministic_tests_can_import_base 
tests/unit/application/cli/test_requirements_commands.py::test_wizard_cmd_back_n
avigation_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_gather_requiremen
ts_cmd_yaml_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_initialize_servic
es_configures_singletons 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_handles_empty_repository 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_renders_rich_table 
tests/unit/application/cli/test_requirements_commands.py::test_create_requiremen
t_invokes_service 
tests/unit/application/cli/test_requirements_gathering.py::test_gather_cmd_loggi
ng_exc_info_succeeds 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs 
tests/unit/application/cli/test_setup_wizard_textual.py::test_textual_and_cli_pa
yloads_match 
tests/unit/application/cli/test_setup_wizard_textual.py::test_requirements_wizar
d_supports_shortcut_navigation 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_planning_cmd_re
turns_structured_plan 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_defaults_when_missing 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_handles_invalid_json 
tests/unit/application/code_analysis/test_analyzer.py::test_analyze_code_simple 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_add_docstring_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_complex_transformations_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_extract_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_remove_unused_imports_and_variables_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_identifier_no_change 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_parameter_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_variable_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_validate_syntax_is_valid 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_complexity_and_readability_metrics_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_differentiate_selects_best_option_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_expand_implementation_options_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_refine_implementation_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_retrospect_code_quality_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_initialization_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_index_files_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_detect_languages_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_categorize_file_assigns_lists 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_infer_architecture_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_identify_components_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_requirements_spec_alignment_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_generate_health_report_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_analyze_maps_dependencies_and_structure 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_initialization_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_architecture_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_unknown 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_layers_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_layer_dependencies_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_check_architecture_violations_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_code_quality_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_improvement_opportunities_succeeds 
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py::test_sel
f_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_transformer.py::TestAstTransformer::te
st_record_change_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestRedundantAssignmen
tRemover::test_remove_redundant_assignments_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestStringLiteralOptim
izer::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeStyleTransform
er::test_improve_code_style_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_code_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_file_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_directory_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_find_python_files_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestSymbolUsageCounter
::test_count_symbol_usage_succeeds 
tests/unit/application/code_analysis/test_transformer_basic.py::test_optimize_st
ring_literals_simple 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_apply_doc
string_spec_inserts_function_docstring 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_met
hod_from_function_respects_method_type 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_cla
ss_from_functions_wraps_functions 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_to_dict 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_accepts_string_payload 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_cr
eate_team_stores_in_memory 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_consensus_outcome_normalizes
_participants_and_metadata 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_peer_review_consensus_error_
embeds_serialized_outcome 
tests/unit/application/collaboration/test_memory_utils_conversion.py::test_task_
round_trip_to_memory_item 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_colla
boration_payload_protocol_support 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_messa
ge_filter_rejects_invalid_input 
tests/unit/application/collaboration/test_message_protocol.py::test_message_filt
er_invalid_timestamp_raises 
tests/unit/application/collaboration/test_peer_review_store.py::test_store_in_me
mory_persists_peer_review_record 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_returns_review_decisions 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_failure_yields_error_decision 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_wraps_consensus_error_with_serialized_outcome 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_summar
ize_voting_result_persists_summary 
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py::test_identify_conflicts_detects_opposing_opinions 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_tie 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_winner 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_consensus_result_methods 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_contradictions 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_different_approaches 
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py::tes
t_peer_review_solution_excludes_author 
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py::te
st_delegate_subtasks_assigns_best_agent 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_html_documentation_extracts_sections 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_markdown_documentation_respects_heading_levels 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_convert_docstrings_to_chunks_builds_expected_metadata 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_version_key_supports_numeric_sorting_and_literals 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_prefers_vector_results 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_falls_back_to_metadata_items 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_basic_creation 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_phase_context 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_details 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery 
tests/unit/application/edrr/test_coordinator.py::test_run_micro_cycles_stops_aft
er_threshold 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure 
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::test_enhanced_dec
ide_next_phase_respects_auto_phase 
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py::test_collect_ph
ase_metrics_uses_stubbed_helpers 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
missing_memory_manager 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flushes_
on_success 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
errors 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flush_fa
ilure_does_not_raise 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_norma
lizes_outputs 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_missi
ng_manager_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_witho
ut_support_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_stores_context 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_uses_deep_copy 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_ignores_empty 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_enforces_dependencies 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_updates_state 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_consumes_manual_override 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_requires_auto_transitions 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_returns_none_for_final_phase 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ne
xt_phase_rejects_final_phase 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error 
tests/unit/application/edrr/test_recursion_termination.py::test_micro_cycle_resp
ects_depth_bounds 
tests/unit/application/edrr/test_recursion_termination.py::test_complexity_thres
hold_triggers_termination 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_sp
rint_planning_phase_constant 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_basic 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_empty 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_partial 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_sprint_retrospective_phase_constant 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_basic 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_empty 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_none 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_partial 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_positive_in
t_handles_out_of_range 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_threshold_c
lamps_invalid_values 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_respects_config 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_returns_none_when_missing 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_micro_cycle_conf
ig_sanitizes_values 
tests/unit/application/ingestion/test_ingestion_pure.py::test_is_artifact_change
d_respects_metadata_differences 
tests/unit/application/ingestion/test_ingestion_pure.py::test_identify_improveme
nt_areas_flags_missing_manifest_information 
tests/unit/application/ingestion/test_ingestion_pure.py::test_generate_recommend
ations_reflects_project_context 
tests/unit/application/ingestion/test_phases.py::test_run_expand_phase_populates
_artifacts 
tests/unit/application/ingestion/test_phases.py::test_run_differentiate_phase_us
es_structure 
tests/unit/application/llm/test_import_without_openai.py::test_import_openai_pro
vider_without_openai_succeeds 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_in
valid_response_raises_model_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_lazy_import 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_caching 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_getattr_delegation 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_call 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_getattr 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_list_downloaded_models 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_configure_default_client 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_exists 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_has_expected_attributes 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_prefixes_with_offline 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_with_context_concatenates 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
et_embedding_is_deterministic 
tests/unit/application/llm/test_openai_env_key_mock.py::test_openai_provider_use
s_mocked_env_key_without_network 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_succ
ess_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_stre
am_yields_tokens_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_inva
lid_response_raises_model_error 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_not_selected_when_flag_false 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_selected_when_flag_true 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_offlin
e_killswitch_overrides_explicit_selection 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault 
tests/unit/application/memory/test_chromadb_store.py::test_store_and_retrieve_wi
th_fallback 
tests/unit/application/memory/test_chromadb_store_typed.py::test_search_normaliz
es_serialized_rows 
tests/unit/application/memory/test_chromadb_store_typed.py::test_fallback_retrie
ve_uses_serialization_helpers 
tests/unit/application/memory/test_circuit_breaker.py::test_circuit_breaker_open
s_after_failures 
tests/unit/application/memory/test_circuit_breaker.py::test_registry_returns_sam
e_instance 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_without_vector_extension_falls_back 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_configures_hnsw_when_enabled 
tests/unit/application/memory/test_error_logger.py::test_log_error_enforces_max_
errors 
tests/unit/application/memory/test_error_logger.py::test_log_error_accepts_neste
d_context 
tests/unit/application/memory/test_error_logger.py::test_persist_errors_respects
_toggle 
tests/unit/application/memory/test_error_logger.py::test_get_recent_errors_and_s
ummary 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_semantic_robustness_testing 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_get_learning_statistics 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_analyze_behavioral_intent 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior 
tests/unit/application/memory/test_faiss_store.py::test_store_and_retrieve_round
_trip_preserves_metadata 
tests/unit/application/memory/test_faiss_store.py::test_transaction_commit_persi
sts_changes 
tests/unit/application/memory/test_faiss_store.py::test_transaction_rollback_res
tores_snapshot 
tests/unit/application/memory/test_faiss_store.py::test_similarity_search_and_st
ats_ignore_deleted_vectors 
tests/unit/application/memory/test_fast_in_memory_components.py::test_graph_memo
ry_adapter_in_memory_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_enhanced_g
raph_memory_adapter_edrr_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_man
ager_sync_hooks_fire 
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
saction_context_commit_and_rollback 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sys
tem_adapter_in_memory_components 
tests/unit/application/memory/test_fast_in_memory_components.py::test_fallback_s
tore_falls_back_on_failure 
tests/unit/application/memory/test_fast_in_memory_components.py::test_json_file_
store_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sna
pshot_save_and_load 
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_traverse_graph_depth_and_missing_nodes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_begin_tran
saction_tracks_and_cleans_up 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_commit_tra
nsaction_persists_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_rollback_t
ransaction_discards_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_get_all_it
ems_returns_everything 
tests/unit/application/memory/test_memory_manager.py::TestRouteQuery::test_route
_query_normalizes_context_mapping 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_regist
er_and_notify_sync_hook_succeeds 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_sync_h
ook_errors_are_logged 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
disabled_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
enabled_uses_adapter_and_store 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_various_backends 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_kuzu_init
ialization_and_fallback 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_lmdb_miss
ing_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_branches_execution 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_cache_and
_transaction_workflow 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_transacti
on_wrappers_raise_without_support 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_round_trip_preserves_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_from_row_handles_stringified_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_build
_memory_record_coerces_legacy_mapping 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_analyze_and_segment_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_multi_hop_reasoning 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_hybrid_llm_processing 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_contextual_prompts 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_integrate_and_validate_results 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_get_system_status 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_benchmark_against_research 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_validate_system_integrity 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_system_performance 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_memory_graph_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execution_learning_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_automata_metacognitive_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt 
tests/unit/application/memory/test_query_router.py::test_direct_query_and_vector
_branch 
tests/unit/application/memory/test_query_router.py::test_cross_store_query_group
s_results 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
tests/unit/application/memory/test_query_router.py::test_context_aware_and_route
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_returns_existing_identifier 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_generates_uuid 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_transactio
n_methods_are_noops 
tests/unit/application/memory/test_search_memory_fallback.py::test_search_memory
_fallback_without_vector_adapter_returns_results 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record 
tests/unit/application/memory/test_sync_manager_transactions.py::test_transactio
n_rollback_uses_normalized_snapshots 
tests/unit/application/memory/test_tiered_cache_termination.py::test_eviction_lo
op_terminates 
tests/unit/application/memory/test_tiered_cache_termination.py::test_preserves_t
yped_values 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_default_
provider_registration 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_optional
_provider_guard 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_edrr_coo
rdinator_delegates_to_helper 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_returns_result 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_logs_consensus_failure 
tests/unit/application/promises/test_agent_create_promise.py::test_create_promis
e_sets_metadata_and_parent_relationship 
tests/unit/application/promises/test_interface_not_implemented.py::test_promise_
interface_id_not_implemented 
tests/unit/application/promises/test_interface_pure.py::test_basic_promise_metad
ata_round_trip 
tests/unit/application/promises/test_interface_pure.py::test_then_on_fulfilled_p
romise_invokes_callback_immediately 
tests/unit/application/promises/test_interface_pure.py::test_catch_on_rejected_p
romise_yields_handler_result 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_success_rate_and_a
verage_feedback_are_computed_from_state 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_performance_score_
combines_success_and_feedback 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_round_trip_seriali
sation_preserves_variant_fields 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_reaches_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_logs_consensus_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_failure_stores_retrospect 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_receives_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_runs_on_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_non_text_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_invalid_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_assess_im
pact_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_argument_parsing_consensus_failure_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_assess_impact_recommendations_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_requirements_collects_dependencies 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_components_merges_sources 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_asse
ss_risk_level_accounts_for_priority 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_esti
mate_effort_scales_with_affected_entities 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_writes_json 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_cancelled 
tests/unit/application/requirements/test_interactions.py::test_gather_requiremen
ts_supports_backtracking 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_updat
e_requirement_uses_typed_dto_and_dialectical_hooks 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_delet
e_requirement_emits_retrospect_phase 
tests/unit/application/requirements/test_wizard.py::test_priority_and_constraint
s_persist_after_navigation 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_each_step 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_exc_info 
tests/unit/application/sprint/test_planning.py::test_map_requirements_to_plan_ex
tracts_fields 
tests/unit/application/test_documentation_fetcher.py::test_download_success_retu
rns_manifest 
tests/unit/application/test_documentation_fetcher.py::test_download_failure_retu
rns_false_manifest 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization_with_memory_port 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_nonexistent 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_info 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_clear_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_operations 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_expiration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_file_corruption 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_as_dict 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_with_docstring 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_ttl_configuration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_unicode_decode_error 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_os_error_handling 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_memory_storage_failure 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_with_extra 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_without_extra 
tests/unit/application/utils/test_extras_helper.py::test_require_optional_packag
e_wraps_importerror 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors 
tests/unit/cli/test_cli_help.py::test_cli_help_exits_zero_and_shows_summary 
tests/unit/cli/test_command_module_loading.py::test_command_modules_register_com
mands_and_build_app 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry 
tests/unit/cli/test_command_registry.py::test_enable_feature_not_top_level 
tests/unit/cli/test_completion_progress.py::test_completion_cmd_outputs_script_a
nd_progress 
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation 
tests/unit/cli/test_entry_points_help.py::test_console_scripts_declared 
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module 
tests/unit/cli/test_help_examples.py::test_get_command_help_includes_examples 
tests/unit/cli/test_help_examples.py::test_get_command_help_unknown_command 
tests/unit/cli/test_import_gating.py::test_import_devsynth_does_not_import_heavy
_optionals 
tests/unit/cli/test_import_gating.py::test_cli_entrypoint_lazy_imports 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_list 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_json 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv0]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv1]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv2]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv3]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv4]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv5]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv6]
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug 
tests/unit/cli/test_mvu_commands.py::test_mvu_help_lists_subcommands 
tests/unit/cli/test_mvu_commands.py::test_mvu_init_creates_config_and_matches_sc
hema 
tests/unit/cli/test_mvuu_command_registration.py::test_mvuu_dashboard_command_re
gistered 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_generat
es_signed_telemetry 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_uses_li
ve_connectors 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_falls_b
ack_on_connector_error 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_force_l
ocal_mode 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests 
tests/unit/cli/test_version.py::test_cli_version_option_prints_version_and_exits
_zero 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env 
tests/unit/config/test_exception_handling.py::test_is_devsynth_managed_project_i
nvalid_toml_returns_false 
tests/unit/config/test_exception_handling.py::test_unified_config_exists_returns
_false_on_invalid_toml 
tests/unit/config/test_exception_handling.py::test_load_config_malformed_toml_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_load_config_invalid_values_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_set_default_memory_dir_handle
s_configuration_error 
tests/unit/config/test_feature_flag_defaults.py::test_feature_flags_default_off 
tests/unit/config/test_feature_flag_defaults.py::test_can_enable_known_feature_f
lag 
tests/unit/config/test_provider_env.py::test_parse_bool_truthy_and_falsy_cases 
tests/unit/config/test_provider_env.py::test_from_env_defaults_and_with_test_def
aults_sets_stub_and_offline 
tests/unit/config/test_provider_env.py::test_apply_to_env_respects_existing_lmst
udio_flag 
tests/unit/config/test_provider_env.py::test_as_dict_roundtrip_and_types 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_sets_e
xpected_vars 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_does_n
ot_override_explicit_lmstudio_flag 
tests/unit/config/test_provider_env_apply_and_parse.py::test_from_env_reads_curr
ent_environment 
tests/unit/config/test_provider_env_behavior.py::test_from_env_defaults_when_uns
et 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_overrid
es_to_safe_when_unset 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_respect
s_explicit_provider 
tests/unit/config/test_provider_env_behavior.py::test_apply_to_env_and_as_dict_r
oundtrip 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_parses_
true_and_false_variants 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_unrecog
nized_values_fall_back_to_defaults 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_as_dict_reflects
_values_and_with_test_defaults_sets_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_sets_offline_stub_and_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_respects_explicit_provider 
tests/unit/config/test_unified_loader.py::test_loads_from_pyproject_succeeds 
tests/unit/core/mvu/test_api.py::test_get_by_trace_id 
tests/unit/core/mvu/test_api.py::test_get_by_affected_path 
tests/unit/core/mvu/test_atomic_rewrite.py::test_cluster_commits_by_file 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_valid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_block 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_bad_traceid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_issue 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_mvuu_false 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_mvuu 
tests/unit/core/mvu/test_mvuu_schema_validation.py::test_mvuu_example_conforms_t
o_schema tests/unit/core/mvu/test_report.py::test_generate_report_markdown 
tests/unit/core/mvu/test_report.py::test_generate_report_html 
tests/unit/core/mvu/test_storage.py::test_format_mvuu_footer_contains_json 
tests/unit/core/mvu/test_storage.py::test_append_mvuu_footer_appends_block 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_accepts_vali
d 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_rejects_bad_
header 
tests/unit/core/mvu/test_validator.py::test_validate_affected_files_reports_mism
atches 
tests/unit/core/test_config_loader.py::test_core_config_normalizes_mvuu_invalid_
entries 
tests/unit/core/test_config_loader_json_types.py::test_load_config_supports_nest
ed_json_resources 
tests/unit/core/test_config_loader_json_types.py::test_environment_override_pres
erves_resources 
tests/unit/core/test_config_loader_json_types.py::test_core_config_rejects_exces
sively_deep_resources 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
tests/unit/core/test_config_loader_optional_deps.py::test_load_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_dump_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_save_global_config_han
dles_missing_yaml 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw2-expected2] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw3-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[not-a-mapping-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload2-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[not-a-mapping-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload2-expected2] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories0-True-expected0] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories1-False-expected1] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories2-False-expected2] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[not-a-mapping-False-expected3] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[15-True] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[16-False] 
tests/unit/core/test_config_loader_validation.py::test_load_yaml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_load_toml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[single_override] 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[multiple_fields] 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[ignores_irrelevant_keys] 
tests/unit/core/test_config_loader_validation.py::test_load_config_merges_source
s_without_mutating_resources 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[github_only] 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[jira_only] 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[both_providers] 
tests/unit/core/test_deterministic_fixtures.py::test_deterministic_seed_sets_env
_and_random_sequence 
tests/unit/core/test_deterministic_fixtures.py::test_mock_datetime_fixture_freez
es_time 
tests/unit/core/test_deterministic_fixtures.py::test_mock_uuid_fixture_returns_f
ixed_uuid tests/unit/core/test_mvu.py::test_schema_has_required_fields 
tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task 
tests/unit/deployment/test_deployment_scripts.py::test_bootstrap_script_exists 
tests/unit/deployment/test_deployment_scripts.py::test_health_check_script_exist
s 
tests/unit/deployment/test_enforcement.py::test_shell_scripts_enforce_non_root_a
nd_env_validation 
tests/unit/deployment/test_enforcement.py::test_docker_compose_enforces_user_and
_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint 
tests/unit/deployment/test_scripts_dir.py::test_scripts_bootstrap_exists 
tests/unit/deployment/test_scripts_dir.py::test_scripts_health_check_exists 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_noo
p_without_flag 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_rai
ses_for_root 
tests/unit/deployment/test_security_hardening.py::test_check_required_env_vars 
tests/unit/deployment/test_security_hardening.py::test_apply_secure_umask 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_invokes_he
lpers 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_raises_whe
n_env_missing 
tests/unit/devsynth/test_consensus.py::test_build_consensus_majority 
tests/unit/devsynth/test_consensus.py::test_build_consensus_no_consensus 
tests/unit/devsynth/test_consensus.py::test_build_consensus_tracks_unique_dissen
ting_options 
tests/unit/devsynth/test_consensus.py::test_build_consensus_invalid_threshold 
tests/unit/devsynth/test_consensus.py::test_build_consensus_empty_votes 
tests/unit/devsynth/test_fallback_reliability.py::test_named_condition_callbacks
_record_metrics 
tests/unit/devsynth/test_fallback_reliability.py::test_circuit_breaker_open_hook
_and_metrics 
tests/unit/devsynth/test_logger.py::test_log_exception_object_normalized 
tests/unit/devsynth/test_logger.py::test_log_true_uses_current_exception 
tests/unit/devsynth/test_logger.py::test_log_invalid_exc_info_dropped 
tests/unit/devsynth/test_metrics.py::test_memory_metrics_increment_and_reset 
tests/unit/devsynth/test_metrics.py::test_provider_and_retry_metrics 
tests/unit/devsynth/test_metrics.py::test_dashboard_metrics 
tests/unit/devsynth/test_metrics.py::test_inc_memory_unhashable_raises_type_erro
r tests/unit/devsynth/test_simple_addition.py::test_add_returns_sum 
tests/unit/devsynth/test_simple_addition.py::test_add_raises_type_error_on_non_n
umeric 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_tests_but_
not_docs 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_docs_but_n
ot_tests 
tests/unit/domain/interfaces/test_interfaces.py::test_cli_interface_raises_not_i
mplemented 
tests/unit/domain/interfaces/test_interfaces.py::test_file_analysis_result_raise
s_not_implemented 
tests/unit/domain/interfaces/test_interfaces.py::test_onnx_runtime_raises_not_im
plemented 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_none_values 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_existing_values 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_initial
ization 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_seriali
zation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_creation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_content_has
h_generation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_serializati
on_roundtrip 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_link_manage
ment 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_salience_up
date 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_lifecycle_m
anagement 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_cognitive_t
ype_properties 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_creati
on 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_serial
ization 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_defa
ult_standard 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_mono
repo 
tests/unit/domain/models/test_project.py::test_artifact_metadata_defaults_to_sep
arate_dicts 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_init
ialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_str_
representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_repr
_representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_project_m
odel_initialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_structure_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_sta
ndard_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_mon
orepo_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
act_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
acts_by_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_relat
ed_artifacts_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_artifact_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_to_dict_s
ucceeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_add_agent_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_dialectical_hook_invok
ed_on_add_solution_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_rotate_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_empty_team_
succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_agent_by_role_succ
eeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ws
de_dataclass_initialises_timestamps 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_te
am_post_init_restores_missing_attributes 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_inserts_validation 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_noop_when_already_secure 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_error_hand
ling_wraps_body 
tests/unit/domain/models/test_wsde_decision_making.py::test_calculate_idea_simil
arity_overlap 
tests/unit/domain/models/test_wsde_decision_making.py::test_evaluate_options_ran
ks_by_weighted_score 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_similar_entries 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_handles_agent_failures 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_limits_count 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_duplicates_with_strict_threshold 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_antithe
sis_returns_typed_draft 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_categorize_criti
ques_by_domain_returns_tuples 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_synthes
is_returns_resolution_plan 
tests/unit/domain/models/test_wsde_dialectical_typing.py::test_dialectical_seque
nce_round_trip 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_dialectical_tas
k_serialization_round_trip 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_dynamic_role_reassignment_selects_expert_primus_succeeds 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_categorize_crit
iques_by_domain_groups_terms 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_identify_domain
_conflicts_finds_performance_security 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_generates_synthesis 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_requires_solution 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_combines_solutions 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_requires_solutions 
tests/unit/domain/models/test_wsde_knowledge.py::test_get_task_id_uses_existing_
id 
tests/unit/domain/models/test_wsde_knowledge.py::test_identify_relevant_knowledg
e_matches_keywords 
tests/unit/domain/models/test_wsde_knowledge.py::test_knowledge_graph_insights_p
arses_payload 
tests/unit/domain/models/test_wsde_knowledge.py::test_integrate_knowledge_builds
_summary 
tests/unit/domain/models/test_wsde_knowledge.py::test_generate_improvement_sugge
stions_deduplicates_entries 
tests/unit/domain/models/test_wsde_roles_personas.py::test_enumerate_research_pe
rsonas_includes_overlays 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Synthesizer] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Contrarian] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Fact Checker] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Planner] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Moderator] 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_s
cores_requirements 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_h
ighlights_gaps 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_identifies_best_solution 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_handles_empty 
tests/unit/domain/models/test_wsde_strategies.py::test_weighted_voting_prefers_d
omain_expertise 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores 
tests/unit/domain/models/test_wsde_strategies.py::test_multidisciplinary_analysi
s_structures_results 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_add_agent_succeed
s 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_rotate_primus_suc
ceeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_empty_
team_succeeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_assign_roles_succ
eeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds 
tests/unit/domain/models/test_wsde_utils.py::test_send_message_invokes_protocol 
tests/unit/domain/models/test_wsde_utils.py::test_broadcast_message_excludes_sen
der tests/unit/domain/models/test_wsde_utils.py::test_get_messages_uses_protocol
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_creates_cy
cle 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_collects_f
eedback 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_handles_mi
ssing_peer_review 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks 
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_logs_warni
ng_on_failure 
tests/unit/domain/models/test_wsde_voting_logic.py::test_deterministic_voting_wi
th_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_determi
nistic_with_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_tie_is_
fair 
tests/unit/domain/models/test_wsde_voting_logic.py::test_handle_tied_vote_produc
es_consensus_result 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_analyzer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_transformer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_simple_file_analysis 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches 
tests/unit/domain/test_wsde_facade.py::test_summarize_consensus_result_outputs_e
xpected_sections 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_peer_review_cross_stor
e_sync_succeeds 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_mvu_helpers_cover_modu
le 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_initial_selection_prefe
rs_unused_agent_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_assign_roles_for_phase_
rotates_after_all_primus_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_first_time_selection_prior
itizes_unused_agents_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_resets_after_all_
have_served_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_when_all_agents_u
sed_resets_flags_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_by_expertise_prefers_doc
umentation_agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotation_resets_after_all_have_served_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_prefers_doc_expertise_vi
a_config_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotate_primus_resets_usage_flags_and_r
ole_map_succeeds 
tests/unit/domain/test_wsde_team.py::test_multiple_task_cycles_reset_primus_flag
s_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds tests/unit/domain/test_wsde_team.py::test_force_wsde_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds 
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie 
tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_rounds 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_primus_t
ie 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_random 
tests/unit/fallback/test_retry_counts.py::test_retry_count_metrics 
tests/unit/fallback/test_retry_counts.py::test_retry_only_network_errors 
tests/unit/fallback/test_retry_predicates.py::test_retry_predicate_triggers_retr
y 
tests/unit/fallback/test_retry_predicates.py::test_integer_predicate_records_met
rics 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_add
_agent_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_agent_type_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_team_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_missing_parameters_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_no_agents_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_type_not_found_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_execution_error_raises_error 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_type_enum_s
ucceeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_init
ialization_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_with
_parameters_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_mvp_capabilities_
succeeds 
tests/unit/general/test_agent_system.py::test_agent_state_keys_has_expected 
tests/unit/general/test_agent_system.py::test_process_input_node_success_is_vali
d 
tests/unit/general/test_agent_system.py::test_process_input_node_empty_input_suc
ceeds 
tests/unit/general/test_agent_system.py::test_process_input_node_adds_tool_list 
tests/unit/general/test_agent_system.py::test_llm_call_node_success_succeeds 
tests/unit/general/test_agent_system.py::test_llm_call_node_llm_failure_fails 
tests/unit/general/test_agent_system.py::test_llm_call_node_skip_on_prior_error_
raises_error 
tests/unit/general/test_agent_system.py::test_llm_call_node_missing_processed_in
put_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_success_is_valid
tests/unit/general/test_agent_system.py::test_parse_output_node_missing_llm_resp
onse_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_skip_on_prior_er
ror_raises_error 
tests/unit/general/test_agent_system.py::test_base_agent_graph_compiles_raises_e
rror 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error 
tests/unit/general/test_api.py::test_verify_token_rejects_invalid_token 
tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token 
tests/unit/general/test_api_health.py::test_health_endpoint_succeeds 
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_help_shows_co
mmand 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_disabled_exit
s_with_guidance 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_enabled_dry_r
un_succeeds 
tests/unit/general/test_backend_resource_flags.py::test_backend_flag_mapping_res
pects_env_vars 
tests/unit/general/test_backend_resource_flags.py::test_rdflib_env_mapping_disab
les_rdflib 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
handles_partial_spec 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error 
tests/unit/general/test_base.py::test_dummy_adapter_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_initiali
zation_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_delete_succee
ds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_persistence_s
ucceeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_exact_
match_matches_expected 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_semant
ic_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_store_and_ret
rieve_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_token_usage_s
ucceeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_lists_comm
ands_succeeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_omits_depr
ecated_aliases_succeeds 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_provider_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_file_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_co
de_analysis_implementation_succeeds 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_fi
le_analysis_implementation_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_code_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_directo
ry_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_file_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_project_structu
re_metrics_succeeds 
tests/unit/general/test_config_loader.py::test_load_yaml_config_succeeds 
tests/unit/general/test_config_loader.py::test_load_pyproject_toml_succeeds 
tests/unit/general/test_config_loader.py::test_autocomplete_succeeds 
tests/unit/general/test_config_loader.py::test_save_persists_version_succeeds 
tests/unit/general/test_config_loader.py::test_version_mismatch_logs_warning_mat
ches_expected 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_default_values_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_from_environment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_llm_set
tings_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[true-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[True-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[TRUE-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[false-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[False-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[FALSE-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_file_not_found_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_with_dotenv_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_invalid_sec
urity_boolean_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_empty_opena
i_api_key_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_defaults_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_from_env_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_embedd
ed_attribute_lookup_succeeds 
tests/unit/general/test_core_config_loader.py::test_precedence_env_over_project_
over_global_succeeds 
tests/unit/general/test_core_config_loader.py::test_load_toml_project_succeeds 
tests/unit/general/test_core_config_loader.py::test_save_global_config_yaml_succ
eeds tests/unit/general/test_core_values.py::test_load_core_values_succeeds 
tests/unit/general/test_core_values.py::test_find_value_conflicts_succeeds 
tests/unit/general/test_core_values.py::test_check_report_for_value_conflicts_su
cceeds 
tests/unit/general/test_core_workflows.py::test_filter_args_removes_none_values_
succeeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[init_project-init-kwargs0-expected0] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_specs-spec-kwargs1-expected1] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_tests-test-kwargs2-expected2] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_code-code-kwargs3-expected3] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[run_pipeline-run-pipeline-kwargs4-expected4] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs5-expected5] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs6-expected6] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[inspect_requirements-inspect-kwargs7-expected7] 
tests/unit/general/test_core_workflows.py::test_gather_requirements_creates_file
_succeeds 
tests/unit/general/test_core_workflows.py::test_workflow_manager_singleton_succe
eds 
tests/unit/general/test_delegate_task_disabled.py::test_delegate_task_collaborat
ion_disabled_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_c
reate_session_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_p
rocess_message_succeeds 
tests/unit/general/test_documentation_fetcher.py::test_fetcher_initialization_su
cceeds tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled 
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency 
tests/unit/general/test_dpg_flag.py::test_dpg_command_enabled 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_no_input_raises_e
rror 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_missing_
raises_error 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_success_
succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_prompt_success_su
cceeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manual_succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_custom_bridge_has
_expected 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_error_handling_ra
ises_error 
tests/unit/general/test_edrr_manifest_string.py::test_start_cycle_from_manifest_
string_succeeds 
tests/unit/general/test_exception_logging.py::test_log_exception_emits_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_message
_only_succeeds 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_error_c
ode_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_details
_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_to_dict_succeeds 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_validation_erro
r_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_configuration_e
rror_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_command_error_r
aises_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_internal_error_rai
ses_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_resource_exhausted
_error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_timeout_
error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_memory_adapter_er
ror_raises_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_agent_error_raises
_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_workflow_error_suc
ceeds 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_dialectical_reason
ing_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_error
_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_state
_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_ingestion_err
or_raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_memory_port_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_provider_port_error_
raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_agent_port_error_rai
ses_error 
tests/unit/general/test_fallback_utils.py::test_bulkhead_limits_concurrency 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_defau
lts_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_custo
m_manifest_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_dry_run_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_validate_o
nly_is_valid 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_verbose_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_forwards_a
uto_phase_flag 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_non_intera
ctive_flag_sets_env 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_env_var_en
ables_non_interactive 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_priority_u
pdates_config 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_manifest_e
rror_raises_error 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_ingestion_
error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_success_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_file_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_schema_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_validation_failed_fails 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_yaml
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_file
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_expand_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_differentiate_phase_has_
expected 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_refine_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected 
tests/unit/general/test_ingestion_edrr_integration.py::test_run_ingestion_invoke
s_edrr_phases_succeeds 
tests/unit/general/test_ingestion_type_hints.py::test_ingestion_type_hints_raise
s_error 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_update_succee
ds 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_prune_succeed
s 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_no_config_suc
ceeds 
tests/unit/general/test_inspect_config_cmd.py::test_analyze_project_structure_re
turns_directories 
tests/unit/general/test_inspect_config_cmd.py::test_compare_with_manifest_return
s_differences 
tests/unit/general/test_inspect_config_cmd.py::test_update_manifest_adds_directo
ry 
tests/unit/general/test_isolation.py::TestIsolation::test_devsynth_dir_isolation
_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_global_config_isolatio
n_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_memory_path_isolation_
succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_no_file_logging_preven
ts_directory_creation_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_path_redirection_in_te
st_environment_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_comprehensive_isolatio
n_succeeds 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_tmp_p
ath_fixture 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_netwo
rk_keyword 
tests/unit/general/test_kuzu_adapter.py::test_store_and_retrieve_vector_succeeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_succeeds 
tests/unit/general/test_kuzu_adapter.py::test_persistence_between_instances_succ
eeds 
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_without_numpy_su
cceeds 
tests/unit/general/test_kuzu_embedded_missing.py::test_ephemeral_kuzu_store_init
ialises_without_kuzu_embedded 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_creation_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_to_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_from_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_checkpoint_path_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_not_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_put_checkpoint_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
create_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
add_step_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
execute_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_save_and_get_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_list_workflows_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_registration 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_configuration_loading 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_initialization_with_defaults 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_environment_variable_handling 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_config_file_integration 
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url 
tests/unit/general/test_logger.py::test_configure_logging_creates_rotating_handl
er tests/unit/general/test_logger.py::test_dev_synth_logger_normalizes_exc_info 
tests/unit/general/test_logging_setup.py::test_log_records_include_request_conte
xt_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_passes_through_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_true_uses_current_except
ion 
tests/unit/general/test_logging_setup.py::test_extra_kwargs_and_reserved_keys_sa
fely_handled 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_idem
potent_no_duplicate_handlers 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_thre
ad_safe 
tests/unit/general/test_logging_setup_idempotent.py::test_no_file_logging_toggle
_prevents_file_handler 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_enu
m_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_ini
tialization_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_wit
h_metadata_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_ali
ases 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_typ
e_alias 
tests/unit/general/test_memory_store.py::test_memory_store_abstract_methods_succ
eeds 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_persistence_su
cceeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_token_usage_su
cceeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_add_and
_get_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_clear_c
ontext_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_get_ful
l_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_add
_and_get_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_cle
ar_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_full_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_relevant_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_per
sistence_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_tok
en_usage_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_file_bas
ed_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_in_memor
y_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_token_us
age_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_with_chromadb_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_without_vector_store_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_memory_and_vector_store_integration_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_context_manager_with_chromadb_succeeds 
tests/unit/general/test_methodology_logging.py::test_phase_timeout_logs_warning_
succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_bulk_add_agents_succeeds 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_success 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_failure 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_combines_streams 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_returns_exit_code 
tests/unit/general/test_mvu_init_cmd.py::test_mvu_init_cmd_creates_file 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure 
tests/unit/general/test_mvuu_dashboard_cli.py::test_mvuu_dashboard_help_succeeds
tests/unit/general/test_mypy_config.py::test_mypy_configuration_raises_error 
tests/unit/general/test_mypy_config.py::test_mypy_project_configuration_raises_e
rror 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_ensure_path_exists_respects_no_file_logging_succeeds 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_settings_respects_no_file_logging_succeeds 
tests/unit/general/test_onnx_port.py::test_onnx_port_load_and_run_succeeds 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds 
tests/unit/general/test_path_restrictions.py::test_configure_logging_within_proj
ect_dir_succeeds 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds 
tests/unit/general/test_primus_selection.py::test_highest_expertise_score_become
s_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_prioritizes_agents_who_have_no
t_served_as_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds 
tests/unit/general/test_primus_selection.py::test_rotation_resets_after_all_agen
ts_served_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_default_ma
nifest_returned_when_missing_returns_expected_result 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_in
itialization_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_di
rect_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_error_raises_error 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_agent_initializ
ation_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_regi
stration_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_and_fulfillment_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_unauthorized_ac
cess_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_not_
found_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_get_available_c
apabilities_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgentMixin::test_mixin_with
_custom_agent_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_initial_state_succe
eds 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_fulfilled_succ
eeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_rejected_succe
eds tests/unit/general/test_promise_system.py::TestPromise::test_catch_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_chaining_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_error_propagation_r
aises_error 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_value_stati
c_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_with_static_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_with_rejection_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_race_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_metadata_succeeds 
tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup 
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_chat_
models_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_diale
ctical_reasoning_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_impac
t_assessment_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_change_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_model_succeeds 
tests/unit/general/test_requirement_repository_interface.py::test_requirement_re
pository_interface_crud 
tests/unit/general/test_requirement_repository_port_interface.py::test_requireme
nt_repository_port_is_abstract 
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
uirement_port_methods_raise_not_implemented 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_app
rove_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_cre
ate_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_del
ete_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_rej
ect_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_upd
ate_requirement_succeeds 
tests/unit/general/test_resource_markers.py::test_is_lmstudio_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_codebase_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds 
tests/unit/general/test_resource_markers.py::test_is_resource_available_succeeds
tests/unit/general/test_resource_markers.py::test_with_resource_marker_succeeds 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds 
tests/unit/general/test_retry_failure_scenarios.py::test_named_retry_condition_a
borts_and_records_metrics 
tests/unit/general/test_retry_failure_scenarios.py::test_circuit_breaker_open_re
cords_abort_metrics 
tests/unit/general/test_speed_option.py::test_speed_option_recognized 
tests/unit/general/test_sync_manager_persistence.py::test_sync_manager_persists_
to_all_stores 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_templat
es_exist_in_temp_location_succeeds 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_can_use
_template_to_create_test_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_analyz
e_commit_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_calcul
ate_metrics_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_genera
te_metrics_report_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_get_co
mmit_history_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_main_s
ucceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_conversat
ion_tokens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_message_t
okens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_tokens_su
cceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_ensure_token_li
mit_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_fallback_tokeni
zer_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_prune_conversat
ion_succeeds 
tests/unit/general/test_unified_agent_code_prompt.py::test_process_code_task_inc
ludes_language_and_paradigm_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_yaml_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_pyproject_succe
eds 
tests/unit/general/test_unified_config_loader.py::test_save_and_exists_succeeds 
tests/unit/general/test_unified_config_loader.py::test_missing_files_succeeds 
tests/unit/general/test_unified_config_loader.py::test_version_mismatch_warning_
succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_yaml
_succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_pypr
oject_succeeds tests/unit/general/test_unit_cli_commands.py::test_cmd 
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds 
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_create_workflow_f
or_command_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_s
ucceeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_f
ailure_fails 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_h
uman_intervention_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
atus_enum_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
ep_initialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_in
itialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_wi
th_steps_succeeds 
tests/unit/general/test_wsde_dynamic_roles.py::test_assign_roles_for_phase_selec
ts_primus_by_expertise_has_expected 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_initialization_s
ucceeds 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_with_custom_valu
es_succeeds 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_wsde_team_init
ialization_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_add_agent_succ
eeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_rotate_primus_
succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_primus_suc
ceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_s
ucceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
by_expertise_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_autonomous_col
laboration_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds 
tests/unit/infrastructure/test_test_infrastructure_sanity.py::test_global_test_i
solation_sets_env_and_dirs 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_and_query_su
ccess 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_disabled_by_
flag 
tests/unit/integrations/test_autoresearch_client.py::test_query_failure_falls_ba
ck 
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_initialization 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_record_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_within_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_exceeds_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_prune_old_requests 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_multiple_clients 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_health_en
dpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_metrics_e
ndpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_init_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_gather_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_synthesiz
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_spec_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_code_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_doctor_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_edrr_cycl
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestRouter::test_router_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimitingIntegration::tes
t_rate_limiting_logic_integration 
tests/unit/interface/test_agentapi_enhanced.py::TestErrorHandling::test_error_re
sponse_structure 
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
quest_models_validation 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_answers_a
nd_defaults 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_confirm_c
hoice_coerces_booleans 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_progress_tr
acks_subtasks 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_rate_limit_
blocks_abusive_clients 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_allow
s_after_window 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_raise
s_when_exceeded 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_records_subtasks 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_normalizes_string_advances 
tests/unit/interface/test_api_endpoints.py::test_enhanced_rate_limit_state_track
s_buckets 
tests/unit/interface/test_api_endpoints.py::test_enhanced_metrics_snapshot_typed
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error 
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_sanitize_
output_succeeds 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_ini
t_with_bad_description_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_upd
ate_with_bad_inputs_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_sub
tasks_with_bad_inputs_use_fallbacks 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_ask_question_us
es_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_confirm_choice_
uses_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_prompt_fallback
_to_rich 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_noninteractive_re
turns_defaults_and_logs 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches 
tests/unit/interface/test_command_output.py::test_format_and_display_message 
tests/unit/interface/test_command_output.py::test_format_error_suggestions 
tests/unit/interface/test_command_output.py::test_list_and_structured_outputs 
tests/unit/interface/test_command_output.py::test_set_console 
tests/unit/interface/test_dpg_ui.py::test_all_buttons_trigger_callbacks_and_prog
ress tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog 
tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog_error 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_actionable_error_suggestion_str_includes_details 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_format_error_wraps_with_footer 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_defaul
t_file 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_specif
ied_file 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_invokes_strea
mlit tests/unit/interface/test_mvuu_dashboard.py::test_require_streamlit_raises 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_snaps
hot 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_witho
ut_optional_sections 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_with_overlays
_loads_telemetry 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_pointer_legacy_env 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_secret_falls_back_to
_legacy 
tests/unit/interface/test_mvuu_dashboard.py::test_resolve_telemetry_path_prefers
_legacy 
tests/unit/interface/test_nicegui_bridge.py::test_session_storage_roundtrip 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_notifies_and_re
cords 
tests/unit/interface/test_nicegui_bridge.py::test_progress_indicator_updates_and
_completes 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_falls_back_with
out_nicegui 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_update 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_complete 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_create_progress 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_main_function
_exists 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_json_yaml_with_and_without_console 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_table_fallback_and_empty_list 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_rich_renderables 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_sanitize_outp
ut_delegates_and_handles_edge_cases 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[ERROR: Disk failure-error] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[warning: Low memory-warning] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Task completed successfully-success] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[INFO: FYI-info] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[# Heading-heading] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Routine update-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_format_messag
e_applies_status_styles 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_highl
ight_branch_emits_panel 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_witho
ut_console_raises_value_error 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_format_
message_error_styles_and_escapes_markup 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_markdow
n_branch_sanitizes_hyperlinks 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_table_b
ranch_sanitizes_script_links 
tests/unit/interface/test_output_formatter_fallbacks.py::test_table_format_falls
_back_to_text_for_nontabular_inputs 
tests/unit/interface/test_output_formatter_fallbacks.py::test_rich_format_select
s_renderables_for_data_shapes 
tests/unit/interface/test_output_formatter_fallbacks.py::test_list_of_dicts_tabl
e_renders_missing_and_complex_values 
tests/unit/interface/test_output_formatter_fallbacks.py::test_set_format_options
_and_command_output_overrides 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-syntax] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-plain] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-syntax] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-plain] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-dict] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-empty-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-heterogeneous] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-dict] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-list-of-dicts] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-bullet-panel] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-falsy-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[text-scalar] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_mark
down_handles_nested_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_to_mark
down_handles_mixed_items 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_tabl
e_serializes_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_of_dict
s_to_table_handles_missing_keys 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-nested] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-list-item] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-dict-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-key] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-table-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_table
_and_list_preserve_sanitized_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_command_outp
ut_unknown_extension_and_highlight_panel 
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds 
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
ed_protocol 
tests/unit/interface/test_progress_helpers.py::test_subtask_snapshot_typed_struc
ture 
tests/unit/interface/test_progress_utils.py::test_progress_manager_create_get_co
mplete_and_context_manager 
tests/unit/interface/test_progress_utils.py::test_progress_manager_track_updates
_on_item_and_slice 
tests/unit/interface/test_progress_utils.py::test_progress_indicator_context_man
ager_completes 
tests/unit/interface/test_progress_utils.py::test_step_progress_sequencing_and_c
omplete 
tests/unit/interface/test_progress_utils.py::test_create_and_track_progress_help
ers_use_manager 
tests/unit/interface/test_progress_utils.py::test_progress_tracker_forced_update
_and_complete 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_prefers_di
alog_selection 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_validates_
input 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_multi_select_re
turns_checkbox_choices 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_produces_timeline_snapshot 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_merges_extended_metadata 
tests/unit/interface/test_research_telemetry.py::test_merge_extended_metadata_in
to_payload_appends_sections 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_invokes_connectors 
tests/unit/interface/test_research_telemetry.py::test_signature_roundtrip_valida
tes 
tests/unit/interface/test_research_telemetry.py::test_signature_failure_with_wro
ng_secret 
tests/unit/interface/test_textual_ux_bridge.py::test_question_and_display_intera
ctions_are_recorded 
tests/unit/interface/test_textual_ux_bridge.py::test_confirm_choice_falls_back_t
o_default 
tests/unit/interface/test_textual_ux_bridge.py::test_progress_updates_capture_ne
sted_subtasks 
tests/unit/interface/test_textual_ux_bridge.py::test_capabilities_reflect_textua
l_availability 
tests/unit/interface/test_textual_ux_bridge.py::test_require_textual_guard 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_enabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_disabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_backward_compatib
ility_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_handle_error_defa
ult_implementation 
tests/unit/interface/test_ux_bridge_coverage.py::test_progress_indicator_context
_manager 
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_fallback_i
mport tests/unit/interface/test_uxbridge_aliases.py::test_function 
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[absent-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants 
tests/unit/interface/test_webui_bridge_cli_parity.py::test_webui_bridge_matches_
cli_prompt_defaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_helpers_normal
ize_mixed_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_prompt_helpers_echo_d
efaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_append
s_documentation_links 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_require_streamlit_cac
hes_and_guides 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_clamps_handle_
invalid_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_and_pr
ogress_use_formatter 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_highli
ght_falls_back_to_write 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_uses
_cached_stub 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_impo
rts_when_missing 
tests/unit/interface/test_webui_bridge_handshake.py::test_adjust_wizard_step_han
dles_invalid_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_routes_
messages_and_sanitizes 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_error_b
ranch_records_message 
tests/unit/interface/test_webui_bridge_handshake.py::test_bridge_prompt_helpers_
return_defaults 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
invalid_total_defaults_to_zero 
tests/unit/interface/test_webui_bridge_normalize.py::test_progress_indicator_rej
ects_missing_parent 
tests/unit/interface/test_webui_bridge_normalize.py::test_display_result_routes_
messages_to_streamlit 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_upda
te_paths 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations 
tests/unit/interface/test_webui_bridge_progress.py::test_require_streamlit_failu
re 
tests/unit/interface/test_webui_bridge_progress.py::test_adjust_wizard_step_edge
s 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_display_re
sult_routes_and_sanitizes 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_session_ac
cess_wrappers 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_prompt_ali
ases_and_progress 
tests/unit/interface/test_webui_bridge_progress.py::test_normalize_wizard_step_v
aried_inputs 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_require_stream
lit_raises 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_progress_indic
ator_status_transitions 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_handshake
_routes_to_streamlit 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_error_rou
te_sanitizes_output 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_respects_
sanitization_flag 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_highlight
_routes_to_info 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_success_r
outes_to_success 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_require_streamlit
_missing_dependency_surfaces_install_guidance 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_navigation
_normalization_matches_state_invariants 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_prompt_defaults_a
lign_with_uxbridge_contract 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_display_result_ch
annels_respect_output_formatter_contract 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_get_wiza
rd_manager_uses_session_state 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_session_
helpers_delegate 
tests/unit/interface/test_webui_bridge_targeted.py::test_adjust_wizard_step_inva
lid_direction_keeps_bounds 
tests/unit/interface/test_webui_bridge_targeted.py::test_normalize_wizard_step_h
andles_strings 
tests/unit/interface/test_webui_bridge_targeted.py::test_question_and_confirmati
on_defaults 
tests/unit/interface/test_webui_bridge_targeted.py::test_display_result_highligh
t_routes_to_info 
tests/unit/interface/test_webui_bridge_targeted.py::test_create_progress_cycles_
statuses 
tests/unit/interface/test_webui_bridge_targeted.py::test_session_helpers_delegat
e_to_state_access 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_requ
ires_session_state 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_nested_completion_and_sanitization 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_wizard_na
vigation_and_display_fallback 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_default_s
tatus_thresholds 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_updates_and_completion 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_nested_su
btask_lifecycle 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_display_r
esult_routes_by_type 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_get_wizar
d_manager_and_create 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute 
tests/unit/interface/test_webui_commands.py::test_cli_returns_none_when_missing 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_pass_thr
ough 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-File not found] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Permission denied] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Invalid value] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Missing key] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Type error] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_generic_
exception 
tests/unit/interface/test_webui_commands.py::test_cli_uses_cli_module_when_avail
able 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_reraises
_devsynth_error 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3] 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key 'api'-key_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-] 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: File not found: config.yaml-Make sure the 
file exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Permission denied: secrets.env-necessary 
permissions] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Invalid value: bad input-Please check your
input] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Missing key: 'api_key'-Verify that the 
referenced key exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Type error: wrong type-Check that all 
inputs] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[warning-warning] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[success-success] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[info-info] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[unexpected-write] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[# Overview-expected_calls0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[## Section-expected_calls1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[### Deep Dive-expected_calls2] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_normalize_ste
p_logs_warning_on_invalid_value 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_adjust_step_w
arns_on_invalid_direction 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors 
tests/unit/interface/test_webui_progress_time.py::test_update_records_time 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/interface/test_webui_rendering_module.py::test_validate_requirements_
step_requires_fields 
tests/unit/interface/test_webui_rendering_module.py::test_handle_requirements_na
vigation_cancel_clears_state 
tests/unit/interface/test_webui_rendering_module.py::test_save_requirements_clea
rs_temporary_keys 
tests/unit/interface/test_webui_rendering_progress.py::test_gather_wizard_render
s_cli_summary 
tests/unit/interface/test_webui_rendering_progress.py::test_render_progress_summ
ary_prefers_checkpoint_eta_strings 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist 
tests/unit/interface/test_webui_routing.py::test_router_uses_session_state 
tests/unit/interface/test_webui_routing.py::test_router_resets_invalid_selection
tests/unit/interface/test_webui_routing.py::test_router_handles_sidebar_exceptio
n 
tests/unit/interface/test_webui_routing.py::test_router_surfaces_page_exception 
tests/unit/interface/test_webui_routing.py::test_router_requires_pages 
tests/unit/interface/test_webui_routing.py::test_router_honors_explicit_default 
tests/unit/interface/test_webui_routing.py::test_router_reports_missing_page_han
dler 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_main_block_succeeds 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_simulatio
n_sanitises_nested_tasks 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_require_s
treamlit_guidance 
tests/unit/interface/test_webui_state_errors.py::test_clear_reraises_after_loggi
ng 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_require_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[error-kwargs0-error] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[warning-kwargs1-warning] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[success-kwargs2-success] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[highlight-kwargs3-info] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[error-kwargs0-error] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[warning-kwargs1-warning] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[success-kwargs2-success] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[highlight-kwargs3-info] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_progre
ss_indicator_nested_lifecycle_and_statuses 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[0-0-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[10-100-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[25-100-Processing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[50-100-Halfway there...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[80-100-Almost done...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[99-100-Finalizing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[100-100-Complete] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_wizard_helpe
rs_clamp_malformed_inputs 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_valid_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_default_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_auto_model_selection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_custom_port 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_lmstudio_unavailable 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_precedence 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_counting_integration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_limit_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_c
ircuit_breaker_initialization 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_r
etry_logic_configuration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_valid_config 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_environment_variable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_without_api_key_raises_error 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_custom_base_url 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_openai_client_unavailable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_with_defaults 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_precedence 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_counting_integration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_limit_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_circu
it_breaker_initialization 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_logic_configuration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_metrics_
collection_setup 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_telemetr
y_emission 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_message_args_
and_mappings 
tests/unit/logging/test_logging_setup.py::test_redact_filter_property_loop_prese
rves_inputs 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_attaches_c
ontext 
tests/unit/logging/test_logging_setup.py::test_json_formatter_serializes_request
_context 
tests/unit/logging/test_logging_setup.py::test_redaction_in_message_and_payload 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_injects_fi
elds_and_clears 
tests/unit/logging/test_logging_setup.py::test_jsonformatter_includes_exception_
block 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_no_file_l
ogging 
tests/unit/logging/test_logging_setup.py::test_get_log_dir_and_file_use_env_over
rides 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_uses_project_dir_f
or_relative_path 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_under_te
st_project_dir 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_absolute
_outside_home 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_project_d
ir_when_file_logging_disabled 
tests/unit/logging/test_logging_setup.py::test_configure_logging_redirects_home_
and_disables_file_handler 
tests/unit/logging/test_logging_setup.py::test_short_secret_not_redacted 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_merges_and_fi
lters_kwargs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_table_normali
zation 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_does_not_muta
te_extra_inputs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_normalizes_tr
uthy_exc_info 
tests/unit/logging/test_logging_setup.py::test_configure_logging_console_only_us
es_caplog 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_secret_tokens
_via_caplog 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_handles_missing_
log_file_path 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_emits_structured
_extras_with_context 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_secrets_f
ilter_masks_values 
tests/unit/logging/test_logging_setup_additional_paths.py::test_json_formatter_i
ncludes_context_and_extras 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_respects_project_dir 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_skips_creation_when_disabled 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_warns_when_creation_fails 
tests/unit/logging/test_logging_setup_additional_paths.py::test_devsynth_logger_
filters_reserved_extra_keys 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_filter_ma
sks_args_and_payload 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_provis
ions_json_file_handler 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_consol
e_only_mode 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[permission-error] 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[file-not-found] 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_idempo
tent_with_identical_configuration 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_e
xplicit_level_overrides_env 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_j
son_handler_writes_structured_output 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_r
econfigures_console_only_toggle 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_file-logging] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_console-only] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_file-logging] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_console-custom] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_idempotent_with_identical_settings 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_preserves_filters_on_reconfigure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_falls_back_to_console_on_file_handler_failure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_create_dir_guard_preserves_console_only_mode 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_cli_context_wires_consol
e_and_json_file_handlers 
tests/unit/logging/test_logging_setup_contexts.py::test_test_context_redirects_a
nd_supports_console_only_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_create_dir_toggle_disabl
es_json_file_handler 
tests/unit/logging/test_logging_setup_contexts.py::test_console_and_json_handler
s_report_consistent_payloads 
tests/unit/logging/test_logging_setup_invariants.py::test_configure_logging_is_i
dempotent_for_handlers 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
masks_known_tokens 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
redacts_payload_and_details 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
survives_mapping_errors 
tests/unit/logging/test_logging_setup_invariants.py::test_cli_to_test_context_sw
itch_updates_log_destination 
tests/unit/logging/test_logging_setup_invariants.py::test_json_formatter_include
s_structured_extras 
tests/unit/logging/test_logging_setup_levels.py::test_configure_logging_honors_e
nv_log_level 
tests/unit/logging/test_logging_setup_levels.py::test_json_formatter_captures_re
quest_context 
tests/unit/logging/test_logging_setup_levels.py::test_dev_logger_attaches_filter
s_and_handlers 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir-disabled] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env-create-dir-disabled] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[home-absolute] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[non-home-absolute] 
tests/unit/memory/test_issue3_regression_guard.py::test_issue3_findings_persist 
tests/unit/memory/test_layered_cache.py::test_promotes_value_to_higher_layer 
tests/unit/memory/test_layered_cache.py::test_hit_ratio_tracking 
tests/unit/memory/test_layered_cache.py::test_read_and_write_alias_methods 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_rel
oad_exposes_runtime_protocol 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_protocol_runtime_
checks_accept_custom_layers 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_pro
tocol_remains_runtime_checkable 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_missing_required_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_m
emory_store_protocol_runtime_check 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_impor
t_and_construction_succeeds 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_accep
ts_optional_backends 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_still
_requires_primary_backend 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_rejec
ts_unknown_backend_names 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_stub_store_matches
_protocol_runtime 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_memory_store_param
eters_are_runtime_typevars 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_parameterised_memo
ry_store_runtime_is_safe 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_snapshot_alias_pre
serves_runtime_origin 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_value_typevar_iden
tity_is_preserved 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_and_s
napshot_share_runtime_typevar 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_transaction_rolls_
back_typed_stores 
tests/unit/memory/test_sync_manager_transaction_failure.py::test_transaction_rol
ls_back_all_stores 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_commit_unknown_tr
ansaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_rollback_unknown_
transaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_double_commit_fai
ls_and_state_persists 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_complete
s_with_deterministic_seed 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_tr
ansitions_and_memory_integration 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_time_bud
get_exceeded 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_consensu
s_error_handling 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_transien
t_error_retry 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_result_t
ype_handling 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_ov
erride_from_result 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_invalid_
result_type_raises 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_determin
istic_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_seeds_random_and_numpy_modules 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_logs_backoff_and_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_coordinator_records_each_phase 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_exits_when_total_budget_elapsed 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_accepts_dialectical_sequence_payload 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_records_unknown_phase_and_next_phase_fallbacks 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_default_path_executes 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_dia
lectical_sequence_records_with_coordinator_fallback 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_tolerates_seed_failures 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_trace_complete 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_configures_seed_providers 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_budget_precheck 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_retries_then_succeeds 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_exhaustion_sets_stop 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_copies_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_handles_dialectical_sequence 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_raises_for_non_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_halts_when_result_missing 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_matrix 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_exhausts_retry_budget_and_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_retries_clamp_sleep_to_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_stops_retry_when_total_budget_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_coordinator_records_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_records_dialectical_sequences_for_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_fallbacks_for_invalid_phase_and_next_phase 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_honors_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_seeds_random_sources 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_preserves_nonstandard_phase_without_hints 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_handles_extended_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_enforces_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retries_until_success 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_fallback_transitions_and_propagation 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_respects_max_iterations_limit 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retry_backoff_respects_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_honors_phase_and_next_phase_fields 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_clamps_retry_when_budget_consumed 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_logs_retry_exhaustion_telemetry 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_exits_when_budget_elapsed_before_iteration 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_retry_sequence_updates_phase_and_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_records_results_before_consensus_failure 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
tries_on_transient 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_emits_debug_and_clamps_sleep 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_without_budget_uses_base_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_clamps_backoff_and_respects_budget 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_remaining_budget_spent 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
cords_consensus_failure_via_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_consensus_failure_without_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_budget_already_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_invalid_next
_phase_falls_back_to_transition_map 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_missing_stat
us_relies_on_max_iterations 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_raises_for_non_mapping_results 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_handles_seed_failures_gracefully 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_logs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_import_he
lper_exposes_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_immediate_timeout_skips_apply_invocation 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_respects_total_budget_and_emits_debug 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_uses_fallback_after_invalid_phase 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_stops_after_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_seeds_random_and_numpy 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_applies_synthesis_to_task 
tests/unit/methodology/test_adhoc_adapter.py::test_should_start_cycle_true 
tests/unit/methodology/test_adhoc_adapter.py::test_should_progress_to_next_phase
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_evaluation
_terminates_with_many_hooks 
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_hooks_cont
inue_after_exception 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations 
tests/unit/methodology/test_edrr_coordinator.py::test_automate_retrospective_rev
iew_summarizes_results 
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs tests/unit/methodology/test_kanban_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_kanban_adapter.py::test_progress_respects_wip_limit 
tests/unit/methodology/test_milestone_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_milestone_adapter.py::test_progress_requires_approva
l_when_configured 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget 
tests/unit/methodology/test_sprint_adapter.py::test_calculate_phase_end_time 
tests/unit/methodology/test_sprint_adapter.py::test_is_phase_time_exceeded_false
tests/unit/methodology/test_sprint_adapter.py::test_should_progress_when_time_ex
ceeded 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase 
tests/unit/methodology/test_sprint_adapter.py::test_before_cycle_provides_contex
t 
tests/unit/methodology/test_sprint_adapter.py::test_before_expand_sets_phase_sta
rt_time 
tests/unit/methodology/test_sprint_adapter.py::test_after_retrospect_captures_sp
rint_plan 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_failure_br
anch_sets_failed 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step 
tests/unit/policies/test_verify_security_policy.py::test_passes_when_all_variabl
es_set 
tests/unit/policies/test_verify_security_policy.py::test_fails_when_variable_mis
sing 
tests/unit/providers/test_provider_contract.py::test_stub_provider_offline_defau
lts_to_stub 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_uses_
safe_provider 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_null_
provider 
tests/unit/providers/test_provider_system_additional.py::test_unknown_provider_f
alls_back 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_us
es_provider_config 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_re
spects_track_metrics_flag 
tests/unit/providers/test_provider_system_additional.py::test_stub_provider_dete
rministic_embeddings 
tests/unit/providers/test_provider_system_additional.py::test_create_tls_config_
uses_settings 
tests/unit/providers/test_provider_system_additional.py::test_provider_factory_p
refers_explicit_tls_config 
tests/unit/providers/test_provider_system_additional.py::test_fallback_async_ski
ps_open_circuit 
tests/unit/providers/test_provider_system_branches.py::test_factory_honors_disab
le_flag 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_s
tub_safe_default 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_n
ull_when_requested 
tests/unit/providers/test_provider_system_branches.py::test_explicit_openai_with
out_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_availabilit
y_guard_returns_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_fallback_fa
ilure_promotes_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_explicit_anthropic_w
ithout_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_anthropic_unsupporte
d_error 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_us
es_next_provider_on_failure 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_pr
opagates_failure_when_all_fail 
tests/unit/providers/test_provider_system_branches.py::test_fallback_disabled_tr
ies_only_first_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_initializat
ion_orders_providers_and_records_circuit_results 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_skips
_open_circuit_breaker 
tests/unit/providers/test_provider_system_branches.py::test_openai_async_retry_e
mits_telemetry 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_circu
it_breaker_recovery 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[primary-success] 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[secondary-success] 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[all-fail] 
tests/unit/providers/test_provider_system_branches.py::test_factory_applies_tls_
and_retry_settings 
tests/unit/providers/test_provider_system_branches.py::test_emit_retry_telemetry
_logs_and_counts 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_comp
lete_builds_payload 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_reje
cts_invalid_temperature 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_embe
d_returns_embeddings 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_co
mplete_uses_custom_messages 
tests/unit/providers/test_provider_system_branches.py::test_fallback_embed_moves
_to_next_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_aembed_reco
vers_from_failure 
tests/unit/providers/test_provider_system_branches.py::test_get_provider_config_
reads_env_file 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ope
nai_success_path 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ant
hropic_requires_key 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_unk
nown_provider_uses_null 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_lms
tudio_fallback_when_openai_missing 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_asyn
c_paths 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_rea
l_module_branches 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_as
ync_paths 
tests/unit/providers/test_provider_system_branches.py::test_complete_helper_incr
ements_metrics_and_propagates_error 
tests/unit/providers/test_provider_system_branches.py::test_embed_helper_wraps_n
on_provider_errors 
tests/unit/providers/test_provider_system_branches.py::test_acomplete_helper_inc
rements_metrics 
tests/unit/providers/test_provider_system_branches.py::test_aembed_helper_promot
es_unexpected_errors 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_components_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_requirements_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_edrr_phas
e_mapping_on_persist 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_evaluatio
n_hook_invoked_on_consensus_true 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[chromadb] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[faiss] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[kuzu] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[tinydb] 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_file_operations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_network_calls 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_global_state 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_fixture_usage 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_simple_test_file 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_file_with_isolation_marker 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_handles_syntax_errors 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_generates_recommendations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages 
tests/unit/scripts/test_analyze_test_dependencies.py::TestIntegration::test_end_
to_end_analysis 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_help 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_missing
_test_dir 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_test_execution_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_coverage_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_validation_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_shell_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_syntax_errors 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_finds_
testing_scripts 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_analyz
es_overlaps 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_git_us
age_frequency 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_genera
tes_consolidation_recommendations 
tests/unit/scripts/test_audit_testing_scripts.py::TestMarkdownGeneration::test_g
enerates_markdown_report 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_help 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_missing_scr
ipts_dir 
tests/unit/scripts/test_audit_testing_scripts.py::test_integration_audit_workflo
w 
tests/unit/scripts/test_auto_issue_comment.py::test_parse_issue_numbers_extracts
_ids 
tests/unit/scripts/test_auto_issue_comment.py::test_dry_run_when_env_missing 
tests/unit/scripts/test_auto_issue_comment.py::test_posts_comment_when_env_prese
nt 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_initialization 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_timeout 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_empty 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_with_data 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_generates_recommendations 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_help 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_invalid_
workers 
tests/unit/scripts/test_benchmark_test_execution.py::test_integration_benchmark_
workflow 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
valid_anchor 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
missing_anchor 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_missing_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_unit 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity 
tests/unit/scripts/test_examples_smoke_script.py::test_main_default_examples_suc
ceeds 
tests/unit/scripts/test_examples_smoke_script.py::test_main_reports_failure_when
_analyze_raises 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors 
tests/unit/scripts/test_gen_ref_pages.py::test_gen_ref_pages_matches_examples 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_with_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_without_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_property_test_metrics 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_calculate_overall_quality_score 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_generate_quality_recommendations 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics 
tests/unit/scripts/test_generate_quality_report.py::test_html_generation 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures 
tests/unit/scripts/test_security_ops.py::test_collect_logs_missing_directory 
tests/unit/scripts/test_security_ops.py::test_run_audit_calls_security_audit 
tests/unit/scripts/test_security_ops.py::test_list_outdated_runs_poetry 
tests/unit/scripts/test_security_ops.py::test_apply_updates_runs_poetry 
tests/unit/scripts/test_security_scan_script.py::test_main_non_strict_no_tools_r
eturns_ok 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_passes_when_above 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_fails_when_below 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_valid 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_issue 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_mvuu 
tests/unit/scripts/test_verify_release_state.py::test_draft_status_missing_tag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_without_t
ag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_with_tag 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_returns
_fields 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_without
_header 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_missing 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_present 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_when_log_mi
ssing 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_unreso
lved_questions 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_only_r
esolved 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_invali
d_json 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache_i
nvalidation 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_path_fi
lter 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_f
lags_missing_docs 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_p
asses_when_documented 
tests/unit/scripts/test_verify_test_markers_cli.py::test_argparser_includes_chan
ged_flag 
tests/unit/scripts/test_verify_test_markers_cli.py::test_verify_files_with_temp_
test 
tests/unit/scripts/test_verify_test_markers_cross_check.py::test_argparser_inclu
des_cross_check_flag 
tests/unit/scripts/test_wsde_edrr_simulation.py::test_simulation_converges 
tests/unit/security/test_api_authentication.py::test_verify_token_valid_is_valid
tests/unit/security/test_api_authentication.py::test_verify_token_invalid_is_val
id 
tests/unit/security/test_api_authentication.py::test_verify_token_missing_succee
ds 
tests/unit/security/test_api_authentication.py::test_verify_token_wrong_format_s
ucceeds 
tests/unit/security/test_api_authentication.py::test_verify_token_access_control
_disabled_succeeds 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_password_hasher_parameters_safe_defaults 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_hash_and_verify_roundtrip 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_generate_key_validates_and_encrypts 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_invalid_key_rejected 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_missing_key_env_raises 
tests/unit/security/test_authentication_optional_dependency.py::test_authenticat
ion_handles_missing_argon2 
tests/unit/security/test_authorization_checks.py::test_require_authorization_all
ows_authorized_action 
tests/unit/security/test_authorization_checks.py::test_require_authorization_rai
ses_forbidden 
tests/unit/security/test_deployment_coverage.py::test_require_non_root_user_when
_not_required 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_missing_vars 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_all_present 
tests/unit/security/test_deployment_coverage.py::test_apply_secure_umask 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_with_requir
ed_env 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_without_req
uired_env 
tests/unit/security/test_encryption.py::test_generate_key_returns_expected_resul
t 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_roundtrip_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_string_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_bytes_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_env_var_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_no_key_raises_error 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_with_env_var_succee
ds 
tests/unit/security/test_encryption.py::test_decrypt_invalid_token_raises_error 
tests/unit/security/test_encryption.py::test_decrypt_with_wrong_key_raises_error
tests/unit/security/test_logging_redaction.py::test_logging_redacts_openai_api_k
ey 
tests/unit/security/test_logging_redaction.py::test_logging_redacts_in_extra_det
ails 
tests/unit/security/test_memory_encryption.py::test_json_file_store_encryption_s
ucceeds 
tests/unit/security/test_memory_encryption.py::test_lmdb_store_encryption_succee
ds 
tests/unit/security/test_memory_encryption.py::test_tinydb_store_encryption_succ
eeds tests/unit/security/test_policy_audit.py::test_audit_detects_violation 
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file 
tests/unit/security/test_review.py::test_review_due_when_interval_elapsed 
tests/unit/security/test_review.py::test_review_not_due_before_interval 
tests/unit/security/test_review.py::test_next_review_date_calculation 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_script_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_control_ch
ars_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_both_succe
eds 
tests/unit/security/test_sanitization.py::test_sanitize_input_strips_whitespace_
succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_script_tags_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_control_chars_s
ucceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_complex_script_tag
s_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_multiple_script_ta
gs_succeeds 
tests/unit/security/test_sanitization.py::test_validate_safe_input_with_safe_inp
ut_returns_expected_result 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_s
cript_raises_error 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_c
ontrol_chars_raises_error 
tests/unit/security/test_security_audit.py::test_run_executes_checks 
tests/unit/security/test_security_audit.py::test_run_raises_on_policy_failure 
tests/unit/security/test_security_audit.py::test_report_writes_results 
tests/unit/security/test_security_audit.py::test_report_records_failure 
tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_runs_che
cks 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_respects
_skip_flags 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_register
ed 
tests/unit/security/test_security_flags_env.py::test_authentication_disabled_all
ows_any_credentials 
tests/unit/security/test_security_flags_env.py::test_authentication_enabled_enfo
rces 
tests/unit/security/test_security_flags_env.py::test_authorization_disabled_allo
ws 
tests/unit/security/test_security_flags_env.py::test_authorization_enabled_enfor
ces 
tests/unit/security/test_security_flags_env.py::test_sanitization_disabled_no_er
ror 
tests/unit/security/test_security_flags_env.py::test_sanitization_enabled_raises
tests/unit/security/test_tls_config.py::test_tls_config_timeout_env_override 
tests/unit/security/test_tls_config.py::test_tls_config_timeout_explicit_overrid
e 
tests/unit/security/test_tls_config.py::test_tls_config_validation_raises_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_partial_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_key_only_succ
eeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_cert_only_suc
ceeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_missing_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_defau
lt_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_verif
y_false_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
ca_file_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_and_key_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_only_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_ca_fi
le_precedence_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_all_p
arams_succeeds 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_valid_string_
is_valid 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[   ] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[None] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_non_string_va
lue_succeeds 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[-5--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[0-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[5-1-10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[1-1-10-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[10-1-10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[-5--10-0--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[0--10-10-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[abc] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[1.5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[None] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value4] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[0-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[-5-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[5-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[0--1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[100-99] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[a-choices0] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[1-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[True-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[None-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[value-choices4] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[5-choices5] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[d-choices0] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[4-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[None-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[missing-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[20-choices4] 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_option 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_with_no_path 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_run_tests_command 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_standard_cli_fallback 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_missing_run_tests_m
odule 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_cli_import_errors 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_runtime_errors 
tests/unit/test_sentinel_speed_markers.py::test_sentinel_fast_bucket_present 
tests/unit/test_simple_addition.py::test_add_returns_sum_for_integers 
tests/unit/test_simple_addition.py::test_add_accepts_floats_and_mixed_numeric_ty
pes 
tests/unit/test_simple_addition.py::test_add_raises_type_error_for_non_numeric_i
nputs 
tests/unit/test_verify_test_organization_sentinel.py::test_verify_test_organizat
ion_returns_zero 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran 
tests/unit/testing/test_collect_cache_sanitize.py::test_sanitize_node_ids_strips
_line_numbers_only_when_no_function_delimiter 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_union_
reaches_threshold_with_overlap 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_thresh
old_detection_matches_cli_expectations 
tests/unit/testing/test_deterministic_seed_fixture.py::test_deterministic_seed_f
ixture_sets_env_vars 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_bad_ttl_env_falls_back_to_
default 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_sanitize_node_ids_preserve
s_function_qualifier_and_strips_line_numbers 
tests/unit/testing/test_failure_tips.py::test_failure_tips_contains_core_guidanc
e 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_can_mutate_addition 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_mutates_addition_to_subtraction 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_cannot_mutate_non_arithmetic 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_can_mutate_equality 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_mutates_equality_to_inequality 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_ca
n_mutate_and_operation 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_mu
tates_and_to_or 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_can_
mutate_not_operation 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_muta
tes_not_by_removal 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_can_mutat
e_boolean_constant 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_t
rue_to_false 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_n
umber_to_zero_and_one 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_mutations_for_simple_code 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_handles
_syntax_errors 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_different_mutation_types 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_initializa
tion 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_killed 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_survived 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
esult_dataclass 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
eport_dataclass 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow 
tests/unit/testing/test_run_tests.py::test_sanitize_node_ids_strips_line_numbers
_without_function_delimiter 
tests/unit/testing/test_run_tests.py::test_failure_tips_contains_key_guidance_li
nes 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches 
tests/unit/testing/test_run_tests.py::test_run_tests_segment_batches 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes 
tests/unit/testing/test_run_tests_additional_coverage.py::test_failure_tips_ment
ions_core_troubleshooting_flags 
tests/unit/testing/test_run_tests_additional_coverage.py::test_ensure_pytest_cov
_plugin_env_injects_and_skips 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_missing_json 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_errors 
tests/unit/testing/test_run_tests_additional_coverage.py::test_sanitize_node_ids
_removes_line_numbers 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker 
tests/unit/testing/test_run_tests_artifacts.py::test_reset_coverage_artifacts_re
moves_stale_files 
tests/unit/testing/test_run_tests_artifacts.py::test_ensure_coverage_artifacts_g
enerates_reports 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_fails_when_pytest
_cov_missing 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_successful_single
_batch 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_h
andles_missing_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_r
ejects_invalid_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_missing_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_s
uccess 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_failure_tips_con
tains_suggestions 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_i
nject_plugins_and_emit_tips 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_failure_tips_surfac
e_cli_remediations 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_batches_fo
llow_segment_size 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_han
dles_resource_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_co
verage_totals 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_skips_placeh
older_artifacts 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_env_passthrough_an
d_coverage_lifecycle 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled 
tests/unit/testing/test_run_tests_collection_cache.py::test_sanitize_node_ids_st
rips_trailing_line_without_function_delimiter 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_reset_coverage_art
ifacts_removes_files_and_directories 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_data_missing 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_no_measured_files 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_generates_reports_and_syncs_legacy 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_html_failure_still_writes_json 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_run_tests_writes_m
anifest_with_coverage_reference 
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py::test_ensure_c
overage_artifacts_combines_fragment_files 
tests/unit/testing/test_run_tests_coverage_short_circuit.py::test_ensure_coverag
e_artifacts_short_circuits_without_measured_files 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_repor
ts_missing_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_flags
_invalid_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_totals 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_html_index 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_rejec
ts_empty_html 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_succe
ss_path 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_failure_tips_formats_
return_code_and_cmd 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_reset_coverage_artifa
cts_handles_oserror 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_ensure_coverage_artif
acts_handles_unreadable_html 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags 
tests/unit/testing/test_run_tests_keyword_exec.py::test_keyword_marker_executes_
matching_node_ids 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success 
tests/unit/testing/test_run_tests_logic.py::test_sanitize_node_ids_strips_line_n
umbers_without_function_delimiter 
tests/unit/testing/test_run_tests_logic.py::test_failure_tips_contains_key_guida
nce_lines 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_success 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_failure 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_segmented_exe
cution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_marker_expres
sion_building 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_env_defaults 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exception_han
dling 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exit_code_5_s
uccess 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_dry_run_skips
_execution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_keyword_filte
r 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_maxfail_optio
n 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_smoke_mode_pl
ugin_injection 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_s
uccess 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_f
rom_existing_cache 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_c
ollection_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_maxfail 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_custom_env 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_execution_failur
e_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_no_target_p
ath_raises_error 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories 
tests/unit/testing/test_run_tests_marker_fallback.py::test_run_tests_marker_fall
back_skips_segmentation 
tests/unit/testing/test_run_tests_marker_fallback.py::test_build_segment_metadat
a_uses_typed_sequences 
tests/unit/testing/test_run_tests_marker_merge.py::test_speed_marker_merged_with
_lmstudio_keyword_filter 
tests/unit/testing/test_run_tests_marker_merge.py::test_global_marker_with_lmstu
dio_keyword_filter 
tests/unit/testing/test_run_tests_module.py::test_sanitize_node_ids_dedup_and_st
rip_line_numbers 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return 
tests/unit/testing/test_run_tests_module.py::test_failure_tips_includes_segmenta
tion_guidance 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_missing_file 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_invalid_json 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions 
tests/unit/testing/test_run_tests_option_parsing.py::test_parse_pytest_addopts_h
andles_balanced_and_unbalanced_quotes 
tests/unit/testing/test_run_tests_option_parsing.py::test_addopts_has_plugin_det
ects_split_and_concatenated_forms 
tests/unit/testing/test_run_tests_option_parsing.py::test_coverage_plugin_disabl
ed_detects_common_overrides 
tests/unit/testing/test_run_tests_orchestration.py::test_verbose_flag_adds_v_to_
pytest_command 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run 
tests/unit/testing/test_run_tests_orchestration.py::test_pytest_addopts_are_pres
erved 
tests/unit/testing/test_run_tests_orchestration.py::test_extra_marker_adds_m_fla
g_to_command 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env0-False---no-cov -s] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env1-True--k smoke -p 
pytest_cov] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env2-False--p 
no:pytest_bdd -s] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env3-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_honors_env_timeout 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_respects_explicit_disables 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_detects_inline_plugin_token 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[--no-cov -s-False---no-cov -s] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[-k smoke-True--k smoke -p pytest_cov] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_pytest_cov_support_
status_missing_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_run_tests_aborts_wh
en_pytest_cov_missing 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_detects_existing_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_respects_explicit_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-p no:pytest_bdd -s-False--p no:pytest_bdd 
-s] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-k feature-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_strips_tra
iling_line_numbers_without_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_keeps_ids_
with_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_deduplicat
es_preserving_order 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_single_speed 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_multiple_speeds 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_no_tests_found 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_failure_with_maxfail 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_reuses_cache_and_preserves_environment 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_dry_run_batches_use_typed_requests 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_command_building 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_multiple_node_ids 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_smoke_mode_env 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_no_parallel 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_batches
_execute 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_honors_
keyword_filter 
tests/unit/testing/test_run_tests_segmented.py::test_run_segmented_tests_stop_af
ter_maxfail 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_failure_appends_tips 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_segmente
d_failure_surfaces_remediation 
tests/unit/testing/test_run_tests_segmented_failures.py::test__run_segmented_tes
ts_aggregates_outputs 
tests/unit/testing/test_run_tests_segmented_failures.py::test_segmented_runs_rei
nject_plugins_without_clobbering_addopts 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_single_b
atch_uses_request_object 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_success_invokes_publish 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_failure_skips_graph 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_reports_append_graph 
tests/unit/testing/test_run_tests_segmented_report_flag.py::test_run_segmented_t
ests_reports_only_last_segment 
tests/unit/testing/test_run_tests_speed_keyword_loop.py::test_speed_loop_uses_ke
yword_filter_and_executes_node_ids 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_merges_fast
_and_medium_collections 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_defaults_to
_fast_and_medium_when_unspecified 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_excludes_gu
i_by_default 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_allows_gui_
when_requested 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_strips_line
_numbers_without_function_selector 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_preserves_o
rder 
tests/unit/testing/test_sanitize_node_ids_minimal.py::test_sanitize_node_ids_str
ips_line_when_no_function 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_tuple_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_invalid
_exc_info 
tests/unit/utils/test_logging_coverage.py::test_get_logger_returns_correct_insta
nce 
tests/unit/utils/test_logging_coverage.py::test_setup_logging_with_different_log
_levels 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_false_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_none_ex
c_info 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
baseexception_direct 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
true_with_active_exception 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
none_and_false_explicit 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_invalid_e
xc_info_to_hit_line_48 
tests/unit/utils/test_logging_final_coverage.py::test_get_logger_function_direct
_call 
tests/unit/utils/test_logging_final_coverage.py::test_setup_logging_function_dir
ect_calls 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_with_kwar
gs 
tests/unit/utils/test_logging_utils.py::test_dev_synth_logger_normalizes_exc_inf
o_tuple_and_exception 
tests/unit/utils/test_logging_utils.py::test_setup_logging_calls_configure_loggi
ng 
tests/unit/utils/test_logging_utils.py::test_get_logger_returns_dev_synth_logger
_instance 
tests/unit/utils/test_serialization.py::test_dumps_deterministic_round_trip_simp
le tests/unit/utils/test_serialization.py::test_dump_and_load_file_round_trip 
tests/unit/utils/test_serialization.py::test_provider_env_as_dict_deterministic_
serialization 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_with_s
tring_already_having_newline 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_ensure
s_single_newline 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_no_trailing_new
line 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_trailing_newlin
e 
tests/unit/utils/test_serialization_coverage.py::test_dump_to_file_complete_cove
rage 
tests/unit/utils/test_serialization_coverage.py::test_load_from_file_complete_co
verage 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_multiple_traili
ng_newlines 
tests/unit/utils/test_serialization_coverage.py::test_serialization_with_unicode
_characters 
tests/unit/utils/test_serialization_coverage.py::test_serialization_edge_cases 
tests/unit/utils/test_serialization_coverage.py::test_file_operations_with_speci
al_paths 
tests/unit/utils/test_serialization_edges.py::test_loads_tolerates_missing_and_s
ingle_trailing_newline 
tests/unit/utils/test_serialization_edges.py::test_dump_to_file_overwrites_and_k
eeps_single_newline 
tests/unit/utils/test_serialization_extra.py::test_dumps_and_loads_deterministic
_round_trip_unicode_and_newline 
tests/unit/utils/test_serialization_extra.py::test_dump_and_load_file_round_trip
_handles_utf8 
tests/unit/utils/test_serialization_extra.py::test_loads_accepts_without_trailin
g_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
direct_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
with_string_that_might_have_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_direct_line_co
verage 
tests/unit/utils/test_serialization_final_coverage.py::test_dump_to_file_direct_
line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_load_from_file_direc
t_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_serialization_functi
ons_with_mock_to_ensure_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_with_various_n
ewline_scenarios 
tests/unit/utils/test_serialization_final_coverage.py::test_file_operations_with
_explicit_paths 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
return_path 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_return_path -m
not memory_intensive and fast and not gui --cov=src/devsynth 
--cov-report=json:test_reports/coverage.json --cov-report=html:htmlcov 
--cov-append --maxfail 1
E         Troubleshooting tips:
E         - Smoke mode: reduce third-party plugin surface to isolate issues:
E           poetry run devsynth run-tests --smoke --speed=fast --no-parallel 
--maxfail=1
E         - Marker discipline: default is '-m not memory_intensive'.
E           Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.
E         - Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using 
--smoke; plugin options may fail otherwise.
E         - Diagnostics: run 'poetry run devsynth doctor' for a quick 
environment check.
E         - Narrow scope: use '-k <expr>' and '-vv' to focus a failure.
E         - Segment large suites to localize failures and flakes:
E           devsynth run-tests --target unit-tests --speed=fast --segment 
--segment-size=50
E         - Limit failures early to speed iteration:
E           poetry run devsynth run-tests --target unit-tests --speed=fast 
--maxfail=1
E         - Disable parallelism if xdist interaction is suspected:
E           devsynth run-tests --target unit-tests --speed=fast --no-parallel
E         - Generate an HTML report for context (saved under test_reports/):
E           devsynth run-tests --target unit-tests --speed=fast --report
E         
E       assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/cli/test_run_tes
ts_regression.py:29: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:33,219 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:33,219 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:36,865 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
____________________ test_configure_llm_settings_reads_env _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ad475c0>

    @pytest.mark.fast
    def test_configure_llm_settings_reads_env(monkeypatch):
        """Environment variables update LLM settings when configured."""
        monkeypatch.setenv("DEVSYNTH_LLM_MODEL", "foo-model")
        monkeypatch.setenv("DEVSYNTH_LLM_MAX_TOKENS", "123")
        monkeypatch.setenv("DEVSYNTH_LLM_TEMPERATURE", "0.9")
        monkeypatch.setenv("DEVSYNTH_LLM_AUTO_SELECT_MODEL", "false")
    
>       importlib.reload(config)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/config/test_conf
ig_llm_env.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = <module 'devsynth.config' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/config/__init
__.py'>

    def reload(module):
        """Reload the module and return it.
    
        The module must have been successfully imported before.
    
        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None
    
        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.config not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
____________________ test_load_config_merges_mvuu_settings _____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_load_config_merges_mvuu_s0')

    @pytest.mark.fast
    def test_load_config_merges_mvuu_settings(tmp_path):
        mvu_cfg = {"schema": "s.json", "storage": {"path": "db.json", "format": 
"json"}}
        dev_dir = tmp_path / ".devsynth"
        dev_dir.mkdir()
        (dev_dir / "mvu.yml").write_text(yaml.safe_dump(mvu_cfg))
        cfg = load_config(start_path=str(tmp_path))
>       assert cfg.mvuu == mvu_cfg
E       AssertionError: assert {} == {'schema': 's...': 'db.json'}}
E         
E         Right contains 2 more items:
E         {'schema': 's.json', 'storage': {'format': 'json', 'path': 'db.json'}}
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_config
_loader_mvu.py:16: AssertionError
______________ test_bootstrap_script_rejects_invalid_environment _______________

    @pytest.mark.fast
    def test_bootstrap_script_rejects_invalid_environment():
        """bootstrap.sh should reject unknown environments.
    
        ReqID: DEP-02"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'bootstrap.sh'} invalid"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Invalid environment" in result.stderr
E           assert 'Invalid environment' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/.../github.com/ravenoak/devsyn
th/scripts/deployment/bootstrap.sh invalid"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:29: AssertionError
____________________ test_bootstrap_script_requires_docker _____________________

    @pytest.mark.fast
    def test_bootstrap_script_requires_docker():
        """bootstrap.sh should require docker to be installed.
    
        ReqID: DEP-03"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'bootstrap.sh'} 
development"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Docker is required" in result.stderr
E           assert 'Docker is required' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...hub.com/ravenoak/devsynth/s
cripts/deployment/bootstrap.sh development"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:49: AssertionError
________________________ test_install_dev_installs_task ________________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_install_dev_installs_task0')

    @pytest.mark.fast
    def test_install_dev_installs_task(tmp_path):
        """install_dev.sh should install go-task when absent.
    
        ReqID: DEP-05"""
    
        home = tmp_path / "home"
>       home.mkdir()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
bootstrap_script.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_install_dev_installs_task0/home')
mode = 511, parents = False, exist_ok = False

    def mkdir(self, mode=0o777, parents=False, exist_ok=False):
        """
        Create a new directory at this given path.
        """
        try:
>           os.mkdir(self, mode)
E           FileExistsError: [Errno 17] File exists: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_install_dev_installs_task0/home'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:1311: FileExistsError
___________________ test_health_check_script_reports_healthy ___________________

    @pytest.mark.fast
    def test_health_check_script_reports_healthy():
        """health_check.sh should succeed when endpoints return 200.
    
        ReqID: DEP-02"""
        server = HTTPServer(("127.0.0.1", 0), _Handler)
        port = server.server_address[1]
        thread = threading.Thread(target=server.serve_forever)
        thread.daemon = True
        thread.start()
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            url = f"http://127.0.0.1:{port}"
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} {url} 
{url} {url}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
>           assert result.returncode == 0
E           assert 1 == 0
E            +  where 1 = CompletedProcess(args=['su', 'nobody', '-s', 
'/bin/bash', '-c', "cd '/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/... 
http://127.0.0.1:62244 http://127.0.0.1:62244 http://127.0.0.1:62244"], 
returncode=1, stdout='', stderr='su: Sorry\n').returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:49: AssertionError
__________________ test_health_check_script_rejects_root_user __________________

    @pytest.mark.fast
    def test_health_check_script_rejects_root_user():
        """health_check.sh should refuse to run as root.
    
        ReqID: CON-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            result = subprocess.run(
                [str(SCRIPTS_DIR / "health_check.sh")],
                cwd=workdir,
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Please run this script as a non-root user." in result.stderr
E           AssertionError: assert 'Please run this script as a non-root user.' 
in 'Missing environment file: .env\n'
E            +  where 'Missing environment file: .env\n' = 
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scr
ipts/deployment/health_check.sh'], returncode=1, stdout='', stderr='Missing 
environment file: .env\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:72: AssertionError
__________________ test_health_check_script_requires_env_file __________________

    @pytest.mark.fast
    def test_health_check_script_requires_env_file():
        """health_check.sh should fail when the env file is missing.
    
        ReqID: DEP-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Missing environment file" in result.stderr
E           assert 'Missing environment file' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/ravenoak/d
evsynth/scripts/deployment/health_check.sh"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:92: AssertionError
_____________ test_health_check_script_requires_strict_permissions _____________

    @pytest.mark.fast
    def test_health_check_script_requires_strict_permissions():
        """health_check.sh should enforce 600 permissions on the env file.
    
        ReqID: CON-04"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o644)
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "must have 600 permissions" in result.stderr
E           assert 'must have 600 permissions' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...jects/github.com/ravenoak/d
evsynth/scripts/deployment/health_check.sh"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:115: AssertionError
_________________ test_health_check_script_rejects_invalid_url _________________

    @pytest.mark.fast
    def test_health_check_script_rejects_invalid_url():
        """health_check.sh should validate provided URLs.
    
        ReqID: DEP-03"""
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            cmd = (
                f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} not-a-url"
                " not-a-url not-a-url"
            )
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert "Invalid URL" in result.stderr
E           assert 'Invalid URL' in 'su: Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/...ynth/scripts/deployment/hea
lth_check.sh not-a-url not-a-url not-a-url"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:141: AssertionError
_____________ test_health_check_script_fails_on_unhealthy_endpoint _____________

    @pytest.mark.fast
    def test_health_check_script_fails_on_unhealthy_endpoint():
        """health_check.sh should report failure when an endpoint is unhealthy.
    
        ReqID: DEP-01"""
        server = HTTPServer(("127.0.0.1", 0), _FailHandler)
        port = server.server_address[1]
        thread = threading.Thread(target=server.serve_forever)
        thread.daemon = True
        thread.start()
        workdir = Path(tempfile.mkdtemp())
        workdir.chmod(0o755)
        try:
            env_file = workdir / ".env"
            env_file.write_text("DEVSYNTH_ENV=testing\n")
            env_file.chmod(0o600)
            url = f"http://127.0.0.1:{port}"
            cmd = f"cd '{workdir}' && {SCRIPTS_DIR / 'health_check.sh'} {url} 
{url} {url}"
            result = subprocess.run(
                ["su", "nobody", "-s", "/bin/bash", "-c", cmd],
                capture_output=True,
                text=True,
            )
            assert result.returncode != 0
>           assert f"Health check failed for {url}" in result.stderr
E           assert 'Health check failed for http://127.0.0.1:62247' in 'su: 
Sorry\n'
E            +  where 'su: Sorry\n' = CompletedProcess(args=['su', 'nobody', 
'-s', '/bin/bash', '-c', "cd 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/... http://127.0.0.1:62247 
http://127.0.0.1:62247 http://127.0.0.1:62247"], returncode=1, stdout='', 
stderr='su: Sorry\n').stderr

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/deployment/test_
health_check_smoke.py:170: AssertionError
____________________ TestWSDETeam.test_get_primus_succeeds _____________________

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x1196d8c50>

    @pytest.mark.fast
    def test_get_primus_succeeds(self):
        """Test getting the current Primus agent.
    
        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.primus_index = 1
        primus = self.team.get_primus()
>       assert primus == self.agent2
E       AssertionError: assert <MagicMock id='5286595328'> == <MagicMock 
id='5286600896'>
E         
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:108: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,566 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_team
2025-10-29 10:48:39,566 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_team
____________ TestWSDETeam.test_assign_roles_with_rotation_succeeds _____________

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x1196d9f10>

    @pytest.mark.fast
    def test_assign_roles_with_rotation_succeeds(self):
        """Test that role assignments change when the Primus rotates.
    
        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.add_agent(self.agent3)
        self.team.add_agent(self.agent4)
        self.team.primus_index = 0
        self.team.assign_roles()
        initial_roles = {
            self.agent1.name: self.agent1.current_role,
            self.agent2.name: self.agent2.current_role,
            self.agent3.name: self.agent3.current_role,
            self.agent4.name: self.agent4.current_role,
        }
        self.team.rotate_primus()
        self.team.assign_roles()
>       assert self.agent1.current_role != initial_roles[self.agent1.name]
E       AssertionError: assert 'Primus' != 'Primus'
E        +  where 'Primus' = <MagicMock id='5284342256'>.current_role
E        +    where <MagicMock id='5284342256'> = 
<tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x1196d9f10>.agent1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:206: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,592 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_team
2025-10-29 10:48:39,593 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_team
2025-10-29 10:48:39,593 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_team
2025-10-29 10:48:39,593 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_team
2025-10-29 10:48:39,594 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2', 
'supervisor': 'agent3', 'designer': 'agent4', 'evaluator': None}
2025-10-29 10:48:39,594 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team test_team: {'primus': 'agent1', 'worker': 'agent2', 
'supervisor': 'agent3', 'designer': 'agent4', 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor': 
'agent3', 'designer': 'agent4', 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team test_team: {'primus': 'agent1', 'worker': 'agent2', 'supervisor': 
'agent3', 'designer': 'agent4', 'evaluator': None}
_ TestWSDETeam.test_apply_dialectical_reasoning_with_knowledge_graph_succeeds __

self = <tests.unit.domain.models.test_wsde.TestWSDETeam object at 0x1196da3c0>

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_with_knowledge_graph_succeeds(self):
        """Test applying dialectical reasoning with knowledge graph integration.
    
        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        task = {
            "id": "task1",
            "type": "code_generation",
            "description": "Implement a secure authentication system",
            "requirements": ["user authentication", "password security"],
        }
        solution = {
            "id": "solution1",
            "agent": "agent1",
            "content": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        self.team.solutions = {task["id"]: [solution]}
        mock_wsde_memory = MagicMock()
        mock_wsde_memory.query_knowledge_for_task.return_value = [
            {"concept": "authentication", "relevance": 0.9},
            {"concept": "password", "relevance": 0.8},
            {"concept": "security", "relevance": 0.7},
        ]
        mock_wsde_memory.query_concept_relationships.return_value = [
            {"relationship": "requires", "direction": "outgoing"}
        ]
>       result = self.team.apply_dialectical_reasoning_with_knowledge_graph(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            task, self.critic_agent, mock_wsde_memory
        )
E       AttributeError: 'WSDETeam' object has no attribute 
'apply_dialectical_reasoning_with_knowledge_graph'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:247: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,602 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_team
2025-10-29 10:48:39,602 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_team
____________________ TestWSDE.test_initialization_succeeds _____________________

self = <tests.unit.domain.models.test_wsde.TestWSDE object at 0x1196da4e0>

    @pytest.mark.fast
    def test_initialization_succeeds(self):
        """Test that a WSDE is initialized correctly.
    
        ReqID: N/A"""
>       wsde = WSDE(name="Test WSDE")
               ^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: WSDE.__init__() got an unexpected keyword argument 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:278: TypeError
_____________ TestWSDE.test_initialization_with_metadata_succeeds ______________

self = <tests.unit.domain.models.test_wsde.TestWSDE object at 0x1196da8d0>

    @pytest.mark.fast
    def test_initialization_with_metadata_succeeds(self):
        """Test that a WSDE is initialized correctly with metadata.
    
        ReqID: N/A"""
        metadata = {"key": "value", "another_key": 123}
>       wsde = WSDE(name="Test WSDE", metadata=metadata)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: WSDE.__init__() got an unexpected keyword argument 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde.py:291: TypeError
__________ test_apply_dialectical_reasoning_invokes_hooks_and_memory ___________

    @pytest.mark.fast
    def test_apply_dialectical_reasoning_invokes_hooks_and_memory() -> None:
        """End-to-end reasoning returns typed artefacts and integrates hooks."""
    
        team = WSDETeam(name="workflow-regression")
        task_factory = DialecticalTaskFactory()
        task = task_factory.build(
            solution={
                "content": "Short solution lacking examples.",
                "code": "print('hello world')",
            }
        )
    
        critic = SimpleNamespace(name="critic-agent")
        memory = MemoryRecorder()
        hook_calls: list[tuple[DialecticalTask, tuple[DialecticalSequence, 
...]]] = []
    
        def hook(task_payload: DialecticalTask, sequences: 
tuple[DialecticalSequence, ...]):
            hook_calls.append((task_payload, sequences))
    
        team.register_dialectical_hook(hook)
    
        result = team.apply_dialectical_reasoning(task, critic, 
memory_integration=memory)
    
        assert isinstance(result, DialecticalSequence)
        serialized = result.to_dict()
        assert serialized["status"] == "completed"
        assert serialized["synthesis"]
    
        assert hook_calls, "Hook should be invoked with the generated sequence"
        hook_task, hook_sequences = hook_calls[0]
        assert isinstance(hook_task, DialecticalTask)
        assert hook_task.identifier == task.identifier
>       assert hook_sequences[0] is result
E       AssertionError: assert {'antithesis': {'agent': 'critic-agent', 
'alternative_approaches': ['Consider a more structured format with sections',...
logging'], ...}, 'id': 'f0013ff8-6568-4df6-a5e1-e77973187673', 'metadata': {}, 
'method': 'dialectical_reasoning', ...} is 
DialecticalSequence(sequence_id='29ce7140-c225-439a-9a71-0374c1bbc000', 
steps=(DialecticalStep(step_id='f0013ff8-6568-... handling', 'Implement a more 
modular design with smaller functions')),), status='completed', reason=None, 
metadata={})

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dialectical_workflow.py:60: AssertionError
__ TestWSDERoleReassignment.test_build_consensus_multiple_solutions_succeeds ___

self = 
<tests.unit.domain.models.test_wsde_dynamic_workflows.TestWSDERoleReassignment 
object at 0x119647e90>

    def test_build_consensus_multiple_solutions_succeeds(self):
        """Test that build consensus multiple solutions succeeds.
    
        ReqID: N/A"""
        task = {"id": "t1", "description": "demo"}
        self.team.add_solution(task, {"agent": "code", "content": "a"})
        self.team.add_solution(task, {"agent": "test", "content": "b"})
        consensus = self.team.build_consensus(task)
>       assert consensus["consensus"] != ""
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'consensus'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_dynamic_workflows.py:43: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_core - INFO - Added agent 
code to team test_dynamic_workflows_team
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team test_dynamic_workflows_team
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_core - INFO - Added agent 
test to team test_dynamic_workflows_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent code 
to team test_dynamic_workflows_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team test_dynamic_workflows_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent test 
to team test_dynamic_workflows_team
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task t1
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task t1
2025-10-29 10:48:39,706 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task t1
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task t1
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
_______________ test_check_security_best_practices_detects_issue _______________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at 
0x13b1fda80>

    @pytest.mark.fast
    def test_check_security_best_practices_detects_issue(wsde_team_factory):
        """ReqID: WSDE-SECURITY-01  flags insecure patterns for escalation."""
    
        team = wsde_team_factory()
        insecure_code = "password = 'secret'\nexec('print(1)')\n"
>       assert team._check_security_best_practices(insecure_code) is False
E       assert {'compliance_level': 'medium', 'issues': ['Hardcoded credentials 
detected'], 'suggestions': ['Use environment variable..., 'Use parameterized 
queries to prevent SQL injection', 'Validate all user input and implement proper
error handling']} is False
E        +  where {'compliance_level': 'medium', 'issues': ['Hardcoded 
credentials detected'], 'suggestions': ['Use environment variable..., 'Use 
parameterized queries to prevent SQL injection', 'Validate all user input and 
implement proper error handling']} = _check_security_best_practices("password = 
'secret'\nexec('print(1)')\n")
E        +    where _check_security_best_practices = 
<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13af8ab40>._check_security_best_practices

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:12: AssertionError
____________ test_check_security_best_practices_accepts_clean_code _____________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at 
0x13b1fd620>

    @pytest.mark.fast
    def 
test_check_security_best_practices_accepts_clean_code(wsde_team_factory):
        """ReqID: WSDE-SECURITY-02  passes checklist when code avoids red 
flags."""
    
        team = wsde_team_factory()
        secure_code = (
            "def process_items(items):\n"
            "    processed_items = []\n"
            "    for element in items:\n"
            "        processed_items.append(element)\n"
            "    return processed_items\n"
        )
>       assert team._check_security_best_practices(secure_code) is True
E       AssertionError: assert {'compliance_level': 'high', 'issues': [], 
'suggestions': []} is True
E        +  where {'compliance_level': 'high', 'issues': [], 'suggestions': []} 
= _check_security_best_practices('def process_items(items):\n    processed_items
= []\n    for element in items:\n        processed_items.append(element)\n    
return processed_items\n')
E        +    where _check_security_best_practices = 
<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13af8bfe0>._check_security_best_practices

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:27: AssertionError
_______________ test_balance_security_and_performance_idempotent _______________

wsde_team_factory = <function wsde_team_factory.<locals>._factory at 
0x13b1fe7a0>

    @pytest.mark.fast
    def test_balance_security_and_performance_idempotent(wsde_team_factory):
        """ReqID: WSDE-SECURITY-03  avoids duplicating checklist 
annotations."""
    
        team = wsde_team_factory()
        code = "def run():\n    return True"
    
        balanced_once = team._balance_security_and_performance(code)
        balanced_twice = team._balance_security_and_performance(balanced_once)
    
        assert balanced_once == balanced_twice
>       assert balanced_once.count("Security and performance balance") == 1
               ^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'count'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_security_checks.py:41: AttributeError
__________________ test_role_assignment_uses_expertise_scores __________________

    @pytest.mark.fast
    def test_role_assignment_uses_expertise_scores():
        agents = [
            DummyAgent(name="lead", expertise=["leadership", "coordination"]),
            DummyAgent(name="builder", expertise=["development", "testing"]),
            DummyAgent(name="designer", expertise=["architecture", "planning"]),
            DummyAgent(name="reviewer", expertise=["evaluation", "analysis"]),
        ]
        team = _bind_team(WSDETeam(name="roles", agents=agents))
    
        assignments = team.assign_roles()
        primus = assignments.as_name_mapping()["primus"]
        assert primus is not None and primus.name == "lead"
>       role_map = team.get_role_map()
                   ^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'get_role_map'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_strategies.py:88: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,824 - devsynth.domain.models.wsde_core - INFO - Added agent 
lead to team roles
2025-10-29 10:48:39,824 - devsynth.domain.models.wsde_core - INFO - Added agent 
builder to team roles
2025-10-29 10:48:39,824 - devsynth.domain.models.wsde_core - INFO - Added agent 
designer to team roles
2025-10-29 10:48:39,824 - devsynth.domain.models.wsde_core - INFO - Added agent 
reviewer to team roles
2025-10-29 10:48:39,824 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team roles: {'primus': 'lead', 'worker': 'builder', 
'supervisor': 'designer', 'designer': 'reviewer', 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent lead 
to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
builder to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
designer to team roles
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
reviewer to team roles
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team roles: {'primus': 'lead', 'worker': 'builder', 'supervisor': 
'designer', 'designer': 'reviewer', 'evaluator': None}
____________________ TestWSDETeam.test_get_primus_succeeds _____________________

self = <tests.unit.domain.models.test_wsde_team.TestWSDETeam object at 
0x119755250>

    @pytest.mark.fast
    def test_get_primus_succeeds(self):
        """Test getting the current Primus agent.
    
        ReqID: N/A"""
        self.team.add_agent(self.agent1)
        self.team.add_agent(self.agent2)
        self.team.primus_index = 1
        primus = self.team.get_primus()
>       assert primus == self.agent2
E       AssertionError: assert <MagicMock id='5287177120'> == <MagicMock 
id='5287215472'>
E         
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_team.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,847 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_wsde_team
2025-10-29 10:48:39,847 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_wsde_team
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_wsde_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_wsde_team
_______ TestWSDETeam.test_analyze_trade_offs_detects_conflicts_succeeds ________

self = <tests.unit.domain.models.test_wsde_team.TestWSDETeam object at 
0x119756480>

    @pytest.mark.fast
    def test_analyze_trade_offs_detects_conflicts_succeeds(self):
        """Trade-off analysis should flag options with similar scores as 
conflicts.
    
        ReqID: N/A"""
        evaluated = [
            {"id": 1, "score": 0.8},
            {"id": 2, "score": 0.78},
            {"id": 3, "score": 0.2},
        ]
        trade_offs = self.team.analyze_trade_offs(
            evaluated, conflict_detection_threshold=0.7
        )
>       opt1 = next(o for o in trade_offs if o["id"] == 1)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       StopIteration

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_team.py:134: StopIteration
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,863 - devsynth.domain.models.wsde_decision_making - INFO - 
Analyzing trade-offs between options
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_decision_making:logging_setup.py:615 
Analyzing trade-offs between options
_________________ test_add_solution_appends_and_triggers_hooks _________________

    def test_add_solution_appends_and_triggers_hooks():
        hook = MagicMock()
        team = SimpleNamespace(solutions={}, dialectical_hooks=[hook])
        task = {"id": "t1"}
        solution = {"content": "data"}
>       result = add_solution(team, task, solution)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/models/te
st_wsde_utils.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

team = namespace(solutions={}, dialectical_hooks=[<MagicMock id='5287672096'>])
task = {'id': 't1'}, solution = {'content': 'data'}

    def add_solution(
        team: Any, task: TaskPayload, solution: SolutionRecord
    ) -> SolutionRecord:
        """Add a solution to the team and trigger dialectical hooks."""
        task_id = task.get("id")
        if not task_id:
            task_id = str(uuid4())
            task["id"] = task_id
        else:
            # Normalise the identifier on the task payload so future calls reuse
it.
            task["id"] = task_id
    
>       team.solutions.add(task_id, solution)
        ^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'add'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_utils.py:236: AttributeError
_______________ test_calculate_expertise_score_multiple_matches ________________

    @pytest.mark.fast
    def test_calculate_expertise_score_multiple_matches():
        """Test that calculate expertise score multiple matches.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeExpertiseScoreTeam")
        agent = DummyAgent(["python", "documentation"])
        team.add_agent(agent)
        task = {"language": "python", "description": "generate documentation"}
        score = team._calculate_expertise_score(agent, task)
>       assert score > 1
E       assert 1 > 1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_expertise_score.py:25: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,945 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent to team TestWsdeExpertiseScoreTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent agent
to team TestWsdeExpertiseScoreTeam
____________ test_summarize_voting_result_reports_winner_and_counts ____________

    @pytest.mark.fast
    def test_summarize_voting_result_reports_winner_and_counts():
        team = WSDETeam(name="facade")
        team.add_agents([SimpleAgent("a"), SimpleAgent("b")])
        voting_result = {
            "status": "completed",
            "result": {
                "method": "majority",
                "winner": "A",
                "tie_broken": True,
                "tie_breaker_method": "primus",
            },
            "vote_counts": {"A": 2, "B": 1},
            "vote_weights": {"a": 1.0, "b": 1.5},
        }
        summary = team.summarize_voting_result(voting_result)
>       assert "Voting was completed using majority." in summary
E       AssertionError: assert 'Voting was completed using majority.' in 'Vote 
distribution: A: 2 votes, B: 1 votes\nVote weights: a: 1.00, b: 1.50'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade.py:50: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,955 - devsynth.domain.models.wsde_core - INFO - Added agent 
a to team facade
2025-10-29 10:48:39,955 - devsynth.domain.models.wsde_core - INFO - Added agent 
b to team facade
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a to 
team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent b to 
team facade
__________________ test_select_primus_updates_index_and_role ___________________

    @pytest.mark.fast
    def test_select_primus_updates_index_and_role():
        team = WSDETeam(name="facade")
        doc = SimpleAgent("doc", ["documentation"])
        coder = SimpleAgent("coder", ["python"])
        team.add_agents([doc, coder])
        team.select_primus_by_expertise({"type": "coding", "language": 
"python"})
>       assert team.get_primus() is coder
E       assert <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x13b2ac440> is <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x13b2ac3e0>
E        +  where <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object 
at 0x13b2ac440> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b2ac4d0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade_roles.py:23: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,961 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team facade
2025-10-29 10:48:39,961 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team facade
2025-10-29 10:48:39,961 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team facade
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
________________ test_dynamic_role_reassignment_rotates_primus _________________

    @pytest.mark.fast
    def test_dynamic_role_reassignment_rotates_primus():
        team = WSDETeam(name="facade")
        doc = SimpleAgent("doc", ["documentation"])
        coder = SimpleAgent("coder", ["python"])
        tester = SimpleAgent("tester", ["testing"])
        team.add_agents([doc, coder, tester])
        team.dynamic_role_reassignment({"type": "documentation"})
        first = team.get_primus()
        team.dynamic_role_reassignment({"type": "coding", "language": "python"})
        second = team.get_primus()
>       assert first is doc
E       assert <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x13b2a8dd0> is <tests.unit.domain.test_wsde_facade_roles.SimpleAgent object at 
0x13b2a88c0>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_facade_roles.py:39: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team facade
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team facade
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team facade
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_roles - INFO - Selected 
coder as primus based on expertise
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_roles - INFO - Selected 
tester as primus based on expertise
2025-10-29 10:48:39,967 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team facade
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team facade
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder 
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester 
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
_________ test_documentation_tasks_pick_documentation_experts_succeeds _________

    @pytest.mark.fast
    def test_documentation_tasks_pick_documentation_experts_succeeds():
        """Test that documentation tasks pick documentation experts succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePhaseRoleRotationTeam")
        coder = _agent("coder", ["python"])
        doc = _agent("doc", ["documentation", "markdown"])
        team.add_agents([coder, doc])
        team.select_primus_by_expertise({"type": "documentation"})
>       assert team.get_primus() is doc
E       AssertionError: assert <MagicMock id='5287778432'> is <MagicMock 
id='5287781840'>
E        +  where <MagicMock id='5287778432'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b2d1790>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_phase_role_rotation.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:39,988 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdePhaseRoleRotationTeam
2025-10-29 10:48:39,988 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdePhaseRoleRotationTeam
2025-10-29 10:48:39,988 - devsynth.domain.models.wsde_roles - INFO - Selected 
coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdePhaseRoleRotationTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdePhaseRoleRotationTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder 
as primus based on expertise
_____________ test_current_primus_considered_in_selection_succeeds _____________

    @pytest.mark.fast
    def test_current_primus_considered_in_selection_succeeds():
        """Current primus remains candidate when reselection occurs.
    
        This prevents ``select_primus_by_expertise`` from skipping the
        existing primus simply because they've served once."""
    
        team = WSDETeam(name="PrimusReselectionTeam")
        py = _agent("py", ["python"])
        js = _agent("js", ["javascript"])
        team.add_agents([py, js])
    
        team.assign_roles()
        team.select_primus_by_expertise({"language": "python"})
    
>       assert team.get_primus() is py
E       AssertionError: assert <MagicMock id='5287773488'> is <MagicMock 
id='5287777472'>
E        +  where <MagicMock id='5287773488'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b2d1520>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:69: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,010 - devsynth.domain.models.wsde_core - INFO - Added agent 
py to team PrimusReselectionTeam
2025-10-29 10:48:40,010 - devsynth.domain.models.wsde_core - INFO - Added agent 
js to team PrimusReselectionTeam
2025-10-29 10:48:40,010 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team PrimusReselectionTeam: {'primus': 'py', 'worker': 'js', 
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-29 10:48:40,010 - devsynth.domain.models.wsde_roles - INFO - Selected js
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to
team PrimusReselectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent js to
team PrimusReselectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team PrimusReselectionTeam: {'primus': 'py', 'worker': 'js', 'supervisor': 
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected js as 
primus based on expertise
_____________ test_documentation_tasks_prefer_doc_experts_succeeds _____________

    @pytest.mark.fast
    def test_documentation_tasks_prefer_doc_experts_succeeds():
        """Test that documentation tasks prefer doc experts succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePrimusSelectionTeam")
        coder = _agent("coder", ["python"])
        doc = _agent("doc", ["documentation", "markdown"])
        team.add_agents([coder, doc])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc
E       AssertionError: assert <MagicMock id='5287499472'> is <MagicMock 
id='5287502064'>
E        +  where <MagicMock id='5287499472'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b28cec0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,017 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,017 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,017 - devsynth.domain.models.wsde_roles - INFO - Selected 
coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder 
as primus based on expertise
_______________ test_nested_task_metadata_is_flattened_succeeds ________________

    @pytest.mark.fast
    def test_nested_task_metadata_is_flattened_succeeds():
        """Test that nested task metadata is flattened succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdePrimusSelectionTeam")
        py = _agent("py", ["python"])
        doc = _agent("doc", ["documentation"])
        team.add_agents([doc, py])
        task = {"context": {"info": [{"language": "python"}]}}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is py
E       AssertionError: assert <MagicMock id='5287577216'> is <MagicMock 
id='5287580768'>
E        +  where <MagicMock id='5287577216'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b2a1370>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:98: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,023 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,023 - devsynth.domain.models.wsde_core - INFO - Added agent 
py to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,023 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent py to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
______________ test_select_primus_by_expertise_coverage_succeeds _______________

    @pytest.mark.fast
    def test_select_primus_by_expertise_coverage_succeeds():
        """Test that select primus by expertise coverage succeeds.
    
        ReqID: N/A"""
        import inspect
        import os
        from types import SimpleNamespace
    
        import coverage
    
        import devsynth.domain.models.wsde_facade as wsde
    
        team = wsde.WSDETeam(name="TestWsdePrimusSelectionTeam")
        cov = coverage.Coverage()
        cov.start()
        team.select_primus_by_expertise({})
        team.add_agent(
            SimpleNamespace(
                name="a1", expertise=["python"], current_role=None, 
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a2",
                config=SimpleNamespace(
                    parameters={"expertise": ["doc_generation", "markdown"]}
                ),
                current_role=None,
                has_been_primus=False,
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a3", expertise=["testing"], current_role=None, 
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a4", expertise=["security"], current_role=None, 
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a5",
                expertise=["javascript"],
                current_role=None,
                has_been_primus=False,
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a6", expertise=["api"], current_role=None, 
has_been_primus=False
            )
        )
        team.add_agent(
            SimpleNamespace(
                name="a7", expertise=["design"], current_role=None, 
has_been_primus=False
            )
        )
        team.select_primus_by_expertise(
            {"type": "documentation", "details": [1, 2], "extra": {"foo": 
"bar"}}
        )
        team.select_primus_by_expertise({"language": "python"})
        team.select_primus_by_expertise({"type": "testing"})
        team.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "security"})
        team.select_primus_by_expertise({"type": "frontend"})
        team.select_primus_by_expertise({"type": "backend"})
        team.select_primus_by_expertise({"type": "design"})
        cov.stop()
        path = inspect.getsourcefile(wsde.WSDETeam.select_primus_by_expertise)
        lines, start = 
inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)
        executable = []
        skip = False
        for i, line in enumerate(lines, start):
            stripped = line.strip()
            if stripped.startswith('"""'):
                if (
                    stripped.count('"""') == 2
                    and stripped.endswith('""')
                    and stripped != '"""'
                ):
                    continue
                skip = not skip
                continue
            if skip:
                if stripped.endswith('"""'):
                    skip = False
                continue
            if stripped:
                executable.append(i)
        executed = set(cov.get_data().lines(path))
        coverage_percent = len(set(executable) & executed) / len(executable) * 
100
>       assert coverage_percent >= 30
E       assert 25.0 >= 30

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_primus_selection.py:210: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_roles - WARNING - Cannot 
select primus: no agents in team
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_core - INFO - Added agent 
a1 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_core - INFO - Added agent 
a2 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_core - INFO - Added agent 
a3 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_core - INFO - Added agent 
a4 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,045 - devsynth.domain.models.wsde_core - INFO - Added agent 
a5 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_core - INFO - Added agent 
a6 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_core - INFO - Added agent 
a7 to team TestWsdePrimusSelectionTeam
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a2
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a3
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a4
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a5
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a6
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a7
as primus based on expertise
2025-10-29 10:48:40,046 - devsynth.domain.models.wsde_roles - INFO - Selected a1
as primus based on expertise
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select 
primus: no agents in team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a4 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a5 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a6 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a7 to
team TestWsdePrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a2 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a3 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a4 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a5 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a6 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a7 as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected a1 as 
primus based on expertise
________ test_vote_on_critical_decision_tie_triggers_consensus_succeeds ________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b28d580>, <MagicMock id='5287503360'>, <MagicMock id='5287499616'>, 
<MagicMock id='5286747248'>)

    @pytest.mark.fast
    def 
test_vote_on_critical_decision_tie_triggers_consensus_succeeds(team_with_agents)
:
        """Test that vote on critical decision tie triggers consensus succeeds.
    
        ReqID: N/A"""
        team, doc, coder, _ = team_with_agents
        doc.process.return_value = {"vote": "A"}
        coder.process.return_value = {"vote": "B"}
        task = {
            "type": "critical_decision",
            "is_critical": True,
            "options": [{"id": "A"}, {"id": "B"}],
        }
        with (
            patch.object(team, "get_primus", return_value=None),
            patch.object(team, "build_consensus", return_value={"consensus": 
"AB"}) as bc,
        ):
            result = team.vote_on_critical_decision(task)
>           assert result["voting_initiated"]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:56: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,061 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,061 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,061 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
___________ test_vote_on_critical_decision_weighted_voting_succeeds ____________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b2d06e0>, <MagicMock id='5287774448'>, <MagicMock id='5287780592'>, 
<MagicMock id='5287639664'>)

    @pytest.mark.fast
    def 
test_vote_on_critical_decision_weighted_voting_succeeds(team_with_agents):
        """Test that vote on critical decision weighted voting succeeds.
    
        ReqID: N/A"""
        team, doc, coder, tester = team_with_agents
        for agent, level in [(doc, "expert"), (coder, "novice"), (tester, 
"novice")]:
            cfg = MagicMock()
            cfg.name = agent.name
            cfg.parameters = {"expertise": agent.expertise, "expertise_level": 
level}
            agent.config = cfg
        doc.process.return_value = {"vote": "A"}
        coder.process.return_value = {"vote": "B"}
        tester.process.return_value = {"vote": "B"}
        task = {
            "type": "critical_decision",
            "domain": "documentation",
            "is_critical": True,
            "options": [{"id": "A"}, {"id": "B"}],
        }
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["winner"] == "A"
E       AssertionError: assert 'B' == 'A'
E         
E         - A
E         + B

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:83: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,070 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,070 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,070 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
______________ test_build_consensus_multiple_and_single_succeeds _______________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b303110>, <MagicMock id='5287981664'>, <MagicMock id='5288362640'>, 
<MagicMock id='5287509408'>)

    @pytest.mark.fast
    def test_build_consensus_multiple_and_single_succeeds(team_with_agents):
        """Test that build consensus multiple and single succeeds.
    
        ReqID: N/A"""
        team, doc, coder, _ = team_with_agents
        task = {"id": "t1", "description": "demo"}
        team.add_solution(task, {"agent": doc.name, "content": "First"})
        single = team.build_consensus(task)
>       assert single["method"] == "single_solution"
               ^^^^^^^^^^^^^^^^
E       KeyError: 'method'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:96: KeyError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,079 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,079 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,079 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,079 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task t1
2025-10-29 10:48:40,079 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task t1
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
__________ test_documentation_task_selects_unused_doc_agent_succeeds ___________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b36e0c0>, <MagicMock id='5288419952'>, <MagicMock id='5288423456'>, 
<MagicMock id='5288427200'>)

    @pytest.mark.fast
    def 
test_documentation_task_selects_unused_doc_agent_succeeds(team_with_agents):
        """Test that documentation task selects unused doc agent succeeds.
    
        ReqID: N/A"""
        team, doc, coder, tester = team_with_agents
        team.select_primus_by_expertise({"type": "coding", "language": 
"python"})
>       assert team.get_primus() is coder
E       AssertionError: assert <MagicMock id='5288419952'> is <MagicMock 
id='5288423456'>
E        +  where <MagicMock id='5288419952'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b36e0c0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:111: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,087 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,087 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,088 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,088 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
_______________ test_vote_on_critical_decision_coverage_succeeds _______________

    @pytest.mark.fast
    def test_vote_on_critical_decision_coverage_succeeds():
        """Test that vote on critical decision coverage succeeds.
    
        ReqID: N/A"""
        import inspect
        from types import SimpleNamespace
    
        import coverage
    
        import devsynth.domain.models.wsde_facade as wsde
    
        team = wsde.WSDETeam(name="TestWsdeTeamTeam")
        a1 = SimpleNamespace(
            name="a1",
            config=SimpleNamespace(
                name="a1", parameters={"expertise": ["python"], 
"expertise_level": "expert"}
            ),
            process=lambda t: {"vote": "A"},
        )
        a2 = SimpleNamespace(
            name="a2",
            config=SimpleNamespace(
                name="a2", parameters={"expertise": ["docs"], "expertise_level":
"novice"}
            ),
            process=lambda t: {"vote": "B"},
        )
        a3 = SimpleNamespace(
            name="a3",
            config=SimpleNamespace(
                name="a3", parameters={"expertise": ["python"], 
"expertise_level": "novice"}
            ),
            process=lambda t: {"vote": "A"},
        )
        team.add_agents([a1, a2, a3])
        cov = coverage.Coverage()
        cov.start()
        team.vote_on_critical_decision({"type": "other"})
        team.vote_on_critical_decision(
            {"type": "critical_decision", "is_critical": True, "options": []}
        )
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "domain": "python",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        a3.process = lambda t: {"vote": "B"}
        team.vote_on_critical_decision(
            {
                "type": "critical_decision",
                "is_critical": True,
                "options": [{"id": "A"}, {"id": "B"}],
            }
        )
        path = wsde.__file__
        lines, start = 
inspect.getsourcelines(wsde.WSDETeam.vote_on_critical_decision)
        executable = list(range(start, start + len(lines)))
>       executed = set(cov.get_data().lines(path))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:253: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,118 - devsynth.domain.models.wsde_core - INFO - Added agent 
a1 to team TestWsdeTeamTeam
2025-10-29 10:48:40,119 - devsynth.domain.models.wsde_core - INFO - Added agent 
a2 to team TestWsdeTeamTeam
2025-10-29 10:48:40,119 - devsynth.domain.models.wsde_core - INFO - Added agent 
a3 to team TestWsdeTeamTeam
2025-10-29 10:48:40,126 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
conduct vote: no options provided
2025-10-29 10:48:40,126 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
conduct vote: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestWsdeTeamTeam
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct 
vote: no options provided
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct 
vote: no options provided
_____________ test_expertise_selection_and_flag_rotation_succeeds ______________

    @pytest.mark.fast
    def test_expertise_selection_and_flag_rotation_succeeds():
        """Test that expertise selection and flag rotation succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeTeamTeam")
        doc = MagicMock()
        doc.name = "doc"
        doc.expertise = ["documentation"]
        coder = MagicMock()
        coder.name = "coder"
        coder.expertise = ["python"]
        tester = MagicMock()
        tester.name = "tester"
        tester.expertise = ["testing"]
        team.add_agents([doc, coder, tester])
        tasks = [
            {"type": "coding", "language": "python"},
            {"type": "documentation"},
            {"type": "testing"},
        ]
        expected = [coder, doc, tester]
        for task, agent in zip(tasks, expected):
            team.select_primus_by_expertise(task)
>           assert team.get_primus() is agent
E           AssertionError: assert <MagicMock id='5288742832'> is <MagicMock 
id='5288638944'>
E            +  where <MagicMock id='5288742832'> = get_primus()
E            +    where get_primus = 
<devsynth.domain.models.wsde_facade.WSDETeam object at 0x13b3ab560>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:301: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,148 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,148 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,148 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
2025-10-29 10:48:40,148 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
_____________________ test_select_primus_coverage_succeeds _____________________

team_with_agents = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b33e6f0>, <MagicMock id='5288227280'>, <MagicMock id='5288227040'>, 
<MagicMock id='5287504512'>)

    @pytest.mark.fast
    def test_select_primus_coverage_succeeds(team_with_agents):
        """Ensure select_primus_by_expertise maintains >80% coverage.
    
        ReqID: N/A"""
        import inspect
    
        import coverage
    
        import devsynth.domain.models.wsde_facade as wsde
    
        team, doc, coder, tester = team_with_agents
        cov = coverage.Coverage()
        cov.start()
        empty = wsde.WSDETeam(name="TestWsdeTeamTeam")
        empty.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "documentation"})
        team.select_primus_by_expertise({"type": "coding", "language": 
"python"})
        team.select_primus_by_expertise({"type": "testing"})
        team.select_primus_by_expertise({"type": "documentation"})
        path = wsde.__file__
        lines, start = 
inspect.getsourcelines(wsde.WSDETeam.select_primus_by_expertise)
        executable = list(range(start, start + len(lines)))
>       executed = set(cov.get_data().lines(path))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_team.py:334: TypeError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,156 - devsynth.domain.models.wsde_core - INFO - Added agent 
doc to team TestWsdeTeamTeam
2025-10-29 10:48:40,156 - devsynth.domain.models.wsde_core - INFO - Added agent 
coder to team TestWsdeTeamTeam
2025-10-29 10:48:40,156 - devsynth.domain.models.wsde_core - INFO - Added agent 
tester to team TestWsdeTeamTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent doc 
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent coder
to team TestWsdeTeamTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
tester to team TestWsdeTeamTeam
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,164 - devsynth.domain.models.wsde_roles - WARNING - Cannot 
select primus: no agents in team
2025-10-29 10:48:40,164 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
2025-10-29 10:48:40,165 - devsynth.domain.models.wsde_roles - INFO - Selected 
coder as primus based on expertise
2025-10-29 10:48:40,165 - devsynth.domain.models.wsde_roles - INFO - Selected 
tester as primus based on expertise
2025-10-29 10:48:40,165 - devsynth.domain.models.wsde_roles - INFO - Selected 
doc as primus based on expertise
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_roles:logging_setup.py:615 Cannot select 
primus: no agents in team
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected coder 
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected tester 
as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected doc as 
primus based on expertise
_________________________ test_majority_voting_simple __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3aa180>

    @pytest.mark.fast
    def test_majority_voting_simple(monkeypatch):
        agents = [DummyAgent("a1", ["a"]), DummyAgent("a2", ["b"]), 
DummyAgent("a3", ["a"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda x: x[0])
        task = {"options": ["option_a", "option_b"], "voting_method": 
"majority"}
        result = team.vote_on_critical_decision(task)
        assert result["status"] == "completed"
>       assert result["result"] == "option_a"
E       AssertionError: assert {'method': 'majority_vote', 'winner': 'option_a'}
== 'option_a'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:46: AssertionError
_____________________ test_handle_tied_vote_primus_breaks ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3ab800>

    @pytest.mark.fast
    def test_handle_tied_vote_primus_breaks(monkeypatch):
        agents = [DummyAgent("primus", ["x"]), DummyAgent("other", ["y"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        voting_result = {
            "votes": {"primus": "A", "other": "B"},
            "options": ["A", "B"],
            "status": "pending",
        }
        vote_counts = {"A": 1, "B": 1}
        res = team._handle_tied_vote(
            {"options": ["A", "B"]}, voting_result, vote_counts, ["A", "B"]
        )
>       assert res["result"] == "A"
E       assert {'consensus_result': {'explanation': "Partial consensus on option
'A' with 0.0% support after 3 rounds of discussion."...nces': {'other': {'A': 
0.0, 'B': 0.0}, 'primus': {'A': 0.0, 'B': 0.0}}, ...}, 'tied': True, 
'tied_options': ['A', 'B']} == 'A'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:62: AssertionError
__________________ test_weighted_voting_tie_primus_resolution __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3abbf0>

    @pytest.mark.fast
    def test_weighted_voting_tie_primus_resolution(monkeypatch):
        a1 = DummyAgent("p", ["frontend"])
        a2 = DummyAgent("s", ["frontend"])
        team = bind(DummyTeam([a1, a2], primus=a1))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {
            "options": ["frontend", "backend"],
            "voting_method": "weighted",
            "domain": "frontend",
        }
        # Force votes to create a tie
        voting_result = {
            "options": ["frontend", "backend"],
            "votes": {"p": "frontend", "s": "backend"},
            "method": "weighted",
            "status": "pending",
        }
        vote_counts = {"frontend": 1, "backend": 1}
        res = team._handle_tied_vote(
            task, voting_result, vote_counts, ["frontend", "backend"]
        )
>       assert res["result"] == "frontend"
E       assert {'consensus_result': {'explanation': "Consensus reached on option
'frontend' with 100.0% support after 1 rounds of dis...'frontend': 1.0}, 's': 
{'backend': 0.0, 'frontend': 1.0}}, ...}, 'tied': True, 'tied_options': 
['frontend', 'backend']} == 'frontend'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:88: AssertionError
___________________ test_vote_on_critical_decision_majority ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3ab710>

    @pytest.mark.fast
    def test_vote_on_critical_decision_majority(monkeypatch):
        agents = [
            DummyAgent("a1", ["opt1"]),
            DummyAgent("a2", ["opt1"]),
            DummyAgent("a3", ["opt2"]),
        ]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {"id": "t", "options": ["opt1", "opt2"], "voting_method": 
"majority"}
        result = team.vote_on_critical_decision(task)
>       assert result["result"] == "opt1"
E       AssertionError: assert {'method': 'majority_vote', 'winner': 'opt1'} == 
'opt1'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:102: AssertionError
___________________ test_vote_on_critical_decision_weighted ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2a69f0>

    @pytest.mark.fast
    def test_vote_on_critical_decision_weighted(monkeypatch):
        a1 = DummyAgent("a1", ["frontend"])
        a2 = DummyAgent("a2", ["backend"])
        team = bind(DummyTeam([a1, a2], primus=a1))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {
            "id": "t2",
            "options": ["frontend", "backend"],
            "voting_method": "weighted",
            "domain": "frontend",
        }
        result = team.vote_on_critical_decision(task)
        assert result["status"] == "completed"
>       assert result["result"] == "frontend"
E       AssertionError: assert {'method': 'weighted_vote', 'winner': 'frontend'}
== 'frontend'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:120: AssertionError
______________________ test_apply_majority_voting_no_tie _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3019a0>

    @pytest.mark.fast
    def test_apply_majority_voting_no_tie(monkeypatch):
        agents = [DummyAgent("a1"), DummyAgent("a2")]
        team = bind(DummyTeam(agents, primus=agents[0]))
        voting_result = {
            "options": ["X", "Y"],
            "votes": {"a1": "X", "a2": "X"},
            "vote_counts": {"X": 2, "Y": 0},
            "status": "pending",
        }
        res = team._apply_majority_voting({}, voting_result)
>       assert res["result"] == "X"
               ^^^^^^^^^^^^^
E       KeyError: 'result'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:134: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,216 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
conduct vote: no options provided
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct 
vote: no options provided
_____________________________ test_consensus_vote ______________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b303770>

    @pytest.mark.fast
    def test_consensus_vote(monkeypatch):
        agents = [DummyAgent("a1", ["front"]), DummyAgent("a2", ["front"])]
        team = bind(DummyTeam(agents, primus=agents[0]))
        monkeypatch.setattr(random, "choice", lambda opts: opts[0])
        task = {"options": ["front", "back"]}
        res = wsde_voting.consensus_vote(team, task)
>       assert res["decision"] in {"front", "back"}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: unhashable type: 'dict'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/domain/test_wsde
_voting_logic.py:145: TypeError
___________ TestAnthropicProvider.test_connection_error_raises_error ___________
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1393: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1375: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x119835dc0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'httpx'> does not have the attribute 'post'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,374 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_________________ TestAnthropicProvider.test_generate_succeeds _________________
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1393: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1375: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x119835af0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'httpx'> does not have the attribute 'post'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,424 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
__________ TestAnthropicProvider.test_generate_with_context_succeeds ___________
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1393: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1375: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x119835820>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'httpx'> does not have the attribute 'post'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,471 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
______________ TestAnthropicProvider.test_get_embedding_succeeds _______________
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1393: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1375: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x119835ca0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'httpx'> does not have the attribute 'post'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,519 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_____________ TestAnthropicProvider.test_model_error_raises_error ______________
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1393: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1375: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x119835ee0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'httpx'> does not have the attribute 'post'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:1437: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,567 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
___________________ test_health_endpoint_accepts_valid_token ___________________

    def _get_testclient():
        """Lazily import TestClient to avoid MRO issues during collection."""
        global TestClient
        if TestClient is None:
            try:
>               from fastapi.testclient import TestClient

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/fastapi/testclient.py:1: in <module>
    from starlette.testclient import TestClient as TestClient  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError

During handling of the above exception, another exception occurred:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b273560>

    @pytest.mark.fast
    def test_health_endpoint_accepts_valid_token(monkeypatch):
        """Health endpoint returns ok when authorized.
    
        ReqID: FR-74"""
        monkeypatch.setattr(api, "settings", 
types.SimpleNamespace(access_token="s3cr3t"))
>       client = _get_testclient()(api.app)
                 ^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
.py:22: in _get_testclient
    from starlette.testclient import TestClient
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError
________________________ test_health_endpoint_succeeds _________________________

api_token_env = None

    @pytest.mark.fast
    def test_health_endpoint_succeeds(api_token_env):
        """Test that health endpoint succeeds.
    
        ReqID: N/A"""
>       resp = _get_client().get("/health", headers={"Authorization": "Bearer 
test-token"})
               ^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _get_client():
        """Get the test client, initializing lazily."""
        global client
>       if client is None:
           ^^^^^^
E       NameError: name 'client' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:28: NameError
________________________ test_metrics_endpoint_succeeds ________________________

api_token_env = None

    @pytest.mark.fast
    def test_metrics_endpoint_succeeds(api_token_env):
        """Test that metrics endpoint succeeds.
    
        ReqID: N/A"""
>       resp = _get_client().get("/metrics", headers={"Authorization": "Bearer 
test-token"})
               ^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _get_client():
        """Get the test client, initializing lazily."""
        global client
>       if client is None:
           ^^^^^^
E       NameError: name 'client' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_api
_health.py:28: NameError
_________ test_skip_if_missing_backend_converts_find_spec_value_error __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b45b4d0>

    @pytest.mark.fast
    def 
test_skip_if_missing_backend_converts_find_spec_value_error(monkeypatch):
        """ValueError from ``find_spec`` should result in a clean skip 
marker."""
    
        from tests.fixtures import resources
    
        calls: list[tuple[str, str]] = []
    
        def raising_find_spec(_name: str):
            raise ValueError("bad spec")
    
        def fake_importorskip(name: str, *, reason: str) -> None:
            calls.append((name, reason))
            raise pytest.skip.Exception(reason)
    
        monkeypatch.setattr(resources.importlib.util, "find_spec", 
raising_find_spec)
        monkeypatch.setattr(resources, "is_resource_available", lambda _: True)
        monkeypatch.setattr(resources.pytest, "importorskip", fake_importorskip)
    
        marks = resources.skip_if_missing_backend("faiss", 
include_requires_resource=False)
    
        assert any(mark.name == "skip" for mark in marks)
>       assert calls and calls[0][0] == "faiss"
E       assert ([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_bac
kend_resource_flags.py:98: AssertionError
_________ TestChromaDBAdapter.test_store_and_retrieve_vector_succeeds __________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x119924ef0>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b44e2a0>

    @pytest.mark.fast
    def test_store_and_retrieve_vector_succeeds(self, adapter):
        """Test storing and retrieving a vector.
    
        ReqID: N/A"""
        vector = MemoryVector(
            id="test-vector-1",
            content="This is a test vector",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"test": "metadata", "category": "test"},
        )
        vector_id = adapter.store_vector(vector)
        assert vector_id == "test-vector-1"
        retrieved_vector = adapter.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector.id
>       assert retrieved_vector.content == vector.content
E       AssertionError: assert None == 'This is a test vector'
E        +  where None = MemoryVector(id='test-vector-1', content=None, 
embedding=<MagicMock 
name='mock.get_collection().get().__getitem__().__getitem__()' id='5285986128'>,
metadata={}, created_at=datetime.datetime(2025, 10, 29, 10, 48, 40, 
748476)).content
E        +  and   'This is a test vector' = MemoryVector(id='test-vector-1', 
content='This is a test vector', embedding=[0.1, 0.2, 0.3, 0.4, 0.5], 
metadata={'test': 'metadata', 'category': 'test'}, 
created_at=datetime.datetime(2025, 10, 29, 10, 48, 40, 747794)).content

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:70: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,747 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpapqd_oh4
2025-10-29 10:48:40,747 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpapqd_oh4
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,747 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-1 in ChromaDB
2025-10-29 10:48:40,748 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Retrieved vector with ID test-vector-1 from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Retrieved vector with ID test-vector-1 from ChromaDB
__________ TestChromaDBAdapter.test_store_vector_without_id_succeeds ___________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x1199253a0>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b4d5910>

    @pytest.mark.fast
    def test_store_vector_without_id_succeeds(self, adapter):
        """Test storing a vector without an ID.
    
        ReqID: N/A"""
        vector = MemoryVector(
            id="",
            content="Vector without ID",
            embedding=[0.5, 0.4, 0.3, 0.2, 0.1],
            metadata={"test": "no-id"},
        )
        vector_id = adapter.store_vector(vector)
        assert vector_id is not None
        assert vector_id != ""
        retrieved_vector = adapter.retrieve_vector(vector_id)
        assert retrieved_vector is not None
        assert retrieved_vector.id == vector_id
>       assert retrieved_vector.content == vector.content
E       AssertionError: assert None == 'Vector without ID'
E        +  where None = MemoryVector(id='666b19bb-3927-47c7-8a24-de565f328ee0',
content=None, embedding=<MagicMock 
name='mock.get_collection()...titem__().__getitem__()' id='5286744032'>, 
metadata={}, created_at=datetime.datetime(2025, 10, 29, 10, 48, 40, 
757648)).content
E        +  and   'Vector without ID' = 
MemoryVector(id='666b19bb-3927-47c7-8a24-de565f328ee0', content='Vector without 
ID', embedding=[0.5, 0.4, 0.3, 0.2, 0.1], metadata={'test': 'no-id'}, 
created_at=datetime.datetime(2025, 10, 29, 10, 48, 40, 756980)).content

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:92: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,756 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpv85ctnto
2025-10-29 10:48:40,756 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpv85ctnto
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,757 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID 666b19bb-3927-47c7-8a24-de565f328ee0 in ChromaDB
2025-10-29 10:48:40,757 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Retrieved vector with ID 666b19bb-3927-47c7-8a24-de565f328ee0 from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID 666b19bb-3927-47c7-8a24-de565f328ee0 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Retrieved vector with ID 666b19bb-3927-47c7-8a24-de565f328ee0 from ChromaDB
_____________ TestChromaDBAdapter.test_similarity_search_succeeds ______________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x119925850>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b2f6a80>

    @pytest.mark.fast
    def test_similarity_search_succeeds(self, adapter):
        """Test similarity search functionality.
    
        ReqID: N/A"""
        vectors = [
            MemoryVector(
                id=f"test-vector-{i}",
                content=f"Test vector {i}",
                embedding=[(float(j) / 10) for j in range(i, i + 5)],
                metadata={"index": i},
            )
            for i in range(1, 6)
        ]
        for vector in vectors:
            adapter.store_vector(vector)
        query_embedding = [0.15, 0.25, 0.35, 0.45, 0.55]
        results = adapter.similarity_search(query_embedding, top_k=3)
>       assert len(results) == 3
E       assert 0 == 3
E        +  where 0 = len([])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:113: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpz6choygy
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpz6choygy
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-1 in ChromaDB
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-2 in ChromaDB
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-3 in ChromaDB
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-4 in ChromaDB
2025-10-29 10:48:40,764 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-5 in ChromaDB
2025-10-29 10:48:40,765 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Found 0 similar vectors in ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-2 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-3 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-4 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-5 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Found 0
similar vectors in ChromaDB
_______________ TestChromaDBAdapter.test_delete_vector_succeeds ________________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x119925d00>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b45eab0>

    @pytest.mark.fast
    def test_delete_vector_succeeds(self, adapter):
        """Test deleting a vector.
    
        ReqID: N/A"""
        vector = MemoryVector(
            id="test-vector-delete",
            content="Vector to delete",
            embedding=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"test": "delete"},
        )
        adapter.store_vector(vector)
        result = adapter.delete_vector(vector.id)
        assert result is True
        retrieved_vector = adapter.retrieve_vector(vector.id)
>       assert retrieved_vector is None
E       AssertionError: assert MemoryVector(id='test-vector-delete', 
content=None, embedding=<MagicMock 
name='mock.get_collection().get().__getitem__().__getitem__()' id='5289633472'>,
metadata={}, created_at=datetime.datetime(2025, 10, 29, 10, 48, 40, 772179)) is 
None

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:132: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,771 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpeybkw4x2
2025-10-29 10:48:40,771 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpeybkw4x2
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,771 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID test-vector-delete in ChromaDB
2025-10-29 10:48:40,771 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Deleted vector with ID test-vector-delete from ChromaDB
2025-10-29 10:48:40,772 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Retrieved vector with ID test-vector-delete from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID test-vector-delete in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted
vector with ID test-vector-delete from ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Retrieved vector with ID test-vector-delete from ChromaDB
_________ TestChromaDBAdapter.test_delete_nonexistent_vector_succeeds __________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x1199261b0>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b452660>

    @pytest.mark.fast
    def test_delete_nonexistent_vector_succeeds(self, adapter):
        """Test deleting a vector that doesn't exist.
    
        ReqID: N/A"""
        result = adapter.delete_vector("nonexistent-vector")
>       assert result is False
E       assert True is False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:140: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,778 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpx0b7wf2k
2025-10-29 10:48:40,778 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmpx0b7wf2k
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,779 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Deleted vector with ID nonexistent-vector from ChromaDB
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Deleted
vector with ID nonexistent-vector from ChromaDB
____________ TestChromaDBAdapter.test_get_collection_stats_succeeds ____________

self = <tests.unit.general.test_chroma_db_adapter.TestChromaDBAdapter object at 
0x119926660>
adapter = <devsynth.adapters.memory.chroma_db_adapter.ChromaDBAdapter object at 
0x13b441fd0>

    @pytest.mark.fast
    def test_get_collection_stats_succeeds(self, adapter):
        """Test getting collection statistics.
    
        ReqID: N/A"""
        vectors = [
            MemoryVector(
                id=f"stats-vector-{i}",
                content=f"Stats vector {i}",
                embedding=[(float(j) / 10) for j in range(i, i + 5)],
                metadata={"index": i},
            )
            for i in range(1, 4)
        ]
        for vector in vectors:
            adapter.store_vector(vector)
        stats = adapter.get_collection_stats()
        assert stats["collection_name"] == "test_vectors"
>       assert stats["vector_count"] >= 3
E       assert 0 >= 3

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_chr
oma_db_adapter.py:160: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:40,784 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5qtaryly
2025-10-29 10:48:40,785 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Using existing ChromaDB collection: test_vectors
------------------------------ Captured log setup ------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Initialized ChromaDB client with persist directory: 
/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5qtaryly
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Using 
existing ChromaDB collection: test_vectors
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:40,786 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID stats-vector-1 in ChromaDB
2025-10-29 10:48:40,786 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID stats-vector-2 in ChromaDB
2025-10-29 10:48:40,786 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Stored vector with ID stats-vector-3 in ChromaDB
2025-10-29 10:48:40,786 - devsynth.adapters.memory.chroma_db_adapter - INFO - 
Retrieved collection statistics: {'collection_name': 'test_vectors', 
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory': 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5qtaryly'}
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID stats-vector-1 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID stats-vector-2 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 Stored 
vector with ID stats-vector-3 in ChromaDB
INFO     devsynth.adapters.memory.chroma_db_adapter:logging_setup.py:615 
Retrieved collection statistics: {'collection_name': 'test_vectors', 
'vector_count': 0, 'embedding_dimensions': 0, 'persist_directory': 
'/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/tmp5qtaryly'}
_____________ TestDialecticalReasoner.test_assess_impact_succeeds ______________

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_assess_impact_succeeds>

        def test_assess_impact_succeeds(self):
            """Test assessing the impact of a change.
    
            ReqID: N/A"""
            self.llm_service.query.side_effect = [
                "This is an impact analysis",
                """- Recommendation 1
    - Recommendation 2
    - Recommendation 3""",
            ]
            dependent_requirement = Requirement(
                id=uuid4(),
                title="Dependent Requirement",
                description="This requirement depends on the test requirement",
                status=RequirementStatus.DRAFT,
                priority=RequirementPriority.MEDIUM,
                type=RequirementType.FUNCTIONAL,
                created_by="test_user",
                dependencies=[self.requirement.id],
            )
            self.requirement_repository.save_requirement(dependent_requirement)
            impact = self.reasoner.assess_impact(self.change)
            self.assertEqual(impact.change_id, self.change.id)
            self.assertEqual(impact.analysis, "This is an impact analysis")
            self.assertEqual(len(impact.recommendations), 3)
            self.assertEqual(impact.recommendations[0], "Recommendation 1")
            self.assertEqual(impact.recommendations[1], "Recommendation 2")
            self.assertEqual(impact.recommendations[2], "Recommendation 3")
            self.assertEqual(len(impact.affected_requirements), 2)
            self.assertIn(self.requirement.id, impact.affected_requirements)
            self.assertIn(dependent_requirement.id, 
impact.affected_requirements)
            saved_impact = 
self.impact_repository.get_impact_assessment(impact.id)
            self.assertIsNotNone(saved_impact)
            self.assertEqual(saved_impact.id, impact.id)
            impact_by_change = 
self.impact_repository.get_impact_assessment_for_change(
                self.change.id
            )
            self.assertIsNotNone(impact_by_change)
            self.assertEqual(impact_by_change.id, impact.id)
>           
self.notification_service.notify_impact_assessment_completed.assert_called_once_
with(
                impact
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.notify_impact_assessment_completed' 
id='5289898256'>
args = (ImpactAssessment(id=UUID('62a8f28a-e682-48c7-9c50-8fcb35eb5996'), 
change_id=UUID('e792585d-1266-4189-9410-e87270992f2...ation 2', 'Recommendation 
3'], created_at=datetime.datetime(2025, 10, 29, 10, 48, 41, 88676), 
created_by='test_user'),)
kwargs = {}
expected = 
call(ImpactAssessment(id=UUID('62a8f28a-e682-48c7-9c50-8fcb35eb5996'), 
change_id=UUID('e792585d-1266-4189-9410-e872709...dation 2', 'Recommendation 
3'], created_at=datetime.datetime(2025, 10, 29, 10, 48, 41, 88676), 
created_by='test_user'))
actual = 
call(ImpactNotificationPayload(assessment=ImpactAssessment(id=UUID('62a8f28a-e68
2-48c7-9c50-8fcb35eb5996'), change_id=...test_user'), 
edrr_phase=<EDRRPhase.REFINE: 'REFINE'>, triggered_at=datetime.datetime(2025, 
10, 29, 10, 48, 41, 88736)))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b4d3100>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: 
notify_impact_assessment_completed(ImpactAssessment(id=UUID('62a8f28a-e682-48c7-
9c50-8fcb35eb5996'), change_id=UUID('e792585d-1266-4189-9410-e87270992f2f'), 
affected_requirements=[UUID('5d04b241-1752-49f9-b12d-d96f45124ac0'), 
UUID('e7971055-ab4c-4d23-976b-8072ddd44201')], affected_components=[], 
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'], 
created_at=datetime.datetime(2025, 10, 29, 10, 48, 41, 88676), 
created_by='test_user'))
E             Actual: 
notify_impact_assessment_completed(ImpactNotificationPayload(assessment=ImpactAs
sessment(id=UUID('62a8f28a-e682-48c7-9c50-8fcb35eb5996'), 
change_id=UUID('e792585d-1266-4189-9410-e87270992f2f'), 
affected_requirements=[UUID('5d04b241-1752-49f9-b12d-d96f45124ac0'), 
UUID('e7971055-ab4c-4d23-976b-8072ddd44201')], affected_components=[], 
risk_level='low', estimated_effort='low', analysis='This is an impact analysis',
recommendations=['Recommendation 1', 'Recommendation 2', 'Recommendation 3'], 
created_at=datetime.datetime(2025, 10, 29, 10, 48, 41, 88676), 
created_by='test_user'), edrr_phase=<EDRRPhase.REFINE: 'REFINE'>, 
triggered_at=datetime.datetime(2025, 10, 29, 10, 48, 41, 88736)))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
________ TestDialecticalReasoner.test_evaluate_change_consensus_failure ________

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_evaluate_change_consensus_failure>

        def test_evaluate_change_consensus_failure(self):
            """Ensure consensus failure triggers logging and memory storage."""
            self.llm_service.query.side_effect = [
                "This is a thesis",
                "This is an antithesis",
                """Argument 1:
    Position: Thesis
    Content: Argument for thesis
    
    Argument 2:
    Position: Antithesis
    Content: Argument for antithesis""",
                "This is a synthesis",
                """Conclusion: This is a conclusion
    
    Recommendation: This is a recommendation""",
                "no",
            ]
            with patch(
                "devsynth.application.requirements.dialectical_reasoner.logger"
            ) as mock_logger:
                with self.assertRaises(ConsensusError):
                    self.reasoner.evaluate_change(self.change)
                mock_logger.error.assert_any_call(
                    "Consensus not reached for change",
                    extra={"change_id": str(self.change.id), "event": 
"consensus_failed"},
                )
>           self.memory_manager.store_with_edrr_phase.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.store_with_edrr_phase' id='5289351776'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id': 
'a9d9b3e9-1a7b-49d8-8919-2dcc989b50e1', 'reasoning_id': 
'0f58c394-ad87-4e2c-a774-3ed7c429432c'}, memory_type=<MemoryType.RELATIONSHIP: 
'relationship'>, edrr_phase='RETROSPECT', metadata={'change_id': 
'a9d9b3e9-1a7b-49d8-8919-2dcc989b50e1', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('0f58c394-ad87-4e2c-a774-3ed7c429432c'), 
'change_id': UUID('a9d9b3e9-1a7b-49d8-8919-2dcc989b50e1'), 'thesis': 'This is a 
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a 
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for 
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion', 
'recommendation': 'This is a recommendation', 'created_at': 
datetime.datetime(2025, 10, 29, 10, 48, 41, 125939), 'updated_at': 
datetime.datetime(2025, 10, 29, 10, 48, 41, 125940), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>, 
edrr_phase='RETROSPECT', metadata={'change_id': 
'a9d9b3e9-1a7b-49d8-8919-2dcc989b50e1'})].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
____________ TestDialecticalReasoner.test_evaluate_change_succeeds _____________

self = <tests.unit.general.test_dialectical_reasoner.TestDialecticalReasoner 
testMethod=test_evaluate_change_succeeds>

        def test_evaluate_change_succeeds(self):
            """Test evaluating a change using dialectical reasoning.
    
            ReqID: N/A"""
            self.llm_service.query.side_effect = [
                "This is a thesis",
                "This is an antithesis",
                """Argument 1:
    Position: Thesis
    Content: Argument for thesis
    
    Argument 2:
    Position: Antithesis
    Content: Argument for antithesis""",
                "This is a synthesis",
                """Conclusion: This is a conclusion
    
    Recommendation: This is a recommendation""",
                "yes",
            ]
            with patch(
                "devsynth.application.requirements.dialectical_reasoner.logger"
            ) as mock_logger:
                reasoning = self.reasoner.evaluate_change(self.change)
                mock_logger.info.assert_any_call(
                    "Consensus reached for change",
                    extra={"change_id": str(self.change.id), "event": 
"consensus_reached"},
                )
>           self.memory_manager.store_with_edrr_phase.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dia
lectical_reasoner.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.store_with_edrr_phase' id='5289512976'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'store_with_edrr_phase' to have been called
once. Called 2 times.
E           Calls: [call({'type': 'requirement_to_reasoning', 'change_id': 
'177654b0-6cc7-40b6-aed7-9cac942b34a7', 'reasoning_id': 
'07097a2d-c9ef-4662-a7b4-a8707e6a86fd'}, memory_type=<MemoryType.RELATIONSHIP: 
'relationship'>, edrr_phase='REFINE', metadata={'change_id': 
'177654b0-6cc7-40b6-aed7-9cac942b34a7', 'link': 'requirement->reasoning'}),
E            call({'id': UUID('07097a2d-c9ef-4662-a7b4-a8707e6a86fd'), 
'change_id': UUID('177654b0-6cc7-40b6-aed7-9cac942b34a7'), 'thesis': 'This is a 
thesis', 'antithesis': 'This is an antithesis', 'synthesis': 'This is a 
synthesis', 'arguments': [{'position': 'Thesis', 'content': 'Argument for 
thesis', 'counterargument': ''}, {'position': 'Antithesis', 'content': 'Argument
for antithesis', 'counterargument': ''}], 'conclusion': 'This is a conclusion', 
'recommendation': 'This is a recommendation', 'created_at': 
datetime.datetime(2025, 10, 29, 10, 48, 41, 149029), 'updated_at': 
datetime.datetime(2025, 10, 29, 10, 48, 41, 149030), 'created_by': 'test_user'},
memory_type=<MemoryType.DIALECTICAL_REASONING: 'dialectical_reasoning'>, 
edrr_phase='REFINE', metadata={'change_id': 
'177654b0-6cc7-40b6-aed7-9cac942b34a7'})].

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
__________________________ test_dpg_command_disabled ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b4739b0>

    def test_dpg_command_disabled(monkeypatch):
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.dpg_cmd.get_settings",
            lambda reload=True: types.SimpleNamespace(gui_enabled=False),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dpg
_flag.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________________ test_dpg_command_missing_dependency ______________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b432ea0>

    def test_dpg_command_missing_dependency(monkeypatch):
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.dpg_cmd.get_settings",
            lambda reload=True: types.SimpleNamespace(gui_enabled=True),
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_dpg
_flag.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________ TestLoadManifest.test_load_manifest_success_is_valid _____________

self = <tests.unit.general.test_ingest_cmd.TestLoadManifest object at 
0x119a05460>
mock_yaml_load = <MagicMock name='safe_load' id='5290040528'>
mock_open = <MagicMock name='open' id='5290041488'>

    @patch("builtins.open")
    @patch("yaml.safe_load")
    @pytest.mark.fast
    def test_load_manifest_success_is_valid(self, mock_yaml_load, mock_open):
        """Test load_manifest with a valid manifest.
    
        ReqID: N/A"""
        mock_yaml_load.return_value = {"projectName": "TestProject", "version": 
"0.1.0"}
        result = load_manifest(Path("manifest.yaml"))
        assert result == {"projectName": "TestProject", "version": "0.1.0"}
>       mock_open.assert_called_once_with(Path("manifest.yaml"), "r")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ing
est_cmd.py:415: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='open' id='5290041488'>
args = (PosixPath('manifest.yaml'), 'r'), kwargs = {}
expected = call(PosixPath('manifest.yaml'), 'r')
actual = call(PosixPath('manifest.yaml'))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b4d3d80>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: open(PosixPath('manifest.yaml'), 'r')
E             Actual: open(PosixPath('manifest.yaml'))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
________________ TestPhases.test_retrospect_phase_has_expected _________________

self = <tests.unit.general.test_ingest_cmd.TestPhases object at 0x119a06f30>
mock_bridge = <MagicMock id='5294617424'>
mock_memory_manager = <MagicMock name='MemoryManager()' id='5289498368'>

    @pytest.mark.fast
    def test_retrospect_phase_has_expected(self, mock_bridge, 
mock_memory_manager):
        """Test retrospect_phase function.
    
        ReqID: N/A"""
        mock_memory_manager.reset_mock()
>       result = retrospect_phase(
            {"projectName": "TestProject"},
            {"relationships_created": 75},
            verbose=True,
            bridge=mock_bridge,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ing
est_cmd.py:504: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

manifest = {'projectName': 'TestProject'}
refine_results = {'relationships_created': 75}, verbose = True

    def retrospect_phase(
        manifest: ManifestModel,
        refine_results: RefinePhaseResult,
        verbose: bool = False,
        *,
        bridge: UXBridge | None = None,
        memory_manager: MemoryManager | None = None,
        code_analyzer: CodeAnalyzer | None = None,
        wsde_team: WSDETeam | None = None,
    ) -> RetrospectPhaseResult:
        """Summarize results and suggest improvements."""
    
        start = time.perf_counter()
    
        bridge = bridge or DEFAULT_BRIDGE
        memory_manager = memory_manager or MemoryManager(
            adapters={"tinydb": TinyDBMemoryAdapter()}
        )
        analyzer = code_analyzer or CodeAnalyzer()
        wsde_team = wsde_team or WSDETeam(name="IngestCmdTeam")
    
        if verbose:
            bridge.print("  Generating retrospective report...")
    
        improvements = refine_results.get("relationships_created", 0)
        gaps = refine_results.get("outdated_items_archived", 0)
    
        learnings = cast(JSONValue, wsde_team.extract_learnings(refine_results, 
True))
        patterns = cast(
            JSONValue,
            wsde_team.recognize_patterns(
                learnings,
                historical_context=memory_manager.retrieve_historical_patterns()
,
                code_analyzer=analyzer,
            ),
        )
        integrated = cast(
            JSONValue,
>           wsde_team.integrate_knowledge(learnings, patterns, memory_manager),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       TypeError: _integrate_knowledge() takes 3 positional arguments but 4 
were given

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/ingest_cmd.py:655: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:41,749 - 
devsynth.application.memory.adapters.tinydb_memory_adapter - INFO - TinyDB 
Memory Adapter initialized
------------------------------ Captured log call -------------------------------
INFO     
devsynth.application.memory.adapters.tinydb_memory_adapter:logging_setup.py:615 
TinyDB Memory Adapter initialized
_____________ test_offline_mode_selects_offline_provider_succeeds ______________

obj = <module 'devsynth.application.llm' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.application.llm' has no attribute 
'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2a5d90>

    @pytest.mark.fast
    def test_offline_mode_selects_offline_provider_succeeds(monkeypatch):
        """Test that offline mode selects offline provider succeeds.
    
        ReqID: N/A"""
        monkeypatch.setattr(
            "devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE", False
        )
        monkeypatch.setattr(
            "devsynth.application.llm.load_config", lambda: _mock_config(True)
        )
>       monkeypatch.setattr(
            "devsynth.application.llm.get_llm_settings",
            lambda: {
                "provider": "openai",
                "openai_api_key": "key",
                "openai_model": "gpt-3.5-turbo",
            },
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_llm
_provider_selection.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.llm' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.llm has no 
attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
______________ test_online_mode_uses_configured_provider_succeeds ______________

obj = <module 'devsynth.application.llm' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
                  ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'devsynth.application.llm' has no attribute 
'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b474ce0>

    @pytest.mark.fast
    def test_online_mode_uses_configured_provider_succeeds(monkeypatch):
        """Test that online mode uses configured provider succeeds.
    
        ReqID: N/A"""
        monkeypatch.setattr(
            "devsynth.application.utils.token_tracker.TIKTOKEN_AVAILABLE", False
        )
        monkeypatch.setattr(
            "devsynth.application.llm.load_config", lambda: _mock_config(False)
        )
>       monkeypatch.setattr(
            "devsynth.application.llm.get_llm_settings",
            lambda: {
                "provider": "openai",
                "openai_api_key": "key",
                "openai_model": "gpt-3.5-turbo",
            },
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_llm
_provider_selection.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.llm' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/l
lm/__init__.py'>
name = 'get_llm_settings', ann = 'devsynth.application.llm'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.llm has no 
attribute 'get_llm_settings'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____ TestLMStudioIntegrationRegression.test_lmstudio_settings_extraction ______

self = 
<tests.unit.general.test_lmstudio_integration_regression.TestLMStudioIntegration
Regression object at 0x119a5c830>

    def test_lmstudio_settings_extraction(self):
        """Test that LLM settings can be extracted for LM Studio.
    
        ReqID: LMSTUDIO-REG-3
        """
>       from devsynth.config import get_llm_settings
E       ImportError: cannot import name 'get_llm_settings' from 
'devsynth.config' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_lms
tudio_integration_regression.py:91: ImportError
_ TestLMStudioIntegrationRegression.test_lmstudio_provider_mock_initialization _

self = 
<tests.unit.general.test_lmstudio_integration_regression.TestLMStudioIntegration
Regression object at 0x119a5d100>
mock_lmstudio = <MagicMock name='lmstudio' id='5294474352'>

    @patch("devsynth.application.llm.lmstudio_provider.lmstudio")
    def test_lmstudio_provider_mock_initialization(self, mock_lmstudio):
        """Test LM Studio provider with mocked LM Studio service.
    
        ReqID: LMSTUDIO-REG-5
        """
>       from ....application.llm.lmstudio_provider import LMStudioProvider
E       ImportError: attempted relative import beyond top-level package

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_lms
tudio_integration_regression.py:149: ImportError
_ 
TestMultiAgentAdapterWorkflow.test_multi_agent_consensus_and_primus_selection_su
cceeds _

self = 
<tests.unit.general.test_multi_agent_adapter_workflow.TestMultiAgentAdapterWorkf
low object at 0x119aab410>

    @pytest.mark.fast
    def test_multi_agent_consensus_and_primus_selection_succeeds(self):
        """Test that multi agent consensus and primus selection succeeds.
    
        ReqID: N/A"""
        task = {"type": "coding", "language": "python"}
        with (
            patch.object(
                self.team,
                "build_consensus",
                return_value={
                    "consensus": "done",
                    "contributors": ["PythonAgent", "JSAgent", "DocAgent"],
                    "method": "consensus_synthesis",
                    "reasoning": "",
                },
            ) as mock_consensus,
            patch.object(
                self.team,
                "select_primus_by_expertise",
                wraps=self.team.select_primus_by_expertise,
            ) as mock_select,
        ):
            result = self.adapter.process_task(task)
>           mock_consensus.assert_called_once_with(task)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mul
ti_agent_adapter_workflow.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='build_consensus' id='5294689168'>
args = ({'id': 'f38bf71e-6e3a-4af9-b63f-4454b656787c', 'language': 'python', 
'solutions': [{'agent': 'PythonAgent', 'confiden...s', 'reasoning': ''}, 
{'agent': 'DocAgent', 'confidence': 1.0, 'content': 'doc', 'reasoning': ''}], 
'type': 'coding'},)
kwargs = {}
expected = call({'type': 'coding', 'language': 'python', 'solutions': [{'agent':
'PythonAgent', 'content': 'py', 'confidence': 1....nt': 'DocAgent', 'content': 
'doc', 'confidence': 1.0, 'reasoning': ''}], 'id': 
'f38bf71e-6e3a-4af9-b63f-4454b656787c'})
actual = call({'type': 'coding', 'language': 'python', 'solutions': [{'agent': 
'PythonAgent', 'content': 'py', 'confidence': 1....', 'confidence': 1.0, 
'reasoning': ''}], 'id': 'f38bf71e-6e3a-4af9-b63f-4454b656787c', 'options': 
['py', 'js', 'doc']})
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b7240e0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: build_consensus({'type': 'coding', 'language': 'python', 
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0, 
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0, 
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0, 
'reasoning': ''}], 'id': 'f38bf71e-6e3a-4af9-b63f-4454b656787c'})
E             Actual: build_consensus({'type': 'coding', 'language': 'python', 
'solutions': [{'agent': 'PythonAgent', 'content': 'py', 'confidence': 1.0, 
'reasoning': ''}, {'agent': 'JSAgent', 'content': 'js', 'confidence': 1.0, 
'reasoning': ''}, {'agent': 'DocAgent', 'content': 'doc', 'confidence': 1.0, 
'reasoning': ''}], 'id': 'f38bf71e-6e3a-4af9-b63f-4454b656787c', 'options': 
['py', 'js', 'doc']})

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:42,274 - devsynth.domain.models.wsde_core - INFO - Added agent 
PythonAgent to team team1
2025-10-29 10:48:42,274 - devsynth.domain.models.wsde_core - INFO - Added agent 
JSAgent to team team1
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_core - INFO - Added agent 
DocAgent to team team1
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_roles - INFO - Role 
assignments for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 
'supervisor': 'DocAgent', 'designer': None, 'evaluator': None}
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
PythonAgent to team team1
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
JSAgent to team team1
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 'supervisor': 
None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
DocAgent to team team1
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Role assignments
for team team1: {'primus': 'PythonAgent', 'worker': 'JSAgent', 'supervisor': 
'DocAgent', 'designer': None, 'evaluator': None}
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_roles - INFO - Selected 
JSAgent as primus based on expertise
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_enhanced_dialectical - 
INFO - Applying enhanced multi-solution dialectical reasoning to task: 
f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 1 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 2 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_solution_analysis - INFO -
Analyzing solution 3 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
2025-10-29 10:48:42,275 - devsynth.domain.models.wsde_solution_analysis - INFO -
Generating comparative analysis for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected JSAgent
as primus based on expertise
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_enhanced_dialectical:logging_setup.py:615 
Applying enhanced multi-solution dialectical reasoning to task: 
f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 
Analyzing solution 1 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 
Analyzing solution 2 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 
Analyzing solution 3 for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
INFO     devsynth.domain.models.wsde_solution_analysis:logging_setup.py:615 
Generating comparative analysis for task: f38bf71e-6e3a-4af9-b63f-4454b656787c
__________________________ test_mvu_lint_cli_success ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b96bc80>

    @pytest.mark.fast
    def test_mvu_lint_cli_success(monkeypatch):
        """CLI should report success when no errors are returned."""
        runner = CliRunner()
        app = build_app()
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.mvu_lint_cmd.lint_range",
            lambda _rev: [],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mvu
_lint_cli.py:36: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
__________________________ test_mvu_lint_cli_failure ___________________________

name = 'commands'

    def __getattr__(name: str) -> object:
        """Lazily expose CLI command callables when requested."""
    
        if (
            name
            in {
                "config_app",
                "inspect_code_cmd",
                "ingest_cmd",
            }
            or name in COMMAND_ATTRIBUTE_NAMES
        ):
            _register_commands()
            if name in globals() and globals()[name] is not None:
                return globals()[name]
            raise AttributeError(f"CLI command '{name}' is unavailable")
>       raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'devsynth.application.cli' has no attribute 
'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/cl
i/__init__.py:101: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b98e690>

    @pytest.mark.fast
    def test_mvu_lint_cli_failure(monkeypatch):
        """CLI should exit with error when linter reports problems."""
        runner = CliRunner()
        app = build_app()
>       monkeypatch.setattr(
            "devsynth.application.cli.commands.mvu_lint_cmd.lint_range",
            lambda _rev: ["abc123: error"],
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_mvu
_lint_cli.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:102: in derive_importpath
    target = resolve(module)
             ^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:84: in resolve
    found = annotated_getattr(found, part, used)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <module 'devsynth.application.cli' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/c
li/__init__.py'>
name = 'commands', ann = 'devsynth.application.cli.commands'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute 
{name!r}"
            ) from e
E           AttributeError: 'module' object at devsynth.application.cli.commands
has no attribute 'commands'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/_pytest/monkeypatch.py:92: AttributeError
_____________ test_ensure_path_exists_within_project_dir_succeeds ______________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_ensure_path_exists_within0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a6f60>

    @pytest.mark.fast
    def test_ensure_path_exists_within_project_dir_succeeds(tmp_path, 
monkeypatch):
        """Test that ensure path exists within project dir succeeds.
    
        ReqID: N/A"""
        project_dir = tmp_path / "project"
        outside_dir = tmp_path / "outside"
        project_dir.mkdir(exist_ok=True)
        outside_dir.mkdir(exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.setenv("DEVSYNTH_NO_FILE_LOGGING", "1")
>       settings = importlib.reload(settings_module)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pat
h_restrictions.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = <module 'devsynth.config.settings' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/config/settin
gs.py'>

    def reload(module):
        """Reload the module and return it.
    
        The module must have been successfully imported before.
    
        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None
    
        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.config.settings not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
________ test_documentation_tasks_prefer_documentation_experts_succeeds ________

    def test_documentation_tasks_prefer_documentation_experts_succeeds():
        """Test that documentation tasks prefer documentation experts succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestPrimusSelectionTeam")
        coder = create_agent("Coder", ["python"])
        doc_agent = create_agent("Doc", ["documentation", "markdown"])
        team.add_agents([coder, doc_agent])
        task = {"type": "documentation", "description": "Update docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc_agent
E       AssertionError: assert <MagicMock id='5294834992'> is <MagicMock 
id='5292746480'>
E        +  where <MagicMock id='5294834992'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b98fce0>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:91: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:43,701 - devsynth.domain.models.wsde_core - INFO - Added agent 
Coder to team TestPrimusSelectionTeam
2025-10-29 10:48:43,701 - devsynth.domain.models.wsde_core - INFO - Added agent 
Doc to team TestPrimusSelectionTeam
2025-10-29 10:48:43,701 - devsynth.domain.models.wsde_roles - INFO - Selected 
Coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc 
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder 
as primus based on expertise
_____________ test_weighted_expertise_prefers_specialist_succeeds ______________

weighted_team = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b7ccc50>, <MagicMock id='5292999888'>, <MagicMock id='5293005648'>)

    def test_weighted_expertise_prefers_specialist_succeeds(weighted_team):
        """Test that weighted expertise prefers specialist succeeds.
    
        ReqID: N/A"""
        team, generalist, specialist = weighted_team
        task = {"type": "coding", "language": "python", "domain": "backend"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is specialist
E       AssertionError: assert <MagicMock id='5292999888'> is <MagicMock 
id='5293005648'>
E        +  where <MagicMock id='5292999888'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b7ccc50>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:102: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:43,707 - devsynth.domain.models.wsde_core - INFO - Added agent 
Generalist to team TestPrimusSelectionTeam
2025-10-29 10:48:43,707 - devsynth.domain.models.wsde_core - INFO - Added agent 
Specialist to team TestPrimusSelectionTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Generalist to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Specialist to team TestPrimusSelectionTeam
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:43,708 - devsynth.domain.models.wsde_roles - INFO - Selected 
Generalist as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
Generalist as primus based on expertise
_________ test_documentation_tasks_prioritize_best_doc_expert_succeeds _________

documentation_team = (<devsynth.domain.models.wsde_facade.WSDETeam object at 
0x13b738e30>, <MagicMock id='5292393632'>, <MagicMock id='5292394400'>, 
<MagicMock id='5292688448'>)

    def 
test_documentation_tasks_prioritize_best_doc_expert_succeeds(documentation_team)
:
        """Test that documentation tasks prioritize best doc expert succeeds.
    
        ReqID: N/A"""
        team, coder, writer, doc = documentation_team
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
        primus = team.get_primus()
>       assert primus in (writer, doc)
E       AssertionError: assert <MagicMock id='5292393632'> in (<MagicMock 
id='5292394400'>, <MagicMock id='5292688448'>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pri
mus_selection.py:128: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:43,719 - devsynth.domain.models.wsde_core - INFO - Added agent 
Coder to team TestPrimusSelectionTeam
2025-10-29 10:48:43,719 - devsynth.domain.models.wsde_core - INFO - Added agent 
Writer to team TestPrimusSelectionTeam
2025-10-29 10:48:43,719 - devsynth.domain.models.wsde_core - INFO - Added agent 
Doc to team TestPrimusSelectionTeam
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Writer to team TestPrimusSelectionTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc 
to team TestPrimusSelectionTeam
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:43,720 - devsynth.domain.models.wsde_roles - INFO - Selected 
Coder as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Coder 
as primus based on expertise
________ TestProjectYamlLoading.test_load_project_yaml_success_succeeds ________

self = <tests.unit.general.test_project_yaml.TestProjectYamlLoading object at 
0x119adf050>
mock_yaml_load = <MagicMock name='safe_load' id='5292574240'>
mock_open = <MagicMock name='open' id='5292579376'>

    @pytest.mark.fast
    @patch("builtins.open")
    @patch("yaml.safe_load")
    def test_load_project_yaml_success_succeeds(self, mock_yaml_load, 
mock_open):
        """Test loading the project configuration file from 
.devsynth/project.yaml.
    
        ReqID: N/A"""
        mock_yaml_load.return_value = {"projectName": "TestProject", "version": 
"0.1.0"}
        result = load_manifest(Path(".devsynth/project.yaml"))
        assert result == {"projectName": "TestProject", "version": "0.1.0"}
>       mock_open.assert_called_once_with(Path(".devsynth/project.yaml"), "r")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pro
ject_yaml.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='open' id='5292579376'>
args = (PosixPath('.devsynth/project.yaml'), 'r'), kwargs = {}
expected = call(PosixPath('.devsynth/project.yaml'), 'r')
actual = call(PosixPath('.devsynth/project.yaml'))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b77d440>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: open(PosixPath('.devsynth/project.yaml'), 'r')
E             Actual: open(PosixPath('.devsynth/project.yaml'))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
__ TestProjectYamlLoading.test_load_project_yaml_fallback_to_legacy_succeeds ___

self = <tests.unit.general.test_project_yaml.TestProjectYamlLoading object at 
0x119afc590>
mock_exists = <MagicMock name='exists' id='5292964800'>
mock_yaml_load = <MagicMock name='safe_load' id='5292956928'>
mock_open = <MagicMock name='open' id='5292949968'>

    @pytest.mark.fast
    @patch("builtins.open")
    @patch("yaml.safe_load")
    @patch("os.path.exists")
    def test_load_project_yaml_fallback_to_legacy_succeeds(
        self, mock_exists, mock_yaml_load, mock_open
    ):
        """Test fallback to legacy manifest.yaml if .devsynth/project.yaml 
doesn't exist.
    
        ReqID: N/A"""
        mock_exists.side_effect = lambda path: str(path) != 
".devsynth/project.yaml"
        mock_file = MagicMock()
        mock_open.return_value = mock_file
        mock_yaml_load.return_value = {"projectName": "TestProject", "version": 
"0.1.0"}
        result = load_manifest(None)
        assert result == {"projectName": "TestProject", "version": "0.1.0"}
>       mock_open.assert_called_once_with(Path("manifest.yaml"), "r")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pro
ject_yaml.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='open' id='5292949968'>
args = (PosixPath('manifest.yaml'), 'r'), kwargs = {}
expected = call(PosixPath('manifest.yaml'), 'r')
actual = call(PosixPath('manifest.yaml'))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b77f880>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: open(PosixPath('manifest.yaml'), 'r')
E             Actual: open(PosixPath('manifest.yaml'))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
______ TestProjectYamlLoading.test_project_yaml_path_preference_succeeds _______

self = <tests.unit.general.test_project_yaml.TestProjectYamlLoading object at 
0x119afc9e0>
mock_exists = <MagicMock name='exists' id='5292847904'>

    @pytest.mark.fast
    @patch("os.path.exists")
    def test_project_yaml_path_preference_succeeds(self, mock_exists):
        """Test that .devsynth/project.yaml is preferred over manifest.yaml when
both exist.
    
        ReqID: N/A"""
        mock_exists.return_value = True
        with patch("builtins.open", MagicMock()) as mock_open:
            with patch(
                "yaml.safe_load", return_value={"projectName": "TestProject"}
            ) as mock_yaml_load:
                result = load_manifest(None)
>               
mock_open.assert_called_once_with(Path(".devsynth/project.yaml"), "r")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pro
ject_yaml.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock id='5292846272'>
args = (PosixPath('.devsynth/project.yaml'), 'r'), kwargs = {}
expected = call(PosixPath('.devsynth/project.yaml'), 'r')
actual = call(PosixPath('.devsynth/project.yaml'))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b77dbc0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: mock(PosixPath('.devsynth/project.yaml'), 'r')
E             Actual: mock(PosixPath('.devsynth/project.yaml'))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
________ TestProjectYamlLoading.test_manifest_version_locking_succeeds _________

self = <tests.unit.general.test_project_yaml.TestProjectYamlLoading object at 
0x119afce30>
mock_yaml_load = <MagicMock name='safe_load' id='5294171072'>
mock_open = <MagicMock name='open' id='5294177936'>

    @pytest.mark.fast
    @patch("builtins.open")
    @patch("yaml.safe_load")
    def test_manifest_version_locking_succeeds(self, mock_yaml_load, mock_open):
        """Version field is preserved when loading project.yaml.
    
        ReqID: N/A"""
        mock_yaml_load.return_value = {"metadata": {"version": "9.9.9"}}
        result = load_manifest(Path(".devsynth/project.yaml"))
        assert result["metadata"]["version"] == "9.9.9"
>       mock_open.assert_called_once_with(Path(".devsynth/project.yaml"), "r")

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_pro
ject_yaml.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:961: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='open' id='5294177936'>
args = (PosixPath('.devsynth/project.yaml'), 'r'), kwargs = {}
expected = call(PosixPath('.devsynth/project.yaml'), 'r')
actual = call(PosixPath('.devsynth/project.yaml'))
_error_message = <function 
NonCallableMock.assert_called_with.<locals>._error_message at 0x13b4d1f80>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: 
%s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: open(PosixPath('.devsynth/project.yaml'), 'r')
E             Actual: open(PosixPath('.devsynth/project.yaml'))

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:949: AssertionError
________________________ test_is_cli_available_succeeds ________________________

    @pytest.mark.fast
    def test_is_cli_available_succeeds():
        """Test that is_cli_available checks environment variables and CLI 
availability.
    
        ReqID: N/A"""
        with patch.dict(os.environ, {"DEVSYNTH_RESOURCE_CLI_AVAILABLE": 
"false"}):
            from tests.conftest import is_cli_available
    
            assert not is_cli_available()
        with patch("subprocess.run") as mock_run:
            mock_run.return_value.returncode = 0
            from tests.conftest import is_cli_available
    
            assert is_cli_available()
        with patch("subprocess.run") as mock_run:
            mock_run.return_value.returncode = 1
            from tests.conftest import is_cli_available
    
            assert not is_cli_available()
        with patch("subprocess.run", side_effect=Exception("Command not 
found")):
            from tests.conftest import is_cli_available
    
>           assert not is_cli_available()
E           assert not True
E            +  where True = <function is_cli_available at 0x109bd8d60>()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_res
ource_markers.py:89: AssertionError
_________________ test_pytest_collection_modifyitems_succeeds __________________

    @pytest.mark.fast
    def test_pytest_collection_modifyitems_succeeds():
        """Test that pytest_collection_modifyitems skips tests with unavailable 
resources.
    
        ReqID: N/A"""
        from tests.conftest import pytest_collection_modifyitems
    
        class MockMarker:
    
            def __init__(self, name, args):
                self.name = name
                self.args = args
    
        class MockItem:
    
            def __init__(self, name, markers=None):
                self.name = name
                self._markers = markers or []
                self.user_properties = []
    
            def iter_markers(self, name=None):
                if name:
                    return [m for m in self._markers if m.name == name]
                return self._markers
    
            def add_marker(self, marker):
                self._markers.append(marker)
    
        class MockConfig:
    
            def __init__(self):
                pass
    
        item1 = MockItem(
            "test_unavailable_resource",
            [MockMarker("requires_resource", ["unavailable_resource"])],
        )
        item2 = MockItem(
            "test_available_resource",
            [MockMarker("requires_resource", ["available_resource"])],
        )
        item3 = MockItem("test_no_resource_marker")
        items = [item1, item2, item3]
        with patch(
            "tests.conftest.is_resource_available", lambda r: r == 
"available_resource"
        ):
>           pytest_collection_modifyitems(MockConfig(), items)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_res
ource_markers.py:161: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

config = 
<tests.unit.general.test_resource_markers.test_pytest_collection_modifyitems_suc
ceeds.<locals>.MockConfig object at 0x13b810ad0>
items = 
[<tests.unit.general.test_resource_markers.test_pytest_collection_modifyitems_su
cceeds.<locals>.MockItem object at 
0x1...nit.general.test_resource_markers.test_pytest_collection_modifyitems_succe
eds.<locals>.MockItem object at 0x13b810a40>]

    def pytest_collection_modifyitems(config, items):
        """
        Normalize and enforce resource gating, smoke-mode behavior, 
property-based testing collection,
        and conservative xdist-parallel safety.
    
        - Resource gating: validate @pytest.mark.requires_resource("<name>") and
skip when unavailable or malformed/unknown.
        - Smoke mode: when PYTEST_DISABLE_PLUGIN_AUTOLOAD=1, skip tests under 
tests/behavior/ that rely on third-party plugins.
        - Property-based tests: skip unless DEVSYNTH_PROPERTY_TESTING is 
enabled.
        - Xdist safety: conservatively mark integration and performance tests as
@pytest.mark.isolation to avoid parallelization issues.
        """
        # Validate and apply resource gating
        for item in items:
            for marker in item.iter_markers(name="requires_resource"):
                # Validate marker arguments
                if (
                    not marker.args
                    or not isinstance(marker.args[0], str)
                    or not marker.args[0].strip()
                ):
                    item.add_marker(
                        pytest.mark.skip(
                            reason="Malformed requires_resource marker: expected
a non-empty resource name"
                        )
                    )
                    continue
                resource = marker.args[0].strip()
    
                # Validate known resource
                known_resources = {
                    "anthropic",
                    "llm_provider",
                    "lmstudio",
                    "openai",
                    "codebase",
                    "cli",
                    "chromadb",
                    "tinydb",
                    "duckdb",
                    "faiss",
                    "kuzu",
                    "lmdb",
                    "rdflib",
                    "memory",
                    "test_resource",
                    "webui",
                }
                if resource not in known_resources:
                    item.add_marker(
                        pytest.mark.skip(
                            reason=f"Unknown resource '{resource}' not 
recognized by test harness"
                        )
                    )
                    continue
    
                # Add a derived static marker to enable '-m resource_<name>' 
selection
                try:
                    item.add_marker(getattr(pytest.mark, 
f"resource_{resource}"))
                except Exception:
                    # Defensive: do not fail collection if dynamic marker 
attachment has issues
                    pass
    
                # Skip if resource is not available
                if not is_resource_available(resource):
                    item.add_marker(
                        pytest.mark.skip(reason=f"Resource '{resource}' not 
available")
                    )
    
        # Smoke-mode behavior: skip behavior tests when plugins are disabled
        smoke = os.environ.get("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "0").lower() in
{
            "1",
            "true",
            "yes",
        }
        if smoke:
            skip_behavior = pytest.mark.skip(
                reason=(
                    "Smoke mode (plugins disabled): skipping behavior tests that
require third-party plugins"
                )
            )
            for item in items:
                try:
                    fspath = getattr(item, "fspath", None)
                    path_str = str(fspath) if fspath is not None else ""
                except Exception:
                    path_str = ""
                norm = path_str.replace("\\", "/")
                if "/tests/behavior/" in norm or 
norm.endswith("/tests/behavior"):
                    item.add_marker(skip_behavior)
    
        # Conservatively mark integration and performance tests as isolation for
xdist safety
        for item in items:
            try:
                fspath = getattr(item, "fspath", None)
                path_str = str(fspath) if fspath is not None else ""
            except Exception:
                path_str = ""
            norm = path_str.replace("\\", "/")
            if "/tests/integration/" in norm or "/tests/performance/" in norm:
                if not item.get_closest_marker("isolation"):
                    item.add_marker(pytest.mark.isolation)
    
        # Broaden isolation auto-marking heuristics for fragile tests that touch
filesystem/network
        # Apply only when a test lacks an explicit @pytest.mark.isolation
        network_keywords = ("network", "http", "https", "socket", "requests", 
"ftp")
        fs_fixtures = {
            "tmp_path",
            "tmpdir",
            "tmpdir_factory",
            "temp_log_dir",
            "tmp_project_dir",
        }
        for item in items:
            try:
                name = getattr(item, "name", "") or ""
                nodeid = getattr(item, "nodeid", "") or ""
                fixturenames = set(getattr(item, "fixturenames", []) or [])
            except Exception:
                name, nodeid, fixturenames = "", "", set()
>           if item.get_closest_marker("isolation"):
               ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'MockItem' object has no attribute 
'get_closest_marker'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/conftest.py:1158: 
AttributeError
_______________________ test_cli_bridge_methods_succeeds _______________________

    @pytest.mark.fast
    def test_cli_bridge_methods_succeeds():
        """Test that cli bridge methods succeeds.
    
        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.prompt.Prompt.ask", return_value="ans") as ask:
            resp = bridge.ask_question("Q?")
>           ask.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ux_
bridge.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='ask' id='5294646784'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'ask' to have been called once. Called 0 
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
----------------------------- Captured stdout call -----------------------------
Q?
______________________ test_webui_bridge_methods_succeeds ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9d86b0>

    @pytest.mark.fast
    def test_webui_bridge_methods_succeeds(monkeypatch):
        """Test that webui bridge methods succeeds.
    
        ReqID: N/A"""
        st = ModuleType("streamlit")
        st.text_input = MagicMock(return_value="text")
        st.selectbox = MagicMock(return_value="choice")
        st.checkbox = MagicMock(return_value=True)
        st.write = MagicMock()
        st.markdown = MagicMock()
        monkeypatch.setitem(sys.modules, "streamlit", st)
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_ux_
bridge.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ TestWorkflowManager.test_handle_human_intervention_succeeds __________

self = <tests.unit.general.test_workflow.TestWorkflowManager object at 
0x119baa690>
workflow_manager = <devsynth.application.orchestration.workflow.WorkflowManager 
object at 0x13b9a8ad0>

    @pytest.mark.fast
    def test_handle_human_intervention_succeeds(self, workflow_manager):
        """Test handling human intervention.
    
        ReqID: N/A"""
        with (
            patch(
                "devsynth.application.orchestration.workflow.console"
            ) as mock_console,
            patch(
                "devsynth.application.orchestration.workflow.Prompt.ask",
                return_value="User input",
            ),
        ):
            response = workflow_manager._handle_human_intervention(
                "workflow-id", "step-id", "Need your input"
            )
>           assert response == "User input"
E           AssertionError: assert '' == 'User input'
E             
E             - User input

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wor
kflow.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
Human intervention required:
Need your input
Your input
__________ TestWorkflowManager.test_add_init_workflow_steps_succeeds ___________

self = <tests.unit.general.test_workflow.TestWorkflowManager object at 
0x119baaf00>
workflow_manager = <devsynth.application.orchestration.workflow.WorkflowManager 
object at 0x13b452900>

    @pytest.mark.fast
    def test_add_init_workflow_steps_succeeds(self, workflow_manager):
        """Test adding steps for init workflow.
    
        ReqID: N/A"""
        mock_workflow = MagicMock()
        workflow_manager._add_init_workflow_steps(
            mock_workflow, {"path": "./test-project"}
        )
>       assert workflow_manager.orchestration_port.add_step.call_count == 3
E       AssertionError: assert 6 == 3
E        +  where 6 = <MagicMock name='OrchestrationPort.add_step' 
id='5289487360'>.call_count
E        +    where <MagicMock name='OrchestrationPort.add_step' 
id='5289487360'> = <MagicMock name='OrchestrationPort' id='5289360032'>.add_step
E        +      where <MagicMock name='OrchestrationPort' id='5289360032'> = 
<devsynth.application.orchestration.workflow.WorkflowManager object at 
0x13b452900>.orchestration_port

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wor
kflow.py:83: AssertionError
_______________ test_assign_roles_with_explicit_mapping_succeeds _______________

    def test_assign_roles_with_explicit_mapping_succeeds():
        """Test that assign roles with explicit mapping succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestWsdeRoleMappingTeam")
        a1 = MagicMock()
        a2 = MagicMock()
        a3 = MagicMock()
        team.add_agents([a1, a2, a3])
        # The primus agent should not also be listed as a worker or the
        # assignment logic will overwrite the Primus role.  Provide a distinct
        # worker mapping to verify role assignment order.
        mapping = {
            "primus": a1,
            "worker": [a2],
            "supervisor": a2,
            "designer": a3,
            "evaluator": None,
        }
>       team.assign_roles(mapping)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_role_mapping.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:664: in assign_roles
    return _manager(self).assign(role_mapping)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:373: in assign
    self._validate_role_mapping(normalised)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = RoleAssignmentManager(team=<devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b9d5b50>)
mapping = {<RoleName.DESIGNER: 'designer'>: <MagicMock id='5295154112'>, 
<RoleName.EVALUATOR: 'evaluator'>: None, <RoleName.PRIMUS: 'primus'>: <MagicMock
id='5295139744'>, <RoleName.SUPERVISOR: 'supervisor'>: <MagicMock 
id='5295132832'>, ...}

    def _validate_role_mapping(
        self, mapping: Mapping[RoleName, SupportsTeamAgent | None]
    ) -> None:
        valid_roles = set(RoleName)
        for role, agent in mapping.items():
            if role not in valid_roles:
                raise ValueError(
                    f"Invalid role: {role}. Valid roles are: {', '.join(r.value 
for r in valid_roles)}"
                )
            if agent is not None and agent not in self.team.agents:
>               raise ValueError(f"Agent {agent.name} is not a member of this 
team")
                                          ^^^^^^^^^^
E               AttributeError: 'list' object has no attribute 'name'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/domain/models/
wsde_roles.py:521: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,573 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5295145184'> to team TestWsdeRoleMappingTeam
2025-10-29 10:48:45,573 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5289230160'> to team TestWsdeRoleMappingTeam
2025-10-29 10:48:45,573 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5289907824'> to team TestWsdeRoleMappingTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5295145184'> to team TestWsdeRoleMappingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5289230160'> to team TestWsdeRoleMappingTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5289907824'> to team TestWsdeRoleMappingTeam
_____________ TestWSDETeam.test_get_role_specific_agents_succeeds ______________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be1430>
mock_agent = <MagicMock spec='BaseAgent' id='5289259856'>

    def test_get_role_specific_agents_succeeds(self, mock_agent):
        """Test getting agents by their specific roles.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = MagicMock(spec=BaseAgent)
        agent2 = MagicMock(spec=BaseAgent)
        agent3 = MagicMock(spec=BaseAgent)
        agent4 = MagicMock(spec=BaseAgent)
        agent5 = MagicMock(spec=BaseAgent)
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        team.add_agent(agent4)
        team.add_agent(agent5)
        agent1.current_role = "Primus"
        agent2.current_role = "Worker"
        agent3.current_role = "Supervisor"
        agent4.current_role = "Designer"
        agent5.current_role = "Evaluator"
>       assert team.get_worker() == agent2
E       AssertionError: assert None == <MagicMock spec='BaseAgent' 
id='5289247424'>
E        +  where None = _get_worker()
E        +    where _get_worker = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b78fb30>.get_worker

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:153: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,620 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5294346976'> to team TestTeam
2025-10-29 10:48:45,620 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5289251792'> to team TestTeam
2025-10-29 10:48:45,620 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5289506144'> to team TestTeam
2025-10-29 10:48:45,620 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5288650816'> to team TestTeam
2025-10-29 10:48:45,620 - devsynth.domain.models.wsde_core - INFO - Added agent 
<MagicMock name='mock.name' id='5288629728'> to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5294346976'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5289251792'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5289506144'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5288650816'> to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
<MagicMock name='mock.name' id='5288629728'> to team TestTeam
_______________ TestWSDETeam.test_peer_based_structure_succeeds ________________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be1d90>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b41ae80>

    def test_peer_based_structure_succeeds(self, mock_agent_with_expertise):
        """Test that all agents are treated as peers with no permanent 
hierarchy.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["skill1"])
        agent2 = mock_agent_with_expertise("Agent2", ["skill2"])
        agent3 = mock_agent_with_expertise("Agent3", ["skill3"])
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        assert team.get_primus() == agent1
        task2 = {"type": "task", "requires": "skill2"}
        team.select_primus_by_expertise(task2)
>       assert team.get_primus() == agent2
E       AssertionError: assert <MagicMock spec='BaseAgent' id='5288220608'> == 
<MagicMock spec='BaseAgent' id='5288342640'>
E        +  where <MagicMock spec='BaseAgent' id='5288220608'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b33d850>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:207: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,641 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent1 to team TestTeam
2025-10-29 10:48:45,641 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent2 to team TestTeam
2025-10-29 10:48:45,641 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent3 to team TestTeam
2025-10-29 10:48:45,641 - devsynth.domain.models.wsde_roles - INFO - Selected 
Agent1 as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent2 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent3 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Agent1 
as primus based on expertise
__________ TestWSDETeam.test_consensus_based_decision_making_succeeds __________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be26f0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b418ea0>

    def test_consensus_based_decision_making_succeeds(self, 
mock_agent_with_expertise):
        """Test facilitating consensus building among agents.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        agent1 = mock_agent_with_expertise("Agent1", ["skill1"])
        agent2 = mock_agent_with_expertise("Agent2", ["skill2"])
        agent3 = mock_agent_with_expertise("Agent3", ["skill3"])
        team.add_agent(agent1)
        team.add_agent(agent2)
        team.add_agent(agent3)
        task = {"type": "decision_task", "description": "A task requiring 
consensus"}
        solution1 = {"agent": "Agent1", "content": "Solution from Agent1"}
        solution2 = {"agent": "Agent2", "content": "Solution from Agent2"}
        solution3 = {"agent": "Agent3", "content": "Solution from Agent3"}
        team.add_solution(task, solution1)
        team.add_solution(task, solution2)
        team.add_solution(task, solution3)
        consensus = team.build_consensus(task)
>       assert "Agent1" in consensus["contributors"]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'contributors'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:262: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,659 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent1 to team TestTeam
2025-10-29 10:48:45,659 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent2 to team TestTeam
2025-10-29 10:48:45,660 - devsynth.domain.models.wsde_core - INFO - Added agent 
Agent3 to team TestTeam
2025-10-29 10:48:45,660 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
2025-10-29 10:48:45,660 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
2025-10-29 10:48:45,660 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
2025-10-29 10:48:45,660 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent2 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Agent3 to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 007fc3a6-4301-4e9b-824a-fcf2e42ff575
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
____________ TestWSDETeam.test_dialectical_review_process_succeeds _____________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be2ba0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b419da0>

    def test_dialectical_review_process_succeeds(self, 
mock_agent_with_expertise):
        """Test the dialectical review process with thesis, antithesis, and 
synthesis.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        code_agent = mock_agent_with_expertise("CodeAgent", ["python", 
"coding"])
        test_agent = mock_agent_with_expertise("TestAgent", ["testing", 
"quality"])
        critic_agent = mock_agent_with_expertise(
            "CriticAgent", ["dialectical_reasoning", "critique"]
        )
        team.add_agent(code_agent)
        team.add_agent(test_agent)
        team.add_agent(critic_agent)
        task = {
            "type": "implementation_task",
            "description": "Implement a user authentication system",
        }
        thesis = {
            "agent": "CodeAgent",
            "content": "Implement authentication using a simple 
username/password check",
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
        }
        team.add_solution(task, thesis)
        dialectical_result = team.apply_dialectical_reasoning(task, 
critic_agent)
        assert "thesis" in dialectical_result
        assert "antithesis" in dialectical_result
        assert "synthesis" in dialectical_result
>       assert dialectical_result["thesis"]["agent"] == "CodeAgent"
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:295: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,671 - devsynth.domain.models.wsde_core - INFO - Added agent 
CodeAgent to team TestTeam
2025-10-29 10:48:45,671 - devsynth.domain.models.wsde_core - INFO - Added agent 
TestAgent to team TestTeam
2025-10-29 10:48:45,671 - devsynth.domain.models.wsde_core - INFO - Added agent 
CriticAgent to team TestTeam
2025-10-29 10:48:45,671 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task dbc22607-aee9-449a-ace3-760d8c13ca27
2025-10-29 10:48:45,671 - devsynth.domain.models.wsde_core - WARNING - Cannot 
apply dialectical reasoning: no solution provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
CodeAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
TestAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task dbc22607-aee9-449a-ace3-760d8c13ca27
WARNING  devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply 
dialectical reasoning: no solution provided
_______ TestWSDETeam.test_peer_review_with_acceptance_criteria_succeeds ________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be3050>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b41afc0>

    def test_peer_review_with_acceptance_criteria_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test the peer review process with specific acceptance criteria.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        author_agent = mock_agent_with_expertise("AuthorAgent", ["python", 
"coding"])
        reviewer1 = mock_agent_with_expertise("ReviewerAgent1", ["testing", 
"quality"])
        reviewer2 = mock_agent_with_expertise(
            "ReviewerAgent2", ["security", "best_practices"]
        )
        team.add_agent(author_agent)
        team.add_agent(reviewer1)
        team.add_agent(reviewer2)
        work_product = {
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        acceptance_criteria = [
            "Code follows security best practices",
            "Function handles edge cases",
            "Code is well-documented",
        ]
        review = team.request_peer_review(
            work_product=work_product,
            author=author_agent,
            reviewer_agents=[reviewer1, reviewer2],
        )
>       review.acceptance_criteria = acceptance_criteria
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: property 'acceptance_criteria' of 'PeerReview' object 
has no setter

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:330: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,683 - devsynth.domain.models.wsde_core - INFO - Added agent 
AuthorAgent to team TestTeam
2025-10-29 10:48:45,683 - devsynth.domain.models.wsde_core - INFO - Added agent 
ReviewerAgent1 to team TestTeam
2025-10-29 10:48:45,683 - devsynth.domain.models.wsde_core - INFO - Added agent 
ReviewerAgent2 to team TestTeam
2025-10-29 10:48:45,683 - devsynth.domain.models.wsde_roles - INFO - Selected 
AuthorAgent as primus based on expertise
2025-10-29 10:48:45,684 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent', 
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
ReviewerAgent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
ReviewerAgent2 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent', 
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
__________ TestWSDETeam.test_peer_review_with_revision_cycle_succeeds __________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be3500>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b44ae80>

    def test_peer_review_with_revision_cycle_succeeds(self, 
mock_agent_with_expertise):
        """Test the peer review process with a revision cycle.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        author_agent = mock_agent_with_expertise("AuthorAgent", ["python", 
"coding"])
        reviewer1 = mock_agent_with_expertise("ReviewerAgent1", ["testing", 
"quality"])
        reviewer2 = mock_agent_with_expertise(
            "ReviewerAgent2", ["security", "best_practices"]
        )
        team.add_agent(author_agent)
        team.add_agent(reviewer1)
        team.add_agent(reviewer2)
        work_product = {
            "code": """def authenticate(username, password):
    return username == 'admin' and password == 'password'""",
            "description": "Simple authentication function",
        }
        review = team.request_peer_review(
            work_product=work_product,
            author=author_agent,
            reviewer_agents=[reviewer1, reviewer2],
        )
        for reviewer in review.reviewers:
            review.reviews[reviewer.name] = {
                "overall_feedback": "The code needs improvement",
                "suggestions": [
                    "Use a secure password hashing algorithm",
                    "Add input validation",
                ],
                "approved": False,
            }
        review.collect_reviews()
        review.request_revision()
        assert review.status == "revision_requested"
        revised_work = {
            "code": """def authenticate(username, password):
    # Validate inputs
    if not username or not password:
        return False
    
    # In a real system, this would use a secure password hashing algorithm
    # and compare against stored hashed passwords
    import hashlib
    hashed_password = hashlib.sha256(password.encode()).hexdigest()
    return username == 'admin' and hashed_password == 
'5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8'
    """,
            "description": "Improved authentication function with input 
validation and hashing",
        }
        new_review = review.submit_revision(revised_work)
        assert new_review.previous_review == review
        assert new_review.work_product == revised_work
        for reviewer in new_review.reviewers:
            new_review.reviews[reviewer.name] = {
                "overall_feedback": "The code is now acceptable",
                "suggestions": [],
                "approved": True,
            }
        new_review.collect_reviews()
        new_review.quality_score = 0.9
        final_result = new_review.finalize(approved=True)
        assert final_result["status"] == "approved"
>       assert final_result["previous_review_id"] == review.review_id
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'previous_review_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:424: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,695 - devsynth.domain.models.wsde_core - INFO - Added agent 
AuthorAgent to team TestTeam
2025-10-29 10:48:45,695 - devsynth.domain.models.wsde_core - INFO - Added agent 
ReviewerAgent1 to team TestTeam
2025-10-29 10:48:45,695 - devsynth.domain.models.wsde_core - INFO - Added agent 
ReviewerAgent2 to team TestTeam
2025-10-29 10:48:45,695 - devsynth.domain.models.wsde_roles - INFO - Selected 
AuthorAgent as primus based on expertise
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent', 
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'ReviewerAgent1', 'worker': 
'ReviewerAgent2', 'supervisor': 'AuthorAgent', 'designer': None, 'evaluator': 
None}
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 59403fd1-8293-4b6e-8187-d38327c8d846
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 59403fd1-8293-4b6e-8187-d38327c8d846
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'ReviewerAgent1', 
'supervisor': 'ReviewerAgent2', 'designer': None, 'evaluator': None}
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 136a6470-068f-4498-987a-b54a34c6524b
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 136a6470-068f-4498-987a-b54a34c6524b
2025-10-29 10:48:45,696 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
2025-10-29 10:48:45,696 - devsynth.application.collaboration.peer_review - ERROR
- Error in finalize: 'dict' object has no attribute 'reviewer'
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
ReviewerAgent1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
ReviewerAgent2 to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'ReviewerAgent2', 'worker': 'AuthorAgent', 
'supervisor': 'ReviewerAgent1', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'ReviewerAgent1', 'worker': 'ReviewerAgent2', 
'supervisor': 'AuthorAgent', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 59403fd1-8293-4b6e-8187-d38327c8d846
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 59403fd1-8293-4b6e-8187-d38327c8d846
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'ReviewerAgent1', 
'supervisor': 'ReviewerAgent2', 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 136a6470-068f-4498-987a-b54a34c6524b
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 136a6470-068f-4498-987a-b54a34c6524b
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
ERROR    devsynth.application.collaboration.peer_review:logging_setup.py:615 
Error in finalize: 'dict' object has no attribute 'reviewer'
_______ TestWSDETeam.test_peer_review_with_dialectical_analysis_succeeds _______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be39b0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b44ab60>

        def test_peer_review_with_dialectical_analysis_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the peer review process with dialectical analysis.
    
            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            author_agent = mock_agent_with_expertise("AuthorAgent", ["python", 
"coding"])
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique"]
            )
            team.add_agent(author_agent)
            team.add_agent(critic_agent)
            work_product = {
                "code": """def authenticate(username, password):
        return username == 'admin' and password == 'password'""",
                "description": "Simple authentication function",
            }
            review = team.request_peer_review(
                work_product=work_product,
                author=author_agent,
                reviewer_agents=[critic_agent],
            )
            dialectical_analysis = {
                "thesis": {
                    "strengths": [
                        "Simple and easy to understand",
                        "Functional for basic use cases",
                    ],
                    "key_points": ["Direct string comparison for 
authentication"],
                },
                "antithesis": {
                    "weaknesses": [
                        "Security vulnerability: Hardcoded credentials",
                        "No input validation",
                        "No error handling",
                        "No password hashing",
                    ],
                    "challenges": [
                        "Insecure for production use",
                        "Vulnerable to timing attacks",
                    ],
                },
                "synthesis": {
                    "improvements": [
                        "Use secure password hashing",
                        "Add input validation",
                        "Implement proper error handling",
                        "Use environment variables or a secure configuration for
credentials",
                    ],
                    "improved_solution": """def authenticate(username, 
password):
        # Validate inputs
        if not username or not password:
            return False
    
        try:
            # In a real system, this would use a secure password hashing 
algorithm
            # and compare against stored hashed passwords
            import hashlib
            import hmac
            import os
    
            # Use constant-time comparison to prevent timing attacks
            stored_hash = hashlib.sha256(os.environ.get('ADMIN_PASSWORD', 
'').encode()).hexdigest()
            user_hash = hashlib.sha256(password.encode()).hexdigest()
    
            return username == os.environ.get('ADMIN_USERNAME', '') and 
hmac.compare_digest(stored_hash, user_hash)
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return False
    """,
                },
            }
            review.reviews[critic_agent.name] = {
                "overall_feedback": "The code needs significant improvement for 
security",
                "dialectical_analysis": dialectical_analysis,
                "approved": False,
            }
            review.collect_reviews()
>           feedback = review.aggregate_feedback()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:505: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:942: in aggregate_feedback
    record = self._build_peer_review_record(decisions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:534: in _build_peer_review_record
    reviewer_names = tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x13b9c2d70>

    reviewer_names = tuple(
>       decision.reviewer or "unknown" for decision in decision_list
        ^^^^^^^^^^^^^^^^^
    )
E   AttributeError: 'dict' object has no attribute 'reviewer'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/co
llaboration/peer_review.py:535: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_core - INFO - Added agent 
AuthorAgent to team TestTeam
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_core - INFO - Added agent 
CriticAgent to team TestTeam
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_roles - INFO - Selected 
AuthorAgent as primus based on expertise
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'CriticAgent', 'worker': 'AuthorAgent', 
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_roles - INFO - Rotated 
roles for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'CriticAgent', 
'supervisor': None, 'designer': None, 'evaluator': None}
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 0ed2826e-0db0-4db5-80b1-f9838ac0c1bd
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_core - WARNING - Cannot 
apply dialectical reasoning: no solution provided
2025-10-29 10:48:45,709 - devsynth.application.collaboration.peer_review - 
WARNING - Error applying dialectical reasoning: 'NoneType' object has no 
attribute 'get'
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 08007352-d307-4da2-8cfd-953a39aed85d
2025-10-29 10:48:45,709 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
build consensus: no options provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
AuthorAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
AuthorAgent as primus based on expertise
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'CriticAgent', 'worker': 'AuthorAgent', 
'supervisor': None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Rotated roles 
for team TestTeam: {'primus': 'AuthorAgent', 'worker': 'CriticAgent', 
'supervisor': None, 'designer': None, 'evaluator': None}
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 0ed2826e-0db0-4db5-80b1-f9838ac0c1bd
WARNING  devsynth.domain.models.wsde_core:logging_setup.py:615 Cannot apply 
dialectical reasoning: no solution provided
WARNING  devsynth.application.collaboration.peer_review:logging_setup.py:615 
Error applying dialectical reasoning: 'NoneType' object has no attribute 'get'
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 08007352-d307-4da2-8cfd-953a39aed85d
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot build 
consensus: no options provided
_____________ TestWSDETeam.test_contextdriven_leadership_succeeds ______________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be27e0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b418ea0>

    def test_contextdriven_leadership_succeeds(self, mock_agent_with_expertise):
        """Test context-driven leadership in the WSDE team.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        python_agent = mock_agent_with_expertise("PythonAgent", ["python", 
"backend"])
        js_agent = mock_agent_with_expertise("JSAgent", ["javascript", 
"frontend"])
        security_agent = mock_agent_with_expertise(
            "SecurityAgent", ["security", "authentication"]
        )
        design_agent = mock_agent_with_expertise("DesignAgent", ["design", "ui",
"ux"])
        doc_agent = mock_agent_with_expertise(
            "DocAgent", ["documentation", "technical_writing"]
        )
        team.add_agent(python_agent)
        team.add_agent(js_agent)
        team.add_agent(security_agent)
        team.add_agent(design_agent)
        team.add_agent(doc_agent)
        doc_task = {
            "type": "documentation_task",
            "description": "Write API documentation",
            "domain": "documentation",
            "requirements": ["Clear examples", "Complete coverage"],
        }
        team.select_primus_by_expertise(doc_task)
        assert team.get_primus() == doc_agent
        assert doc_agent.current_role == "Primus"
        roles = [agent.current_role for agent in team.agents]
>       assert "Worker" in roles
E       AssertionError: assert 'Worker' in [None, None, None, None, 'Primus']

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:547: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_core - INFO - Added agent 
PythonAgent to team TestTeam
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_core - INFO - Added agent 
JSAgent to team TestTeam
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_core - INFO - Added agent 
SecurityAgent to team TestTeam
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_core - INFO - Added agent 
DesignAgent to team TestTeam
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_core - INFO - Added agent 
DocAgent to team TestTeam
2025-10-29 10:48:45,738 - devsynth.domain.models.wsde_roles - INFO - Selected 
DocAgent as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
PythonAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
JSAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
SecurityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
DesignAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
DocAgent to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
DocAgent as primus based on expertise
___ TestWSDETeam.test_dialectical_reasoning_with_external_knowledge_succeeds ___

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119be3bc0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b44ade0>

        def test_dialectical_reasoning_with_external_knowledge_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the dialectical reasoning process with external knowledge 
integration.
    
            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            code_agent = mock_agent_with_expertise("CodeAgent", ["python", 
"coding"])
            security_agent = mock_agent_with_expertise(
                "SecurityAgent", ["security", "authentication"]
            )
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique"]
            )
            task = {
                "type": "implementation_task",
                "description": "Implement a secure user authentication system 
with multi-factor authentication",
            }
            thesis = {
                "agent": "CodeAgent",
                "content": "Implement authentication using username/password 
with JWT tokens",
                "code": """
    def authenticate(username, password):
        if username == 'admin' and password == 'password':
            token = generate_jwt_token(username)
            return token
        return None
    
    def generate_jwt_token(username):
        # Generate a JWT token
        return "jwt_token_placeholder"
                """,
            }
            team.add_solution(task, thesis)
            external_knowledge = {
                "security_best_practices": {
                    "authentication": [
                        "Use multi-factor authentication for sensitive 
operations",
                        "Store passwords using strong, adaptive hashing 
algorithms (e.g., bcrypt, Argon2)",
                        "Implement rate limiting to prevent brute force 
attacks",
                        "Use HTTPS for all authentication requests",
                        "Set secure and HttpOnly flags on authentication 
cookies",
                    ],
                    "data_protection": [
                        "Encrypt sensitive data at rest and in transit",
                        "Implement proper access controls",
                        "Follow the principle of least privilege",
                        "Regularly audit access to sensitive data",
                        "Have a data breach response plan",
                    ],
                },
                "industry_standards": {
                    "OWASP": [
                        "OWASP Top 10 Web Application Security Risks",
                        "OWASP Application Security Verification Standard 
(ASVS)",
                        "OWASP Secure Coding Practices",
                    ],
                    "ISO": [
                        "ISO/IEC 27001 - Information security management",
                        "ISO/IEC 27002 - Code of practice for information 
security controls",
                    ],
                    "NIST": [
                        "NIST Special Publication 800-53 - Security and Privacy 
Controls",
                        "NIST Cybersecurity Framework",
                    ],
                },
                "compliance_requirements": {
                    "GDPR": [
                        "Obtain explicit consent for data collection",
                        "Provide mechanisms for users to access, modify, and 
delete their data",
                        "Report data breaches within 72 hours",
                        "Conduct Data Protection Impact Assessments (DPIA)",
                    ],
                    "HIPAA": [
                        "Implement technical safeguards for PHI",
                        "Conduct regular risk assessments",
                        "Maintain audit trails of PHI access",
                        "Have Business Associate Agreements (BAA) in place",
                    ],
                    "PCI-DSS": [
                        "Maintain a secure network and systems",
                        "Protect cardholder data",
                        "Implement strong access control measures",
                        "Regularly test security systems and processes",
                    ],
                },
            }
>           dialectical_result = 
team.apply_enhanced_dialectical_reasoning_with_knowledge(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^
                task, critic_agent, external_knowledge
            )
E           AttributeError: 'WSDETeam' object has no attribute 
'apply_enhanced_dialectical_reasoning_with_knowledge'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:639: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,751 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task be8b0898-9a9e-4207-af7d-7c52ef5b11db
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task be8b0898-9a9e-4207-af7d-7c52ef5b11db
_____ TestWSDETeam.test_multi_disciplinary_dialectical_reasoning_succeeds ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf80b0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b4487c0>

        def test_multi_disciplinary_dialectical_reasoning_succeeds(
            self, mock_agent_with_expertise
        ):
            """Test the dialectical reasoning process with multiple disciplinary
perspectives.
    
            ReqID: N/A"""
            team = WSDETeam(name="TestTeam")
            code_agent = mock_agent_with_expertise("CodeAgent", ["python", 
"coding"])
            security_agent = mock_agent_with_expertise(
                "SecurityAgent", ["security", "authentication"]
            )
            ux_agent = mock_agent_with_expertise(
                "UXAgent", ["user_experience", "interface_design"]
            )
            performance_agent = mock_agent_with_expertise(
                "PerformanceAgent", ["performance", "optimization"]
            )
            accessibility_agent = mock_agent_with_expertise(
                "AccessibilityAgent", ["accessibility", "inclusive_design"]
            )
            critic_agent = mock_agent_with_expertise(
                "CriticAgent", ["dialectical_reasoning", "critique", 
"synthesis"]
            )
            team.add_agent(code_agent)
            team.add_agent(security_agent)
            team.add_agent(ux_agent)
            team.add_agent(performance_agent)
            team.add_agent(accessibility_agent)
            team.add_agent(critic_agent)
            task = {
                "type": "implementation_task",
                "description": "Implement a user authentication system with a 
focus on security, usability, performance, and accessibility",
            }
            thesis = {
                "agent": "CodeAgent",
                "content": "Implement authentication using username/password 
with JWT tokens",
                "code": """
    def authenticate(username, password):
        if username == 'admin' and password == 'password':
            token = generate_jwt_token(username)
            return token
        return None
    
    def generate_jwt_token(username):
        # Generate a JWT token
        return "jwt_token_placeholder"
                """,
            }
            team.add_solution(task, thesis)
            disciplinary_knowledge = {
                "security": {
                    "authentication_best_practices": [
                        "Use multi-factor authentication for sensitive 
operations",
                        "Store passwords using strong, adaptive hashing 
algorithms (e.g., bcrypt, Argon2)",
                        "Implement rate limiting to prevent brute force 
attacks",
                        "Use HTTPS for all authentication requests",
                        "Set secure and HttpOnly flags on authentication 
cookies",
                    ]
                },
                "user_experience": {
                    "authentication_ux_principles": [
                        "Minimize friction in the authentication process",
                        "Provide clear error messages for failed authentication 
attempts",
                        "Offer password recovery options",
                        "Remember user preferences where appropriate",
                        "Support single sign-on where possible",
                    ]
                },
                "performance": {
                    "authentication_performance_considerations": [
                        "Optimize token validation for minimal latency",
                        "Cache frequently used authentication data",
                        "Use asynchronous processing for non-critical 
authentication tasks",
                        "Implement efficient database queries for user lookup",
                        "Monitor and optimize authentication service response 
times",
                    ]
                },
                "accessibility": {
                    "authentication_accessibility_guidelines": [
                        "Ensure all authentication forms are keyboard 
navigable",
                        "Provide appropriate ARIA labels for authentication form
elements",
                        "Support screen readers for error messages and 
instructions",
                        "Maintain sufficient color contrast for text and 
interactive elements",
                        "Allow authentication timeout extensions for users who 
need more time",
                    ]
                },
            }
            dialectical_result = 
team.apply_multi_disciplinary_dialectical_reasoning(
                task,
                critic_agent,
                disciplinary_knowledge,
                [security_agent, ux_agent, performance_agent, 
accessibility_agent],
            )
>           assert "thesis" in dialectical_result
E           AssertionError: assert 'thesis' in {'reason': 'no_solution', 
'status': 'failed'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:751: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
CodeAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
SecurityAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
UXAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
PerformanceAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
AccessibilityAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_core - INFO - Added agent 
CriticAgent to team TestTeam
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_utils - INFO - Added 
solution for task 02558ec5-6b12-4115-95d7-66873697cbae
2025-10-29 10:48:45,763 - devsynth.domain.models.wsde_multidisciplinary - 
WARNING - Cannot apply multi-disciplinary dialectical reasoning: no solution 
provided
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
CodeAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
SecurityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
UXAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
PerformanceAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
AccessibilityAgent to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
CriticAgent to team TestTeam
INFO     devsynth.domain.models.wsde_utils:logging_setup.py:615 Added solution 
for task 02558ec5-6b12-4115-95d7-66873697cbae
WARNING  devsynth.domain.models.wsde_multidisciplinary:logging_setup.py:615 
Cannot apply multi-disciplinary dialectical reasoning: no solution provided
____ TestWSDETeam.test_assign_roles_for_phase_varied_contexts_has_expected _____

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf8560>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b44a7a0>

    def test_assign_roles_for_phase_varied_contexts_has_expected(
        self, mock_agent_with_expertise
    ):
        """Test that different phases can have different primus agents.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expand_agent = mock_agent_with_expertise(
            "Expand", ["brainstorming", "exploration", "creativity"]
        )
        diff_agent = mock_agent_with_expertise(
            "Diff", ["comparison", "analysis", "evaluation"]
        )
        refine_agent = mock_agent_with_expertise(
            "Refine", ["implementation", "coding", "development"]
        )
        doc_agent = mock_agent_with_expertise(
            "Doc", ["documentation", "reflection", "learning"]
        )
        team.add_agents([expand_agent, diff_agent, refine_agent, doc_agent])
        team.assign_roles_for_phase(Phase.EXPAND, {"description": "demo"})
        team.primus_index = team.agents.index(expand_agent)
>       team.role_assignments["primus"] = expand_agent
        ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'. 
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:800: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_core - INFO - Added agent 
Expand to team TestTeam
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_core - INFO - Added agent 
Diff to team TestTeam
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_core - INFO - Added agent 
Refine to team TestTeam
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_core - INFO - Added agent 
Doc to team TestTeam
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_roles - INFO - Assigning 
roles for phase EXPAND in team TestTeam
2025-10-29 10:48:45,776 - devsynth.domain.models.wsde_roles - INFO - Selected 
Expand as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Expand to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Diff 
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Refine to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc 
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Assigning roles 
for phase EXPAND in team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Expand 
as primus based on expertise
______ TestWSDETeam.test_vote_on_critical_decision_weighted_path_succeeds ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf8ec0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b448cc0>

    def test_vote_on_critical_decision_weighted_path_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that vote on critical decision weighted path succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expert = mock_agent_with_expertise("expert", ["security"])
        expert.config = MagicMock()
        expert.config.name = "expert"
        expert.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "expert",
        }
        intermediate = mock_agent_with_expertise("inter", ["security"])
        intermediate.config = MagicMock()
        intermediate.config.name = "inter"
        intermediate.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "intermediate",
        }
        novice = mock_agent_with_expertise("novice", ["python"])
        novice.config = MagicMock()
        novice.config.name = "novice"
        novice.config.parameters = {
            "expertise": ["python"],
            "expertise_level": "novice",
        }
        for a in [expert, intermediate, novice]:
            a.process = MagicMock()
            team.add_agent(a)
        expert.process.return_value = {"vote": "b"}
        intermediate.process.return_value = {"vote": "a"}
        novice.process.return_value = {"vote": "a"}
        task = {
            "type": "critical_decision",
            "domain": "security",
            "is_critical": True,
            "options": [{"id": "a"}, {"id": "b"}],
        }
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "weighted_vote"
E       AssertionError: assert 'majority_vote' == 'weighted_vote'
E         
E         - weighted_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:882: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,798 - devsynth.domain.models.wsde_core - INFO - Added agent 
expert to team TestTeam
2025-10-29 10:48:45,798 - devsynth.domain.models.wsde_core - INFO - Added agent 
inter to team TestTeam
2025-10-29 10:48:45,798 - devsynth.domain.models.wsde_core - INFO - Added agent 
novice to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
expert to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent inter
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
novice to team TestTeam
_ 
TestWSDETeam.test_documentation_task_selects_doc_agent_and_updates_role_assignme
nts_succeeds _

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf9370>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b418c20>

    def 
test_documentation_task_selects_doc_agent_and_updates_role_assignments_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that documentation task selects doc agent and updates role 
assignments succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        coder = mock_agent_with_expertise("Coder", ["python"])
        coder.has_been_primus = True
        doc_agent = mock_agent_with_expertise("Doc", ["documentation", 
"markdown"])
        doc_agent.has_been_primus = False
        team.add_agents([coder, doc_agent])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
        assert team.get_primus() is doc_agent
>       assert team.role_assignments["primus"] is doc_agent
               ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'. 
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:901: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,810 - devsynth.domain.models.wsde_core - INFO - Added agent 
Coder to team TestTeam
2025-10-29 10:48:45,810 - devsynth.domain.models.wsde_core - INFO - Added agent 
Doc to team TestTeam
2025-10-29 10:48:45,811 - devsynth.domain.models.wsde_roles - INFO - Selected 
Doc as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Coder
to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc 
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected Doc as 
primus based on expertise
______ TestWSDETeam.test_select_primus_fallback_when_no_expertise_matches ______

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf9820>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b449120>

    def test_select_primus_fallback_when_no_expertise_matches(
        self, mock_agent_with_expertise
    ):
        """Test that select primus fallback when no expertise matches.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = mock_agent_with_expertise("A1", ["python"])
        a2 = mock_agent_with_expertise("A2", ["javascript"])
        a3 = mock_agent_with_expertise("A3", ["design"])
        for a in [a1, a2, a3]:
            a.has_been_primus = True
            team.add_agent(a)
        task = {"type": "unknown", "topic": "nothing"}
        team.select_primus_by_expertise(task)
        assert team.get_primus() is a1
>       assert team.role_assignments["primus"] is a1
               ^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute 'role_assignments'. 
Did you mean: 'get_role_assignments'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:920: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,826 - devsynth.domain.models.wsde_core - INFO - Added agent 
A1 to team TestTeam
2025-10-29 10:48:45,826 - devsynth.domain.models.wsde_core - INFO - Added agent 
A2 to team TestTeam
2025-10-29 10:48:45,826 - devsynth.domain.models.wsde_core - INFO - Added agent 
A3 to team TestTeam
2025-10-29 10:48:45,826 - devsynth.domain.models.wsde_roles - INFO - Selected A1
as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent A3 to
team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected A1 as 
primus based on expertise
________ TestWSDETeam.test_documentation_expert_becomes_primus_succeeds ________

self = <tests.unit.general.test_wsde_team_extended.TestWSDETeam object at 
0x119bf9cd0>
mock_agent_with_expertise = <function 
TestWSDETeam.mock_agent_with_expertise.<locals>._create_agent at 0x13b448900>

    def test_documentation_expert_becomes_primus_succeeds(
        self, mock_agent_with_expertise
    ):
        """Test that documentation expert becomes primus succeeds.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        generalist = mock_agent_with_expertise("Generalist", ["python"])
        doc_agent = mock_agent_with_expertise("Doc", ["documentation"])
        team.add_agents([generalist, doc_agent])
        task = {"type": "documentation", "description": "Write docs"}
        team.select_primus_by_expertise(task)
>       assert team.get_primus() is doc_agent
E       AssertionError: assert <MagicMock spec='BaseAgent' id='5290045328'> is 
<MagicMock spec='BaseAgent' id='5288631216'>
E        +  where <MagicMock spec='BaseAgent' id='5290045328'> = get_primus()
E        +    where get_primus = <devsynth.domain.models.wsde_facade.WSDETeam 
object at 0x13b4fa000>.get_primus

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_extended.py:934: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,842 - devsynth.domain.models.wsde_core - INFO - Added agent 
Generalist to team TestTeam
2025-10-29 10:48:45,842 - devsynth.domain.models.wsde_core - INFO - Added agent 
Doc to team TestTeam
2025-10-29 10:48:45,842 - devsynth.domain.models.wsde_roles - INFO - Selected 
Generalist as primus based on expertise
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
Generalist to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent Doc 
to team TestTeam
INFO     devsynth.domain.models.wsde_roles:logging_setup.py:615 Selected 
Generalist as primus based on expertise
___________ test_vote_on_critical_decision_not_critical_raises_error ___________

    def test_vote_on_critical_decision_not_critical_raises_error():
        """Test that vote_on_critical_decision returns an error when task is not
critical.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        result = team.vote_on_critical_decision({"type": "other"})
>       assert result["voting_initiated"] is False
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_voting_invalid.py:14: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,854 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
conduct vote: no agents in team
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct 
vote: no agents in team
____________ test_vote_on_critical_decision_no_options_raises_error ____________

    def test_vote_on_critical_decision_no_options_raises_error():
        """Test that vote_on_critical_decision returns an error when no options 
are provided.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        task = {"type": "critical_decision", "is_critical": True, "options": []}
        result = team.vote_on_critical_decision(task)
>       assert result["voting_initiated"] is False
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_team_voting_invalid.py:25: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,859 - devsynth.domain.models.wsde_voting - WARNING - Cannot 
conduct vote: no agents in team
------------------------------ Captured log call -------------------------------
WARNING  devsynth.domain.models.wsde_voting:logging_setup.py:615 Cannot conduct 
vote: no agents in team
____________ test_majority_vote_with_three_unique_choices_succeeds _____________

    def test_majority_vote_with_three_unique_choices_succeeds():
        """Test that when three agents vote for three different options, it 
results in a tie.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", "option1")
        a2 = _make_agent("a2", "option2")
        a3 = _make_agent("a3", "option3")
        team.add_agents([a1, a2, a3])
        task = _basic_task()
        with patch.object(team, "build_consensus", return_value={"consensus": 
""}):
            result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "tied_vote"
E       AssertionError: assert 'majority_vote' == 'tied_vote'
E         
E         - tied_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:47: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,865 - devsynth.domain.models.wsde_core - INFO - Added agent 
a1 to team TestTeam
2025-10-29 10:48:45,865 - devsynth.domain.models.wsde_core - INFO - Added agent 
a2 to team TestTeam
2025-10-29 10:48:45,865 - devsynth.domain.models.wsde_core - INFO - Added agent 
a3 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a3 to
team TestTeam
_________________ test_tie_triggers_handle_tied_vote_succeeds __________________

    def test_tie_triggers_handle_tied_vote_succeeds():
        """Test that a tie between two options triggers the _handle_tied_vote 
method.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", "option1")
        a2 = _make_agent("a2", "option2")
        team.add_agents([a1, a2])
        task = _basic_task()
        with (
            patch.object(team, "build_consensus", return_value={"consensus": 
""}),
            patch.object(team, "_handle_tied_vote", 
wraps=team._handle_tied_vote) as mocked,
        ):
            team.vote_on_critical_decision(task)
>           mocked.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='_handle_tied_vote' id='5282122352'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected '_handle_tied_vote' to have been called 
once. Called 0 times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,875 - devsynth.domain.models.wsde_core - INFO - Added agent 
a1 to team TestTeam
2025-10-29 10:48:45,876 - devsynth.domain.models.wsde_core - INFO - Added agent 
a2 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a2 to
team TestTeam
______________ test_weighted_voting_prefers_expert_vote_succeeds _______________

    def test_weighted_voting_prefers_expert_vote_succeeds():
        """Test that weighted voting gives preference to expert votes over 
novice votes.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        expert = _make_agent("expert", "option2", ["security"], level="expert")
        novice1 = _make_agent("novice1", "option1", ["security"], 
level="novice")
        novice2 = _make_agent("novice2", "option1", ["security"], 
level="novice")
        team.add_agents([expert, novice1, novice2])
        task = _basic_task()
        task["domain"] = "security"
        result = team.vote_on_critical_decision(task)
>       assert result["result"]["method"] == "weighted_vote"
E       AssertionError: assert 'majority_vote' == 'weighted_vote'
E         
E         - weighted_vote
E         + majority_vote

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:80: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,900 - devsynth.domain.models.wsde_core - INFO - Added agent 
expert to team TestTeam
2025-10-29 10:48:45,900 - devsynth.domain.models.wsde_core - INFO - Added agent 
novice1 to team TestTeam
2025-10-29 10:48:45,900 - devsynth.domain.models.wsde_core - INFO - Added agent 
novice2 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
expert to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
novice1 to team TestTeam
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
novice2 to team TestTeam
_______________ test_vote_on_critical_decision_no_votes_succeeds _______________

    def test_vote_on_critical_decision_no_votes_succeeds():
        """Test that vote_on_critical_decision handles the case when no votes 
are cast.
    
        ReqID: N/A"""
        team = WSDETeam(name="TestTeam")
        a1 = _make_agent("a1", vote=None)
        a1.process.return_value = {}
        team.add_agent(a1)
        task = _basic_task()
        result = team.vote_on_critical_decision(task)
>       assert result["voting_initiated"] is True
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'voting_initiated'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting.py:94: KeyError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:45,909 - devsynth.domain.models.wsde_core - INFO - Added agent 
a1 to team TestTeam
------------------------------ Captured log call -------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent a1 to
team TestTeam
_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_initiates_voting_succeed
s _

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0c1a0>

    def test_vote_on_critical_decision_initiates_voting_succeeds(self):
        """Test that vote_on_critical_decision initiates a voting process.
    
        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option3"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
>       assert "voting_initiated" in result
E       assert 'voting_initiated' in {'explanation': "Option 'option3' received 
2 votes out of 4 (50.0%).", 'id': '49c6f557-3507-42c6-8d79-b97f63a75f81', 
'method': 'majority', 'options': ['option1', 'option2', 'option3'], ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:104: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,916 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,916 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,916 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,916 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
_ TestWSDEVotingMechanisms.test_vote_on_critical_decision_majority_vote_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0c650>

    def test_vote_on_critical_decision_majority_vote_succeeds(self):
        """Test that vote_on_critical_decision uses majority vote to make 
decisions.
    
        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option3"})
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert "result" in result
        assert result["result"] is not None
>       assert "winner" in result["result"]
E       assert 'winner' in {'consensus_result': {'explanation': "Partial 
consensus on option 'option2' with 0.0% support after 3 rounds of 
discus...ption3': 0.0}, 'agent4': {'option2': 0.0, 'option3': 0.0}}, ...}, 
'tied': True, 'tied_options': ['option2', 'option3']}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:128: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,926 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,926 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,927 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,927 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
__ TestWSDEVotingMechanisms.test_vote_on_critical_decision_tied_vote_succeeds __

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0cb00>

    def test_vote_on_critical_decision_tied_vote_succeeds(self):
        """Test that vote_on_critical_decision handles tied votes correctly.
    
        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option2"})
        self.team.build_consensus = MagicMock(
            return_value={
                "consensus": "Use a hybrid architecture combining microservices 
and monolith",
                "contributors": ["agent1", "agent2", "agent3", "agent4"],
                "method": "consensus_synthesis",
                "reasoning": "Combined the best elements from both options",
            }
        )
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert "result" in result
>       assert "tied" in result["result"]
E       AssertionError: assert 'tied' in {'method': 'majority_vote', 'winner': 
'option1'}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:155: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,937 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,937 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,937 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,937 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
_ TestWSDEVotingMechanisms.test_vote_on_critical_decision_weighted_vote_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0cfb0>

    def test_vote_on_critical_decision_weighted_vote_succeeds(self):
        """Test that vote_on_critical_decision uses weighted voting for 
domain-specific decisions.
    
        ReqID: N/A"""
        self.agent1.config.parameters = {
            "expertise": ["security", "encryption", "authentication"],
            "expertise_level": "expert",
        }
        self.agent2.config.parameters = {
            "expertise": ["security", "firewalls"],
            "expertise_level": "intermediate",
        }
        self.agent3.config.parameters = {
            "expertise": ["security"],
            "expertise_level": "novice",
        }
        self.agent4.config.parameters = {
            "expertise": ["python", "javascript"],
            "expertise_level": "intermediate",
        }
        self.agent1.process = MagicMock(return_value={"vote": "option2"})
        self.agent2.process = MagicMock(return_value={"vote": "option1"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option3"})
        result = self.team.vote_on_critical_decision(self.domain_task)
        assert "result" in result
        assert result["result"] is not None
        if "winner" in result["result"]:
>           assert result["result"]["winner"] == "option2"
E           AssertionError: assert 'option1' == 'option2'
E             
E             - option2
E             ?       ^
E             + option1
E             ?       ^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:202: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,949 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,949 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,949 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,949 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_records_results_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0d460>

    def test_vote_on_critical_decision_records_results_succeeds(self):
        """Test that vote_on_critical_decision records the voting results.
    
        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
>       assert "voting_initiated" in result
E       assert 'voting_initiated' in {'explanation': "Option 'option1' received 
2 votes out of 4 (50.0%).", 'id': '6db6b4df-ab6a-45f5-bec8-192505867069', 
'method': 'majority', 'options': ['option1', 'option2', 'option3'], ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:222: AssertionError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,959 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,959 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,959 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,959 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
_ 
TestWSDEVotingMechanisms.test_vote_on_critical_decision_updates_history_succeeds
_

self = <tests.unit.general.test_wsde_voting_mechanisms.TestWSDEVotingMechanisms 
object at 0x119c0d910>

    def test_vote_on_critical_decision_updates_history_succeeds(self):
        """Ensure voting history is recorded after a vote.
    
        ReqID: N/A"""
        self.agent1.process = MagicMock(return_value={"vote": "option1"})
        self.agent2.process = MagicMock(return_value={"vote": "option2"})
        self.agent3.process = MagicMock(return_value={"vote": "option1"})
        self.agent4.process = MagicMock(return_value={"vote": "option1"})
        result = self.team.vote_on_critical_decision(self.critical_task)
        assert len(self.team.voting_history) == 1
        entry = self.team.voting_history[0]
>       assert entry["task_id"] == self.team._get_task_id(self.critical_task)
                                   ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'WSDETeam' object has no attribute '_get_task_id'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/general/test_wsd
e_voting_mechanisms.py:245: AttributeError
---------------------------- Captured stdout setup -----------------------------
2025-10-29 10:48:45,969 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent1 to team test_voting_mechanisms_team
2025-10-29 10:48:45,969 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent2 to team test_voting_mechanisms_team
2025-10-29 10:48:45,969 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent3 to team test_voting_mechanisms_team
2025-10-29 10:48:45,969 - devsynth.domain.models.wsde_core - INFO - Added agent 
agent4 to team test_voting_mechanisms_team
------------------------------ Captured log setup ------------------------------
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent1 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent2 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent3 to team test_voting_mechanisms_team
INFO     devsynth.domain.models.wsde_core:logging_setup.py:615 Added agent 
agent4 to team test_voting_mechanisms_team
_____________ test_fastapi_testclient_guard_allows_minimal_request _____________

    def _get_testclient():
        """Lazily import TestClient to avoid MRO issues during collection."""
        global TestClient
        if TestClient is None:
            try:
>               from fastapi.testclient import TestClient

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
gent_api_fastapi_guard.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/fastapi/testclient.py:1: in <module>
    from starlette.testclient import TestClient as TestClient  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError

During handling of the above exception, another exception occurred:

    def test_fastapi_testclient_guard_allows_minimal_request():
        """Ensure a minimal FastAPI app responds through the TestClient."""
    
        app = FastAPI()
    
        route = ensure_typed_decorator(
            cast(Callable[[Callable[..., Any]], Any], app.get("/ping"))
        )
    
        @route
        def ping() -> dict[str, str]:
            return {"status": "ok"}
    
>       client = _get_testclient()(app)
                 ^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
gent_api_fastapi_guard.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
gent_api_fastapi_guard.py:27: in _get_testclient
    from starlette.testclient import TestClient
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError
_______________ test_enhanced_init_endpoint_returns_typed_error ________________

enhanced_api = <module 'devsynth.interface.agentapi_enhanced' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/age
ntapi_enhanced.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_enhanced_init_endpoint_re0')

    @pytest.mark.fast
    def test_enhanced_init_endpoint_returns_typed_error(enhanced_api, tmp_path):
        """Initialization failures surface typed error payloads.",
    
        ReqID: N/A"""
    
        request = SimpleNamespace(client=SimpleNamespace(host="9.9.9.9"))
        init_request = InitRequest(path=str(tmp_path / "does-not-exist"))
    
>       with pytest.raises(enhanced_api.HTTPException) as exc:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'fastapi.exceptions.HTTPException'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_a
pi_endpoints.py:451: Failed
_____________________ test_display_result_logging_branches _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ba48da0>

    @pytest.mark.fast
    def test_display_result_logging_branches(monkeypatch):
        """display_result should use appropriate logging level and output; error
path uses handler."""
        import devsynth.interface.cli as cli_mod
    
        # Patch logger methods to capture level usage
        debug_mock = MagicMock()
        info_mock = MagicMock()
        warn_mock = MagicMock()
        error_mock = MagicMock()
        monkeypatch.setattr(cli_mod.logger, "debug", debug_mock)
        monkeypatch.setattr(cli_mod.logger, "info", info_mock)
        monkeypatch.setattr(cli_mod.logger, "warning", warn_mock)
        monkeypatch.setattr(cli_mod.logger, "error", error_mock)
    
        # Patch console printing and formatting
        print_mock = MagicMock()
        monkeypatch.setattr("rich.console.Console.print", print_mock)
    
        # Patch SharedBridgeMixin formatter to a simple pass-through marker
        monkeypatch.setattr(
            "devsynth.interface.shared_bridge.SharedBridgeMixin._format_for_outp
ut",
            lambda self, message, *, highlight=False, message_type=None: 
f"FMT:{message}",
        )
    
        bridge = CLIUXBridge()
    
        # Warning branch
        bridge.display_result("warn-msg", message_type="warning")
        warn_mock.assert_called()  # logged at warning
        print_mock.assert_called()  # printed something
        args, kwargs = print_mock.call_args
>       assert args[0] == "FMT:warn-msg"
E       AssertionError: assert 'warn-msg' == 'FMT:warn-msg'
E         
E         - FMT:warn-msg
E         ? ----
E         + warn-msg

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_c
li_uxbridge_noninteractive.py:77: AssertionError
________________ test_cliuxbridge_sanitizes_script_tag_succeeds ________________

clean_state = None

    @pytest.mark.fast
    def test_cliuxbridge_sanitizes_script_tag_succeeds(clean_state):
        """Test that cliuxbridge sanitizes output succeeds.
    
        ReqID: N/A"""
        bridge = CLIUXBridge()
        with patch("rich.console.Console.print") as out:
            bridge.display_result("<script>alert('x')</script>Hello")
            out.assert_called_once()
            printed_text = out.call_args[0][0]
>           assert getattr(printed_text, "plain", str(printed_text)) == "Hello"
E           AssertionError: assert '<script>aler.../script>Hello' == 'Hello'
E             
E             - Hello
E             + <script>alert('x')</script>Hello

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_o
utput_sanitization.py:28: AssertionError
__________________________ test_print_alias_delegates __________________________

    def test_print_alias_delegates():
        bridge = DummyBridge()
>       bridge.print("msg", highlight=True)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_u
xbridge_aliases.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.unit.interface.test_uxbridge_aliases.DummyBridge object at 
0x13bcf20f0>
message = 'msg'

    def print(
        self,
        message: str,
        *,
        highlight: bool = False,
        message_type: str | None = None,
    ) -> None:
        """Backward compatible alias for :meth:`display_result`.
    
        Args:
            message: Message to display to the user.
            highlight: Whether to emphasise the message.
            message_type: Optional semantic type forwarded to
                :meth:`display_result`.
        """
>       self.display_result(message, highlight=highlight, 
message_type=message_type)
E       TypeError: DummyBridge.display_result() got an unexpected keyword 
argument 'message_type'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/ux_b
ridge.py:420: TypeError
___________________ test_lazy_streamlit_forwards_attributes ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcc3260>

    def test_lazy_streamlit_forwards_attributes(monkeypatch: pytest.MonkeyPatch)
-> None:
        """`_LazyStreamlit` proxies attribute lookups through 
`_require_streamlit`."""
    
        calls: list[tuple[str, str | None]] = []
    
        class SentinelStreamlit:
            def header(self, text: str) -> str:
                calls.append(("header", text))
                return f"header::{text}"
    
        sentinel = SentinelStreamlit()
    
        def fake_require_streamlit() -> SentinelStreamlit:
            calls.append(("require", None))
            return sentinel
    
        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
>       monkeypatch.setattr(webui, "_require_streamlit", fake_require_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_require_streamlit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:318: AttributeError
__________________ test_require_streamlit_guidance_and_cache ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ba4b020>

    def test_require_streamlit_guidance_and_cache(monkeypatch: 
pytest.MonkeyPatch) -> None:
        """Lazy loader emits install guidance once and caches the module."""
    
        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
        sentinel = SimpleNamespace(name="streamlit-sentinel")
        import_attempts: list[str] = []
    
        def import_once(name: str) -> SimpleNamespace:
            import_attempts.append(name)
            return sentinel
    
>       monkeypatch.setattr(webui.importlib, "import_module", import_once)
                            ^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'importlib'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:345: AttributeError
____________ test_ask_question_and_confirm_choice_respects_defaults ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd84d0>

    def test_ask_question_and_confirm_choice_respects_defaults(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Select boxes and checkboxes mirror CLI defaults without Streamlit."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:384: TypeError
_____________ test_display_result_routes_error_and_highlight_paths _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcdb950>

    def test_display_result_routes_error_and_highlight_paths(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Error prompts, highlights, headers, and markdown obey spec 
routing."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:460: TypeError
______________ test_display_result_handles_multiple_message_types ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bbcf800>

    def test_display_result_handles_multiple_message_types(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Warnings, successes, info, and unknown types route to the right 
channels."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:546: TypeError
____________ test_display_result_info_and_error_fallbacks_sanitize _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c0275c0>

    def test_display_result_info_and_error_fallbacks_sanitize(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Missing Streamlit channels fall back to ``write`` with sanitized 
payloads."""
    
        monkeypatch.delattr(BehaviorStreamlitStub, "info")
        stub = install_streamlit_stub(monkeypatch)
    
        sanitized_inputs: list[str] = []
    
        def fake_sanitize(text: str) -> str:
            sanitized_inputs.append(text)
            return text.replace("<", "&lt;").replace(">", "&gt;")
    
>       monkeypatch.setattr(webui, "sanitize_output", fake_sanitize)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'sanitize_output'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:591: AttributeError
________________ test_display_result_markup_fallback_uses_write ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c025b80>

    def test_display_result_markup_fallback_uses_write(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Markdown rendering falls back to ``write`` when ``markdown`` is 
absent."""
    
        monkeypatch.delattr(BehaviorStreamlitStub, "markdown")
        stub = install_streamlit_stub(monkeypatch)
    
        sanitized_inputs: list[str] = []
    
        def fake_sanitize(text: str) -> str:
            sanitized_inputs.append(text)
            return text.replace("<", "&lt;")
    
>       monkeypatch.setattr(webui, "sanitize_output", fake_sanitize)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'sanitize_output'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:637: AttributeError
______________ test_display_result_error_prefix_triggers_guidance ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc73830>

    def test_display_result_error_prefix_triggers_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Error prefixes emit suggestions, docs, warnings, and success 
markers."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:661: TypeError
_______________ test_display_result_covers_all_message_channels ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd1d00>

    def test_display_result_covers_all_message_channels(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Sanitized conversion and every message channel execute as 
specified."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:708: TypeError
____________________ test_render_traceback_captures_output _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd1190>

    def test_render_traceback_captures_output(monkeypatch: pytest.MonkeyPatch) 
-> None:
        """Traceback rendering opens an expander and streams the code block."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:860: TypeError
____________________ test_error_mapping_helpers_cover_cases ____________________

    def test_error_mapping_helpers_cover_cases() -> None:
        """Error type and helper tables provide consistent guidance across 
keywords."""
    
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:882: TypeError
___________________ test_ui_progress_estimates_and_subtasks ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd3c50>

    def test_ui_progress_estimates_and_subtasks(monkeypatch: pytest.MonkeyPatch)
-> None:
        """Progress lifecycle mirrors CLI telemetry with sanitized subtasks."""
    
        stub = install_streamlit_stub(monkeypatch)
        times = count(start=0, step=10)
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:932: AttributeError
__________ test_ui_progress_complete_cascades_and_falls_back_to_write __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd1e80>

    def test_ui_progress_complete_cascades_and_falls_back_to_write(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Completing the root task finalizes subtasks and falls back when 
``success`` is absent."""
    
        monkeypatch.delattr(BehaviorStreamlitStub, "success")
        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 20))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1038: AttributeError
______________________ test_ui_progress_eta_formats_hours ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd8ef0>

    def test_ui_progress_eta_formats_hours(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """Long running tasks display hour-level ETAs."""
    
        stub = install_streamlit_stub(monkeypatch)
        times = iter([0, 1000, 2000, 3000, 4000, 5000])
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1071: AttributeError
___________ test_ui_progress_status_transitions_cover_all_thresholds ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bca9220>

    def test_ui_progress_status_transitions_cover_all_thresholds(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Status messages progress from starting through completion with 
sanitized text."""
    
        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 40))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1093: AttributeError
_____________________ test_ui_progress_eta_minutes_branch ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcf0050>

    def test_ui_progress_eta_minutes_branch(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """Slow progress projections report minute-level ETAs."""
    
        stub = install_streamlit_stub(monkeypatch)
        times = iter(range(0, 10))
>       monkeypatch.setattr(webui.time, "time", lambda: next(times))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1159: AttributeError
________________ test_get_layout_config_breakpoints[500-1-True] ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bce6a50>
screen_width = 500, expected_columns = 1, expected_mobile = True

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1191: TypeError
_______________ test_get_layout_config_breakpoints[800-2-False] ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bbbba40>
screen_width = 800, expected_columns = 2, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1191: TypeError
_______________ test_get_layout_config_breakpoints[1300-3-False] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c0080b0>
screen_width = 1300, expected_columns = 3, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1191: TypeError
______________ test_get_layout_config_breakpoints[absent-3-False] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc97cb0>
screen_width = 'absent', expected_columns = 3, expected_mobile = False

    @pytest.mark.parametrize(
        ("screen_width", "expected_columns", "expected_mobile"),
        [
            (500, 1, True),
            (800, 2, False),
            (1300, 3, False),
            ("absent", 3, False),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch,
        screen_width: int | str,
        expected_columns: int,
        expected_mobile: bool,
    ) -> None:
        """Layout config responds to breakpoints and missing measurements."""
    
        stub = install_streamlit_stub(monkeypatch)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1191: TypeError
_______________ test_run_responsive_layout_and_router_invocation _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c0801d0>

    def test_run_responsive_layout_and_router_invocation(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """`run()` applies defaults, injects resize JS, and invokes the 
router."""
    
        stub = install_streamlit_stub(monkeypatch)
    
        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self: 
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1220: AttributeError
________________________ test_run_handles_html_failure _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080950>

    def test_run_handles_html_failure(monkeypatch: pytest.MonkeyPatch) -> None:
        """HTML injection failures surface as display_result messages."""
    
        stub = install_streamlit_stub(monkeypatch)
        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self: 
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1283: AttributeError
______________________ test_run_handles_page_config_error ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080980>

    def test_run_handles_page_config_error(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """Configuration errors surface as WebUI messages without router 
execution."""
    
        stub = install_streamlit_stub(monkeypatch)
        stub.page_config_error = RuntimeError("No display")
        navigation = {"Home": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self: 
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1333: AttributeError
__________________ test_run_without_components_invokes_router __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080d40>

    def test_run_without_components_invokes_router(monkeypatch: 
pytest.MonkeyPatch) -> None:
        """Router still runs when components module is absent."""
    
        stub = install_streamlit_stub(monkeypatch)
        stub.components = None
        navigation = {"Docs": lambda: None}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self: 
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1383: AttributeError
__________________ test_ensure_router_caches_router_instance ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ba4a840>

    def test_ensure_router_caches_router_instance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """`_ensure_router` only instantiates the router once."""
    
        install_streamlit_stub(monkeypatch)
        init_calls: list[str] = []
    
        class RouterSpy:
            def __init__(
                self, owner: webui.WebUI, pages: dict[str, Callable[[], None]]
            ) -> None:
                init_calls.append("init")
    
            def run(self) -> None:  # pragma: no cover - not exercised in this 
test
                raise AssertionError("run should not be called")
    
        monkeypatch.setattr(webui, "Router", RouterSpy)
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1426: TypeError
_________________ test_run_module_entrypoint_invokes_webui_run _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080110>

    def test_run_module_entrypoint_invokes_webui_run(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The module-level run() helper instantiates WebUI and executes 
run()."""
    
        install_streamlit_stub(monkeypatch)
        call_sequence: list[str] = []
    
>       class Runner(webui.WebUI):
E       TypeError: NoneType takes no arguments

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_behavior_checklist_fast.py:1450: TypeError
_____________ test_webui_run_registers_router_and_hydrates_session _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080da0>

    @pytest.mark.fast
    def test_webui_run_registers_router_and_hydrates_session(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
        recorded: dict[str, object] = {}
    
        class DummyRouter:
            def __init__(
                self, ui, pages
            ) -> None:  # noqa: ANN001 - interface dictated by Router
                recorded["ui"] = ui
                recorded["pages"] = dict(pages)
    
            def run(self) -> None:
                recorded["ran"] = True
    
>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:26: AttributeError
_______________ test_webui_command_dispatch_invokes_cli_targets ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c080bf0>

    @pytest.mark.fast
    def test_webui_command_dispatch_invokes_cli_targets(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:45: AttributeError
_______________ test_webui_command_dispatch_reports_value_errors _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcfb980>

    @pytest.mark.fast
    def test_webui_command_dispatch_reports_value_errors(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        dummy_streamlit = DummyStreamlit()
>       monkeypatch.setattr(webui, "st", dummy_streamlit)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute 'st'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bootstrap_fast.py:70: AttributeError
__________ test_z_progress_indicator_extensive_paths_cover_hierarchy ___________

bridge_live = namespace(module=<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c0271d0>

    def test_z_progress_indicator_extensive_paths_cover_hierarchy(
        bridge_live: SimpleNamespace, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        """SpecRef: docs/specifications/webui-integration.md Progress 
indicators;
        docs/developer_guides/progress_indicators.md Testing with Progress 
Indicators.
    
        Exercises fallback descriptions, nested subtasks, and default status
        thresholds so the hierarchy mirrors the documented UX affordances.
        """
    
        bridge = bridge_live.module
    
        class Explodes:
            def __str__(self) -> str:
                raise ValueError("cannot stringify")
    
        sanitize_inputs: list[str] = []
    
        def fake_sanitize(value: str) -> str:
            sanitize_inputs.append(value)
            if value == "raise":
                raise ValueError("boom")
            return f"san::{value}"
    
        monkeypatch.setattr(bridge, "sanitize_output", fake_sanitize)
        tick = iter(range(1000))
        monkeypatch.setattr(bridge.time, "time", lambda: next(tick))
    
        indicator = bridge.WebUIProgressIndicator("start", 12)
    
        indicator.update(description="step-1", status="ok", advance=2)
        assert indicator._description == "san::step-1"
        assert indicator._status == "san::ok"
    
        indicator.update(description=Explodes(), advance=0)
        assert indicator._description == "san::step-1"
    
        indicator.update(status="raise", advance=0)
>       assert indicator._status == "In progress..."
E       AssertionError: assert 'Starting...' == 'In progress...'
E         
E         - In progress...
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_aa_coverage.py:66: AssertionError
__________ test_z_bridge_accessors_and_wizard_paths_cover_invariants ___________

bridge_live = namespace(module=<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcd0bf0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13c21bdd0>

    def test_z_bridge_accessors_and_wizard_paths_cover_invariants(
        bridge_live: SimpleNamespace,
        monkeypatch: pytest.MonkeyPatch,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """SpecRef: 
docs/implementation/requirements_wizard_wizardstate_integration.md Summary;
        docs/implementation/webui_invariants.md Bounded Step Navigation;
        docs/implementation/output_formatter_invariants.md Formatting Rules.
    
        Wizard navigation, session helpers, and Streamlit display channels 
follow
        the published invariants and reuse the shared OutputFormatter pipeline.
        """
    
        bridge = bridge_live.module
        bridge.st = None
        bridge._require_streamlit()
        assert bridge.st is bridge_live.streamlit
    
        ui = bridge.WebUIBridge()
    
        with caplog.at_level("WARNING"):
            assert (
                bridge.WebUIBridge.adjust_wizard_step("1", direction="next", 
total=2) == 1
            )
        caplog.clear()
        with caplog.at_level("WARNING"):
            assert (
                bridge.WebUIBridge.adjust_wizard_step(0, direction="back", 
total="bad") == 0
            )
        caplog.clear()
        with caplog.at_level("WARNING"):
            assert (
                bridge.WebUIBridge.adjust_wizard_step(0, direction="sideways", 
total=1) == 0
            )
    
        assert bridge.WebUIBridge.normalize_wizard_step(1.2, total=3) == 1
        assert bridge.WebUIBridge.normalize_wizard_step(" 2 ", total=3) == 2
        caplog.clear()
        with caplog.at_level("WARNING"):
            assert bridge.WebUIBridge.normalize_wizard_step("bad", total=3) == 0
    
        assert ui.ask_question("Q?", default="answer") == "answer"
        assert ui.confirm_choice("Continue?", default=False) is False
    
        formatter_calls: list[tuple[str, str | None, bool]] = []
    
        def fake_format(
            message: str, message_type: str | None = None, highlight: bool = 
False
        ) -> str:
            formatter_calls.append((message, message_type, highlight))
            return f"{message_type or 'normal'}::{highlight}"
    
        monkeypatch.setattr(
            ui,
            "formatter",
            SimpleNamespace(format_message=fake_format),
            raising=False,
        )
    
        st = bridge_live.streamlit
        for name in ("error", "warning", "success", "info", "write"):
            getattr(st, name).reset_mock()
    
        ui.display_result("serious", message_type="error", highlight=True)
        ui.display_result("heads-up", message_type="warning")
        ui.display_result("victory", message_type="success")
        ui.display_result("note", message_type="info")
        ui.display_result("progress", highlight=True)
        ui.display_result("plain")
    
        assert formatter_calls == [
            ("serious", "error", True),
            ("heads-up", "warning", False),
            ("victory", "success", False),
            ("note", "info", False),
            ("progress", None, True),
            ("plain", None, False),
        ]
        assert st.error.call_args[0][0] == "error::True"
        assert st.warning.call_args[0][0] == "warning::False"
        assert st.success.call_args[0][0] == "success::False"
        assert st.info.call_count == 2
        assert st.info.call_args_list[0][0][0] == "info::False"
        assert st.info.call_args_list[1][0][0] == "normal::True"
        assert st.write.call_args[0][0] == "normal::False"
        assert ui.messages == [
            "error::True",
            "warning::False",
            "success::False",
            "info::False",
            "normal::True",
            "normal::False",
        ]
    
        progress = ui.create_progress("Task", total=3)
        assert isinstance(progress, bridge.WebUIProgressIndicator)
    
        st.session_state.clear()
>       manager = ui.get_wizard_manager("wiz", steps=2, initial_state={"foo": 
"bar"})
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_aa_coverage.py:258: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager
    return self.create_wizard_manager(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:529: in create_wizard_manager
    from devsynth.interface.wizard_state_manager import WizardStateManager
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Wizard state management coordination.
    
    This module provides a centralized way to manage wizard state,
    ensuring consistency between WebUIBridge and WizardState.
    """
    
    import logging
    from pathlib import Path
    from typing import Any, Dict, Optional
    from collections.abc import Sequence
    
>   from devsynth.config import get_project_config, save_config
E   ImportError: cannot import name 'get_project_config' from 'devsynth.config' 
(unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:13: ImportError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:47,172 - devsynth.interface.webui_bridge - WARNING - Invalid 
total steps: bad, defaulting to 1
2025-10-29 10:48:47,173 - devsynth.interface.webui_bridge - WARNING - Invalid 
direction: sideways, keeping current step
2025-10-29 10:48:47,173 - devsynth.interface.webui_bridge - WARNING - Failed to 
normalize step value 'bad': could not convert string to float: 'bad', defaulting
to 0
2025-10-29 10:48:47,173 - devsynth.interface.webui_bridge - ERROR - WebUI 
displaying error: serious
2025-10-29 10:48:47,173 - devsynth.interface.webui_bridge - WARNING - WebUI 
displaying warning: heads-up
2025-10-29 10:48:47,173 - devsynth.interface.webui_bridge - INFO - WebUI 
displaying success: victory
------------------------------ Captured log call -------------------------------
WARNING  devsynth.interface.webui_bridge:logging_setup.py:615 Invalid total 
steps: bad, defaulting to 1
WARNING  devsynth.interface.webui_bridge:logging_setup.py:615 Invalid direction:
sideways, keeping current step
WARNING  devsynth.interface.webui_bridge:logging_setup.py:615 Failed to 
normalize step value 'bad': could not convert string to float: 'bad', defaulting
to 0
ERROR    devsynth.interface.webui_bridge:logging_setup.py:615 WebUI displaying 
error: serious
WARNING  devsynth.interface.webui_bridge:logging_setup.py:615 WebUI displaying 
warning: heads-up
INFO     devsynth.interface.webui_bridge:logging_setup.py:615 WebUI displaying 
success: victory
__________ test_nested_subtask_handles_fallbacks_and_missing_parents ___________

webui_bridge_module = (<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int...ce/webu
i_bridge.py'>, <module 'streamlit' from <function 
StreamlitRecorder.__getattr__.<locals>._noop at 0x13b1577e0>>)

    def test_nested_subtask_handles_fallbacks_and_missing_parents(
        webui_bridge_module: tuple[ModuleType, StreamlitRecorder],
    ) -> None:
        """Nested subtasks fall back to safe labels and ignore missing 
parents."""
    
        bridge, _ = webui_bridge_module
        indicator = bridge.WebUIProgressIndicator("Main task", 10)
    
        parent_id = indicator.add_subtask(BadString(), total=5)
        parent = indicator._subtasks[parent_id]
>       assert parent["description"] == "<subtask>"
               ^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_fast_suite.py:83: TypeError
________ test_nested_subtask_status_progression_without_explicit_status ________

webui_bridge_module = (<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/int...ce/webu
i_bridge.py'>, <module 'streamlit' from <function 
StreamlitRecorder.__getattr__.<locals>._noop at 0x13b4d1300>>)

    def test_nested_subtask_status_progression_without_explicit_status(
        webui_bridge_module: tuple[ModuleType, StreamlitRecorder],
    ) -> None:
        """Omitting ``status`` triggers the automatic status lifecycle."""
    
        bridge, _ = webui_bridge_module
        indicator = bridge.WebUIProgressIndicator("Main task", 100)
        parent_id = indicator.add_subtask("Parent", total=100)
        nested_id = indicator.add_nested_subtask(parent_id, "Child", total=100)
>       nested = indicator._subtasks[parent_id]["nested_subtasks"][nested_id]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_fast_suite.py:122: TypeError
_____________ test_progress_indicator_nested_tasks_cover_fallbacks _____________

bridge_module = <module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>

    def test_progress_indicator_nested_tasks_cover_fallbacks(bridge_module):
        """Nested subtasks use safe placeholders and default statuses."""
    
        indicator = bridge_module.WebUIProgressIndicator("Task", 4)
    
        parent_id = indicator.add_subtask(RaisingStr(), total=4)
        parent = indicator._subtasks[parent_id]
>       assert parent["description"] == "<subtask>"
               ^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_handshake.py:108: TypeError
____________ test_progress_indicator_status_defaults_and_fallbacks _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13babe2d0>
bridge_module = <module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>

    def test_progress_indicator_status_defaults_and_fallbacks(
        monkeypatch: pytest.MonkeyPatch, bridge_module
    ) -> None:
        """Status strings fall back to defaults and sanitize valid updates."""
    
        monkeypatch.setattr(
            bridge_module, "sanitize_output", lambda value: f"S:{value}", 
raising=False
        )
        indicator = bridge_module.WebUIProgressIndicator("Task", 100)
    
        indicator.update(description="Start")
        assert indicator._description == "S:Start"
    
        indicator.update(description=RaisingStr(), status=RaisingStr())
        assert indicator._description == "S:Start"
>       assert indicator._status == "In progress..."
E       AssertionError: assert 'Starting...' == 'In progress...'
E         
E         - In progress...
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_handshake.py:147: AssertionError
____________ test_progress_indicator_subtasks_and_nested_operations ____________

sanitize_spy = <tests.unit.interface.test_webui_bridge_progress._SanitizeSpy 
object at 0x13bcd8170>

    @pytest.mark.fast
    def test_progress_indicator_subtasks_and_nested_operations(
        sanitize_spy: _SanitizeSpy,
    ) -> None:
        """Subtask lifecycle sanitizes text, handles fallbacks, and ignores 
invalid IDs.
    
        ReqID: N/A
        """
    
        indicator = webui_bridge.WebUIProgressIndicator("main", 100)
    
        task_id = indicator.add_subtask("alpha", total=10)
>       assert sanitize_spy.calls[-1] == "alpha"
E       AssertionError: assert 'Starting...' == 'alpha'
E         
E         - alpha
E         + Starting...

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_progress.py:146: AssertionError
___________________ test_nested_subtask_default_status_cycle ___________________

sanitize_spy = <tests.unit.interface.test_webui_bridge_progress._SanitizeSpy 
object at 0x13babf500>

    @pytest.mark.fast
    def test_nested_subtask_default_status_cycle(sanitize_spy: _SanitizeSpy) -> 
None:
        """Nested subtasks update status text according to default progress 
thresholds.
    
        ReqID: N/A
        """
    
        indicator = webui_bridge.WebUIProgressIndicator("root", 100)
    
        subtask_id = indicator.add_subtask("outer", total=100)
        nested_id = indicator.add_nested_subtask(subtask_id, "inner", total=100)
    
        # Descriptions are sanitized through the shared spy helper.
>       assert sanitize_spy.calls[-2:] == ["outer", "inner"]
E       AssertionError: assert ['inner', 'Starting...'] == ['outer', 'inner']
E         
E         At index 0 diff: 'inner' != 'outer'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_progress.py:359: AssertionError
_______________ test_nested_progress_status_defaults_follow_spec _______________

bridge_env = namespace(module=<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13ba43650>

    def test_nested_progress_status_defaults_follow_spec(
        bridge_env: SimpleNamespace, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        """SpecRef: docs/specifications/webui-integration.md Progress 
indicators;
        docs/developer_guides/progress_indicators.md UXBridge Integration.
    
        Nested subtasks surface the documented status thresholds and cascade
        completion through parent subtasks.
        """
    
        bridge = bridge_env.module
        tick = iter(range(1000))
        monkeypatch.setattr(bridge.time, "time", lambda: next(tick))
    
        indicator = bridge.WebUIProgressIndicator("Collect", 100)
        parent_id = indicator.add_subtask("Gather", total=40)
        nested_id = indicator.add_nested_subtask(parent_id, "Validate", total=8)
    
        expectations = [
            (0, "Starting..."),
            (2, "Processing..."),
            (4, "Halfway there..."),
            (6, "Almost done..."),
            (8 * 0.99, "Finalizing..."),
            (8, "Complete"),
        ]
    
        for progress, status in expectations:
>           indicator._subtasks[parent_id]["nested_subtasks"][nested_id][
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                "current"
            ] = progress
E           TypeError: 'SubtaskState' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_spec_alignment.py:90: TypeError
____________ test_wizard_manager_accessors_follow_integration_guide ____________

bridge_env = namespace(module=<module 'devsynth.interface.webui_bridge' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)

    def test_wizard_manager_accessors_follow_integration_guide(
        bridge_env: SimpleNamespace,
    ) -> None:
        """SpecRef: 
docs/implementation/requirements_wizard_wizardstate_integration.md Summary;
        docs/specifications/webui-integration.md Wizard state wiring.
    
        Session-backed managers persist WizardState across calls and guard 
missing
        session_state inputs with DevSynthError.
        """
    
        bridge = bridge_env.module
        bridge_env.streamlit.session_state.clear()
        ui = bridge.WebUIBridge()
    
>       manager = ui.get_wizard_manager(
            "requirements", steps=2, initial_state={"title": "Draft"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_spec_alignment.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager
    return self.create_wizard_manager(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:529: in create_wizard_manager
    from devsynth.interface.wizard_state_manager import WizardStateManager
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Wizard state management coordination.
    
    This module provides a centralized way to manage wizard state,
    ensuring consistency between WebUIBridge and WizardState.
    """
    
    import logging
    from pathlib import Path
    from typing import Any, Dict, Optional
    from collections.abc import Sequence
    
>   from devsynth.config import get_project_config, save_config
E   ImportError: cannot import name 'get_project_config' from 'devsynth.config' 
(unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:13: ImportError
__________ test_webui_bridge_create_wizard_manager_instantiates_stub ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b73be60>

    @pytest.mark.fast
    def test_webui_bridge_create_wizard_manager_instantiates_stub(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
>       import devsynth.interface.wizard_state_manager as wizard_state_module

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_state_fast.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Wizard state management coordination.
    
    This module provides a centralized way to manage wizard state,
    ensuring consistency between WebUIBridge and WizardState.
    """
    
    import logging
    from pathlib import Path
    from typing import Any, Dict, Optional
    from collections.abc import Sequence
    
>   from devsynth.config import get_project_config, save_config
E   ImportError: cannot import name 'get_project_config' from 'devsynth.config' 
(unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:13: ImportError
____________________ test_get_wizard_manager_persists_state ____________________

bridge_under_test = namespace(module=<module 'devsynth.interface.webui_bridge' 
from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui_bridge.py'>, streamlit=<module 'streamlit'>)

    def test_get_wizard_manager_persists_state(
        bridge_under_test: SimpleNamespace,
    ) -> None:
        """Wizard managers reuse the same Streamlit-backed session state.
    
        ReqID: coverage-webui-bridge
        """
    
        bridge = bridge_under_test.module
        bridge_under_test.streamlit.session_state = {}
        ui = bridge.WebUIBridge()
    
>       manager = ui.get_wizard_manager(
            "requirements", steps=3, initial_state={"title": "Draft"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_bridge_targeted.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:507: in get_wizard_manager
    return self.create_wizard_manager(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i_bridge.py:529: in create_wizard_manager
    from devsynth.interface.wizard_state_manager import WizardStateManager
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Wizard state management coordination.
    
    This module provides a centralized way to manage wizard state,
    ensuring consistency between WebUIBridge and WizardState.
    """
    
    import logging
    from pathlib import Path
    from typing import Any, Dict, Optional
    from collections.abc import Sequence
    
>   from devsynth.config import get_project_config, save_config
E   ImportError: cannot import name 'get_project_config' from 'devsynth.config' 
(unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/wiza
rd_state_manager.py:13: ImportError
______________________ test_cli_returns_module_attribute _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b73ae70>

    def test_cli_returns_module_attribute(monkeypatch):
        """ReqID: FR-203 prefer WebUI overrides when resolving commands."""
        dummy = DummyCommands()
    
        def sentinel():
            return "ok"
    
        monkeypatch.setattr(webui, "dummy_cmd", sentinel, raising=False)
>       assert dummy._cli("dummy_cmd") is sentinel
E       AssertionError: assert None is <function 
test_cli_returns_module_attribute.<locals>.sentinel at 0x13b44a7a0>
E        +  where None = _cli('dummy_cmd')
E        +    where _cli = 
<tests.unit.interface.test_webui_commands.DummyCommands object at 
0x13c111bb0>._cli

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_commands.py:40: AssertionError
______________________ test_require_streamlit_lazy_loader ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3e68a0>

    def test_require_streamlit_lazy_loader(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """Lazy ``st`` proxy loads Streamlit only when accessed."""
    
        fake_streamlit = FakeStreamlit()
        monkeypatch.setitem(sys.modules, "streamlit", fake_streamlit)
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_display_and_layout.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________________ test_webui_display_result_highlight_succeeds _________________

mock_streamlit = <module 'streamlit'>, clean_state = None

    def test_webui_display_result_highlight_succeeds(mock_streamlit, 
clean_state):
        """Test that highlighted messages use st.info.
    
        ReqID: N/A"""
        import importlib
    
        from devsynth.interface import webui
    
        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.
    
        The module must have been successfully imported before.
    
        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None
    
        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_________________ test_webui_display_result_error_raises_error _________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_error_raises_error(mock_streamlit):
        """Test that error messages use st.error.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:116: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_warning_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_warning_succeeds(mock_streamlit):
        """Test that warning messages use st.warning.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_success_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_success_succeeds(mock_streamlit):
        """Test that success messages use st.success.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_heading_succeeds __________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_heading_succeeds(mock_streamlit):
        """Test that heading messages use st.header.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_webui_display_result_subheading_succeeds _________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_subheading_succeeds(mock_streamlit):
        """Test that subheading messages use st.subheader.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:180: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_webui_display_result_rich_markup_succeeds ________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_rich_markup_succeeds(mock_streamlit):
        """Test that Rich markup is converted to Markdown/HTML.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_webui_display_result_normal_succeeds ___________________

mock_streamlit = <module 'streamlit'>

    def test_webui_display_result_normal_succeeds(mock_streamlit):
        """Test that normal messages use st.write.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_webui_progress_indicator_succeeds ____________________

mock_streamlit = <module 'streamlit'>

    def test_webui_progress_indicator_succeeds(mock_streamlit):
        """Test the enhanced progress indicator.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_enhanced.py:230: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_get_layout_config_breakpoints[640-expected0] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c8e2540>
screen_width = 640
expected = {'columns': 1, 'content_width': '100%', 'font_size': 'small', 
'is_mobile': True, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen 
width."""
    
        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c8e2540>
stub = namespace(calls=[], session_state=namespace(screen_width=640), 
markdown=<function _make_streamlit_stub.<locals>.record...cals>.method at 
0x13b0c9da0>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b0cac00>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_get_layout_config_breakpoints[820-expected1] _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c992ea0>
screen_width = 820
expected = {'columns': 2, 'content_width': '70%', 'font_size': 'medium', 
'is_mobile': False, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen 
width."""
    
        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c992ea0>
stub = namespace(calls=[], session_state=namespace(screen_width=820), 
markdown=<function _make_streamlit_stub.<locals>.record...cals>.method at 
0x13b3fa8e0>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b771da0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_get_layout_config_breakpoints[1200-expected2] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9914f0>
screen_width = 1200
expected = {'columns': 3, 'content_width': '80%', 'font_size': 'medium', 
'is_mobile': False, ...}

    @pytest.mark.parametrize(
        ("screen_width", "expected"),
        [
            (
                640,
                {
                    "columns": 1,
                    "sidebar_width": "100%",
                    "content_width": "100%",
                    "font_size": "small",
                    "padding": "0.5rem",
                    "is_mobile": True,
                },
            ),
            (
                820,
                {
                    "columns": 2,
                    "sidebar_width": "30%",
                    "content_width": "70%",
                    "font_size": "medium",
                    "padding": "1rem",
                    "is_mobile": False,
                },
            ),
            (
                1200,
                {
                    "columns": 3,
                    "sidebar_width": "20%",
                    "content_width": "80%",
                    "font_size": "medium",
                    "padding": "1.5rem",
                    "is_mobile": False,
                },
            ),
        ],
    )
    def test_get_layout_config_breakpoints(
        monkeypatch: pytest.MonkeyPatch, screen_width, expected
    ):
        """ReqID: N/A - ``WebUI.get_layout_config`` adapts layout by screen 
width."""
    
        stub = _make_streamlit_stub(session_width=screen_width)
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9914f0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13b771120>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b770fe0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
________________ test_display_result_rich_markup_uses_markdown _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9935f0>

    def test_display_result_rich_markup_uses_markdown(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: N/A - Rich markup renders via ``markdown`` with HTML spans."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9935f0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13b41bb00>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b418220>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
________________ test_display_result_error_type_renders_context ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c991790>

    def test_display_result_error_type_renders_context(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: N/A - ``message_type='error'`` surfaces suggestions and 
docs."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c991790>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13b4499e0>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b44bba0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_display_result_message_types[warning-warning] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c92bb90>
message_type = 'warning', expected_method = 'warning'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c92bb90>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13b449620>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b44b600>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
______________ test_display_result_message_types[success-success] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c934da0>
message_type = 'success', expected_method = 'success'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c934da0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae8c400>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae8e8e0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_________________ test_display_result_message_types[info-info] _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c935250>
message_type = 'info', expected_method = 'info'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c935250>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae8fc40>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae8fb00>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_____________ test_display_result_message_types[unexpected-write] ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c990e00>
message_type = 'unexpected', expected_method = 'write'

    @pytest.mark.parametrize(
        ("message_type", "expected_method"),
        [
            ("warning", "warning"),
            ("success", "success"),
            ("info", "info"),
            ("unexpected", "write"),
        ],
    )
    def test_display_result_message_types(
        monkeypatch: pytest.MonkeyPatch, message_type, expected_method
    ) -> None:
        """ReqID: N/A - ``display_result`` delegates to Streamlit per type."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c990e00>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae8fd80>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae8d6c0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
___________________ test_display_result_highlight_uses_info ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c833350>

    def test_display_result_highlight_uses_info(monkeypatch: pytest.MonkeyPatch)
-> None:
        """ReqID: N/A - Highlighting without type uses ``info`` output."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c833350>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae137e0>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae109a0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
____________________ test_display_result_defaults_to_write _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c871bb0>

    def test_display_result_defaults_to_write(monkeypatch: pytest.MonkeyPatch) 
-> None:
        """ReqID: N/A - Plain messages fall back to ``write`` output."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c871bb0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae13060>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae13e20>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_______ test_display_result_renders_headings[# Overview-expected_calls0] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c8708c0>
message = '# Overview', expected_calls = [('header', ('Overview',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c8708c0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae13100>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae116c0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_______ test_display_result_renders_headings[## Section-expected_calls1] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86d4c0>
message = '## Section', expected_calls = [('subheader', ('Section',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86d4c0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13ae8f880>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13ae8f9c0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
_____ test_display_result_renders_headings[### Deep Dive-expected_calls2] ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86f2c0>
message = '### Deep Dive'
expected_calls = [('markdown', ('**Deep Dive**',), {})]

    @pytest.mark.parametrize(
        ("message", "expected_calls"),
        [
            ("# Overview", [("header", ("Overview",), {})]),
            ("## Section", [("subheader", ("Section",), {})]),
            ("### Deep Dive", [("markdown", ("**Deep Dive**",), {})]),
        ],
    )
    def test_display_result_renders_headings(
        monkeypatch: pytest.MonkeyPatch, message: str, expected_calls
    ) -> None:
        """ReqID: N/A - Markdown headings map onto Streamlit helpers."""
    
        stub = _make_streamlit_stub()
>       _install_streamlit_stub(monkeypatch, stub)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:230: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c86f2c0>
stub = namespace(calls=[], session_state=namespace(screen_width=1200), 
markdown=<function _make_streamlit_stub.<locals>.recor...cals>.method at 
0x13b44b920>, subheader=<function 
_make_streamlit_stub.<locals>.record.<locals>.method at 0x13b44bce0>)

    def _install_streamlit_stub(
        monkeypatch: pytest.MonkeyPatch, stub: SimpleNamespace
    ) -> None:
        """Patch :mod:`devsynth.interface.webui` to use the provided Streamlit 
stub."""
    
>       monkeypatch.setattr(webui_module, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_layout_and_display_branching.py:52: AttributeError
____________________ test_lazy_streamlit_proxy_imports_once ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c07cad0>

    def test_lazy_streamlit_proxy_imports_once(monkeypatch: pytest.MonkeyPatch) 
-> None:
        """``st`` forwards attribute access while caching the loaded module."""
    
        sentinel = SimpleNamespace(marker="streamlit-sentinel")
        imports: list[str] = []
    
        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
    
        def fake_import(name: str) -> SimpleNamespace:
            imports.append(name)
            return sentinel
    
>       monkeypatch.setattr(webui.importlib, "import_module", fake_import)
                            ^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'importlib'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:107: AttributeError
____________________ test_ui_progress_tracks_status_and_eta ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8660f0>

    def test_ui_progress_tracks_status_and_eta(monkeypatch: pytest.MonkeyPatch) 
-> None:
        """Progress lifecycle updates sanitize text, surface ETA, and mark 
completion."""
    
        stub = DummyStreamlit()
        monkeypatch.setattr(webui, "st", stub, raising=False)
    
        time_ticks = count(start=100, step=10)
>       monkeypatch.setattr(webui.time, "time", lambda: next(time_ticks))
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:126: AttributeError
__________________ test_ensure_router_creates_single_instance __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b866060>

    def test_ensure_router_creates_single_instance(monkeypatch: 
pytest.MonkeyPatch) -> None:
        """Router initialization defers until first access and memoizes the 
instance."""
    
        created: list[tuple[webui.WebUI, dict[str, str]]] = []
    
        class RecordingRouter:
            def __init__(self, owner: webui.WebUI, navigation: dict[str, str]) 
-> None:
                created.append((owner, navigation))
                self.owner = owner
                self.navigation = navigation
    
        monkeypatch.setattr(webui, "Router", RecordingRouter)
    
        navigation = {"Home": "render_home", "Docs": "render_docs"}
>       monkeypatch.setattr(webui.WebUI, "navigation_items", lambda self: 
navigation)
E       AttributeError: None has no attribute 'navigation_items'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_loader_fast.py:189: AttributeError
_______________ test_missing_streamlit_surfaces_install_guidance _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc4b740>

    def test_missing_streamlit_surfaces_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Import failures raise :class:`DevSynthError` with actionable 
guidance."""
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_progress_suggestions_fast.py:140: AttributeError
_____________________ test_lazy_streamlit_import_is_cached _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c1bb410>

    def test_lazy_streamlit_import_is_cached(monkeypatch: pytest.MonkeyPatch) ->
None:
        """Feature: webui_core.feature Scenario: Lazy Streamlit loader caches 
module."""
    
        from devsynth.interface import webui
    
        call_log: list[str] = []
        streamlit_stub = make_streamlit_mock()
    
        original_import = importlib.import_module
    
        def fake_import(name: str, package: str | None = None):
            call_log.append(name)
            if name == "streamlit":
                return streamlit_stub
            return original_import(name, package)
    
        monkeypatch.setattr(importlib, "import_module", fake_import)
    
>       module = importlib.reload(webui)
                 ^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_lazy_streamlit_and_wizard.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.
    
        The module must have been successfully imported before.
    
        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None
    
        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
___________ test_ui_progress_eta_displays_seconds_when_under_minute ____________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c293170>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_seconds_when_under_minute(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA in seconds when less than a minute remains."""
    
>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 5.0, 10.0],
            description="ETA Seconds",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:244: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_ui_progress_eta_displays_minutes_when_under_hour _____________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3852e0>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_minutes_when_under_hour(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA rounded to whole minutes when under an hour remains."""
    
>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 100.0, 200.0],
            description="ETA Minutes",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:263: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________ test_ui_progress_eta_displays_hours_and_minutes ________________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c387fb0>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_eta_displays_hours_and_minutes(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Render ETA with hours and minutes when exceeding an hour."""
    
>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 100.0, 200.0],
            description="ETA Hours",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_________ test_ui_progress_status_transitions_without_explicit_status __________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13baca0c0>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_status_transitions_without_explicit_status(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Ensure automatic status text transitions at documented thresholds."""
    
>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
            description="Status Thresholds",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:301: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_ui_progress_subtasks_update_with_frozen_time _______________

mock_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3bb620>
clean_state = None

    @pytest.mark.fast
    def test_ui_progress_subtasks_update_with_frozen_time(
        mock_streamlit, monkeypatch, clean_state
    ):
        """Subtask updates still render when the clock is frozen."""
    
>       progress, _, time_container = _init_progress_with_time(
            mock_streamlit,
            monkeypatch,
            times=[100.0],
            description="Frozen Subtasks",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:335: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:52: in _init_progress_with_time
    module = _get_webui_module()
             ^^^^^^^^^^^^^^^^^^^
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_progress.py:16: in _get_webui_module
    import devsynth.interface.webui as webui
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________ TestProjectSetupPages.test_project_setup_pages_inheritance __________

self = <tests.unit.interface.test_webui_rendering.TestProjectSetupPages object 
at 0x11dbc2d20>

    def test_project_setup_pages_inheritance(self):
        """Test that ProjectSetupPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:98: ImportError
_____________ TestLifecyclePages.test_lifecycle_pages_inheritance ______________

self = <tests.unit.interface.test_webui_rendering.TestLifecyclePages object at 
0x11dbc2060>

    def test_lifecycle_pages_inheritance(self):
        """Test that LifecyclePages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:129: ImportError
____________ TestOperationsPages.test_operations_pages_inheritance _____________

self = <tests.unit.interface.test_webui_rendering.TestOperationsPages object at 
0x11dbd4500>

    def test_operations_pages_inheritance(self):
        """Test that OperationsPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:161: ImportError
_______________ TestSupportPages.test_support_pages_inheritance ________________

self = <tests.unit.interface.test_webui_rendering.TestSupportPages object at 
0x11dbd52b0>

    def test_support_pages_inheritance(self):
        """Test that SupportPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:192: ImportError
________ TestWebUIRenderingUtilities.test_rendering_import_dependencies ________

self = <tests.unit.interface.test_webui_rendering.TestWebUIRenderingUtilities 
object at 0x11dbd6150>

    def test_rendering_import_dependencies(self):
        """Test that rendering imports work correctly."""
        # Test that key imports are available
>       from devsynth.interface.webui.rendering import (
            LifecyclePages,
            OperationsPages,
            PageRenderer,
            ProjectSetupPages,
            SupportPages,
        )
E       ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_rendering.py:295: ImportError
____________________ test_require_streamlit_returns_module _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9430e0>

    @pytest.mark.fast
    def test_require_streamlit_returns_module(monkeypatch):
        """Successful import returns module.
    
        ReqID: N/A"""
        module = types.SimpleNamespace()
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_require_streamlit.py:16: AttributeError
________________________ test_require_streamlit_raises _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c942e70>

    @pytest.mark.fast
    def test_require_streamlit_raises(monkeypatch):
        """Import failure raises DevSynthError.
    
        ReqID: N/A"""
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_require_streamlit.py:26: AttributeError
___________________ test_requirements_wizard_initialization ____________________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_initialization(stub_streamlit, clean_state):
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________ test_requirements_wizard_step_navigation_succeeds _______________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_step_navigation_succeeds(stub_streamlit, 
clean_state):
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_requirements_wizard_save_requirements_succeeds ______________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_requirements_wizard_save_requirements_succeeds(stub_streamlit, 
clean_state):
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_______________________ test_validate_requirements_step ________________________

stub_streamlit = <module 'streamlit'>

    def test_validate_requirements_step(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
___________________ test_handle_requirements_navigation_next ___________________

stub_streamlit = <module 'streamlit'>

    def test_handle_requirements_navigation_next(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_save_requirements_writes_file ______________________

stub_streamlit = <module 'streamlit'>

    def test_save_requirements_writes_file(stub_streamlit):
>       from devsynth.interface.webui import WebUI

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
__________________ test_priority_persists_through_navigation ___________________

stub_streamlit = <module 'streamlit'>

    def test_priority_persists_through_navigation(stub_streamlit):
        """Priority selection should persist when navigating steps."""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
______________________ test_title_and_description_persist ______________________

stub_streamlit = <module 'streamlit'>

    def test_title_and_description_persist(stub_streamlit):
        """Title and description should persist across navigation."""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_requirements_wizard.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________ test_run_method_with_invalid_navigation_option ________________

stub_streamlit = <module 'streamlit'>, clean_state = None

    def test_run_method_with_invalid_navigation_option(stub_streamlit, 
clean_state):
        """Test the run method with an invalid navigation option.
    
        ReqID: N/A"""
        import importlib
    
        from devsynth.interface import webui
    
        # Reload the module to ensure clean state
>       importlib.reload(webui)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'>

    def reload(module):
        """Reload the module and return it.
    
        The module must have been successfully imported before.
    
        """
        try:
            name = module.__spec__.name
        except AttributeError:
            try:
                name = module.__name__
            except AttributeError:
                raise TypeError("reload() argument must be a module") from None
    
        if sys.modules.get(name) is not module:
>           raise ImportError(f"module {name} not in sys.modules", name=name)
E           ImportError: module devsynth.interface.webui not in sys.modules

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/importlib/__init__.py:111: ImportError
_______________ test_run_method_with_page_exception_raises_error _______________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_page_exception_raises_error(stub_streamlit):
        """Test the run method when a page method raises an exception.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_run_method_with_streamlit_exception_raises_error _____________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9e6ff0>

    def test_run_method_with_streamlit_exception_raises_error(stub_streamlit, 
monkeypatch):
        """Test the run method when streamlit raises an exception.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_run_method_with_sidebar_exception_raises_error ______________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_sidebar_exception_raises_error(stub_streamlit):
        """Test the run method when sidebar raises an exception.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________ test_run_method_with_multiple_exceptions_raises_error _____________

stub_streamlit = <module 'streamlit'>

    def test_run_method_with_multiple_exceptions_raises_error(stub_streamlit):
        """Test the run method when multiple exceptions occur.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
____________________ test_standalone_run_function_succeeds _____________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c5cbd70>

    def test_standalone_run_function_succeeds(stub_streamlit, monkeypatch):
        """Test the standalone run function.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:169: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
________________________ test_run_webui_alias_succeeds _________________________

stub_streamlit = <module 'streamlit'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c533b60>

    def test_run_webui_alias_succeeds(stub_streamlit, monkeypatch):
        """Test the run_webui alias function.
    
        ReqID: N/A"""
        import importlib
    
>       import devsynth.interface.webui as webui

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_run_edge_cases.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Web UI module for DevSynth.
    
    This module provides web interface components for DevSynth.
    """
    
>   from .rendering import (
        LifecyclePages,
        OperationsPages,
        PageRenderer,
        ProjectSetupPages,
        SupportPages,
    )
E   ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/webu
i/__init__.py:7: ImportError
_____________ test_rendering_simulation_records_summary_and_errors _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c533740>

    @pytest.mark.fast
    def test_rendering_simulation_records_summary_and_errors(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """simulate_progress_rendering mirrors CLI telemetry and sanitises 
errors."""
    
        stub = BehaviorStreamlitStub()
        harness = RenderHarness(stub)
    
        summary = {
            "description": "Gather <docs>",
            "progress": 0.75,
            "remaining": 45.0,
            "elapsed": 90.0,
            "eta": 200.0,
            "history": [
                {"status": "Queued <init>", "progress": 0.25, "time": 100.0},
                {"status": "Collecting", "progress": 0.5, "time": 150.0},
            ],
            "checkpoints": [
                {"progress": 0.25, "time": 110.0, "eta": 200.0},
                {"progress": 0.5, "time": 160.0, "eta": 200.0},
            ],
            "subtasks_detail": [
                {
                    "description": "Docs <survey>",
                    "progress": 1.0,
                    "status": "Complete",
                    "history": [
                        {"status": "Scanning", "progress": 0.5, "time": 140.0},
                    ],
                    "checkpoints": [
                        {"progress": 0.5, "time": 145.0, "eta": 150.0},
                    ],
                }
            ],
        }
    
        clock = _LinearClock(start=100.0, step=5.0)
>       result = rendering.simulate_progress_rendering(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            harness,
            summary,
            errors=["<boom & fail>"],
            clock=clock,
        )
E       AttributeError: module 'devsynth.interface.webui.rendering' has no 
attribute 'simulate_progress_rendering'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:84: AttributeError
__________ test_rendering_simulation_handles_nested_summary_and_clock __________

    @pytest.mark.fast
    def test_rendering_simulation_handles_nested_summary_and_clock() -> None:
        """simulate_progress_rendering formats nested history with a scripted 
clock."""
    
        stub = BehaviorStreamlitStub()
        harness = RenderHarness(stub)
        primary = stub.container()
    
        summary = {
            "description": "Main <summary>",
            "progress": 0.5,
            "eta": 250.0,
            "remaining": 120.0,
            "elapsed": 80.0,
            "history": [
                {"status": "Queued <1>", "progress": 0.1, "time": 50.0},
                {
                    "status": "Processing",
                    "completed": 30,
                    "total": 100,
                    "time": 70.0,
                },
            ],
            "checkpoints": [
                {"progress": 0.25, "time": 60.0, "eta": 250.0},
                {"completed": 80, "total": 100, "time": 80.0, "eta": 250.0},
            ],
            "subtasks_detail": [
                {
                    "description": "stage <alpha>",
                    "progress": 0.75,
                    "status": "Working <soon>",
                    "history": [
                        {"status": "Primed <1>", "progress": 0.25, "time": 
40.0},
                        {
                            "status": "Working",
                            "completed": 30,
                            "total": 40,
                            "time": 70.0,
                        },
                    ],
                    "checkpoints": [
                        {"progress": 0.5, "time": 45.0, "eta": 250.0},
                    ],
                },
                {
                    "description": "stage beta",
                    "completed": 20,
                    "total": 40,
                    "status": "Queued",
                    "history": [
                        {
                            "status": "Queued",
                            "completed": 10,
                            "total": 40,
                            "time": 65.0,
                        }
                    ],
                    "checkpoints": [
                        {"completed": 20, "total": 40, "time": 75.0, "eta": 
250.0},
                    ],
                },
            ],
        }
    
        clock = _LinearClock(start=200.0, step=0.0)
>       result = rendering.simulate_progress_rendering(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            harness,
            summary,
            container=primary,
            errors=["<primary>", "<secondary>"],
            clock=clock,
        )
E       AttributeError: module 'devsynth.interface.webui.rendering' has no 
attribute 'simulate_progress_rendering'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:170: AttributeError
____________ test_ui_progress_simulation_drives_eta_and_completion _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c531760>

    @pytest.mark.fast
    def test_ui_progress_simulation_drives_eta_and_completion(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """WebUI._UIProgress under a stubbed Streamlit emits ETA and success 
messages."""
    
        stub = BehaviorStreamlitStub()
        monkeypatch.setattr(webui, "st", stub, raising=False)
        monkeypatch.setattr(webui, "_STREAMLIT", stub, raising=False)
    
        clock_values = iter([0.0, 2.0, 4.0, 6.5, 9.0, 12.0, 15.5, 18.0, 20.0])
    
        def fake_time() -> float:
            try:
                return float(next(clock_values))
            except StopIteration:
                return 20.0
    
>       monkeypatch.setattr(webui.time, "time", fake_time, raising=False)
                            ^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'time'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:244: AttributeError
__________________ test_webui_display_result_sanitises_error ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c9aa240>

    @pytest.mark.fast
    def test_webui_display_result_sanitises_error(monkeypatch: 
pytest.MonkeyPatch) -> None:
        """display_result escapes markup before routing to Streamlit error 
channel."""
    
        stub = BehaviorStreamlitStub()
        monkeypatch.setattr(webui, "st", stub, raising=False)
        monkeypatch.setattr(webui, "_STREAMLIT", stub, raising=False)
    
>       ui = webui.WebUI()
             ^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not callable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:279: TypeError
______________________ test_webui_require_streamlit_cache ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c53a6f0>

    @pytest.mark.fast
    def test_webui_require_streamlit_cache(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """The WebUI lazy loader imports Streamlit once and caches the 
module."""
    
        sentinel = object()
        calls: list[str] = []
    
        def fake_import(name: str) -> object:
            calls.append(name)
            return sentinel
    
        monkeypatch.setattr(webui, "_STREAMLIT", None, raising=False)
        monkeypatch.setattr(importlib, "import_module", fake_import)
    
>       loaded = webui._require_streamlit()
                 ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'devsynth.interface.webui' has no attribute 
'_require_streamlit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_simulations_fast.py:336: AttributeError
____________ test_webui_require_streamlit_reports_install_guidance _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c498620>

    @pytest.mark.fast
    def test_webui_require_streamlit_reports_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Missing Streamlit surfaces actionable guidance."""
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:356: AttributeError
__ test_webui_display_result_sanitizes_without_streamlit[error-kwargs0-error] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c49bbf0>
branch = 'error', kwargs = {'message_type': 'error'}, expected_method = 'error'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed 
Streamlit API."""
    
        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[warning-kwargs1-warning]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6f6420>
branch = 'warning', kwargs = {'message_type': 'warning'}
expected_method = 'warning'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed 
Streamlit API."""
    
        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[success-kwargs2-success]
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6f6840>
branch = 'success', kwargs = {'message_type': 'success'}
expected_method = 'success'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed 
Streamlit API."""
    
        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
_ test_webui_display_result_sanitizes_without_streamlit[highlight-kwargs3-info] 
_

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6f6c90>
branch = 'highlight', kwargs = {'highlight': True}, expected_method = 'info'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("branch", "kwargs", "expected_method"),
        [
            ("error", {"message_type": "error"}, "error"),
            ("warning", {"message_type": "warning"}, "warning"),
            ("success", {"message_type": "success"}, "success"),
            ("highlight", {"highlight": True}, "info"),
        ],
    )
    def test_webui_display_result_sanitizes_without_streamlit(
        monkeypatch: pytest.MonkeyPatch,
        branch: str,
        kwargs: dict[str, object],
        expected_method: str,
    ) -> None:
        """`WebUI.display_result` routes sanitized text through the stubbed 
Streamlit API."""
    
        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:411: AttributeError
______________________ test_webui_ui_progress_eta_formats ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c65c0b0>

    @pytest.mark.fast
    def test_webui_ui_progress_eta_formats(monkeypatch: pytest.MonkeyPatch) -> 
None:
        """The Streamlit UI progress indicator formats ETA strings across 
ranges."""
    
        stub = _WebUIStreamlitStub()
>       monkeypatch.setattr(webui, "_STREAMLIT", stub)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_free_regressions.py:521: AttributeError
_______________ test_missing_streamlit_surfaces_install_guidance _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c6d8cb0>

    def test_missing_streamlit_surfaces_install_guidance(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Import failures raise DevSynthError with installation 
instructions."""
    
>       monkeypatch.setattr(webui, "_STREAMLIT", None)
E       AttributeError: <module 'devsynth.interface.webui' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/interface/web
ui/__init__.py'> has no attribute '_STREAMLIT'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test_w
ebui_streamlit_stub.py:249: AttributeError
__________ TestProjectSetupPages.test_project_setup_pages_inheritance __________

self = <test_rendering.TestProjectSetupPages object at 0x11dca0b60>

    def test_project_setup_pages_inheritance(self):
        """Test that ProjectSetupPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:98: ImportError
_____________ TestLifecyclePages.test_lifecycle_pages_inheritance ______________

self = <test_rendering.TestLifecyclePages object at 0x11dca1940>

    def test_lifecycle_pages_inheritance(self):
        """Test that LifecyclePages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:129: ImportError
____________ TestOperationsPages.test_operations_pages_inheritance _____________

self = <test_rendering.TestOperationsPages object at 0x11dca26c0>

    def test_operations_pages_inheritance(self):
        """Test that OperationsPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:161: ImportError
_______________ TestSupportPages.test_support_pages_inheritance ________________

self = <test_rendering.TestSupportPages object at 0x11dca1eb0>

    def test_support_pages_inheritance(self):
        """Test that SupportPages inherits from CommandHandlingMixin."""
>       from devsynth.interface.webui.commands import CommandHandlingMixin
E       ImportError: cannot import name 'CommandHandlingMixin' from 
'devsynth.interface.webui.commands' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:192: ImportError
________ TestWebUIRenderingUtilities.test_rendering_import_dependencies ________

self = <test_rendering.TestWebUIRenderingUtilities object at 0x11dcc5340>

    def test_rendering_import_dependencies(self):
        """Test that rendering imports work correctly."""
        # Test that key imports are available
>       from devsynth.interface.webui.rendering import (
            LifecyclePages,
            OperationsPages,
            PageRenderer,
            ProjectSetupPages,
            SupportPages,
        )
E       ImportError: cannot import name 'LifecyclePages' from 
'devsynth.interface.webui.rendering' (unknown location)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/webui/
test_rendering.py:295: ImportError
__ TestLMStudioProviderAvailabilityProbing.test_server_availability_detection __

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11dcd9730>

    @pytest.mark.fast
    def test_server_availability_detection(self):
        """Test detection of LM Studio server availability."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            # Mock successful model list response
            mock_lmstudio.sync_api.models.list.return_value = [
                {"id": "model1", "object": "model"},
                {"id": "model2", "object": "model"},
            ]
    
            provider = LMStudioProvider(config)
    
            # Should probe server availability on initialization
>           mock_lmstudio.sync_api.models.list.assert_called_once()

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:470: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='_require_lmstudio().sync_api.models.list' 
id='5297721600'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'list' to have been called once. Called 0 
times.

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/unittest/mock.py:928: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,381 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,382 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,382 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,382 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,383 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,383 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
___ TestLMStudioProviderAvailabilityProbing.test_server_unavailable_handling ___

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11dcd97c0>

    @pytest.mark.fast
    def test_server_unavailable_handling(self):
        """Test handling when LM Studio server is unavailable."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            # Mock connection error
            from requests.exceptions import ConnectionError
    
            mock_lmstudio.sync_api.models.list.side_effect = ConnectionError(
                "Connection refused"
            )
    
            provider = LMStudioProvider(config)
    
            # Should handle gracefully and set server_unavailable flag
>           assert hasattr(provider, "server_unavailable")
E           AssertionError: assert False
E            +  where False = 
hasattr(<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13ba4a600>, 'server_unavailable')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:493: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,409 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,409 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,410 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,410 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,410 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,410 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______ TestLMStudioProviderAvailabilityProbing.test_model_list_retrieval _______

self = <test_lmstudio_provider.TestLMStudioProviderAvailabilityProbing object at
0x11dcd9c40>

    @pytest.mark.fast
    def test_model_list_retrieval(self):
        """Test retrieval of available models list."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        available_models = [
            {"id": "llama-2-7b", "object": "model"},
            {"id": "codellama-7b", "object": "model"},
            {"id": "mistral-7b", "object": "model"},
        ]
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            mock_lmstudio.sync_api.models.list.return_value = available_models
    
            provider = LMStudioProvider(config)
    
            # Should retrieve and store model list
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute 
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:518: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,420 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,420 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,420 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,420 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,421 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,421 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
______ TestLMStudioProviderConfiguration.test_configuration_with_defaults ______

self = <test_lmstudio_provider.TestLMStudioProviderConfiguration object at 
0x11dcda570>

    @pytest.mark.fast
    def test_configuration_with_defaults(self):
        """Test configuration with default values."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            provider = LMStudioProvider(config)
    
            assert provider.temperature == 0.7  # Default
>           assert provider.max_tokens == 4096  # Default
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           assert 1024 == 4096
E            +  where 1024 = 
<devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13bcf3b60>.max_tokens

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:561: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,439 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,439 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,439 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,439 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,439 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,439 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_______ TestLMStudioProviderErrorHandling.test_invalid_temperature_range _______

self = <test_lmstudio_provider.TestLMStudioProviderErrorHandling object at 
0x11dcec230>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            provider = LMStudioProvider(config)
    
            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello", {"temperature": -0.1})
    
>           assert "temperature must be between" in str(exc_info.value)
E           assert 'temperature must be between' in "LM Studio API error: module
'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible."
E            +  where "LM Studio API error: module 'lmstudio' has no attribute 
'llm'. Check that LM Studio is running and accessible." = 
str(LMStudioConnectionError("LM Studio API error: module 'lmstudio' has no 
attribute 'llm'. Check that LM Studio is running and accessible."))
E            +    where LMStudioConnectionError("LM Studio API error: module 
'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible.") = <ExceptionInfo LMStudioConnectionError("LM Studio API error: 
module 'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible.") tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:684: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,483 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,483 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,483 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,483 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,484 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,484 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-29 10:48:50,484 - devsynth.application.llm.lmstudio_provider - ERROR - 
LM Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM 
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM Studio
is running and accessible.
__________ TestLMStudioProviderErrorHandling.test_invalid_max_tokens ___________

self = <test_lmstudio_provider.TestLMStudioProviderErrorHandling object at 
0x11dcec650>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            provider = LMStudioProvider(config)
    
            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello", {"max_tokens": 0})
    
>           assert "max_tokens must be positive" in str(exc_info.value)
E           assert 'max_tokens must be positive' in "LM Studio API error: module
'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible."
E            +  where "LM Studio API error: module 'lmstudio' has no attribute 
'llm'. Check that LM Studio is running and accessible." = 
str(LMStudioConnectionError("LM Studio API error: module 'lmstudio' has no 
attribute 'llm'. Check that LM Studio is running and accessible."))
E            +    where LMStudioConnectionError("LM Studio API error: module 
'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible.") = <ExceptionInfo LMStudioConnectionError("LM Studio API error: 
module 'lmstudio' has no attribute 'llm'. Check that LM Studio is running and 
accessible.") tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:702: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,494 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,494 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,495 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,495 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,495 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,495 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-29 10:48:50,495 - devsynth.application.llm.lmstudio_provider - ERROR - 
LM Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM 
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM Studio
is running and accessible.
_________ TestLMStudioProviderEdgeCases.test_empty_model_list_handling _________

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x11dcda1e0>

    @pytest.mark.fast
    def test_empty_model_list_handling(self):
        """Test handling of empty model list."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            # Mock empty model list
            mock_lmstudio.sync_api.models.list.return_value = []
    
            provider = LMStudioProvider(config)
    
            # Should handle empty model list gracefully
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute 
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:774: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,505 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,505 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,506 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,506 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,506 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,506 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_____________ TestLMStudioProviderEdgeCases.test_timeout_handling ______________

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x11dced5e0>

    @pytest.mark.fast
    def test_timeout_handling(self):
        """Test timeout handling during model listing."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            # Mock timeout during model listing
            import requests
    
            mock_lmstudio.sync_api.models.list.side_effect = (
                requests.exceptions.Timeout("Request timed out")
            )
    
            provider = LMStudioProvider(config)
    
            # Should handle timeout gracefully
>           models = provider.get_available_models()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'LMStudioProvider' object has no attribute 
'get_available_models'. Did you mean: 'list_available_models'?

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:798: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,517 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,517 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,518 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,518 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,518 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,518 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
_________ TestLMStudioProviderEdgeCases.test_unicode_content_handling __________

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13b30c1d0>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.
    
        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation
    
        Returns:
            The generated text
    
        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)
    
        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)
    
        try:
            result = self._execute_with_resilience(
>               self._lmstudio.llm(self.model).complete,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                prompt,
                config=params,
            )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:501: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.lmstudio_provider._AttrForwarder object at 
0x1188023f0>
args = ('qwen/qwen3-4b-2507',), kwargs = {}, real = <module 'lmstudio'>

    def __call__(self, *args, **kwargs):  # pragma: no cover - thin forwarder
        real = self._proxy._ensure()
>       return getattr(real, self._attr)(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'lmstudio' has no attribute 'llm'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:64: AttributeError

During handling of the above exception, another exception occurred:

self = <test_lmstudio_provider.TestLMStudioProviderEdgeCases object at 
0x11dceda60>

    @pytest.mark.fast
    def test_unicode_content_handling(self):
        """Test handling of Unicode content."""
        config = {"base_url": "http://localhost:1234/v1"}
    
        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "test-model",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! ! Hola! ",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10, 
"total_tokens": 15},
        }
    
        with patch(
            "devsynth.application.llm.lmstudio_provider._require_lmstudio"
        ) as mock_require:
            mock_lmstudio = MagicMock()
            mock_require.return_value = mock_lmstudio
    
            mock_lmstudio.sync_api.chat.completions.create.return_value = (
                unicode_response
            )
    
            provider = LMStudioProvider(config)
>           response = provider.generate("Say hello in multiple languages")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_lmstudi
o_provider.py:835: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.lmstudio_provider.LMStudioProvider object at 
0x13b30c1d0>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: dict[str, Any] = None) -> str:
        """Generate text from a prompt using LM Studio.
    
        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation
    
        Returns:
            The generated text
    
        Raises:
            LMStudioConnectionError: If there's an issue connecting to LM Studio
            LMStudioModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)
    
        params = {
            "temperature": self.temperature,
            "maxTokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)
    
        try:
            result = self._execute_with_resilience(
                self._lmstudio.llm(self.model).complete,
                prompt,
                config=params,
            )
            content = getattr(result, "content", None)
            if isinstance(content, str) and content:
                return content
            raise LMStudioModelError("Invalid response from LM Studio")
        except LMStudioModelError:
            raise
        except LMStudioTokenLimitError:
            raise  # Re-raise token limit errors as-is
        except LMStudioConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:  # noqa: BLE001
            error_msg = f"LM Studio API error: {str(e)}. Check that LM Studio is
running and accessible."
            logger.error(error_msg)
>           raise LMStudioConnectionError(error_msg)
E           devsynth.application.llm.lmstudio_provider.LMStudioConnectionError: 
LM Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM 
Studio is running and accessible.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/lmstudio_provider.py:518: LMStudioConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,530 - devsynth.application.llm.lmstudio_provider - INFO - LM
Studio resource disabled; skipping default client configuration
2025-10-29 10:48:50,530 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,530 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,530 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not verify specified model 'stub-model': Failed to connect to LM Studio: 
Network access disabled during tests, falling back to auto-selection
2025-10-29 10:48:50,531 - devsynth.application.llm.lmstudio_provider - ERROR - 
Failed to connect to LM Studio: Network access disabled during tests
2025-10-29 10:48:50,531 - devsynth.application.llm.lmstudio_provider - WARNING -
Could not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
2025-10-29 10:48:50,531 - devsynth.application.llm.lmstudio_provider - ERROR - 
LM Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM 
Studio is running and accessible.
------------------------------ Captured log call -------------------------------
INFO     devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio resource disabled; skipping default client configuration
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not verify specified model 'stub-model': Failed to connect to LM Studio: Network
access disabled during tests, falling back to auto-selection
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Failed 
to connect to LM Studio: Network access disabled during tests
WARNING  devsynth.application.llm.lmstudio_provider:logging_setup.py:615 Could 
not connect to LM Studio: Failed to connect to LM Studio: Network access 
disabled during tests. Using fallback: qwen/qwen3-4b-2507
ERROR    devsynth.application.llm.lmstudio_provider:logging_setup.py:615 LM 
Studio API error: module 'lmstudio' has no attribute 'llm'. Check that LM Studio
is running and accessible.
___ TestOpenAIProviderInitialization.test_initialization_with_default_model ____

self = <test_openai_provider.TestOpenAIProviderInitialization object at 
0x11dd102c0>

    @pytest.mark.fast
    def test_initialization_with_default_model(self):
        """Test initialization with default model."""
        config = {"api_key": "test-key"}
    
        provider = OpenAIProvider(config)
    
>       assert provider.model == "gpt-3.5-turbo"  # Default model
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 'stub-model' == 'gpt-3.5-turbo'
E         
E         - gpt-3.5-turbo
E         + stub-model

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,564 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,564 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
________ TestOpenAIProviderErrorHandling.test_invalid_temperature_range ________

self = <test_openai_provider.TestOpenAIProviderErrorHandling object at 
0x11dd10ce0>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)
    
        with pytest.raises(DevSynthError) as exc_info:
            provider.generate("Hello", {"temperature": -0.1})
    
>       assert "temperature must be between" in str(exc_info.value)
E       AssertionError: assert 'temperature must be between' in 'OpenAI 
temperature must be a number between 0.0 and 2.0, got -0.1'
E        +  where 'OpenAI temperature must be a number between 0.0 and 2.0, got 
-0.1' = str(OpenAIConfigurationError('OpenAI temperature must be a number 
between 0.0 and 2.0, got -0.1'))
E        +    where OpenAIConfigurationError('OpenAI temperature must be a 
number between 0.0 and 2.0, got -0.1') = <ExceptionInfo 
OpenAIConfigurationError('OpenAI temperature must be a number between 0.0 and 
2.0, got -0.1') tblen=3>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:462: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,584 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,584 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
___________ TestOpenAIProviderErrorHandling.test_invalid_max_tokens ____________

self = <test_openai_provider.TestOpenAIProviderErrorHandling object at 
0x11dd110d0>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)
    
        with pytest.raises(DevSynthError) as exc_info:
            provider.generate("Hello", {"max_tokens": 0})
    
>       assert "max_tokens must be positive" in str(exc_info.value)
E       AssertionError: assert 'max_tokens must be positive' in 'OpenAI 
max_tokens must be a positive integer, got 0'
E        +  where 'OpenAI max_tokens must be a positive integer, got 0' = 
str(OpenAIConfigurationError('OpenAI max_tokens must be a positive integer, got 
0'))
E        +    where OpenAIConfigurationError('OpenAI max_tokens must be a 
positive integer, got 0') = <ExceptionInfo OpenAIConfigurationError('OpenAI 
max_tokens must be a positive integer, got 0') tblen=3>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:473: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,594 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,594 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
______________ TestOpenAIProviderHeaders.test_correct_headers_set ______________

self = <test_openai_provider.TestOpenAIProviderHeaders object at 0x11dd28200>

    @pytest.mark.fast
    def test_correct_headers_set(self):
        """Test that correct headers are set for OpenAI."""
        config = {"api_key": "test-key"}
        provider = OpenAIProvider(config)
    
        expected_headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer test-key",
        }
    
        # Verify headers are set correctly
        for key, value in expected_headers.items():
>           assert provider.headers[key] == value
                   ^^^^^^^^^^^^^^^^
E           AttributeError: 'OpenAIProvider' object has no attribute 'headers'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:653: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,652 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,652 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
_____________ TestOpenAIProviderHeaders.test_custom_api_key_header _____________

self = <test_openai_provider.TestOpenAIProviderHeaders object at 0x11dd28620>

    @pytest.mark.fast
    def test_custom_api_key_header(self):
        """Test custom API key header configuration."""
        config = {"api_key": "custom-key"}
        provider = OpenAIProvider(config)
    
>       assert provider.headers["Authorization"] == "Bearer custom-key"
               ^^^^^^^^^^^^^^^^
E       AttributeError: 'OpenAIProvider' object has no attribute 'headers'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:661: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,662 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,662 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
___________ TestOpenAIProviderEdgeCases.test_empty_response_handling ___________

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11dd28ef0>

    @pytest.mark.fast
    def test_empty_response_handling(self):
        """Test handling of empty responses."""
        config = {"api_key": "test-key"}
    
        empty_response = {
            "id": "chatcmpl-empty",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-3.5-turbo",
            "choices": [],
        }
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=empty_response,
                status=200,
            )
    
            provider = OpenAIProvider(config)
    
            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello")
    
>           assert "Invalid response" in str(exc_info.value)
E           assert 'Invalid response' in "OpenAI API error: 'NoneType' object 
has no attribute 'choices'. Check your API key and model configuration."
E            +  where "OpenAI API error: 'NoneType' object has no attribute 
'choices'. Check your API key and model configuration." = 
str(OpenAIConnectionError("OpenAI API error: 'NoneType' object has no attribute 
'choices'. Check your API key and model configuration."))
E            +    where OpenAIConnectionError("OpenAI API error: 'NoneType' 
object has no attribute 'choices'. Check your API key and model configuration.")
= <ExceptionInfo OpenAIConnectionError("OpenAI API error: 'NoneType' object has 
no attribute 'choices'. Check your API key and model configuration.") 
tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:693: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,672 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,672 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
2025-10-29 10:48:50,673 - devsynth.application.llm.openai_provider - ERROR - 
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API 
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI 
API error: 'NoneType' object has no attribute 'choices'. Check your API key and 
model configuration.
_________ TestOpenAIProviderEdgeCases.test_malformed_response_handling _________

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11dd29370>

    @pytest.mark.fast
    def test_malformed_response_handling(self):
        """Test handling of malformed responses."""
        config = {"api_key": "test-key"}
    
        malformed_response = {"invalid": "response", "structure": True}
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=malformed_response,
                status=200,
            )
    
            provider = OpenAIProvider(config)
    
            with pytest.raises(DevSynthError) as exc_info:
                provider.generate("Hello")
    
>           assert "Invalid response" in str(exc_info.value)
E           assert 'Invalid response' in "OpenAI API error: 'NoneType' object 
has no attribute 'choices'. Check your API key and model configuration."
E            +  where "OpenAI API error: 'NoneType' object has no attribute 
'choices'. Check your API key and model configuration." = 
str(OpenAIConnectionError("OpenAI API error: 'NoneType' object has no attribute 
'choices'. Check your API key and model configuration."))
E            +    where OpenAIConnectionError("OpenAI API error: 'NoneType' 
object has no attribute 'choices'. Check your API key and model configuration.")
= <ExceptionInfo OpenAIConnectionError("OpenAI API error: 'NoneType' object has 
no attribute 'choices'. Check your API key and model configuration.") 
tblen=2>.value

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:715: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,683 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,683 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
2025-10-29 10:48:50,683 - devsynth.application.llm.openai_provider - ERROR - 
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API 
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI 
API error: 'NoneType' object has no attribute 'choices'. Check your API key and 
model configuration.
______________ TestOpenAIProviderEdgeCases.test_unicode_handling _______________

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x13baa2930>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenAI.
    
        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation
    
        Returns:
            The generated text
    
        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)
    
        # Validate runtime parameters
        self._validate_runtime_parameters(parameters or {})
    
        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)
    
        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]
    
        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
>           message = getattr(response.choices[0], "message", None)
                              ^^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'choices'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:347: AttributeError

During handling of the above exception, another exception occurred:

self = <test_openai_provider.TestOpenAIProviderEdgeCases object at 0x11dd297f0>

    @pytest.mark.fast
    def test_unicode_handling(self):
        """Test handling of Unicode content."""
        config = {"api_key": "test-key"}
    
        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-3.5-turbo",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! ! Hola! ",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10, 
"total_tokens": 15},
        }
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://api.openai.com/v1/chat/completions",
                json=unicode_response,
                status=200,
            )
    
            provider = OpenAIProvider(config)
>           response = provider.generate("Say hello in multiple languages")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openai_
provider.py:749: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openai_provider.OpenAIProvider object at 
0x13baa2930>
prompt = 'Say hello in multiple languages', parameters = None

    def generate(self, prompt: str, parameters: dict[str, Any] = None) -> str:
        """Generate text from a prompt using OpenAI.
    
        Args:
            prompt: The prompt to generate text from
            parameters: Additional parameters for the generation
    
        Returns:
            The generated text
    
        Raises:
            OpenAIConnectionError: If there's an issue connecting to OpenAI
            OpenAIModelError: If there's an issue with the model or response
            TokenLimitExceededError: If the prompt exceeds the token limit
        """
        # Ensure the prompt doesn't exceed token limits
        self.token_tracker.ensure_token_limit(prompt, self.max_tokens)
    
        # Validate runtime parameters
        self._validate_runtime_parameters(parameters or {})
    
        # Merge default parameters with provided parameters
        params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        if parameters:
            params.update(parameters)
    
        # Prepare the request payload
        messages = [{"role": "user", "content": prompt}]
    
        try:
            response = self._execute_with_resilience(
                self.client.chat.completions.create,
                model=self.model,
                messages=messages,
                **params,
            )
            message = getattr(response.choices[0], "message", None)
            content = getattr(message, "content", None)
            if content is None:
                raise OpenAIModelError("Invalid response from OpenAI")
            return content
        except OpenAIModelError:
            raise
        except OpenAITokenLimitError:
            raise  # Re-raise token limit errors as-is
        except OpenAIConnectionError:
            raise  # Re-raise connection errors as-is
        except Exception as e:
            error_msg = f"OpenAI API error: {str(e)}. Check your API key and 
model configuration."
            logger.error(error_msg)
>           raise OpenAIConnectionError(error_msg)
E           devsynth.application.llm.openai_provider.OpenAIConnectionError: 
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API 
key and model configuration.

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openai_provider.py:361: OpenAIConnectionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,694 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
2025-10-29 10:48:50,694 - devsynth.application.llm.openai_provider - INFO - 
Initialized OpenAI provider with model: stub-model
2025-10-29 10:48:50,694 - devsynth.application.llm.openai_provider - ERROR - 
OpenAI API error: 'NoneType' object has no attribute 'choices'. Check your API 
key and model configuration.
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
INFO     devsynth.application.llm.openai_provider:logging_setup.py:615 
Initialized OpenAI provider with model: stub-model
ERROR    devsynth.application.llm.openai_provider:logging_setup.py:615 OpenAI 
API error: 'NoneType' object has no attribute 'choices'. Check your API key and 
model configuration.
__ TestOpenRouterProviderInitialization.test_initialization_with_valid_config __

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd2ad50>

    @pytest.mark.fast
    def test_initialization_with_valid_config(self):
        """Test initialization with valid configuration."""
        config = {
            "openrouter_api_key": "test-key",
            "openrouter_model": "google/gemini-flash-1.5",
            "max_tokens": 1000,
            "temperature": 0.8,
        }
    
        with patch.dict(os.environ, {"OPENROUTER_API_KEY": "env-key"}):
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc70560>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,710 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_ 
TestOpenRouterProviderInitialization.test_initialization_with_environment_variab
le _

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd2b140>

    @pytest.mark.fast
    def test_initialization_with_environment_variable(self):
        """Test initialization using environment variable."""
        config = {"openrouter_model": "meta-llama/llama-3.1-8b-instruct"}
    
        with patch.dict(os.environ, {"OPENROUTER_API_KEY": "env-api-key"}):
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc4df10>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,723 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_ 
TestOpenRouterProviderInitialization.test_initialization_without_api_key_raises_
error _

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd2b5c0>

    @pytest.mark.fast
    def test_initialization_without_api_key_raises_error(self):
        """Test that initialization fails without API key."""
        config = {"openrouter_model": "google/gemini-flash-1.5"}
    
        with pytest.raises(OpenRouterConnectionError) as exc_info:
>           OpenRouterProvider(config)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc480e0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,737 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_ 
TestOpenRouterProviderInitialization.test_initialization_with_default_free_tier_
model _

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd2ba40>

    @pytest.mark.fast
    def test_initialization_with_default_free_tier_model(self):
        """Test initialization with default free-tier model."""
        config = {"openrouter_api_key": "test-key"}
    
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b451fa0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,750 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_ 
TestOpenRouterProviderInitialization.test_initialization_with_httpx_unavailable 
_

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd40380>

    @pytest.mark.fast
    def test_initialization_with_httpx_unavailable(self):
        """Test initialization when httpx is not available."""
        config = {"openrouter_api_key": "test-key"}
    
        with patch("devsynth.application.llm.openrouter_provider.httpx", None):
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b9fca70>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,763 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_ TestOpenRouterProviderInitialization.test_initialization_with_custom_base_url 
_

self = <test_openrouter_provider.TestOpenRouterProviderInitialization object at 
0x11dd40800>

    @pytest.mark.fast
    def test_initialization_with_custom_base_url(self):
        """Test initialization with custom base URL."""
        config = {
            "openrouter_api_key": "test-key",
            "base_url": "https://custom.openrouter.com/api/v1",
        }
    
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b2aecc0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,777 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
______ TestOpenRouterProviderErrorHandling.test_invalid_temperature_range ______

self = <test_openrouter_provider.TestOpenRouterProviderErrorHandling object at 
0x11dd40920>

    @pytest.mark.fast
    def test_invalid_temperature_range(self):
        """Test error handling for invalid temperature range."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc4b650>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,790 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_________ TestOpenRouterProviderErrorHandling.test_invalid_max_tokens __________

self = <test_openrouter_provider.TestOpenRouterProviderErrorHandling object at 
0x11dd40d10>

    @pytest.mark.fast
    def test_invalid_max_tokens(self):
        """Test error handling for invalid max_tokens."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc4cad0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,803 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
______ TestOpenRouterProviderConfiguration.test_configuration_validation _______

self = <test_openrouter_provider.TestOpenRouterProviderConfiguration object at 
0x11dd41700>

    @pytest.mark.fast
    def test_configuration_validation(self):
        """Test configuration validation."""
        # Valid configuration should not raise errors
        config = {
            "openrouter_api_key": "test-key",
            "temperature": 0.7,
            "max_tokens": 1000,
        }
    
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:470: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc3a7b0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,816 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_____ TestOpenRouterProviderConfiguration.test_configuration_with_defaults _____

self = <test_openrouter_provider.TestOpenRouterProviderConfiguration object at 
0x11dd41af0>

    @pytest.mark.fast
    def test_configuration_with_defaults(self):
        """Test configuration with default values."""
        config = {"openrouter_api_key": "test-key"}
    
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc735f0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,829 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
______ TestOpenRouterProviderConfiguration.test_configuration_precedence _______

self = <test_openrouter_provider.TestOpenRouterProviderConfiguration object at 
0x11dd2b260>

    @pytest.mark.fast
    def test_configuration_precedence(self):
        """Test configuration precedence (config overrides defaults)."""
        config = {
            "openrouter_api_key": "test-key",
            "temperature": 0.9,
            "max_tokens": 2000,
        }
    
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:494: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b273da0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,841 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_____ TestOpenRouterProviderTokenTracking.test_token_counting_integration ______

self = <test_openrouter_provider.TestOpenRouterProviderTokenTracking object at 
0x11dd42000>

    @pytest.mark.fast
    def test_token_counting_integration(self):
        """Test that token counting is integrated."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:507: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b44f200>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,854 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_______ TestOpenRouterProviderTokenTracking.test_token_limit_validation ________

self = <test_openrouter_provider.TestOpenRouterProviderTokenTracking object at 
0x11dd422d0>

    @pytest.mark.fast
    def test_token_limit_validation(self):
        """Test token limit validation."""
        config = {"openrouter_api_key": "test-key", "max_tokens": 10}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:517: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc73560>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,868 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_____ TestOpenRouterProviderResilience.test_circuit_breaker_initialization _____

self = <test_openrouter_provider.TestOpenRouterProviderResilience object at 
0x11dd427e0>

    @pytest.mark.fast
    def test_circuit_breaker_initialization(self):
        """Test circuit breaker initialization."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc4f350>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,880 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_______ TestOpenRouterProviderResilience.test_retry_logic_configuration ________

self = <test_openrouter_provider.TestOpenRouterProviderResilience object at 
0x11dd42c00>

    @pytest.mark.fast
    def test_retry_logic_configuration(self):
        """Test retry logic configuration."""
        config = {
            "openrouter_api_key": "test-key",
            "max_retries": 5,
        }
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:547: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13c4bbd70>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,895 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_________ TestOpenRouterProviderMetrics.test_metrics_collection_setup __________

self = <test_openrouter_provider.TestOpenRouterProviderMetrics object at 
0x11dd43590>

    @pytest.mark.fast
    def test_metrics_collection_setup(self):
        """Test that metrics collection is set up."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:583: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc3a540>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,907 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
____________ TestOpenRouterProviderMetrics.test_telemetry_emission _____________

self = <test_openrouter_provider.TestOpenRouterProviderMetrics object at 
0x11dd439b0>

    @pytest.mark.fast
    def test_telemetry_emission(self):
        """Test that telemetry is emitted correctly."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b450ec0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,920 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
____________ TestOpenRouterProviderHeaders.test_correct_headers_set ____________

self = <test_openrouter_provider.TestOpenRouterProviderHeaders object at 
0x11dd50320>

    @pytest.mark.fast
    def test_correct_headers_set(self):
        """Test that correct headers are set for OpenRouter."""
        config = {"openrouter_api_key": "test-key"}
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:612: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b361310>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,933 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
___________ TestOpenRouterProviderHeaders.test_custom_referer_header ___________

self = <test_openrouter_provider.TestOpenRouterProviderHeaders object at 
0x11dd507a0>

    @pytest.mark.fast
    def test_custom_referer_header(self):
        """Test custom referer header configuration."""
        config = {
            "openrouter_api_key": "test-key",
            "http_referer": "https://custom-app.com",
        }
>       provider = OpenRouterProvider(config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:632: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13c4bbb30>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,946 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_________ TestOpenRouterProviderEdgeCases.test_empty_response_handling _________

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at 
0x11dd50830>

    @pytest.mark.fast
    def test_empty_response_handling(self):
        """Test handling of empty responses."""
        config = {"openrouter_api_key": "test-key"}
    
        empty_response = {
            "id": "chatcmpl-empty",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "google/gemini-flash-1.5",
            "choices": [],
        }
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=empty_response,
                status=200,
            )
    
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b4534a0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,958 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
_______ TestOpenRouterProviderEdgeCases.test_malformed_response_handling _______

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at 
0x11dd50c50>

    @pytest.mark.fast
    def test_malformed_response_handling(self):
        """Test handling of malformed responses."""
        config = {"openrouter_api_key": "test-key"}
    
        malformed_response = {"invalid": "response", "structure": True}
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=malformed_response,
                status=200,
            )
    
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:683: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13bc70350>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,971 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
____________ TestOpenRouterProviderEdgeCases.test_unicode_handling _____________

self = <test_openrouter_provider.TestOpenRouterProviderEdgeCases object at 
0x11dd510d0>

    @pytest.mark.fast
    def test_unicode_handling(self):
        """Test handling of Unicode content."""
        config = {"openrouter_api_key": "test-key"}
    
        unicode_response = {
            "id": "chatcmpl-unicode",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "google/gemini-flash-1.5",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! ! Hola! ",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {"prompt_tokens": 5, "completion_tokens": 10, 
"total_tokens": 15},
        }
    
        with responses.RequestsMock() as rsps:
            rsps.add(
                responses.POST,
                "https://openrouter.ai/api/v1/chat/completions",
                json=unicode_response,
                status=200,
            )
    
>           provider = OpenRouterProvider(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/llm/test_openrou
ter_provider.py:721: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py in __init__
    self._init_clients()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <devsynth.application.llm.openrouter_provider.OpenRouterProvider object 
at 0x13b2ac7d0>

    def _init_clients(self):
        """Initialize HTTP clients for OpenRouter API."""
        # Set up headers with required attribution for OpenRouter
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://devsynth.dev",
            "X-Title": "DevSynth AI Platform",
        }
    
        # Initialize clients if dependencies are available
        try:
            import httpx
    
            # Create sync and async clients for different use cases
>           self.sync_client = httpx.Client(
                               ^^^^^^^^^^^^
                base_url=self.base_url,
                headers=self.headers,
                timeout=self.timeout,
            )
E           AttributeError: module 'httpx' has no attribute 'Client'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/ll
m/openrouter_provider.py:119: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:50,984 - devsynth.application.utils.token_tracker - WARNING - 
Skipping tiktoken initialization in test environment for model 'gpt-3.5-turbo'. 
Falling back to approximate token counting
------------------------------ Captured log call -------------------------------
WARNING  devsynth.application.utils.token_tracker:logging_setup.py:615 Skipping 
tiktoken initialization in test environment for model 'gpt-3.5-turbo'. Falling 
back to approximate token counting
____________ test_configure_logging_invokes_directory_creation_once ____________

logging_setup_module = <module 'devsynth.logging_setup' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_configure_logging_invokes0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b3aa7b0>

    @pytest.mark.fast
    def test_configure_logging_invokes_directory_creation_once(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: LOG-CONF-05A  ensure_log_dir_exists executes only on first 
configuration."""
    
        logging_setup = logging_setup_module
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(tmp_path))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
    
        calls: list[str] = []
    
        def fake_ensure(path: str) -> str:
            calls.append(path)
            return path
    
        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", fake_ensure)
    
        logging_setup.configure_logging(log_dir="logs")
        logging_setup.configure_logging(log_dir="logs")
    
>       assert calls == [str(tmp_path / "logs")]
E       AssertionError: assert ['/private/va...nvokes0/logs'] == 
['/private/va...nvokes0/logs']
E         
E         Left contains 3 more items, first extra item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_invokes0/logs'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_configure_logging.py:183: AssertionError
----------------------------- Captured stdout call -----------------------------
WARNING: File logging failed - 2025-10-29 10:48:51,230 - root - WARNING - Failed
to set up file logging: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_invokes0/logs/devsynth.log'
WARNING: File logging failed - 2025-10-29 10:48:51,230 - root - INFO - Logging 
configured for console output only (no file logging).
WARNING: File logging failed - 2025-10-29 10:48:51,230 - root - WARNING - Failed
to set up file logging: [Errno 2] No such file or directory: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_invokes0/logs/devsynth.log'
WARNING: File logging failed - 2025-10-29 10:48:51,230 - root - INFO - Logging 
configured for console output only (no file logging).
______ test_configure_logging_reenables_file_handler_after_console_toggle ______

logging_setup_module = <module 'devsynth.logging_setup' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_configure_logging_reenabl0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b1b5d90>

    @pytest.mark.fast
    def test_configure_logging_reenables_file_handler_after_console_toggle(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: LOG-CONF-09  toggling create_dir back on reinstates JSON 
handler.
    
        Issue: issues/coverage-below-threshold.md
        """
    
        logging_setup = logging_setup_module
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(tmp_path))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
    
        logging_setup.configure_logging(log_dir="retention", create_dir=False)
    
        assert all(
            not isinstance(handler, logging.FileHandler)
            for handler in logging.getLogger().handlers
        ), "Initial console-only run should not attach a file handler."
    
        calls: list[str] = []
    
        def track_directory(path: str) -> str:
            calls.append(path)
            Path(path).mkdir(parents=True, exist_ok=True)
            return path
    
        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", 
track_directory)
    
        logging_setup.configure_logging(log_dir="retention", create_dir=True)
    
>       assert calls == [str(tmp_path / "retention")]
E       AssertionError: assert ['/private/va...l0/retention'] == 
['/private/va...l0/retention']
E         
E         Left contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_reenabl0/retention'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_configure_logging.py:353: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:51,257 - root - INFO - Logging configured for console output 
only (no file logging).
2025-10-29 10:48:51,257 - root - INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_configure_logging_reenabl0/retention/devsynth.log
_____________ test_configure_logging_retention_matrix[create-dir] ______________

logging_setup_module = <module 'devsynth.logging_setup' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_configure_logging_retenti0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcf0e90>
create_dir = True, no_file_env = None, expected_effective = True

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("create_dir", "no_file_env", "expected_effective"),
        [
            pytest.param(True, None, True, id="create-dir"),
            pytest.param(True, "1", False, id="no-file-env"),
            pytest.param(False, None, False, id="create-dir-disabled"),
            pytest.param(False, "1", False, 
id="no-file-env-create-dir-disabled"),
        ],
    )
    def test_configure_logging_retention_matrix(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        create_dir: bool,
        no_file_env: str | None,
        expected_effective: bool,
    ) -> None:
        """Exercise retention decisions across create_dir and environment 
flags."""
    
        logging_setup = logging_setup_module
        project_dir = tmp_path / "retention_project"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
        if no_file_env is not None:
            monkeypatch.setenv("DEVSYNTH_NO_FILE_LOGGING", no_file_env)
    
        log_dir_argument = "relative/logs"
        ensure_calls: list[str | None] = []
        real_ensure = logging_setup.ensure_log_dir_exists
    
        def tracking(log_dir: str | None = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)
    
        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)
    
        logging_setup.configure_logging(log_dir=log_dir_argument, 
create_dir=create_dir)
    
        expected_dir = os.path.join(str(project_dir), log_dir_argument)
        expected_file = os.path.join(
            expected_dir,
            os.environ.get("DEVSYNTH_LOG_FILENAME", 
logging_setup.DEFAULT_LOG_FILENAME),
        )
    
        assert logging_setup._configured_log_dir == expected_dir
        assert logging_setup._configured_log_file == expected_file
        assert logging_setup._last_effective_config[0] == expected_dir
        assert logging_setup._last_effective_config[1] == expected_file
        assert logging_setup._last_effective_config[3] is expected_effective
    
        if expected_effective:
>           assert ensure_calls == [expected_dir]
E           AssertionError: assert ['/private/va...elative/logs'] == 
['/private/va...elative/logs']
E             
E             Left contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_retenti0/retention_project/relative/logs'
E             Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:108: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:51,342 - root - INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_configure_logging_retenti0/retention_project/relative/logs/devsynth.lo
g
________ test_configure_logging_relocates_absolute_paths[home-absolute] ________

logging_setup_module = <module 'devsynth.logging_setup' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_configure_logging_relocat0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b17eea0>
log_dir_input = PosixPath('/Users/caitlyn/devsynth/logs')
log_file_name = 'home.json'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("log_dir_input", "log_file_name"),
        [
            pytest.param(
                Path.home() / "devsynth" / "logs", "home.json", 
id="home-absolute"
            ),
            pytest.param(
                Path("/var/tmp/devsynth/logs"), "system.json", 
id="non-home-absolute"
            ),
        ],
    )
    def test_configure_logging_relocates_absolute_paths(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        log_dir_input: Path,
        log_file_name: str,
    ) -> None:
        """Absolute paths are redirected into the sandboxed project 
directory."""
    
        logging_setup = logging_setup_module
        project_dir = tmp_path / "sandbox_relocation"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
    
        log_file_input = log_dir_input / log_file_name
    
        ensure_calls: list[str | None] = []
        real_ensure = logging_setup.ensure_log_dir_exists
    
        def tracking(log_dir: str | None = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)
    
        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)
    
        logging_setup.configure_logging(
            log_dir=str(log_dir_input),
            log_file=str(log_file_input),
            create_dir=True,
        )
    
        home_prefix = str(Path.home())
    
        def expected_relative(path: Path) -> str:
            path_str = str(path)
            if path_str.startswith(home_prefix):
                return path_str.replace(home_prefix, "", 1).lstrip("/\\")
            return str(path.relative_to(path.anchor))
    
        expected_dir = os.path.join(str(project_dir), 
expected_relative(log_dir_input))
        expected_file = os.path.join(str(project_dir), 
expected_relative(log_file_input))
    
>       assert ensure_calls == [expected_dir]
E       AssertionError: assert ['/private/va...evsynth/logs'] == 
['/private/va...evsynth/logs']
E         
E         Left contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_relocat0/sandbox_relocation/Users/caitlyn/devsynth/
logs'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:51,368 - root - INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_configure_logging_relocat0/sandbox_relocation/Users/caitlyn/devsynth/l
ogs/home.json
______ test_configure_logging_relocates_absolute_paths[non-home-absolute] ______

logging_setup_module = <module 'devsynth.logging_setup' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/logging_setup
.py'>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_configure_logging_relocat1')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bc395b0>
log_dir_input = PosixPath('/var/tmp/devsynth/logs')
log_file_name = 'system.json'

    @pytest.mark.fast
    @pytest.mark.parametrize(
        ("log_dir_input", "log_file_name"),
        [
            pytest.param(
                Path.home() / "devsynth" / "logs", "home.json", 
id="home-absolute"
            ),
            pytest.param(
                Path("/var/tmp/devsynth/logs"), "system.json", 
id="non-home-absolute"
            ),
        ],
    )
    def test_configure_logging_relocates_absolute_paths(
        logging_setup_module: ModuleType,
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        log_dir_input: Path,
        log_file_name: str,
    ) -> None:
        """Absolute paths are redirected into the sandboxed project 
directory."""
    
        logging_setup = logging_setup_module
        project_dir = tmp_path / "sandbox_relocation"
        project_dir.mkdir(parents=True, exist_ok=True)
        monkeypatch.setenv("DEVSYNTH_PROJECT_DIR", str(project_dir))
        monkeypatch.delenv("DEVSYNTH_NO_FILE_LOGGING", raising=False)
    
        log_file_input = log_dir_input / log_file_name
    
        ensure_calls: list[str | None] = []
        real_ensure = logging_setup.ensure_log_dir_exists
    
        def tracking(log_dir: str | None = None) -> str:
            ensure_calls.append(log_dir)
            return real_ensure(log_dir)
    
        monkeypatch.setattr(logging_setup, "ensure_log_dir_exists", tracking)
    
        logging_setup.configure_logging(
            log_dir=str(log_dir_input),
            log_file=str(log_file_input),
            create_dir=True,
        )
    
        home_prefix = str(Path.home())
    
        def expected_relative(path: Path) -> str:
            path_str = str(path)
            if path_str.startswith(home_prefix):
                return path_str.replace(home_prefix, "", 1).lstrip("/\\")
            return str(path.relative_to(path.anchor))
    
        expected_dir = os.path.join(str(project_dir), 
expected_relative(log_dir_input))
        expected_file = os.path.join(str(project_dir), 
expected_relative(log_file_input))
    
>       assert ensure_calls == [expected_dir]
E       AssertionError: assert ['/private/va...evsynth/logs'] == 
['/private/va...evsynth/logs']
E         
E         Left contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devsynth/logs'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/logging/test_log
ging_setup_retention.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:51,376 - root - INFO - Logging configured. Log file: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_configure_logging_relocat1/sandbox_relocation/var/tmp/devsynth/logs/sy
stem.json
___________ TestSyncManagerProtocol.test_sync_manager_initialization ___________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddf8b30>

    def test_sync_manager_initialization(self):
        """Test SyncManager initializes correctly with required stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13b3a9160>, 's...MemoryStore object at 0x13b2a86b0>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
________ TestSyncManagerProtocol.test_sync_manager_write_to_all_stores _________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddf9a60>

    def test_sync_manager_write_to_all_stores(self):
        """Test write operation propagates to all configured stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13b44ebd0>, 's...MemoryStore object at 0x13b2e5e80>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_______ TestSyncManagerProtocol.test_sync_manager_read_from_first_store ________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddf9ee0>

    def test_sync_manager_read_from_first_store(self):
        """Test read operation returns from first store containing the key."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13bc58aa0>, 's...MemoryStore object at 0x13bc5a6f0>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
___ TestSyncManagerProtocol.test_sync_manager_read_fallback_to_second_store ____

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddfa360>

    def test_sync_manager_read_fallback_to_second_store(self):
        """Test read operation falls back to second store if first doesn't have 
key."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13b2ac530>, 's...MemoryStore object at 0x13b2ad850>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_ TestSyncManagerProtocol.test_sync_manager_read_raises_keyerror_if_not_found __

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddf9700>

    def test_sync_manager_read_raises_keyerror_if_not_found(self):
        """Test read operation raises KeyError if key not found in any store."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13b1b5eb0>, 's...MemoryStore object at 0x13b1b6060>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_________ TestSyncManagerProtocol.test_sync_manager_transaction_commit _________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddfa6c0>

    def test_sync_manager_transaction_commit(self):
        """Test transaction commits changes to all stores."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(
            stores={"tinydb": store1, "store2": store2}, 
required_stores={"tinydb"}
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'tinydb': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13b2abda0>, 's...MemoryStore object at 0x13b2ab380>}, 
required_stores={'tinydb'}, optional_stores=frozenset({'duckdb', 'kuzu', 
'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
            raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
    
        allowed = self.required_stores | self.optional_stores
        unexpected = configured - allowed
        if unexpected:
>           raise ValueError(
                "Unexpected stores configured: " f"{', 
'.join(sorted(unexpected))}"
            )
E           ValueError: Unexpected stores configured: store2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:88: ValueError
_ TestSyncManagerProtocol.test_sync_manager_transaction_rollback_on_exception __

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddfab40>

    def test_sync_manager_transaction_rollback_on_exception(self):
        """Test transaction rolls back changes if exception occurs."""
        store1 = MockMemoryStore()
        store2 = MockMemoryStore()
    
>       sync_manager = SyncManager(stores={"store1": store1, "store2": store2})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'store1': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13bcd9430>, 's... object at 0x13bcdb5f0>}, 
required_stores=frozenset({'tinydb'}), optional_stores=frozenset({'duckdb', 
'kuzu', 'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
>           raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
E           ValueError: Missing stores: tinydb

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:83: ValueError
_________ TestSyncManagerProtocol.test_sync_manager_with_generic_type __________

self = <tests.unit.memory.test_sync_manager_protocol.TestSyncManagerProtocol 
object at 0x11ddfb440>

    def test_sync_manager_with_generic_type(self):
        """Test SyncManager works with different value types."""
>       sync_manager = SyncManager[str](stores={"mock": MockMemoryStore()})
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/memory/test_sync
_manager_protocol.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/typing.py:1184: in __call__
    result = self.__origin__(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<string>:6: in __init__
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SyncManager(stores={'mock': 
<tests.unit.memory.test_sync_manager_protocol.MockMemoryStore object at 
0x13bcf0e90>}, required_stores=frozenset({'tinydb'}), 
optional_stores=frozenset({'duckdb', 'kuzu', 'lmdb'}))

    def __post_init__(self) -> None:
        configured = frozenset(self.stores)
        missing = self.required_stores - configured
        if missing:
>           raise ValueError(f"Missing stores: {', '.join(sorted(missing))}")
E           ValueError: Missing stores: tinydb

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/memory/sync_ma
nager.py:83: ValueError
___________________ test_reasoning_loop_runs_until_complete ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b363bc0>

    @pytest.mark.fast
    def test_reasoning_loop_runs_until_complete(monkeypatch):
        """It continues until the reasoning process is complete.
    
        ReqID: DR-4
        """
    
        calls = []
    
        def fake_apply(team, task, critic, memory):
            calls.append(task.get("solution"))
            if len(calls) == 1:
                return {"status": "in_progress", "synthesis": "next"}
            return {"status": "completed", "synthesis": "final"}
    
>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fake_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:32: AttributeError
__________________ test_reasoning_loop_logs_consensus_failure __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b991100>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13b75fe60>

    @pytest.mark.fast
    def test_reasoning_loop_logs_consensus_failure(monkeypatch, caplog):
        """It logs and swallows consensus failures.
    
        ReqID: DR-5
        """
    
        def fail_apply(team, task, critic, memory):
            raise DummyConsensusError("no consensus")
    
>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fail_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:49: AttributeError
_________________ test_reasoning_loop_respects_max_iterations __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b756ba0>

    @pytest.mark.fast
    def test_reasoning_loop_respects_max_iterations(monkeypatch):
        """It stops after reaching the iteration limit.
    
        ReqID: DR-6
        """
    
        calls = []
    
        def fake_apply(team, task, critic, memory):
            calls.append(1)
            return {"status": "in_progress"}
    
>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", fake_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_dialectical_reasoning_loop.py:71: AttributeError
________________ test_reasoning_loop_respects_total_time_budget ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b829c40>

    @pytest.mark.fast
    @pytest.mark.unit
    def test_reasoning_loop_respects_total_time_budget(monkeypatch):
        """It stops when the total time budget is exhausted.
    
        ReqID: DR-6
        """
    
>       monkeypatch.setattr(rl, "_apply_dialectical_reasoning", _slow_apply)
E       AttributeError: <module 'devsynth.methodology.edrr.reasoning_loop' from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/methodology/e
drr/reasoning_loop.py'> has no attribute '_apply_dialectical_reasoning'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_reasoning_loop_time_budget.py:25: AttributeError
________________________ test_ceremony_mapping_to_phase ________________________

    @pytest.mark.fast
    def test_ceremony_mapping_to_phase():
        """Configured ceremonies map to the correct EDRR phases."""
        config = {
            "settings": {
                "ceremonyMapping": {
                    "planning": "retrospect.iteration_planning",
                    "dailyStandup": "phase_progression_tracking",
                    "review": "refine.outputs_review",
                    "retrospective": "retrospect.process_evaluation",
                }
            }
        }
        adapter = SprintAdapter(config)
        assert adapter.get_ceremony_phase("planning") == Phase.RETROSPECT
        assert adapter.get_ceremony_phase("review") == Phase.REFINE
        assert adapter.get_ceremony_phase("retrospective") == Phase.RETROSPECT
>       assert adapter.get_ceremony_phase("dailyStandup") is None
E       AssertionError: assert <Phase.DIFFERENTIATE: 'differentiate'> is None
E        +  where <Phase.DIFFERENTIATE: 'differentiate'> = 
get_ceremony_phase('dailyStandup')
E        +    where get_ceremony_phase = 
<devsynth.methodology.sprint.SprintAdapter object at 
0x13b7c0740>.get_ceremony_phase

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_adapter.py:63: AssertionError
_____________________ test_map_ceremony_to_phase_defaults ______________________

    @pytest.mark.fast
    def test_map_ceremony_to_phase_defaults():
        """Common ceremonies resolve to their default EDRR phases."""
>       assert map_ceremony_to_phase("planning") == Phase.RETROSPECT
E       AssertionError: assert <Phase.EXPAND: 'expand'> == <Phase.RETROSPECT: 
'retrospect'>
E        +  where <Phase.EXPAND: 'expand'> = map_ceremony_to_phase('planning')
E        +  and   <Phase.RETROSPECT: 'retrospect'> = Phase.RETROSPECT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_hooks.py:11: AssertionError
_____________________ test_adapter_uses_ceremony_defaults ______________________

    @pytest.mark.fast
    def test_adapter_uses_ceremony_defaults():
        """SprintAdapter falls back to default phase mapping for bare ceremony 
names."""
        config = {
            "settings": {
                "ceremonyMapping": {
                    "planning": "planning",
                    "review": "review",
                    "retrospective": "retrospective",
                }
            }
        }
        adapter = SprintAdapter(config)
>       assert adapter.get_ceremony_phase("planning") == Phase.RETROSPECT
E       AssertionError: assert <Phase.EXPAND: 'expand'> == <Phase.RETROSPECT: 
'retrospect'>
E        +  where <Phase.EXPAND: 'expand'> = get_ceremony_phase('planning')
E        +    where get_ceremony_phase = 
<devsynth.methodology.sprint.SprintAdapter object at 
0x13b48cfb0>.get_ceremony_phase
E        +  and   <Phase.RETROSPECT: 'retrospect'> = Phase.RETROSPECT

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/methodology/test
_sprint_hooks.py:31: AssertionError
_______________________ test_graph_transitions_complete ________________________

engine = 
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine 
object at 0x13b783650>

    @pytest.mark.fast
    def test_graph_transitions_complete(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1", 
agent_type="t")
        step2 = WorkflowStep(id="s2", name="Step 2", description="d2", 
agent_type="t")
        wf = engine.add_step(wf, step1)
        wf = engine.add_step(wf, step2)
    
        # Patch orchestration to be a no-op that appends a message
        class FakeService:
            def process_step(self, state, step):
                state.messages.append({"role": "system", "content": f"ran 
{step.id}"})
                return state
    
        with patch(
            "devsynth.orchestration.step_executor.OrchestrationService", 
FakeService
        ):
            result = engine.execute_workflow(wf, context={})
    
>       assert result.status == WorkflowStatus.COMPLETED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> == 
<WorkflowStatus.COMPLETED: 'completed'>
E        +  where <WorkflowStatus.FAILED: 'failed'> = 
Workflow(id='f0fcc649-0d13-4ef4-89b0-30e993906f5f', name='wf', 
description='desc', steps=[WorkflowStep(id='s1', 
name='...at=datetime.datetime(2025, 10, 29, 10, 48, 52, 393461), 
updated_at=datetime.datetime(2025, 10, 29, 10, 48, 52, 393515)).status
E        +  and   <WorkflowStatus.COMPLETED: 'completed'> = 
WorkflowStatus.COMPLETED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:52,393 - devsynth.adapters.orchestration.langgraph_adapter - 
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 
Error executing workflow: object() takes no arguments
_________________ test_retry_branch_succeeds_with_max_retries __________________

engine = 
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine 
object at 0x13b9b7e60>

    @pytest.mark.fast
    def test_retry_branch_succeeds_with_max_retries(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1", 
agent_type="t")
        wf = engine.add_step(wf, step1)
    
        calls = {"n": 0}
    
        class FlakyService:
            def process_step(self, state, step):
                calls["n"] += 1
                if calls["n"] == 1:
                    raise RuntimeError("transient")
                state.messages.append({"role": "system", "content": "ok"})
                return state
    
        with patch(
            "devsynth.orchestration.step_executor.OrchestrationService", 
FlakyService
        ):
            # Allow one retry
            result = engine.execute_workflow(wf, context={"max_retries": 1})
    
>       assert calls["n"] == 2
E       assert 0 == 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:76: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:52,406 - devsynth.adapters.orchestration.langgraph_adapter - 
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 
Error executing workflow: object() takes no arguments
________________________ test_streaming_callback_called ________________________

engine = 
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine 
object at 0x13b9b7980>

    @pytest.mark.fast
    def test_streaming_callback_called(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1", 
agent_type="t")
        wf = engine.add_step(wf, step1)
    
        class Service:
            def process_step(self, state, step):
                state.messages.append({"role": "agent", "content": "result"})
                return state
    
        stream_events = []
    
        def stream_cb(evt):
            stream_events.append(evt)
    
        with patch("devsynth.orchestration.step_executor.OrchestrationService", 
Service):
            result = engine.execute_workflow(wf, context={"stream_callback": 
stream_cb})
    
>       assert result.status == WorkflowStatus.COMPLETED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> == 
<WorkflowStatus.COMPLETED: 'completed'>
E        +  where <WorkflowStatus.FAILED: 'failed'> = 
Workflow(id='5737a55f-69b1-4748-b6ab-b7e40137f3b8', name='wf', 
description='desc', steps=[WorkflowStep(id='s1', 
name='...at=datetime.datetime(2025, 10, 29, 10, 48, 52, 414452), 
updated_at=datetime.datetime(2025, 10, 29, 10, 48, 52, 414502)).status
E        +  and   <WorkflowStatus.COMPLETED: 'completed'> = 
WorkflowStatus.COMPLETED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:99: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:52,414 - devsynth.adapters.orchestration.langgraph_adapter - 
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 
Error executing workflow: object() takes no arguments
__________________ test_cancellation_pauses_before_first_step __________________

engine = 
<devsynth.adapters.orchestration.langgraph_adapter.LangGraphWorkflowEngine 
object at 0x13b9909e0>

    @pytest.mark.fast
    def test_cancellation_pauses_before_first_step(engine):
        wf = engine.create_workflow("wf", "desc")
        step1 = WorkflowStep(id="s1", name="Step 1", description="d1", 
agent_type="t")
        wf = engine.add_step(wf, step1)
    
        spy = MagicMock()
    
        class SpyService:
            def process_step(self, state, step):
                spy()
                return state
    
        with patch("devsynth.orchestration.step_executor.OrchestrationService", 
SpyService):
            result = engine.execute_workflow(wf, context={"is_cancelled": 
lambda: True})
    
>       assert result.status == WorkflowStatus.PAUSED
E       AssertionError: assert <WorkflowStatus.FAILED: 'failed'> == 
<WorkflowStatus.PAUSED: 'paused'>
E        +  where <WorkflowStatus.FAILED: 'failed'> = 
Workflow(id='4414cbd9-f22a-45c8-a4c5-99ab2f972fd6', name='wf', 
description='desc', steps=[WorkflowStep(id='s1', 
name='...at=datetime.datetime(2025, 10, 29, 10, 48, 52, 422077), 
updated_at=datetime.datetime(2025, 10, 29, 10, 48, 52, 422169)).status
E        +  and   <WorkflowStatus.PAUSED: 'paused'> = WorkflowStatus.PAUSED

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/orchestration/te
st_graph_transitions_and_controls.py:122: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:52,422 - devsynth.adapters.orchestration.langgraph_adapter - 
INFO - Error executing workflow: object() takes no arguments
------------------------------ Captured log call -------------------------------
INFO     devsynth.adapters.orchestration.langgraph_adapter:logging_setup.py:615 
Error executing workflow: object() takes no arguments
__________________ test_adapter_openai_provider_stub_offline ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bcf2450>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_adapter_openai_provider_stub_offline(monkeypatch):
        """
        When DEVSYNTH_OFFLINE=true and normalized stubs are applied (default),
        adapter-level OpenAIProvider should return deterministic responses.
        """
        # Ensure offline and that provider availability flags don't accidentally
enable real backend
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")
        monkeypatch.setenv("DEVSYNTH_PROVIDER", "stub")
    
        # Import locally to ensure stub application via tests/conftest autouse 
fixture has run
        import devsynth.adapters.provider_system as provider_system  # type: 
ignore
    
>       p = provider_system.OpenAIProvider()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: OpenAIProvider.__init__() missing 1 required positional 
argument: 'api_key'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/providers/test_p
rovider_stub_offline.py:20: TypeError
________________________ test_generate_arguments_sorted ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b124050>

    @pytest.mark.fast
    def test_generate_arguments_sorted(monkeypatch):
        # Reverse order in text, expect sorted by position/content
        text = (
            "Argument 2\n"
            "Position: AGAINST\n"
            "Content: zeta\n"
            "Counterargument: aaa\n\n"
            "Argument 1\n"
            "Position: FOR\n"
            "Content: alpha\n"
            "Counterargument: bbb\n"
        )
    
        class LL(StubLLM):
            def query(self, prompt: str) -> str:
                if "Arguments" in prompt or "arguments" in prompt:
                    return text
                if "Determine if the following reasoning" in prompt:
                    return "yes"
                return "ok"
    
        service = make_service(llm=LL())
        change = RequirementChange(requirement_id=uuid4(), 
change_type=ChangeType.ADD)
        change.new_state = Requirement(title="T", description="D")
        args = service._generate_arguments(change, "t", "a")
        # After sorting, FOR/alpha should come before AGAINST/zeta
>       assert args[0]["content"].lower() == "alpha"
               ^^^^^^^^^^^^^^^^^^
E       TypeError: 'ParsedDialecticalArgument' object is not subscriptable

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/requirements/tes
t_dialectical_reasoner_determinism.py:185: TypeError
___________ TestRecommendationGeneration.test_calculates_percentages ___________

self = 
<tests.unit.scripts.test_analyze_test_dependencies.TestRecommendationGeneration 
object at 0x11f1c9f70>

    def test_calculates_percentages(self):
        """Test percentage calculations in recommendations."""
        analysis_results = [
            {"has_isolation_marker": True, "safe_for_parallel": True, 
"risk_score": 0},
            {"has_isolation_marker": True, "safe_for_parallel": True, 
"risk_score": 1},
            {
                "has_isolation_marker": True,
                "safe_for_parallel": False,
                "risk_score": 10,
            },
            {
                "has_isolation_marker": True,
                "safe_for_parallel": False,
                "risk_score": 15,
            },
        ]
    
>       recommendations = generate_recommendations(analysis_results)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ana
lyze_test_dependencies.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

analysis_results = [{'has_isolation_marker': True, 'risk_score': 0, 
'safe_for_parallel': True}, {'has_isolation_marker': True, 'risk_scor..._score':
10, 'safe_for_parallel': False}, {'has_isolation_marker': True, 'risk_score': 
15, 'safe_for_parallel': False}]

    def generate_recommendations(analysis_results: list[dict[str, Any]]) -> 
dict[str, Any]:
        """Generate recommendations based on analysis results."""
        total_files = len(analysis_results)
        files_with_isolation = sum(
            1 for r in analysis_results if r.get("has_isolation_marker", False)
        )
        safe_for_removal = sum(
            1
            for r in analysis_results
            if r.get("safe_for_parallel", False) and 
r.get("has_isolation_marker", False)
        )
    
        # Categorize by risk level
        low_risk = [
            r
            for r in analysis_results
            if r.get("risk_score", 0) <= 2 and r.get("has_isolation_marker", 
False)
        ]
        medium_risk = [
            r
            for r in analysis_results
            if 3 <= r.get("risk_score", 0) <= 8 and 
r.get("has_isolation_marker", False)
        ]
        high_risk = [
            r
            for r in analysis_results
            if r.get("risk_score", 0) > 8 and r.get("has_isolation_marker", 
False)
        ]
    
        return {
            "summary": {
                "total_test_files": total_files,
                "files_with_isolation_markers": files_with_isolation,
                "safe_for_removal": safe_for_removal,
                "removal_percentage": round(
                    (
                        (safe_for_removal / files_with_isolation * 100)
                        if files_with_isolation > 0
                        else 0
                    ),
                    1,
                ),
            },
            "risk_categories": {
                "low_risk": {
                    "count": len(low_risk),
>                   "files": [r["relative_path"] for r in low_risk],
                              ^^^^^^^^^^^^^^^^^^
                },
                "medium_risk": {
                    "count": len(medium_risk),
                    "files": [r["relative_path"] for r in medium_risk],
                },
                "high_risk": {
                    "count": len(high_risk),
                    "files": [r["relative_path"] for r in high_risk],
                },
            },
            "recommendations": {
                "immediate_removal": [r["relative_path"] for r in low_risk],
                "careful_review": [r["relative_path"] for r in medium_risk],
                "keep_isolation": [r["relative_path"] for r in high_risk],
            },
        }
E       KeyError: 'relative_path'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/analyze_test_depend
encies.py:313: KeyError
____________ TestTestExecutionBenchmark.test_run_benchmark_success _____________

self = 
<tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchmark 
object at 0x11f1f6a20>
mock_run = <MagicMock name='run' id='5293378160'>

    @patch("subprocess.run")
    def test_run_benchmark_success(self, mock_run):
        """Test successful benchmark execution."""
        # Mock successful subprocess result
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_result.stdout = "===== 5 passed, 0 failed, 2 skipped in 1.23s 
====="
        mock_result.stderr = ""
        mock_run.return_value = mock_result
    
        benchmark = TestExecutionBenchmark()
    
        with patch("time.time", side_effect=[1000.0, 1001.23]):  # 1.23 second 
duration
            result = benchmark.run_benchmark("unit-tests", "fast", 2)
    
        assert result["target"] == "unit-tests"
        assert result["speed"] == "fast"
        assert result["workers"] == 2
>       assert result["duration"] == 1.23
E       assert 1.2300000000000182 == 1.23

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ben
chmark_test_execution.py:54: AssertionError
----------------------------- Captured stdout call -----------------------------
  Running unit-tests (speed: fast, workers: 2)...
____________ TestTestExecutionBenchmark.test_run_benchmark_failure _____________

self = 
<tests.unit.scripts.test_benchmark_test_execution.TestTestExecutionBenchmark 
object at 0x11f1f72c0>
mock_run = <MagicMock name='run' id='5294006080'>

    @patch("subprocess.run")
    def test_run_benchmark_failure(self, mock_run):
        """Test benchmark failure handling."""
        # Mock subprocess failure
        mock_result = MagicMock()
        mock_result.returncode = 1
        mock_result.stdout = "===== 2 passed, 3 failed in 2.45s ====="
        mock_result.stderr = "Some error"
        mock_run.return_value = mock_result
    
        benchmark = TestExecutionBenchmark()
    
        with patch("time.time", side_effect=[1000.0, 1002.45]):
            result = benchmark.run_benchmark("unit-tests", None, 1)
    
        assert result["success"] == False
>       assert result["passed"] == 2
E       assert 0 == 2

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ben
chmark_test_execution.py:93: AssertionError
----------------------------- Captured stdout call -----------------------------
  Running unit-tests (speed: all, workers: 1)...
_____________________ test_parametrize_speed_marker_parity _____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_parametrize_speed_marker_0')

    @pytest.mark.fast
    def test_parametrize_speed_marker_parity(tmp_path: Path):
        # Create a synthetic test file that uses only pytest.param speed markers
        test_code = textwrap.dedent(
            """
            import pytest
    
            @pytest.mark.parametrize(
                "x",
                [
                    pytest.param(1, marks=pytest.mark.medium),
                    pytest.param(2, marks=pytest.mark.medium),
                ],
            )
            def test_derived_speed_from_params(x):
                assert x in (1, 2)
            """
        )
        file_path = tmp_path / "test_sample_param_speed.py"
        file_path.write_text(test_code)
    
        # Load enhanced_test_parser from scripts
        etp = _import_module_from_path(
            "enhanced_test_parser",
            Path(__file__).parents[3] / "scripts" / "enhanced_test_parser.py",
        )
        vtm = _import_module_from_path(
            "verify_test_markers",
            Path(__file__).parents[3] / "scripts" / "verify_test_markers.py",
        )
    
        # Parse using enhanced test parser
        parsed = etp.parse_test_file(str(file_path))
        # Find our test
        target = [
>           t for t in parsed["tests"] if t["name"] == 
"test_derived_speed_from_params"
                       ^^^^^^^^^^^^^^^
        ]
E       TypeError: list indices must be integers or slices, not str

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_enh
anced_test_parser_marker_parity.py:54: TypeError
__________________ test_returns_error_when_syntax_is_invalid ___________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_returns_error_when_syntax0')

    def test_returns_error_when_syntax_is_invalid(tmp_path: Path) -> None:
        bad_file = tmp_path / "bad.py"
        bad_file.write_text("def broken(:\n    pass\n", encoding="utf-8")
    
        result = run_script(tmp_path)
    
>       assert result.returncode == 1
E       AssertionError: assert 0 == 1
E        +  where 0 = 
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.ve
nv/bin/python', 
'/Users/caitlyn/Projec...st-of-caitlyn/pytest-57/test_returns_error_when_syntax0
'], returncode=0, stdout='[find_syntax_errors] OK\n', stderr='').returncode

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_fin
d_syntax_errors.py:32: AssertionError
_______________________ test_returns_zero_with_no_errors _______________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_returns_zero_with_no_erro0')

    def test_returns_zero_with_no_errors(tmp_path: Path) -> None:
        good_file = tmp_path / "good.py"
        good_file.write_text("print('ok')\n", encoding="utf-8")
    
        result = run_script(tmp_path)
    
        assert result.returncode == 0
>       assert "No syntax errors found" in result.stdout
E       AssertionError: assert 'No syntax errors found' in '[find_syntax_errors]
OK\n'
E        +  where '[find_syntax_errors] OK\n' = 
CompletedProcess(args=['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.ve
nv/bin/python', 
'/Users/caitlyn/Projec...st-of-caitlyn/pytest-57/test_returns_zero_with_no_erro0
'], returncode=0, stdout='[find_syntax_errors] OK\n', stderr='').stdout

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_fin
d_syntax_errors.py:43: AssertionError
_____ TestQualityReportGenerator.test_quality_score_with_missing_mutation ______

self = 
<tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerator 
object at 0x11f22a090>

    def test_quality_score_with_missing_mutation(self):
        """Test quality score calculation when mutation testing is skipped."""
        metrics = {
            "coverage": {"line_coverage": 90.0},
            "mutation": {"mutation_score": None},  # Skipped
            "property": {"total_property_tests": 5, "enabled": False},
            "organization": {"marker_compliance": 100.0},
            "performance": {"parallel_speedup": 3.5},
        }
    
        score = calculate_overall_quality_score(metrics)
    
        # Should handle None mutation score gracefully
>       assert 60 <= score <= 85
E       assert 60 <= 58.5

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_gen
erate_quality_report.py:124: AssertionError
_______ TestQualityReportGenerator.test_recommendations_for_good_metrics _______

self = 
<tests.unit.scripts.test_generate_quality_report.TestQualityReportGenerator 
object at 0x11f22b620>

    def test_recommendations_for_good_metrics(self):
        """Test recommendations when metrics are already good."""
        metrics = {
            "coverage": {"line_coverage": 95.0},
            "mutation": {"skipped": False, "mutation_score": 85.0},
            "property": {"total_property_tests": 20, "enabled": True},
            "organization": {"marker_compliance": 98.0},
            "performance": {"parallel_speedup": 5.0},
        }
    
        recommendations = generate_quality_recommendations(metrics)
    
        # Should have fewer recommendations for good metrics
        assert len(recommendations) <= 2
        # Should have positive reinforcement
>       assert any("excellent" in rec.lower() for rec in recommendations)
E       assert False
E        +  where False = any(<generator object 
TestQualityReportGenerator.test_recommendations_for_good_metrics.<locals>.<genex
pr> at 0x13c1cef60>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_gen
erate_quality_report.py:142: AssertionError
__________________ test_verify_test_markers_collection_error ___________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_verify_test_markers_colle0')

    @pytest.mark.fast
    def test_verify_test_markers_collection_error(tmp_path: Path) -> None:
        """Reports collection errors. ReqID: QA-02"""
        test_file = tmp_path / "test_bad.py"
        test_file.write_text(
            "import pytest\nimport nonexistent_module\n\n@pytest.mark.fast\ndef 
test_fail():\n    pass\n",
            encoding="utf-8",
        )
    
        vtm.PERSISTENT_CACHE.clear()
        vtm.FILE_SIGNATURES.clear()
    
        result = vtm.verify_directory_markers(str(tmp_path))
>       assert result["files_with_issues"] == 1
E       assert 0 == 1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/scripts/test_ver
ify_test_markers.py:43: AssertionError
_________________________ test_audit_detects_violation _________________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_audit_detects_violation0')

    @pytest.mark.fast
    def test_audit_detects_violation(tmp_path: Path) -> None:
        """A file with forbidden patterns should be reported."""
        config = tmp_path / "unsafe.cfg"
        config.write_text("password=secret")
>       results = policy_audit.audit([config])
                  ^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'policy_audit' has no attribute 'audit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_po
licy_audit.py:18: AttributeError
_________________________ test_audit_passes_clean_file _________________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_audit_passes_clean_file0')

    @pytest.mark.fast
    def test_audit_passes_clean_file(tmp_path: Path) -> None:
        """A clean file should yield no findings."""
        config = tmp_path / "safe.cfg"
        config.write_text("value=1")
>       assert policy_audit.audit([config]) == []
               ^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'policy_audit' has no attribute 'audit'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_po
licy_audit.py:27: AttributeError
_________________________ test_run_requires_pre_deploy _________________________

mock_policy = <MagicMock name='main' id='5293842336'>
mock_safety = <MagicMock name='run_safety' id='5293837008'>
mock_bandit = <MagicMock name='run_bandit' id='5294845312'>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13aef4e00>

    @patch("security_audit.audit.run_bandit")
    @patch("security_audit.audit.run_safety")
    @patch("security_audit.verify_security_policy.main", return_value=0)
    @pytest.mark.fast
    def test_run_requires_pre_deploy(
        mock_policy: MagicMock,
        mock_safety: MagicMock,
        mock_bandit: MagicMock,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """The audit aborts if pre-deploy approval is missing."""
        monkeypatch.delenv("DEVSYNTH_PRE_DEPLOY_APPROVED", raising=False)
>       with pytest.raises(RuntimeError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'RuntimeError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/security/test_se
curity_audit.py:105: Failed
----------------------------- Captured stdout call -----------------------------
[security_audit] Environment: DEVSYNTH_VERSION=0.1.0a1, DEVSYNTH_ENV=alpha
[security] Pre-deploy policy checks not approved, but allowing for alpha release
_________________ test_mvuu_config_schema_and_sample_validate __________________

    @pytest.mark.fast
    def test_mvuu_config_schema_and_sample_validate():
        """Ensure the MVUU config schema is valid and the sample conforms to it.
    
        This test is intentionally strict but local-only:
        - validates the schema structure using Draft7Validator.check_schema
        - validates the sample instance against the schema using 
jsonschema.validate
        """
        repo_root = Path(__file__).resolve().parents[3]
        schema_path = repo_root / "docs/specifications/mvuu_config.schema.json"
        sample_path = repo_root / "docs/specifications/mvuu_config.sample.json"
    
        assert schema_path.exists(), f"Schema file not found: {schema_path}"
        assert sample_path.exists(), f"Sample config file not found: 
{sample_path}"
    
        schema = json.loads(schema_path.read_text(encoding="utf-8"))
        sample = json.loads(sample_path.read_text(encoding="utf-8"))
    
        # 1) Validate the schema is itself a valid Draft-07 schema
        Draft7Validator.check_schema(schema)
    
        # 2) Validate the sample instance against the schema
>       validate(instance=sample, schema=schema)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/specifications/t
est_mvuu_config_schema_validation.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

instance = {'$schema': './mvuu_config.schema.json', 'issues': {}, 'schema': 
'docs/specifications/mvuuschema.json', 'storage': {'format': 'json', 'path': 
'docs/specifications/mvuu_database.json'}}
schema = {'$schema': 'http://json-schema.org/draft-07/schema#', 
'additionalProperties': False, 'properties': {'issues': {'addit...ype': 
'string'}}, 'required': ['path', 'format'], 'type': 'object'}}, 'required': 
['schema', 'storage', 'issues'], ...}
cls = <class 'jsonschema.validators.Draft7Validator'>, args = (), kwargs = {}
validator = Draft7Validator(schema={'$schema': 'http://json-...ft-07/schema#', 
'additionalProperties': False, 'properties': {'issu...uired': ['path', 
'format'], 'type': 'object'}}, 'required': ['schema', 'storage', 'issues'], 
...}, format_checker=None)
error = <ValidationError: "Additional properties are not allowed ('$schema' was 
unexpected)">

    def validate(instance, schema, cls=None, *args, **kwargs):  # noqa: D417
        """
        Validate an instance under the given schema.
    
            >>> validate([2, 3, 4], {"maxItems": 2})
            Traceback (most recent call last):
                ...
            ValidationError: [2, 3, 4] is too long
    
        :func:`~jsonschema.validators.validate` will first verify that the
        provided schema is itself valid, since not doing so can lead to less
        obvious error messages and fail in less obvious or consistent ways.
    
        If you know you have a valid schema already, especially
        if you intend to validate multiple instances with
        the same schema, you likely would prefer using the
        `jsonschema.protocols.Validator.validate` method directly on a
        specific validator (e.g. ``Draft202012Validator.validate``).
    
    
        Arguments:
    
            instance:
    
                The instance to validate
    
            schema:
    
                The schema to validate with
    
            cls (jsonschema.protocols.Validator):
    
                The class that will be used to validate the instance.
    
        If the ``cls`` argument is not provided, two things will happen
        in accordance with the specification. First, if the schema has a
        :kw:`$schema` keyword containing a known meta-schema [#]_ then the
        proper validator will be used. The specification recommends that
        all schemas contain :kw:`$schema` properties for this reason. If no
        :kw:`$schema` property is found, the default validator class is the
        latest released draft.
    
        Any other provided positional and keyword arguments will be passed
        on when instantiating the ``cls``.
    
        Raises:
    
            `jsonschema.exceptions.ValidationError`:
    
                if the instance is invalid
    
            `jsonschema.exceptions.SchemaError`:
    
                if the schema itself is invalid
    
        .. rubric:: Footnotes
        .. [#] known by a validator registered with
            `jsonschema.validators.validates`
    
        """
        if cls is None:
            cls = validator_for(schema)
    
        cls.check_schema(schema)
        validator = cls(schema, *args, **kwargs)
        error = exceptions.best_match(validator.iter_errors(instance))
        if error is not None:
>           raise error
E           jsonschema.exceptions.ValidationError: Additional properties are not
allowed ('$schema' was unexpected)
E           
E           Failed validating 'additionalProperties' in schema:
E               {'$schema': 'http://json-schema.org/draft-07/schema#',
E                'title': 'DevSynth MVUU Config Schema',
E                'type': 'object',
E                'required': ['schema', 'storage', 'issues'],
E                'additionalProperties': False,
E                'properties': {'schema': {'type': 'string',
E                                          'description': 'Path to the MVUU 
record '
E                                                         'schema (JSON Schema 
file).',
E                                          'default': 
'docs/specifications/mvuuschema.json'},
E                               'storage': {'type': 'object',
E                                           'required': ['path', 'format'],
E                                           'additionalProperties': False,
E                                           'properties': {'path': {'type': 
'string',
E                                                                   
'description': 'Location '
E                                                                               
'for '
E                                                                               
'storing '
E                                                                               
'MVUU '
E                                                                               
'records '
E                                                                               
'(e.g., '
E                                                                               
'JSON '
E                                                                               
'database).',
E                                                                   'default': 
'docs/specifications/mvuu_database.json'},
E                                                          'format': {'type': 
'string',
E                                                                     'enum': 
['json'],
E                                                                     
'description': 'Storage '
E                                                                               
'format. '
E                                                                               
'Currently '
E                                                                               
'only '
E                                                                               
"'json' "
E                                                                               
'is '
E                                                                               
'supported.',
E                                                                     'default':
'json'}}},
E                               'issues': {'type': 'object',
E                                          'required': [],
E                                          'additionalProperties': False,
E                                          'properties': {'github': {'type': 
'object',
E                                                                    'required':
['base_url',
E                                                                               
'token'],
E                                                                    
'additionalProperties': False,
E                                                                    
'properties': {'base_url': {'type': 'string'},
E                                                                               
'token': {'type': 'string'}}},
E                                                         'jira': {'type': 
'object',
E                                                                  'required': 
['base_url',
E                                                                               
'token'],
E                                                                  
'additionalProperties': False,
E                                                                  'properties':
{'base_url': {'type': 'string'},
E                                                                               
'token': {'type': 'string'}}}}}}}
E           
E           On instance:
E               {'$schema': './mvuu_config.schema.json',
E                'schema': 'docs/specifications/mvuuschema.json',
E                'storage': {'path': 'docs/specifications/mvuu_database.json',
E                            'format': 'json'},
E                'issues': {}}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/jsonschema/validators.py:1332: ValidationError
____________ test_collect_behavior_tests_fallback_when_no_tests_ran ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9b48c0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_behavior_tests_fa0')

    @pytest.mark.fast
    def test_collect_behavior_tests_fallback_when_no_tests_ran(monkeypatch, 
tmp_path):
        """ReqID: TR-RT-01  Behavior/integration fallback when no tests ran."""
        # Simulate the behavior/integration fallback branch when a 
speed_category is
        # provided and the initial collection yields "no tests ran". The 
function
        # should retry with a relaxed marker expression and return the second 
set of
        # collected node ids.
    
        calls: list[dict[str, Any]] = []
    
        def fake_run(
            cmd, check=False, capture_output=False, text=False
        ):  # type: ignore[no-untyped-def]
            # Record the call for assertions
            calls.append({"cmd": cmd[:]})
            joined = " ".join(cmd)
            if "--collect-only" in cmd and "-m" in cmd:
                # First path: the pre-check for behavior/integration when
                # speed_category is set
                if "tests/behavior/" in joined or cmd[-1] == ".":
                    # Return a signal equivalent to no tests collected under 
this
                    # filter
                    return _CP(stdout="no tests ran\n", returncode=0)
            # The subsequent actual collect_cmd should run with either relaxed
            # marker or same path. Return a couple of synthetic node ids to be
            # sanitized.
            out = (
                "tests/behavior/test_fake.py::test_case\n"
                "tests/behavior/test_other.py::test_ok\n"
            )
            return _CP(stdout=out, returncode=0)
    
        monkeypatch.setattr("subprocess.run", fake_run)
    
        # Use a temporary cache dir so we don't affect the real project cache
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        # Force cache directory to tmp by changing CWD since function writes 
relative path
        monkeypatch.chdir(tmp_path)
    
        # Create a minimal behavior tests tree so pruning-by-existence keeps our
ids
        (tmp_path / "tests" / "behavior").mkdir(parents=True, exist_ok=True)
        (tmp_path / "tests" / "behavior" / "test_fake.py").write_text(
            "def test_case():\n    assert True\n"
        )
        (tmp_path / "tests" / "behavior" / "test_other.py").write_text(
            "def test_ok():\n    assert True\n"
        )
    
>       result = collect_tests_with_cache(target="behavior-tests", 
speed_category="fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_behavior_fallback.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_collect_behavior_tests_fallback_when_no_tests_ran.<locals>.fake_run() got 
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:55,742 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=behavior-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=behavior-tests (fast)  collecting via pytest
_________ test_collect_tests_with_cache_prunes_nonexistent_and_caches __________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a9f40>

    @pytest.mark.fast
    def test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path, 
monkeypatch):
        # Create a fake tests directory with a simple structure
        tests_dir = tmp_path / "tests"
        unit_dir = tests_dir / "unit"
        unit_dir.mkdir(parents=True)
        # Create some dummy test files
        t1 = unit_dir / "test_a.py"
        t1.write_text("def test_a():\n    assert True\n")
        t2 = unit_dir / "test_b.py"
        t2.write_text("def test_b():\n    assert True\n")
    
        # Monkeypatch TARGET_PATHS lookup by pretending 'all-tests' maps to our 
tmp tests dir
        from devsynth.testing import run_tests as rt
    
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tests_dir))
    
        # Simulate pytest --collect-only -q output lines via subprocess.run
        class FakeProc:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str = 
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr
    
        def fake_run(
            cmd, check=False, capture_output=True, text=True, timeout=None
        ):  # noqa: D401
            # Emit node ids including a non-existent file and a line-number 
suffix
            lines = [
                f"{t1}::test_a",
                f"{t2}:42",  # should be sanitized to path only then pruned 
check happens on path
                f"{tests_dir}/missing_test.py::test_missing",
            ]
            return FakeProc("\n".join(lines), 0, "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Ensure os.path.exists behaves normally but returns False for 
missing_test.py
        real_exists = os.path.exists
    
        def fake_exists(path):
            if str(path).endswith("missing_test.py"):
                return False
            return real_exists(path)
    
        monkeypatch.setattr(rt.os.path, "exists", fake_exists)
    
        # Point cache directory to tmp so we don't touch repo state
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path / 
".cache"))
    
>       out = collect_tests_with_cache("all-tests")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_cache_sanitize.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'all-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_____________ test_collect_tests_with_cache_synthesizes_when_empty _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04d370>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_1')

    def test_collect_tests_with_cache_synthesizes_when_empty(monkeypatch, 
tmp_path):
        """ReqID: TR-RT-02  Synthesize minimal list when collection is empty.
    
        When both primary and fallback collections return no node ids and no 
cache exists,
        collect_tests_with_cache should synthesize a minimal file list by 
scanning the
        filesystem under the target path for test_*.py files.
        """
        # Arrange: create an isolated temporary test tree
        test_root = tmp_path / "tests" / "unit"
        test_root.mkdir(parents=True)
        dummy = test_root / "test_dummy.py"
        dummy.write_text("def test_ok():\n    assert True\n")
    
        # Import module under test
        from devsynth.testing import run_tests as rt
    
        # Point the target mapping to our isolated test directory
        old_target = rt.TARGET_PATHS.get("unit-tests")
        rt.TARGET_PATHS["unit-tests"] = str(test_root)
    
        # Ensure cwd is the temp directory so the cache dir is local and empty
        monkeypatch.chdir(tmp_path)
    
        # Monkeypatch subprocess.run to simulate empty collection outputs for 
both
        # the primary and fallback collectors.
        def _fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            return types.SimpleNamespace(returncode=0, stdout="", stderr="")
    
        monkeypatch.setattr(subprocess, "run", _fake_run)
    
        # Act: call collect with speed_category=None to exercise the synthesize 
path
>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_synthesize_on_empty.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_collect_tests_with_cache_synthesizes_when_empty.<locals>._fake_run() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:55,792 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (all)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (all)  collecting via pytest
____________________ test_collect_tests_with_cache_bad_json ____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_2')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04d700>

    @pytest.mark.fast
    def test_collect_tests_with_cache_bad_json(tmp_path, monkeypatch):
        """Malformed cache file triggers regeneration.
    
        ReqID: N/A"""
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path))
        cache = tmp_path / "unit-tests_all_tests.json"
        cache.write_text("{bad json}")
    
        class Res:
            stdout = "tests/unit/sample_test.py::test_a\n"
            returncode = 0
    
        monkeypatch.setattr(subprocess, "run", lambda *a, **k: Res())
>       out = rt.collect_tests_with_cache("unit-tests", None)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_bad_json.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________________ test_cache_invalidation_on_file_change ____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cache_invalidation_on_fil0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b95f800>

    @pytest.mark.fast
    def test_cache_invalidation_on_file_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        # Arrange: create an isolated tests directory
        tests_dir = tmp_path / "isolated_tests"
        tests_dir.mkdir(parents=True, exist_ok=True)
    
        test_file = tests_dir / "test_sample.py"
        test_file.write_text(
            """
    import pytest
    
    @pytest.mark.fast
    def test_example():
        assert 1 + 1 == 2
    """
        )
    
        # Redirect cache dir to tmp
        cache_dir = tmp_path / ".cache"
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")
        # Patch globals on the module for cache dir and target path
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
    
        outputs = ["test_sample.py::test_example\n"]
        call_index = {"value": 0}
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            idx = min(call_index["value"], len(outputs) - 1)
            call_index["value"] += 1
            return SimpleNamespace(stdout=outputs[idx], stderr="", returncode=0)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Act: initial collection (populates cache)
>       first = collect_tests_with_cache("unit-tests", "fast")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___________________ test_cache_invalidation_on_marker_change ___________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cache_invalidation_on_mar0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b430a40>

    @pytest.mark.fast
    def test_cache_invalidation_on_marker_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        tests_dir = tmp_path / "isolated_tests_marker"
        tests_dir.mkdir(parents=True, exist_ok=True)
    
        test_file = tests_dir / "test_marker.py"
        test_file.write_text(
            """
    import pytest
    
    @pytest.mark.fast
    def test_fast_case():
        assert True
    """
        )
    
        cache_dir = tmp_path / ".cache"
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
    
        outputs = [
            "test_marker.py::test_fast_case\n",
            "test_marker.py::test_medium_case\n",
        ]
        call_index = {"value": 0}
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            idx = min(call_index["value"], len(outputs) - 1)
            call_index["value"] += 1
            return SimpleNamespace(stdout=outputs[idx], stderr="", returncode=0)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Populate cache for fast
>       fast_list = collect_tests_with_cache("unit-tests", "fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
________________ test_cache_invalidation_on_target_path_change _________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cache_invalidation_on_tar0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a3cb0>

    @pytest.mark.fast
    def test_cache_invalidation_on_target_path_change(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch
    ) -> None:
        tests_dir_one = tmp_path / "suite_one"
        tests_dir_one.mkdir(parents=True, exist_ok=True)
        (tests_dir_one / "test_alpha.py").write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_alpha():\n    assert 
True\n"
        )
    
        tests_dir_two = tmp_path / "suite_two"
        tests_dir_two.mkdir(parents=True, exist_ok=True)
        (tests_dir_two / "test_beta.py").write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_beta():\n    assert 
True\n"
        )
    
        cache_dir = tmp_path / ".cache"
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir_one))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999)
    
        calls = {"count": 0}
    
        class FakeProc:
            def __init__(self, stdout: str) -> None:
                self.stdout = stdout
                self.stderr = ""
                self.returncode = 0
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):
            assert "--collect-only" in cmd
            calls["count"] += 1
            cwd = os.getcwd()
            if cwd == str(tests_dir_one):
                return FakeProc("test_alpha.py::test_alpha\n")
            if cwd == str(tests_dir_two):
                return FakeProc("test_beta.py::test_beta\n")
            raise AssertionError(f"Unexpected cwd {cwd}")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       first = collect_tests_with_cache("unit-tests", "fast")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_invalidation.py:215: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_____________ test_cache_uses_fresh_cache_without_subprocess_call ______________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cache_uses_fresh_cache_wi0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04cda0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_cache_uses_fresh_cache_without_subprocess_call(tmp_path, 
monkeypatch):
        """ReqID: CACHE-TTL-1"""
        # Arrange isolated tests dir and cache dir
        tests_dir = tmp_path / "isolated_tests_fresh"
        tests_dir.mkdir(parents=True, exist_ok=True)
        t1 = tests_dir / "test_alpha.py"
        # File existence matters for pruning; create it
        t1.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n"
        )
    
        # Redirect module globals
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
    
        # Stable mtime for fingerprint match
        monkeypatch.setattr(rt.os.path, "getmtime", lambda p: 123.456)
    
        # Prepare a fresh cache file that should be reused
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_key = "unit-tests_fast"
        cache_file = cache_dir / f"{cache_key}_tests.json"
        payload = {
            "timestamp": datetime.now().isoformat(),  # fresh
            "tests": [str(t1) + "::test_ok"],
            "fingerprint": {
                "latest_mtime": 123.456,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir) + "/",
                "node_set_hash": 111,
            },
        }
        cache_file.write_text(json.dumps(payload))
    
        # If subprocess.run is called, fail the test (cache should be used)
        def forbid_run(*args, **kwargs):  # pragma: no cover - should not be hit
            raise AssertionError("subprocess.run should not be called for fresh 
cache")
    
        monkeypatch.setattr(rt.subprocess, "run", forbid_run)
    
        # Act
>       out = collect_tests_with_cache("unit-tests", "fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_ttl.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________ test_cache_ttl_expired_triggers_subprocess_and_refresh ____________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cache_ttl_expired_trigger0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04e270>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_cache_ttl_expired_triggers_subprocess_and_refresh(tmp_path, 
monkeypatch):
        """ReqID: CACHE-TTL-2"""
        # Arrange isolated tests dir and cache dir
        tests_dir = tmp_path / "isolated_tests_ttl"
        tests_dir.mkdir(parents=True, exist_ok=True)
        t1 = tests_dir / "test_beta.py"
        t1.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_beta():\n    assert 
True\n"
        )
    
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir) + "/")
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
    
        # Keep fingerprint matching except TTL
        monkeypatch.setattr(rt.os.path, "getmtime", lambda p: 999.0)
    
        # Expire quickly: TTL = 1 second
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        # Re-import TTL constant by reloading module-level var
        # Note: the module reads TTL at import; our code in run_tests.py guards
        # ValueError and sets default.
        # For simplicity we won't force re-import. collect_tests_with_cache 
reads
        # the env only at import time for TTL, but it uses the module-level int
        # COLLECTION_CACHE_TTL_SECONDS.
        # We'll monkeypatch that directly for this test.
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 1)
    
        # Prepare an old cache file whose timestamp is older than TTL
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_key = "unit-tests_fast"
        cache_file = cache_dir / f"{cache_key}_tests.json"
        old_time = (datetime.now() - timedelta(seconds=5)).isoformat()
        payload = {
            "timestamp": old_time,
            "tests": [str(t1) + "::test_beta"],
            "fingerprint": {
                "latest_mtime": 999.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir) + "/",
                "node_set_hash": 222,
            },
        }
        cache_file.write_text(json.dumps(payload))
    
        # Track subprocess invocations and emit a predictable collection
        calls = {"count": 0}
    
        class FakeProc:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str = 
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):
            calls["count"] += 1
            lines = [f"{t1}::test_beta"]
            return FakeProc("\n".join(lines), 0, "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Act: TTL expired so it should call subprocess and refresh cache
>       out = collect_tests_with_cache("unit-tests", "fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_cache_ttl.py:135: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
______________ test_collect_tests_with_cache_respects_ttl_expiry _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a5730>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_3')

    @pytest.mark.fast
    def test_collect_tests_with_cache_respects_ttl_expiry(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-01  TTL expiry invalidates stale cache entries."""
    
        tests_dir = tmp_path / "suite_ttl"
        tests_dir.mkdir()
        node = tests_dir / "test_ttl.py"
        node.write_text("def test_ttl():\n    assert True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_ttl"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt.os.path, "getmtime", lambda path: 42.0)
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 0)
    
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        payload = {
            "timestamp": (datetime.now() - timedelta(seconds=5)).isoformat(),
            "tests": [f"{node}::test_ttl"],
            "fingerprint": {
                "latest_mtime": 42.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 1,
            },
        }
        cache_file.write_text(json.dumps(payload))
    
        calls: list[list[str]] = []
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(
                returncode=0,
                stdout=f"{node}::test_ttl\n",
                stderr="",
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       result = rt.collect_tests_with_cache("unit-tests", "fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
______ test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a6420>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_4')

    @pytest.mark.fast
    def test_collect_tests_with_cache_regenerates_on_fingerprint_mismatch(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-02  Fingerprint mismatch forces a fresh collection."""
    
        tests_dir = tmp_path / "suite_fingerprint"
        tests_dir.mkdir()
        node = tests_dir / "test_fp.py"
        node.write_text("def test_fp():\n    assert True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_fp"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        cache_dir.mkdir(parents=True, exist_ok=True)
    
        monkeypatch.setattr(rt.os.path, "getmtime", lambda path: 321.0)
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 3600)
    
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        payload = {
            "timestamp": datetime.now().isoformat(),
            "tests": [f"{node}::test_fp"],
            "fingerprint": {
                "latest_mtime": 111.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 5,
            },
        }
        cache_file.write_text(json.dumps(payload))
    
        calls: list[list[str]] = []
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(
                returncode=0,
                stdout=f"{node}::test_fp\n",
                stderr="",
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       result = rt.collect_tests_with_cache("unit-tests", "fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___ test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a75c0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_5')

    @pytest.mark.fast
    def test_collect_tests_with_cache_falls_back_to_cache_when_collection_empty(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-03  Empty collections trigger fallback to pruned 
cache."""
    
        tests_dir = tmp_path / "suite_fallback"
        tests_dir.mkdir()
        keep = tests_dir / "test_keep.py"
        keep.write_text("def test_keep():\n    assert True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_fb"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_TTL_SECONDS", 0)
        cache_dir.mkdir(parents=True, exist_ok=True)
    
        cache_file = cache_dir / "unit-tests_all_tests.json"
        payload = {
            "timestamp": (datetime.now() - timedelta(seconds=10)).isoformat(),
            "tests": [
                f"{keep}::test_ok",
                f"{tests_dir / 'test_missing.py'}::test_missing",
            ],
            "fingerprint": {
                "latest_mtime": 10.0,
                "category_expr": "not memory_intensive",
                "test_path": str(tests_dir),
                "node_set_hash": 10,
            },
        }
        cache_file.write_text(json.dumps(payload))
    
        calls: list[list[str]] = []
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls.append(cmd)
            return SimpleNamespace(returncode=0, stdout="", stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        real_exists = rt.os.path.exists
    
        def fake_exists(path: str) -> bool:
            if path == str(keep):
                return True
            if path == str(tests_dir / "test_missing.py"):
                return False
            if path == str(cache_file):
                return True
            return real_exists(path)
    
        monkeypatch.setattr(rt.os.path, "exists", fake_exists)
    
>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:194: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
________ test_collect_tests_with_cache_synthesizes_and_caches_node_ids _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b884680>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_6')

    @pytest.mark.fast
    def test_collect_tests_with_cache_synthesizes_and_caches_node_ids(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: CTC-04  Synthesized node list persists to cache when 
empty."""
    
        tests_dir = tmp_path / "suite_synth"
        tests_dir.mkdir(parents=True)
        file_a = tests_dir / "test_alpha.py"
        file_a.write_text("def test_alpha():\n    assert True\n")
        nested = tests_dir / "nested"
        nested.mkdir()
        file_b = nested / "test_beta.py"
        file_b.write_text("def test_beta():\n    assert True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        cache_dir = tmp_path / ".cache_synth"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout="", stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       result = rt.collect_tests_with_cache("unit-tests", None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_additional_paths.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
__________ test_collect_uses_cached_and_prunes_when_collection_empty ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a5dc0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_uses_cached_and_p0')

>   ???

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_fallback.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_______ test_collect_falls_back_to_unfiltered_and_returns_sanitized_ids ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04dd90>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_falls_back_to_unf0')

>   ???

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_col
lect_tests_with_cache_fallback.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
____________ test_html_report_artifacts_created_with_stable_naming _____________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_html_report_artifacts_cre0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c2460c0>

    @pytest.mark.fast
    def test_html_report_artifacts_created_with_stable_naming(tmp_path, 
monkeypatch):
        """
        Verify that --report produces an HTML report under test_reports/ with a
        timestamped folder and target subdirectory, and that the naming is 
stable
        enough for tooling (YYYYMMDD_HHMMSS/target/report.html).
        """
        # Arrange: create a minimal passing test in an isolated directory
        test_file = tmp_path / "test_passes.py"
        test_file.write_text(
            """
    import pytest
    
    @pytest.mark.fast
    def test_will_pass():
        assert True
    """
        )
        # Route the unit-tests target to our isolated directory
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))
    
        # Act: run tests with HTML report generation enabled
        success, output = run_tests("unit-tests", ["fast"], report=True, 
parallel=False)
    
        # Assert: tests should pass and a report should be created in 
test_reports/
>       assert success, f"Expected success, got failure. Output was:\n{output}"
E       AssertionError: Expected success, got failure. Output was:
E         ERROR: file or directory not found: test_passes.py::test_will_pass
E         
E         ============================= test session starts 
==============================
E         platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
E         rootdir: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_html_report_artifacts_cre0/project
E         plugins: cov-7.0.0
E         asyncio: mode=Mode.AUTO, debug=False, 
asyncio_default_fixture_loop_scope=None, 
asyncio_default_test_loop_scope=function
E         collected 0 items
E         
E         ============================ no tests ran in 0.00s 
=============================
E         
E         Pytest exited with code 4. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest 
test_passes.py::test_will_pass -m not memory_intensive and fast and not gui 
--cov=src/devsynth --cov-report=json:test_reports/coverage.json 
--cov-report=html:htmlcov --cov-append
E         Troubleshooting tips:
E         - Smoke mode: reduce third-party plugin surface to isolate issues:
E           poetry run devsynth run-tests --smoke --speed=fast --no-parallel 
--maxfail=1
E         - Marker discipline: default is '-m not memory_intensive'.
E           Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.
E         - Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using 
--smoke; plugin options may fail otherwise.
E         - Diagnostics: run 'poetry run devsynth doctor' for a quick 
environment check.
E         - Narrow scope: use '-k <expr>' and '-vv' to focus a failure.
E         - Segment large suites to localize failures and flakes:
E           devsynth run-tests --target unit-tests --speed=fast --segment 
--segment-size=50
E         - Limit failures early to speed iteration:
E           poetry run devsynth run-tests --target unit-tests --speed=fast 
--maxfail=1
E         - Disable parallelism if xdist interaction is suspected:
E           devsynth run-tests --target unit-tests --speed=fast --no-parallel
E         - Generate an HTML report for context (saved under test_reports/):
E           devsynth run-tests --target unit-tests --speed=fast --report
E         
E       assert False

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_htm
l_report_artifacts.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:56,117 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:56,117 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:56,659 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
______________________ test_integration_mutation_workflow ______________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_integration_mutation_work0')

    @pytest.mark.fast
    def test_integration_mutation_workflow(tmp_path):
        """Test complete mutation testing workflow."""
        # Create a simple source file
        source_dir = tmp_path / "src"
        source_dir.mkdir()
        source_file = source_dir / "calculator.py"
        source_file.write_text(
            """
    def add(x, y):
        return x + y
    
    def is_positive(x):
        return x > 0
    
    def is_equal(x, y):
        return x == y
    """
        )
    
        # Create corresponding tests
        test_dir = tmp_path / "tests"
        test_dir.mkdir()
        test_file = test_dir / "test_calculator.py"
        test_file.write_text(
            """
    import sys
    sys.path.append('../src')
    from calculator import add, is_positive, is_equal
    
    def test_add():
        assert add(2, 3) == 5
        assert add(-1, 1) == 0
    
    def test_is_positive():
        assert is_positive(5) == True
        assert is_positive(-5) == False
    
    def test_is_equal():
        assert is_equal(5, 5) == True
        assert is_equal(5, 6) == False
    """
        )
    
        # Mock subprocess to avoid actually running tests
        with patch("subprocess.run") as mock_run:
            # Simulate that mutations are caught (tests fail)
            mock_result = MagicMock()
            mock_result.returncode = 1  # Test failed (mutation killed)
            mock_result.stdout = "FAILED"
            mock_result.stderr = ""
            mock_run.return_value = mock_result
    
            tester = MutationTester(timeout_seconds=5)
    
            # Run with limited mutations for testing
>           report = tester.run_mutations(str(source_dir), str(test_dir), 
max_mutations=5)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_mut
ation_testing.py:425: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/mutati
on_testing.py:453: in run_mutations
    file_key = str(Path(result.file_path).relative_to(Path.cwd()))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_integration_mutation_work0/src/calculator.py')
other = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_integration_mutation_work0/project')
walk_up = False, _deprecated = (), step = 0

    def relative_to(self, other, /, *_deprecated, walk_up=False):
        """Return the relative path to another path identified by the passed
        arguments.  If the operation is not possible (because this is not
        related to the other path), raise ValueError.
    
        The *walk_up* parameter controls whether `..` may be used to resolve
        the path.
        """
        if _deprecated:
            msg = ("support for supplying more than one positional argument "
                   "to pathlib.PurePath.relative_to() is deprecated and "
                   "scheduled for removal in Python {remove}")
            warnings._deprecated("pathlib.PurePath.relative_to(*args)", msg,
                                 remove=(3, 14))
        other = self.with_segments(other, *_deprecated)
        for step, path in enumerate([other] + list(other.parents)):
            if self.is_relative_to(path):
                break
            elif not walk_up:
>               raise ValueError(f"{str(self)!r} is not in the subpath of 
{str(other)!r}")
E               ValueError: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_integration_mutation_work0/src/calculator.py' is not in the subpath 
of 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_integration_mutation_work0/project'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/pathlib.py:682: ValueError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:56,785 - devsynth.testing.mutation_testing - INFO - Starting 
mutation testing: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_integration_mutation_work0/src -> 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_integration_mutation_work0/tests
2025-10-29 10:48:56,785 - devsynth.testing.mutation_testing - INFO - Found 1 
Python files to mutate
2025-10-29 10:48:56,787 - devsynth.testing.mutation_testing - INFO - Generated 8
total mutations
2025-10-29 10:48:56,787 - devsynth.testing.mutation_testing - INFO - Limited to 
5 mutations
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Starting 
mutation testing: 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_integration_mutation_work0/src -> 
/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytes
t-57/test_integration_mutation_work0/tests
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Found 1 Python 
files to mutate
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Generated 8 
total mutations
INFO     devsynth.testing.mutation_testing:logging_setup.py:615 Limited to 5 
mutations
___________________ test_run_tests_keyword_filter_no_matches ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c287c80>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_keyword_filter_0')

    @pytest.mark.fast
    def test_run_tests_keyword_filter_no_matches(monkeypatch, tmp_path):
        """ReqID: FR-11.2  Keyword path handles empty collection gracefully."""
        # Ensure test path exists to satisfy chdir logic
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)
    
        # Patch target path mapping to our tmp tests dir for unit-tests
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
    
        # Mock subprocess.run for collection to return no node ids
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            # Simulate --collect-only output with no matching lines
            return _DummyCompleted(stdout="", stderr="", returncode=0)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )
        assert ok is True
>       assert "No tests matched" in output
E       AssertionError: assert 'No tests matched' in 'Marker fallback 
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0 
_______________\n\n============================ no tests ran in 0.01s 
=============================\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests.py:121: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:56,822 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:56,822 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:56,823 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (medium)  collecting via pytest
2025-10-29 10:48:56,823 - devsynth.testing.run_tests - INFO - marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
2025-10-29 10:48:57,100 - devsynth.testing.run_tests - INFO - Coverage data file
detected at .coverage (53248 bytes)
2025-10-29 10:48:57,101 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: no measured files present
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (medium)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage data file 
detected at .coverage (53248 bytes)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: no measured files present
___________ test_collect_tests_with_cache_writes_cache_and_sanitizes ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a5280>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_7')

    @pytest.mark.fast
    def test_collect_tests_with_cache_writes_cache_and_sanitizes(monkeypatch, 
tmp_path):
        """ReqID: FR-11.2  Collection cache stores sanitized node ids."""
        tests_dir = tmp_path / "tests"
        (tests_dir / "unit").mkdir(parents=True)
        # Create dummy test files to satisfy synthesized fallback if needed
        (tests_dir / "unit" / "test_x.py").write_text("\n")
        monkeypatch.chdir(tmp_path)
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir / 
"unit"))
    
        # First run: provide some noisy output with line numbers and duplicates
        noisy = [
            "tests/unit/test_x.py:10",
            "tests/unit/test_x.py::test_ok",
            "tests/unit/test_x.py:10",  # duplicate
        ]
    
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            return _DummyCompleted(stdout="\n".join(noisy), stderr="", 
returncode=0)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        out = rt.collect_tests_with_cache("unit-tests", "fast")
        # Sanitized and deduped: line numbers removed when no '::'
>       assert out[0] == "tests/unit/test_x.py"
E       AssertionError: assert 'tests/unit/t...x.py::test_ok' == 
'tests/unit/test_x.py'
E         
E         - tests/unit/test_x.py
E         + tests/unit/test_x.py::test_ok
E         ?                     +++++++++

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests.py:215: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,119 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
________________ test_collect_tests_with_cache_handles_timeout _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a29f0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_8')

    def test_collect_tests_with_cache_handles_timeout(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Timeouts during collection yield tips but no crash.
    
        ReqID: coverage-run-tests
        """
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "1")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests")
        (tmp_path / "tests").mkdir()
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(stdout="", stderr="", returncode=-1)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       collected = rt.collect_tests_with_cache("unit-tests", 
speed_category="fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_coverage.py:180: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
        result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
        if result.returncode != 0:
            if result.stderr:
                error_message = f"Test collection failed: {result.stderr}"
            else:
                error_message = f"Test collection failed with exit code 
{result.returncode}"
    
            # Log more details for debugging
            logger.warning(
                error_message,
                extra={
                    "event": "test_collection_failed",
                    "target": target,
                    "returncode": result.returncode,
                    "stdout": (
                        result.stdout[:500] if result.stdout else None
                    ),  # First 500 chars
                    "stderr": (
                        result.stderr[:500] if result.stderr else None
                    ),  # First 500 chars
                    "speed_category": category_expr,
                },
            )
>           raise RuntimeError(error_message)
E           RuntimeError: Test collection failed with exit code -1

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1367: RuntimeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,169 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,170 - devsynth.testing.run_tests - WARNING - Test collection
failed with exit code -1
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection failed 
with exit code -1
__________ test_collect_tests_with_cache_handles_subprocess_exception __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04d3a0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_9')
caplog = <_pytest.logging.LogCaptureFixture object at 0x13b899fa0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_handles_subprocess_exception(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RT-ERR-01  Collection errors log tips and return empty list.
    
        Issue: issues/coverage-below-threshold.md
        """
    
        tests_dir = tmp_path / "tests_root"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        caplog.set_level(logging.ERROR)
    
        def boom(*_args, **_kwargs):  # noqa: ANN002
            raise RuntimeError("collection failed")
    
        monkeypatch.setattr(rt.subprocess, "run", boom)
    
>       result = rt.collect_tests_with_cache("unit-tests", 
speed_category="fast")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

_args = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_9/tests_root', '--collect-only', '-q', ...],)
_kwargs = {'capture_output': True, 'cwd': 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_C...'1', 'AWS_SDK_LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'text': True, 
...}

    def boom(*_args, **_kwargs):  # noqa: ANN002
>       raise RuntimeError("collection failed")
E       RuntimeError: collection failed

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:27: RuntimeError
______________ test_run_tests_handles_unexpected_execution_error _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b80cc20>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_handles_unexpec0')

    @pytest.mark.fast
    def test_run_tests_handles_unexpected_execution_error(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RT-ERR-02  Unexpected execution errors surface 
troubleshooting tips.
    
        Issue: issues/coverage-below-threshold.md
        """
    
        tests_dir = tmp_path / "tests_exec"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
    
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
    
        def failing_popen(*_args, **_kwargs):  # noqa: ANN002
            raise RuntimeError("boom popen")
    
        monkeypatch.setattr(rt.subprocess, "Popen", failing_popen)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )
    
        assert success is False
        assert "boom popen" in output
>       assert "Troubleshooting tips" in output
E       AssertionError: assert 'Troubleshooting tips' in 'boom popen'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:72: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,219 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,220 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
__________________ test_run_tests_segment_merges_extra_marker __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b810860>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_segment_merges_0')

    @pytest.mark.fast
    def test_run_tests_segment_merges_extra_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RT-ERR-03  Segmented runs combine marker filters with extra 
expressions.
    
        Issue: issues/coverage-below-threshold.md
        """
    
        tests_dir = tmp_path / "tests_segment"
        tests_dir.mkdir()
        (tests_dir / "test_demo.py").write_text("def test_ok():\n    assert 
True\n")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
    
        node_ids = "test_demo.py::test_ok\n"
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            marker_index = len(cmd) - 1 - cmd[::-1].index("-m")
            expr = cmd[marker_index + 1]
            assert "fast" in expr and "not memory_intensive" in expr
            assert "not slow" in expr
            return SimpleNamespace(returncode=0, stdout=node_ids, stderr="")
    
        captured_batches: list[list[str]] = []
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                captured_batches.append(cmd)
                self.returncode = 0
    
            def communicate(self):
                return ("ok\n", "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker="not slow",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cmd = ['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', 
'/private/var/folders/2v/lbs...nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_r
un_tests_segment_merges_0/tests_segment', '--collect-only', '-q', ...]
check = False, capture_output = True, text = True, timeout = 60.0
cwd = '/Users/caitlyn/Projects/github.com/ravenoak/devsynth'
env = {'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '{}', 
'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': '1', 'AWS_SDK_LOAD_CONFIG': 
'true', 'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}

    def fake_run(
        cmd,
        check=False,
        capture_output=True,
        text=True,
        timeout=None,
        cwd=None,
        env=None,
    ):  # noqa: ANN001
        assert "--collect-only" in cmd
        marker_index = len(cmd) - 1 - cmd[::-1].index("-m")
        expr = cmd[marker_index + 1]
        assert "fast" in expr and "not memory_intensive" in expr
>       assert "not slow" in expr
E       AssertionError: assert 'not slow' in 'fast and not memory_intensive'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_additional_error_paths.py:104: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,228 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,228 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
______________ test_coverage_artifacts_status_detects_empty_html _______________

coverage_paths = 
(PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-c
aitlyn/pytest-57/test_coverage_artifac.../var/folders/2v/lbss3by10y51bg7c07nd23r
h0000gn/T/pytest-of-caitlyn/pytest-57/test_coverage_artifacts_status5/htmlcov'))

    @pytest.mark.fast
    def test_coverage_artifacts_status_detects_empty_html(
        coverage_paths: tuple[Path, Path],
    ) -> None:
        """HTML reports that note missing data should trigger remediation."""
    
        coverage_json, html_dir = coverage_paths
        coverage_json.parent.mkdir(parents=True, exist_ok=True)
        coverage_json.write_text(json.dumps({"totals": {"percent_covered": 
90.0}}))
        html_dir.mkdir(parents=True, exist_ok=True)
        (html_dir / "index.html").write_text("No coverage data available")
    
        ok, reason = run_tests_module.coverage_artifacts_status()
        assert ok is False
>       assert reason is not None and "No coverage data" in reason
E       AssertionError: assert ('Coverage HTML indicates no recorded data' is 
not None and 'No coverage data' in 'Coverage HTML indicates no recorded data')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_artifacts.py:264: AssertionError
__________________ test_failure_tips_includes_command_context __________________

    @pytest.mark.fast
    def test_failure_tips_includes_command_context() -> None:
        """Failure tips prefix the command and retain segmentation guidance."""
    
        cmd = ["python", "-m", "pytest", "tests/unit"]
        tips = run_tests_module._failure_tips(2, cmd)
    
        assert tips.startswith(
            "\nPytest exited with code 2. Command: python -m pytest tests/unit"
        )
        assert "Troubleshooting tips:" in tips
        assert "Segment large suites" in tips
>       assert "Re-run failing segments" in tips
E       AssertionError: assert 'Re-run failing segments' in '\nPytest exited 
with code 2. Command: python -m pytest tests/unit\nTroubleshooting tips:\n- 
Smoke mode: reduce third-...HTML report for context (saved under 
test_reports/):\n  devsynth run-tests --target unit-tests --speed=fast 
--report\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_artifacts.py:294: AssertionError
____________ test_segmented_run_treats_benchmark_warning_as_success ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b433dd0>

    @pytest.mark.fast
    def test_segmented_run_treats_benchmark_warning_as_success(monkeypatch):
        """
        ReqID: FR-11.2
        When running in segmented mode, if a batch returns a nonzero exit code 
but
        stderr contains PytestBenchmarkWarning, the batch should be treated as
        successful. This test simulates that path deterministically.
        """
    
        # Simulate collection returning two node ids
        class DummyCompleted:
            def __init__(self, stdout: str = "", stderr: str = "") -> None:
                self.stdout = stdout
                self.stderr = stderr
                self.returncode = 0
    
        def fake_run(
            cmd: list[str], check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            # Return minimal collect-only output resembling pytest's -q 
--collect-only
            # Use python path style entries so _sanitize_node_ids accepts them
            out = "\n".join(
                [
                    "tests/unit/sample_test.py::test_one",
                    "tests/unit/sample_test.py::test_two",
                ]
            )
            return DummyCompleted(stdout=out, stderr="")
    
        monkeypatch.setattr("subprocess.run", fake_run)
    
        # Simulate pytest execution batches: nonzero returncode
        # but with a benchmark warning in stderr
        class DummyPopen:
            def __init__(
                self,
                cmd: list[str],
                stdout: Any,
                stderr: Any,
                text: bool,
                env: dict[str, str],
            ):  # noqa: D401
                # store for potential assertions/debug
                self.cmd = cmd
                # nonzero return code; handled as success due to warning in 
stderr
                self._returncode = 1
    
            def communicate(self):
                stdout = ""
                stderr = "PytestBenchmarkWarning: benchmark plugin present\n"
                return stdout, stderr
    
            @property
            def returncode(self) -> int:
                return self._returncode
    
        monkeypatch.setattr("subprocess.Popen", DummyPopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            segment=True,
            segment_size=1,  # force two batches
            parallel=False,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_benchmark_warning.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_segmented_run_treats_benchmark_warning_as_success.<locals>.fake_run() got 
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,337 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,338 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_________ test_collect_tests_with_cache_prunes_nonexistent_and_caches __________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_10')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9aa5a0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_prunes_nonexistent_and_caches(tmp_path, 
monkeypatch):
        # Direct cache into a temp directory
        import devsynth.testing.run_tests as rt
    
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path / "cache"))
    
        # Simulate pytest --collect-only output with one non-existent and one 
existent file
        existing = "tests/unit/synthetic_test_file.py::test_ok"
        missing = "tests/unit/missing_test_file.py::test_missing"
    
        stdout = f"{missing}\n{existing}\n"
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout=stdout, stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Make os.path.exists return True only for the existing path part
        real_exists = os.path.exists
    
        def fake_exists(path):  # noqa: ANN001
            if isinstance(path, str) and path.startswith(
                "tests/unit/synthetic_test_file.py"
            ):
                return True
            if isinstance(path, str) and 
path.startswith("tests/unit/missing_test_file.py"):
                return False
            return real_exists(path)
    
        monkeypatch.setattr(rt.os.path, "exists", fake_exists)
    
        results = collect_tests_with_cache(target="unit-tests", 
speed_category=None)
        assert existing in results
>       assert all(missing not in r for r in results)
E       assert False
E        +  where False = all(<generator object 
test_collect_tests_with_cache_prunes_nonexistent_and_caches.<locals>.<genexpr> 
at 0x13c8ac1e0>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cache_prune_and_tips.py:65: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,376 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (all)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (all)  collecting via pytest
_________________ test_prunes_nonexistent_paths_and_uses_cache _________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_prunes_nonexistent_paths_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c04f8c0>

    @pytest.mark.fast
    def test_prunes_nonexistent_paths_and_uses_cache(tmp_path, monkeypatch):
        """ReqID: CACHE-PRUNE-1"""
        # Prepare a fake cache with a nonexistent test path and an existent one
        os.makedirs(COLLECTION_CACHE_DIR, exist_ok=True)
        cache_file = os.path.join(COLLECTION_CACHE_DIR, 
"unit-tests_all_tests.json")
        existent = "tests/unit/test_example.py::test_ok"
        nonexistent = "tests/unit/test_deleted.py::test_gone"
        # Ensure the existent path exists on filesystem for the pruning check
        exist_dir = Path("tests/unit")
        exist_dir.mkdir(parents=True, exist_ok=True)
        exist_file = exist_dir / "test_example.py"
        if not exist_file.exists():
            exist_file.write_text(
                "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n"
            )
    
        cache_payload = {
            "timestamp": "2099-01-01T00:00:00",
            "tests": [existent, nonexistent],
            "fingerprint": {
                "latest_mtime": 0.0,
                "category_expr": "not memory_intensive",
                "test_path": "tests/",
                "node_set_hash": 123,
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cache_payload, f)
    
        # Monkeypatch TTL to be huge so cache would be used if fingerprint 
matches
        monkeypatch.setenv("DEVSYNTH_COLLECTION_CACHE_TTL_SECONDS", "999999")
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            """Return the cached node ids without invoking pytest."""
    
            assert "--collect-only" in cmd, cmd
            return SimpleNamespace(
                returncode=0,
                stdout="\n".join([existent, nonexistent]),
                stderr="",
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Force fingerprint mismatch by changing latest_mtime via
        # monkeypatching os.path.getmtime
        original_getmtime = os.path.getmtime
    
        def fake_getmtime(path):
            return 1.0
    
        monkeypatch.setattr(os.path, "getmtime", fake_getmtime)
    
        # Now call collection; it should regenerate and prune the nonexistent 
path
        out = collect_tests_with_cache(target="all-tests", speed_category=None)
        assert existent in out
>       assert all(nonexistent != nid for nid in out)
E       assert False
E        +  where False = all(<generator object 
test_prunes_nonexistent_paths_and_uses_cache.<locals>.<genexpr> at 0x13cb0bc60>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cache_pruning.py:77: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,384 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=all-tests (all)  decomposing all-tests into dependent 
targets
2025-10-29 10:48:57,384 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (all)  collecting via pytest
2025-10-29 10:48:57,384 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=integration-tests (all)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=all-tests (all)  decomposing all-tests into dependent targets
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (all)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=integration-tests (all)  collecting via pytest
____________ test_segmented_batch_exception_emits_tips_and_plugins _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3705c0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_batch_exception0')

    @pytest.mark.fast
    def test_segmented_batch_exception_emits_tips_and_plugins(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENT-CLI-2  Exceptions surface tips and preserve
plugins."""
    
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_segment.py"
        test_file.write_text("def test_fail():\n    assert True\n")
    
        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None, 
**kwargs
        ):  # noqa: ANN001
            return SimpleNamespace(
                returncode=0, stdout=f"{test_file}::test_fail\n", stderr=""
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
    
        captured_envs: list[dict[str, str]] = []
    
        def exploding_batch(
            cmd, stdout=None, stderr=None, text=True, env=None
        ):  # noqa: ANN001
            captured_envs.append(dict(env or {}))
            raise RuntimeError("segmented batch crashed")
    
        monkeypatch.setattr(rt.subprocess, "Popen", exploding_batch)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=3,
            extra_marker=None,
        )
    
        assert success is False
        assert "segmented batch crashed" in output
>       assert output.count("Troubleshooting tips:") == 2
E       AssertionError: assert 0 == 2
E        +  where 0 = <built-in method count of str object at 
0x13cb57a50>('Troubleshooting tips:')
E        +    where <built-in method count of str object at 0x13cb57a50> = 
'Failed to run tests: segmented batch crashed'.count

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:186: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,402 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-29 10:48:57,402 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-29 10:48:57,402 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,402 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,403 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 1 for target=unit-tests
2025-10-29 10:48:57,403 - devsynth.testing.run_tests - INFO - Running segment 
1/1 (1 tests)
2025-10-29 10:48:57,403 - devsynth.testing.run_tests - ERROR - Failed to run 
tests: segmented batch crashed
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov 
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)
ERROR    devsynth.testing.run_tests:logging_setup.py:615 Failed to run tests: 
segmented batch crashed
_______________ test_segmented_batches_reinject_when_env_mutates _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c3721e0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_batches_reinjec0')

    @pytest.mark.fast
    def test_segmented_batches_reinject_when_env_mutates(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """Segments reapply plugin directives even if previous runs stripped 
them."""
    
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        first = tmp_path / "test_first.py"
        second = tmp_path / "test_second.py"
        first.write_text("def test_one():\n    assert True\n")
        second.write_text("def test_two():\n    assert True\n")
    
        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None, 
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout="\n".join([f"{first}::test_one", f"{second}::test_two"]),
                stderr="",
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
    
        popen_envs: list[dict[str, str]] = []
    
        class MutatingPopen:
            call_index = 0
    
            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=True,
                env=None,
            ):  # noqa: ANN001
                MutatingPopen.call_index += 1
                env_map = dict(env or {})
                popen_envs.append(env_map)
                _assert_plugins_in_env(env_map)
    
                if env is not None:
                    tokens = env.get("PYTEST_ADDOPTS", "").split()
                    filtered = [
                        token
                        for token in tokens
                        if token not in {"-p", "pytest_cov", 
"pytest_bdd.plugin"}
                    ]
                    env["PYTEST_ADDOPTS"] = " ".join(filtered)
    
                self.returncode = 0
                self._stdout = f"segment {MutatingPopen.call_index} ok"
                self._stderr = ""
    
            def communicate(self):  # noqa: D401 - subprocess API emulation
                """Return the stubbed stdout/stderr pair."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "Popen", MutatingPopen)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )
    
        assert success is True
        assert "segment 1 ok" in output
        assert "segment 2 ok" in output
        assert len(popen_envs) == 2
        for env_snapshot in popen_envs:
            _assert_plugins_in_env(env_snapshot)
>       _assert_plugins_in_env(dict(os.environ))

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:280: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:39: in _assert_plugins_in_env
    _assert_plugins_in_addopts(env.get("PYTEST_ADDOPTS", ""))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

addopts = '-q'

    def _assert_plugins_in_addopts(addopts: str) -> None:
        """Assert pytest-cov and pytest-bdd plugins are present in addopts."""
    
        normalized = addopts.strip()
        assert normalized, "PYTEST_ADDOPTS should not be empty when plugins are 
injected"
>       assert (
            "-p pytest_cov" in normalized or "-ppytest_cov" in normalized
        ), f"pytest-cov plugin missing: {normalized}"
E       AssertionError: pytest-cov plugin missing: -q
E       assert ('-p pytest_cov' in '-q' or '-ppytest_cov' in '-q')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:28: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,413 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-29 10:48:57,413 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-29 10:48:57,413 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,414 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,414 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-29 10:48:57,414 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (1 tests)
2025-10-29 10:48:57,414 - devsynth.testing.run_tests - INFO - Running segment 
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov 
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1 
tests)
_________ test_run_tests_env_var_propagation_retains_existing_addopts __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31e570>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_env_var_propaga0')

    @pytest.mark.fast
    def test_run_tests_env_var_propagation_retains_existing_addopts(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-ENV-1  CLI helper preserves existing 
PYTEST_ADDOPTS."""
    
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")
        monkeypatch.setenv("PYTEST_ADDOPTS", "-q")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_env.py"
        test_file.write_text("def test_env():\n    assert True\n")
    
        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None, 
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(returncode=0, 
stdout=f"{test_file}::test_env", stderr="")
    
        recorded: list[tuple[list[str], dict[str, str]]] = []
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                recorded.append((list(cmd), dict(env or {})))
                self.returncode = 0
                self._stdout = "pass"
                self._stderr = ""
    
            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            maxfail=1,
            extra_marker=None,
        )
    
        assert success is True
        assert output == "pass"
        assert recorded, "Expected subprocess invocation to be recorded"
    
        process_addopts = os.environ.get("PYTEST_ADDOPTS", "")
        assert "-q" in process_addopts
>       _assert_plugins_in_addopts(process_addopts)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:340: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

addopts = '-q'

    def _assert_plugins_in_addopts(addopts: str) -> None:
        """Assert pytest-cov and pytest-bdd plugins are present in addopts."""
    
        normalized = addopts.strip()
        assert normalized, "PYTEST_ADDOPTS should not be empty when plugins are 
injected"
>       assert (
            "-p pytest_cov" in normalized or "-ppytest_cov" in normalized
        ), f"pytest-cov plugin missing: {normalized}"
E       AssertionError: pytest-cov plugin missing: -q
E       assert ('-p pytest_cov' in '-q' or '-ppytest_cov' in '-q')

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:28: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,430 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-29 10:48:57,430 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-29 10:48:57,430 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,431 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov 
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_____________ test_run_tests_option_wiring_includes_expected_flags _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31ef00>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_option_wiring_i0')

    @pytest.mark.fast
    def test_run_tests_option_wiring_includes_expected_flags(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
    ) -> None:
        """ReqID: RUN-TESTS-PYTEST-OPTS-1  Command wiring emits coverage/report
args."""
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_opts.py"
        test_file.write_text("def test_opts():\n    assert True\n")
    
        class FakeDT:
            @staticmethod
            def now(tz=None) -> SimpleNamespace:  # type: ignore[no-untyped-def]
                return SimpleNamespace(
                    isoformat=lambda: "2025-01-02T00:00:00",
                    strftime=lambda fmt: "20250102_000000",
                )
    
        monkeypatch.setattr(rt, "datetime", FakeDT)
    
        def fake_collect(
            cmd, check=False, capture_output=True, text=True, timeout=None, 
**kwargs
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0, stdout=f"{test_file}::test_opts", stderr=""
            )
    
        recorded: list[list[str]] = []
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                recorded.append(list(cmd))
                self.returncode = 0
                self._stdout = "opts"
                self._stderr = ""
    
            def communicate(self):
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=True,
            report=True,
            parallel=False,
            segment=False,
            maxfail=3,
            extra_marker=None,
        )
    
        assert success is True
        assert "opts" in output  # The main output should contain "opts"
        assert recorded, "Expected pytest command to be recorded"
    
        cmd = recorded[0]
>       assert "--maxfail=3" in cmd
E       AssertionError: assert '--maxfail=3' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 
'/private/var/folders/2v/lbs...st-57/test_run_tests_option_wiring_i0/test_opts.p
y::test_opts', '-m', 'not memory_intensive and fast and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_helpers_focus.py:412: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,573 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,573 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,574 - devsynth.testing.run_tests - WARNING - Skipping 
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json
_______________ test_cli_marker_expression_includes_extra_marker _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31d760>

    @pytest.mark.fast
    def test_cli_marker_expression_includes_extra_marker(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-1  CLI marker flag augments default 
filters."""
    
        commands: list[list[str]] = []
    
        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                commands.append(cmd)
                self.returncode = 0
    
            def communicate(self) -> tuple[str, str]:
                return ("collected 1 item", "")
    
        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="custom_marker",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_cli_marker_expression_includes_extra_marker.<locals>.DummyProcess.__init__(
) got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,590 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,590 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
__________________ test_cli_failure_surfaces_actionable_tips ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31f710>

    @pytest.mark.fast
    def test_cli_failure_surfaces_actionable_tips(
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ERROR-1  CLI surfaces troubleshooting tips."""
    
        commands: list[list[str]] = []
    
        class FailingProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                commands.append(cmd)
                raise RuntimeError("simulated failure")
    
        monkeypatch.setattr(rt.subprocess, "Popen", FailingProcess)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_cli_failure_surfaces_actionable_tips.<locals>.FailingProcess.__init__() got
an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,635 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,635 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
________________ test_cli_segment_failure_emits_aggregate_tips _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31c320>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_segment_failure_emits0')

    @pytest.mark.fast
    def test_cli_segment_failure_emits_aggregate_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-SEGMENT-2  failing batch surfaces aggregate 
tips."""
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_segmented.py"
        test_file.write_text(
            "def test_batch_one():\n    assert True\n\n"
            "def test_batch_two():\n    assert True\n"
        )
    
        node_ids = [
            f"{test_file}::test_batch_one",
            f"{test_file}::test_batch_two",
        ]
    
        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            return SimpleNamespace(stdout="\n".join(node_ids), stderr="", 
returncode=0)
    
        batch_commands: list[list[str]] = []
        responses = [
            ("batch one ok", "", 0),
            ("batch two fail", "collected errors", 2),
        ]
        tips_record: list[tuple[int, tuple[str, ...], str]] = []
    
        def fake_failure_tips(returncode: int, cmd: list[str]) -> str:
            tip = f"[tip {returncode} #{len(tips_record)}]"
            tips_record.append((returncode, tuple(cmd), tip))
            return tip
    
        call_index = {"value": 0}
    
        class DummyBatchProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                idx = call_index["value"]
                batch_commands.append(cmd)
                stdout_payload, stderr_payload, returncode = responses[idx]
                self._stdout = stdout_payload
                self._stderr = stderr_payload
                self.returncode = returncode
                call_index["value"] += 1
    
            def communicate(self) -> tuple[str, str]:
                return (self._stdout, self._stderr)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyBatchProcess)
        monkeypatch.setattr(rt, "_failure_tips", fake_failure_tips)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
        )
    
        assert success is False
        assert len(batch_commands) == 2, "Segmented execution should spawn two 
batches"
        assert tips_record, "Expected failure tips to be generated"
    
        failing_tip = tips_record[0]
>       aggregate_tip = tips_record[1]
                        ^^^^^^^^^^^^^^
E       IndexError: list index out of range

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:281: IndexError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,688 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,688 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,688 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-29 10:48:57,688 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (1 tests)
2025-10-29 10:48:57,689 - devsynth.testing.run_tests - INFO - Running segment 
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1 
tests)
__________________ test_cli_marker_filters_merge_extra_marker __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31de50>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_marker_filters_merge_0')

    @pytest.mark.fast
    def test_cli_marker_filters_merge_extra_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-3  speed markers combine with extra 
filter."""
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_filters.py"
        test_file.write_text("def test_placeholder():\n    assert True\n")
    
        collect_invocations: list[list[str]] = []
    
        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            collect_invocations.append(cmd)
            return SimpleNamespace(
                stdout=f"{test_file}::test_placeholder\n", stderr="", 
returncode=0
            )
    
        run_commands: list[list[str]] = []
    
        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                run_commands.append(cmd)
                self.returncode = 0
    
            def communicate(self) -> tuple[str, str]:
                return ("ok", "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast", "slow"],
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="custom_marker",
        )
    
        assert success is True
        assert run_commands, "Expected run commands for each speed"
        assert collect_invocations, "Expected collection to occur"
        assert output.count("ok") == len(run_commands)
    
        collect_strings = [" ".join(cmd) for cmd in collect_invocations]
>       assert any(
            "(fast and not memory_intensive) and (custom_marker)" in cmd
            for cmd in collect_strings
        )
E       assert False
E        +  where False = any(<generator object 
test_cli_marker_filters_merge_extra_marker.<locals>.<genexpr> at 0x13caa1d80>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:437: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,710 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,710 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,711 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (slow)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (slow)  collecting via pytest
___________________ test_cli_report_mode_adds_html_argument ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8e9340>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_report_mode_adds_html0')

    @pytest.mark.fast
    def test_cli_report_mode_adds_html_argument(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-REPORT-1  report flag appends HTML output 
options."""
    
        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        test_file = tmp_path / "test_report.py"
        test_file.write_text("def test_report():\n    assert True\n")
    
        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            return SimpleNamespace(
                stdout=f"{test_file.name}::test_report\n", stderr="", 
returncode=0
            )
    
        run_commands: list[list[str]] = []
    
        class DummyProcess:
            def __init__(
                self,
                cmd: list[str],
                stdout=None,
                stderr=None,
                text: bool = False,
                env: dict[str, str] | None = None,
            ) -> None:
                run_commands.append(cmd)
                self.returncode = 0
    
            def communicate(self) -> tuple[str, str]:
                return ("report ok", "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", DummyProcess)
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=True,
            parallel=False,
        )
    
        assert success is True
        assert "report ok" in output
        assert run_commands, "Expected report-mode command to run"
    
        html_args = [arg for arg in run_commands[0] if 
arg.startswith("--html=")]
>       assert html_args, run_commands[0]
E       AssertionError: 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 'test_report.py::test_report', '-m', 'not memory_intensive and (fast 
or medium) and not gui', ...]
E       assert []

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:508: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,726 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,726 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,727 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (medium)  collecting via pytest
2025-10-29 10:48:57,727 - devsynth.testing.run_tests - WARNING - Skipping 
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (medium)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json
___________ test_cli_keyword_filter_returns_success_when_no_matches ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a9820>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_cli_keyword_filter_return0')

    @pytest.mark.fast
    def test_cli_keyword_filter_returns_success_when_no_matches(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RUN-TESTS-CLI-ARGS-4  keyword fallback exits cleanly when 
empty."""
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
    
        lifecycle: list[str] = []
    
        def fake_reset() -> None:
            lifecycle.append("reset")
    
        def fake_ensure() -> None:
            lifecycle.append("ensure")
    
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", fake_reset)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", fake_ensure)
    
        collect_commands: list[list[str]] = []
    
        def fake_collect(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
            timeout: float | None = None,
            cwd: str | None = None,
            env: dict[str, str] | None = None,
        ) -> SimpleNamespace:
            collect_commands.append(cmd)
            return SimpleNamespace(stdout="", stderr="", returncode=0)
    
        def fail_popen(*args, **kwargs):
            raise AssertionError("Popen should not be invoked when no tests 
match")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", fail_popen)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="requires_resource('lmstudio')",
        )
    
        assert collect_commands, "Expected keyword-filter collection to execute"
        collect_tokens = " ".join(collect_commands[0])
        assert "-k lmstudio" in collect_tokens
    
>       assert success is True
E       assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:722: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,761 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,762 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:57,762 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (medium)  collecting via pytest
2025-10-29 10:48:57,763 - devsynth.testing.run_tests - INFO - marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
2025-10-29 10:48:57,763 - devsynth.testing.run_tests - ERROR - Failed to run 
tests: Popen should not be invoked when no tests match
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (medium)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
ERROR    devsynth.testing.run_tests:logging_setup.py:615 Failed to run tests: 
Popen should not be invoked when no tests match
____________ test_run_tests_generates_artifacts_for_normal_profile _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b7a8410>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_generates_artif0')

    @pytest.mark.fast
    def test_run_tests_generates_artifacts_for_normal_profile(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Normal run writes `.coverage`, JSON, and HTML artifacts via the 
harness."""
    
        monkeypatch.chdir(tmp_path)
    
        coverage_json = tmp_path / "reports" / "coverage.json"
        html_dir = tmp_path / "htmlcov"
        monkeypatch.setattr(rt, "COVERAGE_JSON_PATH", coverage_json)
        monkeypatch.setattr(rt, "COVERAGE_HTML_DIR", html_dir)
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests/unit")
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", "tests")
    
        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda *_: 
["tests/unit/test_ok.py::test_one"]
        )
    
        lifecycle: list[str] = []
        monkeypatch.setattr(
            rt, "_reset_coverage_artifacts", lambda: lifecycle.append("reset")
        )
        monkeypatch.setattr(
            rt, "_ensure_coverage_artifacts", lambda: lifecycle.append("ensure")
        )
    
        popen_envs: list[dict[str, str]] = []
    
        def fake_single_batch(
            config: rt.SingleBatchRequest,
        ) -> rt.BatchExecutionResult:
            popen_envs.append(dict(config.env))
            tmp_path.joinpath(".coverage").write_text("data")
            html_dir.mkdir(parents=True, exist_ok=True)
            (html_dir / "index.html").write_text("<html>ok</html>")
            coverage_json.parent.mkdir(parents=True, exist_ok=True)
            coverage_json.write_text(json.dumps({"totals": {"percent_covered": 
98.7}}))
            return True, "batch ok", build_batch_metadata("batch-cli-artifacts")
    
        monkeypatch.setattr(rt, "_run_single_test_batch", fake_single_batch)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=True,
            parallel=False,
        )
    
        assert success is True
>       assert output == "batch ok"
E       AssertionError: assert 'batch ok\n[k...hold=70.00%\n' == 'batch ok'
E         
E         - batch ok
E         + batch ok
E         ?         +
E         + [knowledge-graph] coverage gate pass  QualityGate 
e387790e-23a9-4dff-8436-cb054742acfe (new), TestRun 
9783a613-c4b3-448f-8ecc-2e80a41e7b3f (new), Evidence 
[03b38f0c-f2ba-4f67-b6a8-17a4f2971cc3 (new), 
3aeea957-0745-4a34-b130-a3d2d0bca096 (new)] via networkx; coverage=98.70% 
threshold=70.00%

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:779: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,776 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
__________ test_run_tests_generates_artifacts_with_autoload_disabled ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8e91c0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_generates_artif1')

    @pytest.mark.fast
    def test_run_tests_generates_artifacts_with_autoload_disabled(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """Smoke-style environments still create coverage artifacts with plugin 
injection."""
    
        monkeypatch.chdir(tmp_path)
    
        coverage_json = tmp_path / "reports" / "coverage.json"
        html_dir = tmp_path / "htmlcov"
        monkeypatch.setattr(rt, "COVERAGE_JSON_PATH", coverage_json)
        monkeypatch.setattr(rt, "COVERAGE_HTML_DIR", html_dir)
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", "tests/unit")
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", "tests")
    
        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda *_: 
["tests/unit/test_ok.py::test_one"]
        )
    
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        captured_envs: list[dict[str, str]] = []
    
        def fake_single_batch(
            config: rt.SingleBatchRequest,
        ) -> rt.BatchExecutionResult:
            captured_envs.append(dict(config.env))
            tmp_path.joinpath(".coverage").write_text("data")
            html_dir.mkdir(parents=True, exist_ok=True)
            (html_dir / "index.html").write_text("<html>smoke</html>")
            coverage_json.parent.mkdir(parents=True, exist_ok=True)
            coverage_json.write_text(json.dumps({"totals": {"percent_covered": 
94.2}}))
            return True, "smoke ok", build_batch_metadata("batch-cli-smoke")
    
        monkeypatch.setattr(rt, "_run_single_test_batch", fake_single_batch)
    
        env = {"PYTEST_DISABLE_PLUGIN_AUTOLOAD": "1"}
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=True,
            report=True,
            parallel=True,
            env=env,
        )
    
        assert success is True
>       assert output == "smoke ok"
E       AssertionError: assert 'smoke ok\n[k...hold=70.00%\n' == 'smoke ok'
E         
E         - smoke ok
E         + smoke ok
E         ?         +
E         + [knowledge-graph] coverage gate pass  QualityGate 
a46698d1-a93f-492c-a88d-e8e16725fe1e (new), TestRun 
91f57ccd-3d1e-45e6-ad2f-69916edff966 (new), Evidence 
[e58d8882-1262-49ce-986c-00b9f31d8f39 (new), 
464cf868-62e6-4388-919c-696719efa4db (new)] via networkx; coverage=94.20% 
threshold=70.00%

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_cli_invocation.py:838: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,792 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_cov into PYTEST_ADDOPTS to preserve coverage instrumentation
2025-10-29 10:48:57,792 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
2025-10-29 10:48:57,792 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p pytest_cov 
into PYTEST_ADDOPTS to preserve coverage instrumentation
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_bdd.plugin into PYTEST_ADDOPTS to preserve pytest-bdd hooks
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
_________ test_ensure_coverage_artifacts_skips_when_module_unavailable _________

coverage_test_environment = 
namespace(coverage_json=PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd2
3rh0000gn/T/pytest-of-caitlyn/pytest-5.../folders/2v/lbss3by10y51bg7c07nd23rh000
0gn/T/pytest-of-caitlyn/pytest-57/test_ensure_coverage_artifacts4/legacy/html'))
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b48ef30>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13b7ce3c0>

    @pytest.mark.fast
    def test_ensure_coverage_artifacts_skips_when_module_unavailable(
        coverage_test_environment: SimpleNamespace,
        monkeypatch: pytest.MonkeyPatch,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """ReqID: COV-ART-02A  Missing coverage module logs debug message and 
exits."""
    
        monkeypatch.delitem(sys.modules, "coverage", raising=False)
    
        original_import = builtins.__import__
    
        def fake_import(name, *args, **kwargs):  # noqa: ANN001
            if name == "coverage":
                raise ImportError("coverage module unavailable")
            return original_import(name, *args, **kwargs)
    
        monkeypatch.setattr(builtins, "__import__", fake_import)
    
        with caplog.at_level(logging.DEBUG, 
logger="devsynth.testing.run_tests"):
            rt._ensure_coverage_artifacts()
    
>       assert any("coverage library unavailable" in message for message in 
caplog.messages)
E       assert False
E        +  where False = any(<generator object 
test_ensure_coverage_artifacts_skips_when_module_unavailable.<locals>.<genexpr> 
at 0x11c928380>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_coverage_artifacts.py:218: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,841 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
________________ test_keyword_filter_no_matches_returns_success ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9e6030>

    @pytest.mark.fast
    def test_keyword_filter_no_matches_returns_success(monkeypatch) -> None:
        """ReqID: RT-01  keyword filter with no matches returns success and 
message."""
    
        # Simulate collect-only returning no matching node ids under keyword 
filter
        def fake_run(
            cmd,  # noqa: ANN001
            check: bool = False,
            capture_output: bool = False,
            text: bool = False,
        ):  # type: ignore[no-redef]
            class R:
                def __init__(self) -> None:
                    self.returncode = 0
                    # nothing that matches the node id regex
                    self.stdout = "\n"
                    self.stderr = ""
    
            return R()
    
        # Ensure Popen is not invoked if there are no node ids
        def fail_popen(*args, **kwargs):  # type: ignore[no-redef]
            pytest.fail("Popen should not be called when no node ids match")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", fail_popen)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_keyword_filter_no_matches_returns_success.<locals>.fake_run() got an 
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,935 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,936 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_________________ test_failure_tips_appended_on_nonzero_return _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9e6810>

    @pytest.mark.fast
    def test_failure_tips_appended_on_nonzero_return(monkeypatch) -> None:
        """ReqID: RT-02  non-zero exit appends troubleshooting tips."""
    
        # Make the simple '-m not memory_intensive' path run and return a 
non-zero code
        class DummyProc:
            def __init__(self) -> None:
                self.returncode = 2
    
            def communicate(self) -> tuple[str, str]:
                return ("", "boom")
    
        def fake_popen(
            args,  # noqa: ANN001
            stdout=None,  # noqa: ANN001
            stderr=None,  # noqa: ANN001
            text=None,  # noqa: ANN001
            env=None,  # noqa: ANN001
        ):  # type: ignore[no-redef]
            return DummyProc()
    
        monkeypatch.setattr(rt.subprocess, "Popen", fake_popen)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_failure_tips_appended_on_nonzero_return.<locals>.fake_popen() got an 
unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:57,966 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:57,966 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
___________ test_keyword_filter_lmstudio_no_matches_returns_success ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9e4c20>

    @pytest.mark.fast
    def test_keyword_filter_lmstudio_no_matches_returns_success(monkeypatch):
        """ReqID: TR-RT-08  Keyword 'lmstudio' no-match returns success.
    
        When extra_marker includes requires_resource('lmstudio') and collection
        yields no matching node ids, run_tests should return success=True with a
        helpful message without attempting to execute pytest on any node ids.
        """
    
        class DummyRunResult:
            def __init__(self):
                self.stdout = ""  # no node ids collected
                self.stderr = ""
                self.returncode = 0
    
        calls = {
            "run": [],
            "popen": [],
        }
    
        def fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            calls["run"].append(cmd)
            return DummyRunResult()
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-redef]
                calls["popen"].append(cmd)
                # This path should not be reached because no node ids => early 
return
                self._stdout = ""
                self._stderr = ""
                self.returncode = 0
    
            def communicate(self):
                return self._stdout, self._stderr
    
        import subprocess
    
        monkeypatch.setattr(subprocess, "run", fake_run)
        monkeypatch.setattr(subprocess, "Popen", FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_keyword_filter_lmstudio_no_matches_returns_success.<locals>.fake_run() got 
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,007 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,007 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
__________________ test_extra_marker_merges_into_m_expression __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9e4f20>

    @pytest.mark.fast
    def test_extra_marker_merges_into_m_expression(monkeypatch):
        """ReqID: TR-RT-09  Non-keyword extra_marker merges into -m 
expression."""
        """
        When extra_marker does not invoke the keyword fallback, ensure it is 
merged
        into the -m expression and subprocess.Popen is called once; run_tests 
returns
        success if the process returncode is 0.
        """
    
        class FakePopen:
            last_cmd = None
    
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-redef]
                FakePopen.last_cmd = cmd
                self.returncode = 0
                self._stdout = "pytest ok\n"
                self._stderr = ""
    
            def communicate(self):
                return self._stdout, self._stderr
    
        import subprocess
    
        def fake_run(
            cmd, check=False, capture_output=True, text=True
        ):  # type: ignore[override]
            # Not used in this path; ensure it's not called unnecessarily
            raise AssertionError("subprocess.run should not be called for 
non-keyword path")
    
        monkeypatch.setattr(subprocess, "Popen", FakePopen)
        monkeypatch.setattr(subprocess, "run", fake_run)
    
        extra = "slow or (requires_resource('codebase'))"
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=extra,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_extra_marker_merges_into_m_expression.<locals>.fake_run() got an unexpected
keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,037 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,037 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_________ test_run_tests_merges_extra_marker_into_category_expression __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31f7d0>

    @pytest.mark.fast
    def 
test_run_tests_merges_extra_marker_into_category_expression(monkeypatch):
        """ReqID: TR-RT-09  Merge extra_marker into -m expression.
    
        When speed_categories is None and extra_marker is a normal expression
        (not a requires_resource('lmstudio') keyword case), run_tests should 
pass
        a combined -m expression that includes both the base filter and the 
extra
        marker.
        """
        import devsynth.testing.run_tests as rt
    
        captured_cmd = {}
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # The command should include '-m' with the merged expression
                assert "-m" in cmd, f"-m missing in: {cmd}"
                # There are two '-m' flags: Python module and pytest marker; use
the last
                idx = len(cmd) - 1 - cmd[::-1].index("-m")
                expr = cmd[idx + 1]
                assert "not memory_intensive" in expr
                assert "(not slow)" in expr or "not slow" in expr
                captured_cmd["cmd"] = cmd
                self.returncode = 0
    
            def communicate(self):
                return ("ok\n", "")
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="not slow",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_marker_passthrough.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_run_tests_merges_extra_marker_into_category_expression.<locals>.FakePopen._
_init__() got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,066 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,067 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_______________ test_collect_fallback_on_behavior_speed_no_tests _______________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_fallback_on_behav0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b956db0>

    @pytest.mark.fast
    def test_collect_fallback_on_behavior_speed_no_tests(tmp_path, monkeypatch):
        """When behavior-tests with a speed filter yields no tests, fallback to 
marker_expr.
    
        We simulate the preliminary check returning a 'no tests ran' message and
assert that
        the second collection call (fallback) is invoked and its items are 
returned.
        ReqID: C3 (coverage of fallback branch)
        """
        # Arrange
        from devsynth.testing import run_tests as rt
    
        # Redirect cache dir and target path to isolated temp locations
        cache_dir = tmp_path / ".cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "behavior-tests", str(tmp_path))
        # Create a real file that the pruning check will accept
        real_file = tmp_path / "test_example.py"
        real_file.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n"
        )
    
        # Prepare fake subprocess.run that returns two different responses:
        calls = {"invocations": []}
    
        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str = 
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            calls["invocations"].append(cmd)
            # First invocation is the preliminary check (with same 
category_expr)
            if len(calls["invocations"]) == 1:
                return FakeCompleted(stdout="no tests ran\n", returncode=0)
            # Second invocation should be the fallback collection using 
marker_expr only
            # Return a couple of node ids
            return FakeCompleted(stdout=f"{real_file}::test_ok\n")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Act
>       out = rt.collect_tests_with_cache(target="behavior-tests", 
speed_category="fast")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'behavior-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
___________________ test_collect_malformed_cache_regenerates ___________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_malformed_cache_r0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b955f70>

    @pytest.mark.fast
    def test_collect_malformed_cache_regenerates(tmp_path, monkeypatch):
        """Malformed JSON cache should be ignored and collection regenerated.
    
        ReqID: C3 (coverage of malformed cache read path)
        """
        from devsynth.testing import run_tests as rt
    
        # Point cache dir and target path
        cache_dir = tmp_path / ".cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        # Create a real file for existence pruning
        real2 = tmp_path / "test_a.py"
        real2.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_b():\n    assert 
True\n"
        )
    
        # Write malformed JSON to the expected cache file for key all-tests_all
        cache_file = cache_dir / "all-tests_all_tests.json"
        cache_file.write_text("{ not-json }")
    
        # Fake subprocess.run to return one id
        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str = 
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr
    
        monkeypatch.setattr(
            rt.subprocess,
            "run",
            lambda *a, **k: FakeCompleted(stdout=f"{real2}::test_b\n"),
        )
    
>       out = rt.collect_tests_with_cache(target="all-tests", 
speed_category=None)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'all-tests', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
__________ test_run_tests_lmstudio_extra_marker_keyword_early_success __________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_lmstudio_extra_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a10a0>

    @pytest.mark.fast
    def test_run_tests_lmstudio_extra_marker_keyword_early_success(tmp_path, 
monkeypatch):
        """With extra_marker requires_resource('lmstudio') and no matches, 
run_tests should
        perform keyword-based collection and return success with a friendly 
message.
    
        ReqID: C3 (coverage of extra_marker keyword path with early success)
        """
        from devsynth.testing import run_tests as rt
    
        # Point the unit-tests target to an isolated path (not used directly 
here)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
    
        # Fake subprocess.run for the collect-only path to return no matching 
node IDs
        class FakeCompleted:
            def __init__(self, stdout: str, returncode: int = 0, stderr: str = 
""):
                self.stdout = stdout
                self.returncode = returncode
                self.stderr = stderr
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            # Simulate collect-only returning nothing for '-k lmstudio'
            return FakeCompleted(stdout="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            parallel=False,
            extra_marker="requires_resource('lmstudio')",
        )
    
        assert success is True
>       assert "No tests matched" in output
E       AssertionError: assert 'No tests matched' in 'Marker fallback 
executed.\n/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python
3.12/site-packages/co... 3.12.12-final-0 
_______________\n\n============================ no tests ran in 0.01s 
=============================\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_extra_paths.py:144: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,148 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,149 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:58,149 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (medium)  collecting via pytest
2025-10-29 10:48:58,149 - devsynth.testing.run_tests - INFO - marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
2025-10-29 10:48:58,421 - devsynth.testing.run_tests - INFO - Coverage data file
detected at .coverage (53248 bytes)
2025-10-29 10:48:58,423 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: no measured files present
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (medium)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage data file 
detected at .coverage (53248 bytes)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: no measured files present
____________________ test_failure_tips_include_common_flags ____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_failure_tips_include_comm0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c31fec0>

    @pytest.mark.fast
    def test_failure_tips_include_common_flags(tmp_path, monkeypatch):
        """ReqID: FR-59 Ensure failure tips include common flags examples.
    
        We create a minimal failing test and assert that the output contains
        actionable hints for --smoke, --segment/--segment-size, --maxfail,
        --no-parallel, and --report.
        """
        test_file = tmp_path / "test_fails.py"
        test_file.write_text(
            """
    import pytest
    
    @pytest.mark.fast
    def test_will_fail():
        assert False
    """
        )
        # Point unit-tests target to our tmp_path
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))
    
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        def fake_collect(cmd, check=False, capture_output=True, text=True):  # 
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_will_fail",
                stderr="",
            )
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                self.cmd = list(cmd)
                self.returncode = 1
                self._stdout = ""
                self._stderr = "FAIL Required test coverage of 90% not reached."
    
            def communicate(self):  # noqa: D401 - mimic subprocess signature
                """Return predetermined stdout/stderr."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests("unit-tests", ["fast"], parallel=False)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_failure_tips.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_failure_tips_include_common_flags.<locals>.fake_collect() got an unexpected
keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,433 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,433 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
____________ test_keyword_filter_no_matches_returns_success_message ____________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_keyword_filter_no_matches1')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8ea1b0>

    @pytest.mark.fast
    def test_keyword_filter_no_matches_returns_success_message(tmp_path, 
monkeypatch):
        """ReqID: FR-11.2  Keyword filter path returns success when no matches.
    
        Triggers the lmstudio keyword path and expects a friendly message.
        """
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        def fake_collect(cmd, check=False, capture_output=True, text=True):  # 
noqa: ANN001
            assert "--collect-only" in cmd
            assert "-k" in cmd and "lmstudio" in cmd
            return SimpleNamespace(returncode=0, stdout="", stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
    
        # Use a very specific marker expression to trigger keyword path.
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_keyword_filter_no_matches_returns_success_message.<locals>.fake_collect() 
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,472 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,473 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
________ test_keyword_filter_honors_report_flag_and_creates_report_dir _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c253860>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_keyword_filter_honors_rep0')

    @pytest.mark.fast
    def test_keyword_filter_honors_report_flag_and_creates_report_dir(
        monkeypatch, tmp_path
    ):
        """ReqID: FR-11.2  Report flag creates deterministic report directory.
    
        Use keyword path with report=True and patch datetime to assert directory
path.
        """
    
        class FakeDT:
            @staticmethod
            def now():
                # Fixed timestamp for stable directory path
                class _DT:
                    def strftime(self, fmt: str) -> str:
                        return "20250101_000000"
    
                return _DT()
    
        monkeypatch.setattr(rt, "datetime", FakeDT)
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        test_file = tmp_path / "test_lmstudio.py"
        test_file.write_text("def test_placeholder():\n    assert True\n")
    
        def fake_collect(cmd, check=False, capture_output=True, text=True):  # 
noqa: ANN001
            assert "--collect-only" in cmd
            assert "-k" in cmd and "lmstudio" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_placeholder",
                stderr="",
            )
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                self.cmd = list(cmd)
                self.returncode = 0
                self._stdout = "ok"
                self._stderr = ""
    
            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=True,
            parallel=False,
            segment=False,
            extra_marker='requires_resource("lmstudio")',
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_keyword_filter_honors_report_flag_and_creates_report_dir.<locals>.fake_coll
ect() got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,504 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,505 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
____ test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a3c20>

    @pytest.mark.fast
    def 
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success(monkeypat
ch):
        """
        ReqID: C3-05
        When extra_marker requires_resource('lmstudio') is provided and the 
keyword-filtered
        collection yields no tests, run_tests should short-circuit and return 
success=True
        with a friendly message instead of attempting to invoke pytest with 
empty args.
        """
    
        # Simulate `pytest --collect-only -q -m <expr> -k lmstudio` returning no
node IDs.
        class DummyCompleted:
            def __init__(self, stdout: str = "", returncode: int = 0):
                self.stdout = stdout
                self.stderr = ""
                self.returncode = returncode
    
        def fake_run(
            cmd: list[str],
            check: bool = False,
            capture_output: bool = True,
            text: bool = True,
        ):  # type: ignore[no-redef]
            # Ensure we're calling a python -m pytest command with 
'--collect-only'
            assert cmd[:3] == [sys.executable, "-m", "pytest"], cmd
            assert "--collect-only" in cmd
            # Return empty stdout to indicate no matched tests
            return DummyCompleted(stdout="", returncode=0)
    
        monkeypatch.setenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")  # keep 
hermetic and fast
        monkeypatch.setenv("DEVSYNTH_OFFLINE", "true")
    
        monkeypatch.setattr("subprocess.run", fake_run)
    
        # Call run_tests with an lmstudio resource marker expression
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,  # triggers the single-pass branch
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_keyword_filter_empty.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_run_tests_lmstudio_keyword_filter_with_no_matches_returns_success.<locals>.
fake_run() got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,535 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,535 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
___________________ test_collect_tests_with_cache_uses_cache ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a3f50>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_11')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_uses_cache(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-1  collect_tests_with_cache uses the cache."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 1.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)
    
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
>           tests = rt.collect_tests_with_cache("unit-tests", "fast")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
        result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
        if result.returncode != 0:
            if result.stderr:
                error_message = f"Test collection failed: {result.stderr}"
            else:
                error_message = f"Test collection failed with exit code 
{result.returncode}"
    
            # Log more details for debugging
            logger.warning(
                error_message,
                extra={
                    "event": "test_collection_failed",
                    "target": target,
                    "returncode": result.returncode,
                    "stdout": (
                        result.stdout[:500] if result.stdout else None
                    ),  # First 500 chars
                    "stderr": (
                        result.stderr[:500] if result.stderr else None
                    ),  # First 500 chars
                    "speed_category": category_expr,
                },
            )
>           raise RuntimeError(error_message)
E           RuntimeError: Test collection failed: <MagicMock name='run().stderr'
id='5288446096'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1367: RuntimeError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - WARNING - Test collection
failed: <MagicMock name='run().stderr' id='5288446096'>
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection failed:
<MagicMock name='run().stderr' id='5288446096'>
____________ test_collect_tests_with_cache_regenerates_when_expired ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8e9010>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_12')

    @typed_freeze_time("2025-01-02")
    @pytest.mark.fast
    def test_collect_tests_with_cache_regenerates_when_expired(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-2  collect_tests_with_cache regenerates when 
expired."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 0.5,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)
    
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            tests = rt.collect_tests_with_cache("unit-tests", "fast")
    
>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E         
E         Right contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_with_cache_12/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:118: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-01-01 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
______________________ test_collect_tests_with_cache_miss ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8eb620>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_13')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_miss(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-3  collect_tests_with_cache handles a cache 
miss."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
    
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "test_file.py") + "\n",
                returncode=0,
                stderr="",
            )
            tests = rt.collect_tests_with_cache("unit-tests", "fast")
    
>       assert tests == [os.path.join(tmp_path, "test_file.py")]
E       AssertionError: assert [] == ['/private/va...test_file.py']
E         
E         Right contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_with_cache_13/test_file.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:143: AssertionError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
______________ test_collect_tests_with_cache_invalidated_by_mtime ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8ea4e0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_14')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_invalidated_by_mtime(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-4  collect_tests_with_cache is invalidated by 
mtime."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 0.5,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)
    
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            with freeze_time("2025-01-02"):
                tests = rt.collect_tests_with_cache("unit-tests", "fast")
    
>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E         
E         Right contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_with_cache_14/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:183: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-01-01 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_____________ test_collect_tests_with_cache_invalidated_by_marker ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8e8ad0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_15')

    @typed_freeze_time("2025-01-01")
    @pytest.mark.fast
    def test_collect_tests_with_cache_invalidated_by_marker(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-COLL-5  collect_tests_with_cache is invalidated by 
marker."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        cache_dir = tmp_path / rt.COLLECTION_CACHE_DIR
        cache_dir.mkdir()
        cache_file = cache_dir / "unit-tests_fast_tests.json"
        cached_data = {
            "timestamp": "2025-01-01T00:00:00.000000",
            "tests": [os.path.join(str(tmp_path), "test_file.py")],
            "fingerprint": {
                "latest_mtime": 1.0,
                "category_expr": "fast and not memory_intensive",
                "test_path": str(tmp_path),
            },
        }
        with open(cache_file, "w") as f:
            json.dump(cached_data, f)
    
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
        with (
            patch.object(rt.os.path, "getmtime", return_value=1.0),
            patch.object(rt.subprocess, "run") as mock_run,
            patch.object(rt, "COLLECTION_CACHE_TTL_SECONDS", 999999),
        ):
            mock_run.return_value = SimpleNamespace(
                stdout=os.path.join(str(tmp_path), "new_test.py") + "\n",
                returncode=0,
                stderr="",
            )
            with freeze_time("2025-01-01"):
                tests = rt.collect_tests_with_cache("unit-tests", "slow")
    
>       assert tests == [os.path.join(tmp_path, "new_test.py")]
E       AssertionError: assert [] == ['/private/va.../new_test.py']
E         
E         Right contains one more item: 
'/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pyte
st-57/test_collect_tests_with_cache_15/new_test.py'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_logic.py:223: AssertionError
----------------------------- Captured stdout call -----------------------------
2024-12-31 16:00:00,000 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (slow)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (slow)  collecting via pytest
________________________ test_run_tests_basic_execution ________________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_basic_execution0')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b968860>

    @pytest.mark.fast
    def test_run_tests_basic_execution(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test basic run_tests execution with default parameters."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        success, output = rt.run_tests(target="unit-tests")
    
        assert success is True
        assert output == ""  # Default mock_subprocess_popen output
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "pytest" in cmd
        assert f"{test_file}::test_pass" in cmd
        assert "--verbose" not in cmd
        assert "--cov" not in cmd
>       assert "PYTEST_ADDOPTS" not in env
E       AssertionError: assert 'PYTEST_ADDOPTS' not in 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '{}', 
'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': '1', 'AWS_SDK_LOAD_CONFIG': 
'true', 'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:188: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,864 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
________________ test_run_tests_with_markers_and_keyword_filter ________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_with_markers_an0')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b4e4dd0>

    @pytest.mark.fast
    def test_run_tests_with_markers_and_keyword_filter(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with extra_marker and keyword_filter."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        success, output = rt.run_tests(
            target="unit-tests", extra_marker="slow", keyword_filter="example"
        )
    
        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "-m" in cmd
>       assert "not memory_intensive and (slow)" in cmd
E       AssertionError: assert 'not memory_intensive and (slow)' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 
'/private/var/folders/2v/lbs...markers_an0/test_example.py::test_pass', '-m', 
'not memory_intensive and (fast or medium) and (slow) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:257: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,882 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
_______________ test_run_tests_collection_failure_returns_false ________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_collection_fail0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2d1160>

    @pytest.mark.fast
    def test_run_tests_collection_failure_returns_false(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests returns False on test collection failure."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
    
        def failing_collect(target, speed_category):
            raise RuntimeError("Collection failed")
    
        monkeypatch.setattr(rt, "collect_tests_with_cache", failing_collect)
    
        success, output = rt.run_tests(target="unit-tests")
    
        assert success is False
>       assert output == "Test collection failed"
E       AssertionError: assert 'Collection failed' == 'Test collection failed'
E         
E         - Test collection failed
E         ? ^^^^^^
E         + Collection failed
E         ? ^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:341: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,905 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
_________ test_run_tests_no_tests_collected_returns_true_with_message __________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_no_tests_collec0')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2cc680>

    @pytest.mark.fast
    def test_run_tests_no_tests_collected_returns_true_with_message(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests returns True and message when no tests are 
collected."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
    
        monkeypatch.setattr(
            rt, "collect_tests_with_cache", lambda target, speed_category: []
        )
    
        success, output = rt.run_tests(target="unit-tests")
    
        assert success is True
>       assert "No tests collected" in output
E       AssertionError: assert 'No tests collected' in 'Marker fallback 
executed.\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:362: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,918 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,918 - devsynth.testing.run_tests - INFO - marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback 
triggered for target=unit-tests (speeds=fast,medium)
______________________ test_run_tests_segmented_execution ______________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_segmented_execu1')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b2cf080>

    @pytest.mark.fast
    def test_run_tests_segmented_execution(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with segmented execution."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_a = tmp_path / "test_a.py"
        test_b = tmp_path / "test_b.py"
        test_c = tmp_path / "test_c.py"
        test_a.write_text("def test_a(): pass")
        test_b.write_text("def test_b(): pass")
        test_c.write_text("def test_c(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [
                f"{test_a}::test_a",
                f"{test_b}::test_b",
                f"{test_c}::test_c",
            ],
        )
    
        # Mock Popen to return success for all batches
        class FakePopenSuccess(rt.subprocess.Popen):
            def __init__(self, *args: Any, **kwargs: Any) -> None:
                # Append to the fixture's recorded_calls
                mock_subprocess_popen.append((args[0], kwargs.get("env", {})))
                super().__init__(*args, **kwargs)
                self.returncode = 0
                self._stdout = "ok"
                self._stderr = ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopenSuccess)
    
        success, output = rt.run_tests(target="unit-tests", segment=True, 
segment_size=1)
    
        assert success is True
>       assert len(mock_subprocess_popen) == 3  # Three batches
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 6 == 3
E        +  where 6 = 
len([(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:439: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,936 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,936 - devsynth.testing.run_tests - INFO - Running 3 tests in
3 segments of size 1 for target=unit-tests
2025-10-29 10:48:58,936 - devsynth.testing.run_tests - INFO - Running segment 
1/3 (1 tests)
2025-10-29 10:48:58,936 - devsynth.testing.run_tests - INFO - Running segment 
2/3 (1 tests)
2025-10-29 10:48:58,936 - devsynth.testing.run_tests - INFO - Running segment 
3/3 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 3 tests in 3 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/3 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/3 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 3/3 (1 
tests)
_______________ test_run_tests_segmented_execution_with_failure ________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_segmented_execu2')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b8eb890>

    @pytest.mark.fast
    def test_run_tests_segmented_execution_with_failure(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with segmented execution where one batch fails."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_a = tmp_path / "test_a.py"
        test_b = tmp_path / "test_b.py"
        test_a.write_text("def test_a(): pass")
        test_b.write_text("def test_b(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [
                f"{test_a}::test_a",
                f"{test_b}::test_b",
            ],
        )
    
        # Mock Popen to simulate failure in the first batch
        class FakePopenMixed(rt.subprocess.Popen):
            call_count = 0
    
            def __init__(self, *args: Any, **kwargs: Any) -> None:
                # Append to the fixture's recorded_calls
                mock_subprocess_popen.append((args[0], kwargs.get("env", {})))
                super().__init__(*args, **kwargs)
                FakePopenMixed.call_count += 1
                if FakePopenMixed.call_count == 1:
                    self.returncode = 1  # Fail first batch
                    self._stdout = "fail"
                    self._stderr = "error"
                else:
                    self.returncode = 0
                    self._stdout = "ok"
                    self._stderr = ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopenMixed)
    
        success, output = rt.run_tests(target="unit-tests", segment=True, 
segment_size=1)
    
        assert success is False
        assert "Troubleshooting tips" in output  # Should include failure tips
>       assert len(mock_subprocess_popen) == 2  # Both batches should run
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 4 == 2
E        +  where 4 = 
len([(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:492: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,949 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,949 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-29 10:48:58,949 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (1 tests)
2025-10-29 10:48:58,950 - devsynth.testing.run_tests - INFO - Running segment 
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1 
tests)
______________________ test_run_tests_parallel_execution _______________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_parallel_execut0')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b358650>

    @pytest.mark.fast
    def test_run_tests_parallel_execution(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test run_tests with parallel execution."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        success, output = rt.run_tests(target="unit-tests", parallel=True)
    
        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
>       assert "-n auto" in cmd  # Should add parallel flag
        ^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert '-n auto' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 
'/private/var/folders/2v/lbs...tests_parallel_execut0/test_example.py::test_pass
', '-m', 'not memory_intensive and (fast or medium) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:520: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,961 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
____________ test_run_tests_parallel_execution_disabled_by_segment _____________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_parallel_execut1')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b358ce0>

    @pytest.mark.fast
    def test_run_tests_parallel_execution_disabled_by_segment(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test parallel execution is disabled when segment is True."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        success, output = rt.run_tests(target="unit-tests", parallel=True, 
segment=True)
    
        assert success is True
        assert len(mock_subprocess_popen) == 1  # Only one batch due to 
segment=True
        cmd, env = mock_subprocess_popen[0]
        assert "-n auto" not in cmd  # Parallel should be disabled
>       assert [f"{test_file}::test_pass"] == cmd[
            3:
        ]  # Check if node_ids are correctly passed
E       AssertionError: assert ['/private/va...y::test_pass'] == 
['/private/va...htmlcov', ...]
E         
E         Right contains 8 more items, first extra item: '-m'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:548: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:58,973 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:58,973 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 50 for target=unit-tests
2025-10-29 10:48:58,973 - devsynth.testing.run_tests - INFO - Running segment 
1/1 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 
segments of size 50 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)
___________________ test_run_tests_with_env_var_propagation ____________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_with_env_var_pr0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9a0380>

    @pytest.mark.fast
    def test_run_tests_with_env_var_propagation(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        # Set some initial environment variables
        os.environ["EXISTING_VAR"] = "initial_value"
        os.environ["PYTEST_ADDOPTS"] = "-q"
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        custom_env = {"NEW_VAR": "new_value", "PYTEST_ADDOPTS": 
"--strict-markers"}
    
>       success, output = rt.run_tests(target="unit-tests", env=custom_env)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:572: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests
    nodes = collect_callable(target, category)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

>       lambda target, speed_category: [f"{test_file}::test_pass"],
                                           ^^^^^^^^^
    )
E   NameError: name 'test_file' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:567: NameError
_____________ test_run_tests_with_empty_speed_categories_uses_all ______________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_with_empty_spee0')
mock_subprocess_run = [], mock_subprocess_popen = []
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bb63530>

    @pytest.mark.fast
    def test_run_tests_with_empty_speed_categories_uses_all(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
>       success, output = rt.run_tests(target="unit-tests", speed_categories=[])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1750: in run_tests
    nodes = collect_callable(target, category)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

>       lambda target, speed_category: [f"{test_file}::test_pass"],
                                           ^^^^^^^^^
    )
E   NameError: name 'test_file' is not defined

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:614: NameError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,015 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
________________ test_run_tests_with_specific_speed_categories _________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_with_specific_s0')
mock_subprocess_run = []
mock_subprocess_popen = 
[(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', '/private/var/folders/2v/l...GNOSTIC_CHANNEL': '1', 
'AWS_SDK_LOAD_CONFIG': 'true', 'BRAVE_SEARCH_API_KEY': 
'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...})]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c027bc0>

    @pytest.mark.fast
    def test_run_tests_with_specific_speed_categories(
        tmp_path: Path,
        mock_subprocess_run: list[list[str]],
        mock_subprocess_popen: list[tuple[list[str], dict[str, str]]],
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        """Test that specific speed_categories are correctly applied."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        test_file = tmp_path / "test_example.py"
        test_file.write_text("def test_pass(): pass")
    
        monkeypatch.setattr(
            rt,
            "collect_tests_with_cache",
            lambda target, speed_category: [f"{test_file}::test_pass"],
        )
    
        success, output = rt.run_tests(
            target="unit-tests", speed_categories=["fast", "medium"]
        )
    
        assert success is True
        assert len(mock_subprocess_popen) == 1
        cmd, env = mock_subprocess_popen[0]
        assert "-m" in cmd
>       assert "not memory_intensive and (fast or medium)" in cmd
E       AssertionError: assert 'not memory_intensive and (fast or medium)' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 
'/private/var/folders/2v/lbs...tests_with_specific_s0/test_example.py::test_pass
', '-m', 'not memory_intensive and (fast or medium) and not gui', ...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_main_logic.py:654: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,041 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
__________ test_collect_tests_with_cache_uses_cache_and_respects_ttl ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c025dc0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_19')

    @pytest.mark.fast
    def test_collect_tests_with_cache_uses_cache_and_respects_ttl(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-02  collect_tests_with_cache caches and reuses results
        respecting TTL and fingerprint."""
        # Point TARGET_PATHS to tmp tests dir
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        (tests_dir / "test_sample.py").write_text("def test_ok():\n    assert 
True\n")
    
        # Monkeypatch TARGET_PATHS and subprocess to avoid invoking real pytest
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests": 
str(tests_dir)}
        )
    
        class DummyProc:
            def __init__(self, out: str):
                self.stdout = out
                self.returncode = 0
    
        def fake_run(
            cmd: list[str], check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            assert "--collect-only" in cmd
            # emulate -q output: one per line
            return DummyProc(out=f"{tests_dir}/test_sample.py\n")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        # Speed None path
>       ids1 = rt.collect_tests_with_cache("unit-tests", speed_category=None)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_collect_tests_with_cache_uses_cache_and_respects_ttl.<locals>.fake_run() 
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,083 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (all)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (all)  collecting via pytest
___________ test_run_tests_translates_args_and_handles_return_codes ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c026270>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_translates_args0')

    @pytest.mark.fast
    def test_run_tests_translates_args_and_handles_return_codes(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-03  run_tests translates args, treats code 0/5 as 
success,
        and omits -n when parallel=False."""
        # Arrange base to avoid plugin interactions and filesystem writes
        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        (tests_dir / "test_x.py").write_text("def test_one():\n    assert 
True\n")
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests": 
str(tests_dir)}
        )
    
        # Capture the command built for the run path (no speed_categories 
provided)
        captured = {"cmd": None, "env": None}
    
        class CollectResult:
            def __init__(self, out: str) -> None:
                self.stdout = out
                self.stderr = ""
                self.returncode = 0
    
        def fake_run(
            cmd: list[str],
            check: bool,
            capture_output: bool,
            text: bool,
        ) -> CollectResult:  # type: ignore[override]
            assert "--collect-only" in cmd
            return CollectResult(f"{tests_dir}/test_x.py::test_one\n")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        class P:
            def __init__(self, cmd: list[str], code: int, out: str = "ok\n", 
err: str = ""):
                self.args = cmd
                self._code = code
                self._out = out
                self._err = err
    
            def communicate(self, *_args: Any, **_kwargs: Any):
                return self._out, self._err
    
            @property
            def returncode(self) -> int:
                return self._code
    
            def __enter__(self) -> "P":
                return self
    
            def __exit__(
                self,
                _exc_type: Any,
                _exc: Any,
                _tb: Any,
            ) -> bool:
                return False
    
            def kill(self) -> None:  # pragma: no cover - subprocess.run 
compatibility
                pass
    
            def wait(self) -> int:  # pragma: no cover - subprocess.run 
compatibility
                return self._code
    
            def poll(self) -> int:  # pragma: no cover - subprocess.run 
compatibility
                return self._code
    
        def fake_popen(
            cmd: list[str],
            stdout,
            stderr,
            text: bool,
            env: dict[str, str] | None = None,
        ):  # type: ignore[override]
            captured["cmd"] = cmd
            captured["env"] = env
            # Succeed with code 0 first
            return P(cmd, 0)
    
        monkeypatch.setattr(rt.subprocess, "Popen", fake_popen)
    
>       ok, output = rt.run_tests(
            target="unit-tests", speed_categories=["fast"], parallel=False
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:174: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_run_tests_translates_args_and_handles_return_codes.<locals>.fake_run() got 
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,109 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
___________ test_run_tests_keyword_filter_for_extra_marker_lmstudio ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b33de80>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_keyword_filter_1')

    @pytest.mark.fast
    def test_run_tests_keyword_filter_for_extra_marker_lmstudio(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ):
        """ReqID: RTM-04  extra_marker 'requires_resource("lmstudio")' uses
        keyword filter and early success on no matches."""
        # Arrange: ensure keyword narrowing path is exercised with no matches ->
        # early success
        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests": 
str(tests_dir)}
        )
    
        class Dummy:
            def __init__(self, stdout: str, returncode: int = 0):
                self.stdout = stdout
                self.returncode = returncode
    
        def fake_run(
            cmd, check: bool, capture_output: bool, text: bool
        ):  # type: ignore[override]
            # '--collect-only' path with '-k lmstudio' produces no items
            assert "--collect-only" in cmd
            return Dummy(stdout="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
>       ok, msg = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            extra_marker="requires_resource('lmstudio')",
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_run_tests_keyword_filter_for_extra_marker_lmstudio.<locals>.fake_run() got 
an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,141 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_________ test_run_tests_handles_popen_exception_without_speed_filters _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b4e6ff0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_handles_popen_e0')

    @pytest.mark.fast
    def test_run_tests_handles_popen_exception_without_speed_filters(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RTM-05  run_tests surfaces subprocess errors with 
guidance."""
    
        tests_dir = tmp_path / "tests"
        tests_dir.mkdir()
        monkeypatch.setattr(
            rt, "TARGET_PATHS", {"unit-tests": str(tests_dir), "all-tests": 
str(tests_dir)}
        )
    
        # ``run_tests`` should not perform collection when no speed categories 
are
        # provided. Guard against unexpected subprocess.run usage.
        def fail_run(*_args: Any, **_kwargs: Any) -> None:  # pragma: no cover -
safety
            raise AssertionError("subprocess.run should not be invoked in this 
branch")
    
        monkeypatch.setattr(rt.subprocess, "run", fail_run)
    
        captured: dict[str, list[str]] = {}
    
        def boom_popen(
            cmd: list[str],
            stdout: Any = None,
            stderr: Any = None,
            text: bool = True,
            env: dict[str, str] | None = None,
        ) -> Any:  # pragma: no cover - behavior exercised via exception path
            captured["cmd"] = cmd
            raise RuntimeError("intentional popen failure")
    
        monkeypatch.setattr(rt.subprocess, "Popen", boom_popen)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:278: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

_args = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...bg7c07nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/t
est_run_tests_handles_popen_e0/tests', '--collect-only', '-q', ...],)
_kwargs = {'capture_output': True, 'cwd': 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_C...'1', 'AWS_SDK_LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'text': True, 
...}

    def fail_run(*_args: Any, **_kwargs: Any) -> None:  # pragma: no cover - 
safety
>       raise AssertionError("subprocess.run should not be invoked in this 
branch")
E       AssertionError: subprocess.run should not be invoked in this branch

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:260: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,171 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_______________ test_collect_unknown_target_uses_all_tests_path ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b4e5610>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_unknown_target_us0')

    @pytest.mark.fast
    def test_collect_unknown_target_uses_all_tests_path(monkeypatch, tmp_path):
        """ReqID: RTM-06  Unknown target falls back to all-tests mapping."""
    
        tests_dir = tmp_path / "some_tests"
        tests_dir.mkdir()
        cache_dir = tmp_path / "cache"
        cache_dir.mkdir()
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(cache_dir))
        monkeypatch.setattr(rt, "TARGET_PATHS", {"all-tests": str(tests_dir)})
    
        observed: list[list[str]] = []
    
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            observed.append(cmd[:])
            return SimpleNamespace(
                stdout="test_sample.py::test_ok\n",
                stderr="",
                returncode=0,
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        original_isdir = rt.os.path.isdir
    
        def fake_isdir(path: str) -> bool:
            if path == str(tests_dir):
                return False
            return original_isdir(path)
    
        monkeypatch.setattr(rt.os.path, "isdir", fake_isdir)
    
        original_exists = rt.os.path.exists
    
        def fake_exists(path: str) -> bool:
            if path == "test_sample.py":
                return True
            return original_exists(path)
    
        monkeypatch.setattr(rt.os.path, "exists", fake_exists)
    
>       result = rt.collect_tests_with_cache("custom-target", 
speed_category=None)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'custom-target', speed_category = None

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
_______________ test_enforce_coverage_threshold_exit_and_return ________________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_enforce_coverage_threshol2')

    @pytest.mark.fast
    def test_enforce_coverage_threshold_exit_and_return(tmp_path):
        """ReqID: RTM-05  Coverage helper returns percent and exits on 
failure."""
    
        cov_file = tmp_path / "coverage.json"
        cov_file.write_text(json.dumps({"totals": {"percent_covered": 95.25}}))
        percent = rt.enforce_coverage_threshold(
            coverage_file=cov_file, exit_on_failure=False
        )
        assert percent == pytest.approx(95.25)
    
        cov_file.write_text(json.dumps({"totals": {"percent_covered": 81.7}}))
>       with pytest.raises(RuntimeError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'RuntimeError'>

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:365: Failed
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,225 - devsynth.testing.run_tests - INFO - Coverage 95.25% 
meets the 70.00% threshold.
2025-10-29 10:48:59,225 - devsynth.testing.run_tests - INFO - Coverage 81.70% 
meets the 70.00% threshold.
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage 95.25% meets 
the 70.00% threshold.
INFO     devsynth.testing.run_tests:logging_setup.py:615 Coverage 81.70% meets 
the 70.00% threshold.
_______________ test_run_tests_segment_appends_aggregation_tips ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b438470>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_segment_appends0')

    @pytest.mark.fast
    def test_run_tests_segment_appends_aggregation_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path
    ) -> None:
        """ReqID: RTM-07  Segmented failures append aggregate troubleshooting 
tips."""
    
        tests_dir = tmp_path / "segmented"
        tests_dir.mkdir()
        (tests_dir / "test_one.py").write_text("def test_one():\n    assert 
True\n")
        (tests_dir / "test_two.py").write_text("def test_two():\n    assert 
True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        class CollectProc:
            def __init__(self, out: str) -> None:
                self.stdout = out
                self.stderr = ""
                self.returncode = 0
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd
            stdout = "\n".join(["test_one.py::test_one", 
"test_two.py::test_two"])
            return CollectProc(stdout)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        batch_calls: list[list[str]] = []
    
        class FakeBatchProcess:
            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=False,
                env=None,
            ) -> None:  # noqa: ANN001
                batch_calls.append(cmd)
                self.args = cmd
                index = len(batch_calls) - 1
                if index == 0:
                    self._stdout = "batch-1\n"
                    self._stderr = ""
                    self._returncode = 0
                else:
                    self._stdout = "batch-2\n"
                    self._stderr = "boom"
                    self._returncode = 1
    
            def communicate(self):  # noqa: D401 - simple stub
                return self._stdout, self._stderr
    
            @property
            def returncode(self) -> int:
                return self._returncode
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakeBatchProcess)
    
        success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )
    
        assert success is False
        assert len(batch_calls) == 2
    
        expected_agg_cmd = [
            rt.sys.executable,
            "-m",
            "pytest",
            f"--cov={rt.COVERAGE_TARGET}",
            "--cov-report=term-missing",
            f"--cov-report=json:{rt.COVERAGE_JSON_PATH}",
            f"--cov-report=html:{rt.COVERAGE_HTML_DIR}",
            "--cov-append",
            str(tests_dir),
        ]
        aggregate_tip = rt._failure_tips(1, expected_agg_cmd)
    
>       assert aggregate_tip in output
E       AssertionError: assert '\nPytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m 
pytest...HTML report for context (saved under test_reports/):\n  devsynth 
run-tests --target unit-tests --speed=fast --report\n' in 
'batch-1\n\nbatch-2\n\nPytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/...HTML report for 
context (saved under test_reports/):\n  devsynth run-tests --target unit-tests 
--speed=fast --report\n'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_module.py:484: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,241 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,241 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-29 10:48:59,241 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (1 tests)
2025-10-29 10:48:59,241 - devsynth.testing.run_tests - INFO - Running segment 
2/2 (1 tests)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1 
tests)
______________ test_run_tests_completes_without_xdist_assertions _______________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_completes_witho0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b453890>

    @pytest.mark.fast
    def test_run_tests_completes_without_xdist_assertions(tmp_path, 
monkeypatch):
        """run_tests completes without INTERNALERROR when run in parallel. 
ReqID: FR-22"""
        test_file = tmp_path / "test_dummy.py"
        test_file.write_text(
            "import pytest\n\n@pytest.mark.fast\ndef test_ok():\n    assert 
True\n"
        )
        monkeypatch.setitem(TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        def fake_collect(cmd, check=False, capture_output=True, text=True):  # 
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(
                returncode=0,
                stdout=f"{test_file}::test_ok",
                stderr="",
            )
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                assert "-n" in cmd and "auto" in cmd
                self.returncode = 0
                self._stdout = "passed"
                self._stderr = ""
    
            def communicate(self):  # noqa: D401 - mimic subprocess API
                """Return deterministic stdout/stderr."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests("unit-tests", ["fast"], parallel=True)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_no_xdist_assertions.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_run_tests_completes_without_xdist_assertions.<locals>.fake_collect() got an
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,263 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,263 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_________________ test_report_flag_adds_html_report_to_command _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b33d820>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_report_flag_adds_html_rep0')

    @pytest.mark.fast
    def test_report_flag_adds_html_report_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-2  report=True adds --html to the pytest 
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
    
        recorded_cmds: list[list[str]] = []
    
        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0
    
            def communicate(self):
                return "ok", ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        with (
            patch.object(rt, "collect_tests_with_cache", 
return_value=["test_file.py"]),
            patch.object(rt, "datetime") as mock_dt,
        ):
            mock_dt.now.return_value.strftime.return_value = "20250101_000000"
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=True,
                parallel=False,
                segment=False,
                maxfail=None,
                extra_marker=None,
            )
    
        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert any(arg.startswith("--html=") for arg in pytest_cmd)
E       assert False
E        +  where False = any(<generator object 
test_report_flag_adds_html_report_to_command.<locals>.<genexpr> at 0x13cbbdb10>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:101: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,317 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,318 - devsynth.testing.run_tests - WARNING - Skipping 
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json
___________________ test_no_parallel_flag_adds_n0_to_command ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bb81400>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_no_parallel_flag_adds_n0_0')

    @pytest.mark.fast
    def test_no_parallel_flag_adds_n0_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-3  parallel=False adds -n0 to the pytest 
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
    
        recorded_cmds: list[list[str]] = []
    
        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0
    
            def communicate(self):
                return "ok", ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        with patch.object(rt, "collect_tests_with_cache", 
return_value=["test_file.py"]):
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=False,
                maxfail=None,
                extra_marker=None,
            )
    
        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert "-n0" in pytest_cmd
E       AssertionError: assert '-n0' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 'test_file.py', '-m', 'not memory_intensive and fast and not gui', 
...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:138: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,326 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
__________________ test_maxfail_flag_adds_maxfail_to_command ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b443aa0>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_maxfail_flag_adds_maxfail0')

    @pytest.mark.fast
    def test_maxfail_flag_adds_maxfail_to_command(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-4  maxfail=N adds --maxfail=N to the pytest 
command."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
    
        recorded_cmds: list[list[str]] = []
    
        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0
    
            def communicate(self):
                return "ok", ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        with patch.object(rt, "collect_tests_with_cache", 
return_value=["test_file.py"]):
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=False,
                maxfail=5,
                extra_marker=None,
            )
    
        assert recorded_cmds, "run_tests should have invoked Popen"
        pytest_cmd = recorded_cmds[0]
>       assert "--maxfail=5" in pytest_cmd
E       AssertionError: assert '--maxfail=5' in 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 'test_file.py', '-m', 'not memory_intensive and fast and not gui', 
...]

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:175: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,335 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
___________________ test_segment_flags_trigger_segmented_run ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x10b422420>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segment_flags_trigger_seg0')

    @pytest.mark.fast
    def test_segment_flags_trigger_segmented_run(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path
    ) -> None:
        """ReqID: RUN-TESTS-ORCH-5  segment=True triggers a segmented run."""
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "test_file.py").write_text("def test_example(): pass")
    
        recorded_cmds: list[list[str]] = []
    
        class FakePopen:
            def __init__(self, cmd, *args, **kwargs):
                recorded_cmds.append(list(cmd))
                self.returncode = 0
    
            def communicate(self):
                return "ok", ""
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        with patch.object(rt.subprocess, "run") as mock_run:
            mock_run.return_value.stdout = "test_file.py\ntest_file2.py"
            mock_run.return_value.returncode = 0
            rt.run_tests(
                target="unit-tests",
                speed_categories=["fast"],
                verbose=False,
                report=False,
                parallel=False,
                segment=True,
                segment_size=1,
                maxfail=None,
                extra_marker=None,
            )
    
>       assert len(recorded_cmds) == 2, "Expected two Popen calls for a 
segmented run"
E       AssertionError: Expected two Popen calls for a segmented run
E       assert 1 == 2
E        +  where 1 = 
len([['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', 
'-m', 'pytest', 
'/private/var/folders/2v/lb.../pytest-of-caitlyn/pytest-57/test_segment_flags_tr
igger_seg0', '-m', 'not memory_intensive and fast and not gui', ...]])

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_orchestration.py:213: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,344 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,345 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,345 - devsynth.testing.run_tests - INFO - marker fallback 
triggered for target=unit-tests (speeds=fast)
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 marker fallback 
triggered for target=unit-tests (speeds=fast)
_______________ test_run_tests_parallel_includes_cov_and_n_auto ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b33e360>

    @pytest.mark.fast
    def test_run_tests_parallel_includes_cov_and_n_auto(monkeypatch):
        """ReqID: RUN-TESTS-PARALLEL-1
    
        When parallel=True and no explicit node ids are collected (single-pass 
branch),
        run_tests should include '-n auto' and explicit coverage instrumentation
in the
        pytest command.
        """
    
        import devsynth.testing.run_tests as rt
    
        # We won't validate the collection step here; the single-pass branch 
does not
        # pre-collect node ids when speed_categories is None.
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Assert parallel-related flags are present
                assert "-n" in cmd and "auto" in cmd, f"parallel flags missing 
in: {cmd}"
                cov_flag = f"--cov={rt.COVERAGE_TARGET}"
                json_flag = f"--cov-report=json:{rt.COVERAGE_JSON_PATH}"
                html_flag = f"--cov-report=html:{rt.COVERAGE_HTML_DIR}"
                assert cov_flag in cmd, f"{cov_flag} missing in: {cmd}"
                assert json_flag in cmd, f"{json_flag} missing in: {cmd}"
                assert html_flag in cmd, f"{html_flag} missing in: {cmd}"
                assert "--cov-append" in cmd, f"--cov-append missing in: {cmd}"
                self.returncode = 0
    
            def communicate(self):
                return ("ok\n", "")
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,  # triggers non-collection single-pass branch
            verbose=False,
            report=False,
            parallel=True,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_parallel_flags.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_run_tests_parallel_includes_cov_and_n_auto.<locals>.FakePopen.__init__() 
got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,367 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,367 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_______________ test_parallel_injects_cov_reports_and_xdist_auto _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9d9520>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_parallel_injects_cov_repo0')

    @pytest.mark.fast
    def test_parallel_injects_cov_reports_and_xdist_auto(monkeypatch, tmp_path: 
Path):
        """ReqID: TR-RT-11  Parallel path injects -n auto with coverage 
reports.
    
        Verify that when parallel=True, run_tests injects xdist flags and 
preserves
        coverage instrumentation so JSON/HTML artifacts are generated.
        """
    
        called = {}
    
        class FakeCompleted:
            def __init__(self, stdout: str = "", stderr: str = "", returncode: 
int = 0):
                self.stdout = stdout
                self.stderr = stderr
                self.returncode = returncode
    
        test_a = tmp_path / "test_alpha.py"
        test_b = tmp_path / "test_beta.py"
        test_a.write_text("def test_one():\n    assert True\n")
        test_b.write_text("def test_two():\n    assert True\n")
    
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        monkeypatch.setitem(rt.TARGET_PATHS, "all-tests", str(tmp_path))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
    
        def fake_run(
            cmd, check=False, capture_output=False, text=False
        ):  # type: ignore[no-untyped-def]
            # Simulate collection with two node ids; pattern
            # ".*\\.py(::|$)" will match them.
            stdout = "\n".join(
                [
                    f"{test_a}::test_one",
                    f"{test_b}::test_two",
                ]
            )
            return FakeCompleted(stdout=stdout, stderr="", returncode=0)
    
        # pragma: no cover - communicate() path is asserted via effects
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # type: ignore[no-untyped-def]
                called["cmd"] = cmd
                self.returncode = 0
    
            def communicate(self):  # type: ignore[no-untyped-def]
                return ("", "")
    
        # Patch subprocess in module under test
        monkeypatch.setattr("devsynth.testing.run_tests.subprocess.run", 
fake_run)
        monkeypatch.setattr("devsynth.testing.run_tests.subprocess.Popen", 
FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=True,
            segment=False,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_parallel_no_cov.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_parallel_injects_cov_reports_and_xdist_auto.<locals>.fake_run() got an 
unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,407 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,407 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
___________ test_collect_tests_with_cache_handles_subprocess_timeout ___________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_20')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13bb80410>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13bb002c0>

    @pytest.mark.fast
    def test_collect_tests_with_cache_handles_subprocess_timeout(
        tmp_path: Path,
        monkeypatch: pytest.MonkeyPatch,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """Timeouts during collection surface a warning and yield no tests."""
    
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", tmp_path / ".cache")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tmp_path))
        (tmp_path / "sample_test.py").write_text("def test_sample():\n    assert
True\n")
    
        def fake_run(*_args: object, **_kwargs: object) -> 
subprocess.CompletedProcess[str]:
            raise subprocess.TimeoutExpired(
                cmd=["pytest"], timeout=rt.DEFAULT_COLLECTION_TIMEOUT_SECONDS
            )
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        with caplog.at_level(logging.WARNING, 
logger="devsynth.testing.run_tests"):
            collected = rt.collect_tests_with_cache("unit-tests")
    
        assert collected == []
>       assert "Test collection failed" in caplog.text
E       AssertionError: assert 'Test collection failed' in 'WARNING  
devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout for 
target=unit-tests (all); falling back to path\n'
E        +  where 'WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test
collection timeout for target=unit-tests (all); falling back to path\n' = 
<_pytest.logging.LogCaptureFixture object at 0x13bb002c0>.text

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_plugin_timeouts.py:35: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,460 - devsynth.testing.run_tests - WARNING - Test collection
timeout for target=unit-tests (all); falling back to path
------------------------------ Captured log call -------------------------------
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout
for target=unit-tests (all); falling back to path
________________ test_pytest_plugins_registers_pytest_bdd_once _________________

    @pytest.mark.fast
    def test_pytest_plugins_registers_pytest_bdd_once() -> None:
        """Ensure the centralized helper exports pytest-bdd exactly once."""
    
        import importlib
    
        registry = importlib.import_module("tests.pytest_plugin_registry")
        plugin_list = list(registry.PYTEST_PLUGINS)
    
        assert plugin_list.count("pytest_bdd.plugin") == 1
    
        root_conftest = importlib.import_module("tests.conftest")
>       assert (
            root_conftest.pytest_plugins.count("tests.behavior.steps._pytest_bdd
_proxy")
            == 1
        )
E       AssertionError: assert 0 == 1
E        +  where 0 = <built-in method count of list object at 
0x10867c280>('tests.behavior.steps._pytest_bdd_proxy')
E        +    where <built-in method count of list object at 0x10867c280> = 
['tests.conftest_extensions', 'tests.fixtures.backends', 'tests.fixtures.ports',
'tests.fixtures.kuzu', 'tests.fixtures.state_access_fixture', 
'tests.fixtures.webui_wizard_state_fixture', ...].count
E        +      where ['tests.conftest_extensions', 'tests.fixtures.backends', 
'tests.fixtures.ports', 'tests.fixtures.kuzu', 
'tests.fixtures.state_access_fixture', 
'tests.fixtures.webui_wizard_state_fixture', ...] = <module 'tests.conftest' 
from 
'/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/conftest.py'>.pytest
_plugins

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_pytest_plugins_bdd.py:110: AssertionError
___________ test_run_tests_report_injects_html_args_and_creates_dir ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b43a540>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_report_injects_0')

    @pytest.mark.fast
    def test_run_tests_report_injects_html_args_and_creates_dir(monkeypatch, 
tmp_path):
        """
        ReqID: TR-RT-12  Report HTML generation and directory creation.
    
        Validate that when report=True, run_tests:
        - adds --html=<test_reports/.../target>/report.html and 
--self-contained-html
        - creates the report directory path
        - executes pytest with node ids (non-parallel path)
        """
    
        # Arrange a tmp tests dir and map unit-tests target to it
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
    
        # Collection returns a couple of node ids
        collected = [
            "tests/unit/test_alpha.py::test_a",
            "tests/unit/test_beta.py::test_b",
        ]
    
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            if "--collect-only" in cmd:
                return SimpleNamespace(stdout="\n".join(collected), stderr="", 
returncode=0)
            return SimpleNamespace(stdout="", stderr="", returncode=0)
    
        seen_cmds: list[list[str]] = []
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=True, env=None
            ):  # noqa: ANN001
                seen_cmds.append(cmd)
                self.returncode = 0
    
            def communicate(self):  # noqa: D401
                return ("", "")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        # Act
        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=[
                "fast"
            ],  # go through segmented-speed path without segmentation
            verbose=False,
            report=True,
            parallel=False,
            segment=False,
            segment_size=50,
            maxfail=None,
            extra_marker=None,
        )
    
        # Assert
        assert ok is True
>       assert output == ""
E       AssertionError: assert '\n[knowledge...verage.json\n' == ''
E         
E         + 
E         + [knowledge-graph] coverage ingestion skipped: Coverage JSON missing 
at test_reports/coverage.json

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_report.py:78: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,560 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,560 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,561 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
2025-10-29 10:48:59,561 - devsynth.testing.run_tests - WARNING - Skipping 
release graph publication: Coverage JSON missing at test_reports/coverage.json
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Skipping release graph 
publication: Coverage JSON missing at test_reports/coverage.json
_____________ test_single_pass_non_keyword_returncode_5_is_success _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13babc7a0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_single_pass_non_keyword_returncode_5_is_success(monkeypatch) -> 
None:
        """ReqID: TR-RT-10  Return code 5 is success in single-pass non-keyword
path.
    
        In the single-pass, non-keyword path (no speed_categories), pytest 
return
        code 5 (no tests collected) should be treated as success. This exercises
        the branch where we do not pre-collect node ids and simply pass a 
category
        expression to pytest via '-m'.
        """
    
        # Force the branch: speed_categories=None, no extra_marker or keyword 
filter,
        # parallel=False to avoid xdist flags.
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Ensure the '-m' category expression is present and no '-k' 
keyword filter
                assert "-m" in cmd, f"expected -m category expression in cmd: 
{cmd}"
                assert "-k" not in cmd, f"did not expect -k in cmd: {cmd}"
                # Simulate pytest exit code 5 (no tests collected)
                self.returncode = 5
    
            def communicate(self):
                return ("", "")
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = run_tests(
            target="unit-tests",
            speed_categories=None,
            verbose=False,
            report=False,
            parallel=False,
            segment=False,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_returncode5_success.py:36: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: in _collect_via_pytest
    result = subprocess.run(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 60.0, check = False
popenargs = 
(['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m',
'pytest', 
'/private/var/folders/2v/lb...7nd23rh0000gn/T/pytest-of-caitlyn/pytest-57/test_c
ollect_tests_with_cache_1/tests/unit', '--collect-only', '-q', ...],)
kwargs = {'cwd': '/Users/caitlyn/Projects/github.com/ravenoak/devsynth', 'env': 
{'APPLICATIONINSIGHTS_CONFIGURATION_CONTENT': '...LOAD_CONFIG': 'true', 
'BRAVE_SEARCH_API_KEY': 'BSANtaq4PsTJtfCuz8MtVOksRFBo_Xi', ...}, 'stderr': -1, 
'stdout': -1, ...}

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, 
**kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those 
attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture 
them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return 
code
        in the returncode attribute, and output & stderr attributes if those 
streams
        were captured.
    
        If timeout (seconds) is given and the process takes too long,
         a TimeoutExpired exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this 
argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" 
should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings 
decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or 
universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be 
used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not 
None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
>       with Popen(*popenargs, **kwargs) as process:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 
test_single_pass_non_keyword_returncode_5_is_success.<locals>.FakePopen.__init__
() got an unexpected keyword argument 'cwd'

/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.
12/lib/python3.12/subprocess.py:548: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,568 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,569 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
_______ test_segmented_batches_surface_plugin_fallbacks_and_failure_tips _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b43a180>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_batches_surface0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x13c024800>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segmented_batches_surface_plugin_fallbacks_and_failure_tips(
        monkeypatch: pytest.MonkeyPatch, tmp_path: Path, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENTATION-1  Segmented failures emit rich 
diagnostics.
    
        This test simulates a segmented execution where the first batch fails 
with a
        coverage gate error. It verifies that fallback plugin injection occurs 
for the
        subprocess environment, coverage warnings propagate to stdout/stderr, 
and the
        aggregated failure guidance from :func:`_failure_tips` is appended 
exactly once.
        """
    
        caplog.set_level(logging.INFO)
    
        tests_dir = tmp_path / "segmented"
        tests_dir.mkdir()
        test_one = tests_dir / "test_one.py"
        test_two = tests_dir / "test_two.py"
        test_one.write_text("def test_one():\n    assert True\n")
        test_two.write_text("def test_two():\n    assert True\n")
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", str(tmp_path / "cache"))
    
        # Avoid mutating real coverage artifacts while exercising segmentation 
logic.
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        coverage_calls: list[str] = []
        monkeypatch.setattr(
            rt, "_ensure_coverage_artifacts", lambda: 
coverage_calls.append("ensured")
        )
    
        ensure_calls: list[tuple[str, bool, str]] = []
    
        def fake_cov(env: dict[str, str]) -> bool:
            ensure_calls.append(("cov", env is os.environ, 
env.get("PYTEST_ADDOPTS", "")))
            if env is os.environ:
                # Simulate a no-op at the process level so the subprocess copy 
applies the fix.
                return False
            env["PYTEST_ADDOPTS"] = (
                env.get("PYTEST_ADDOPTS", "") + " -p pytest_cov"
            ).strip()
            return True
    
        def fake_bdd(env: dict[str, str]) -> bool:
            ensure_calls.append(("bdd", env is os.environ, 
env.get("PYTEST_ADDOPTS", "")))
            if env is os.environ:
                return False
            env["PYTEST_ADDOPTS"] = (
                env.get("PYTEST_ADDOPTS", "") + " -p pytest_bdd.plugin"
            ).strip()
            return True
    
        monkeypatch.setattr(rt, "ensure_pytest_cov_plugin_env", fake_cov)
        monkeypatch.setattr(rt, "ensure_pytest_bdd_plugin_env", fake_bdd)
    
        collect_output = "\n".join(
            [
                f"{test_one}::test_one",
                f"{test_two}::test_two",
            ]
        )
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            assert "--collect-only" in cmd, "collection command expected"
            return SimpleNamespace(returncode=0, stdout=collect_output, 
stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
    
        batch_plan = iter(
            [
                {
                    "returncode": 1,
                    "stdout": "batch-one\n",
                    "stderr": "FAIL Required test coverage of 90% not 
reached.\n",
                },
                {
                    "returncode": 0,
                    "stdout": "batch-two\n",
                    "stderr": "",
                },
            ]
        )
        popen_calls: list[dict[str, object]] = []
    
        class FakePopen:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                popen_calls.append({"cmd": list(cmd), "env": dict(env or {})})
                try:
                    result = next(batch_plan)
                except StopIteration as exc:  # pragma: no cover - guards test 
integrity
                    raise AssertionError("Unexpected extra Popen invocation") 
from exc
                self.returncode = result["returncode"]
                self._stdout = result["stdout"]
                self._stderr = result["stderr"]
    
            def communicate(self):  # noqa: D401 - signature mirrors subprocess 
API
                """Return the stubbed stdout/stderr pair."""
    
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmentation.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'unit-tests', speed_category = 'fast'

    def collect_tests_with_cache(
        target: str,
        speed_category: str | None = None,
        *,
        keyword_filter: str | None = None,
        _allow_all_target_decomposition: bool = True,
        _timeout_override: float | None = None,
        _propagate_timeout: bool = False,
    ) -> list[str]:
        """Collect tests for the given target and speed category.
    
        Args:
            target: Logical test target such as ``unit-tests`` or ``all-tests``.
            speed_category: Optional speed marker used to scope collection.
            keyword_filter: Optional ``-k`` expression applied during 
collection.
    
        Returns:
            A list of pytest node identifiers matching the requested filters.
        """
        test_path = TARGET_PATHS.get(target, TARGET_PATHS["all-tests"])
    
        # Build the marker expression we'll use and compute a simple fingerprint
of
        # the test tree (latest mtime) to detect changes that should invalidate 
cache.
        marker_expr = "not memory_intensive"
        category_expr = marker_expr
        if speed_category:
            category_expr = f"{speed_category} and {marker_expr}"
    
        normalized_filter = _normalize_keyword_filter(keyword_filter)
        latest_mtime = _latest_mtime(test_path)
        base_cache_key = f"{target}_{speed_category or 'all'}"
        suffix = _cache_key_suffix(normalized_filter)
        cache_key = f"{base_cache_key}_{suffix}" if suffix else base_cache_key
        cache_file = (
>           COLLECTION_CACHE_DIR / f"{cache_key}_tests.json"
        )  # Use Path object for cache_file
E       TypeError: unsupported operand type(s) for /: 'str' and 'str'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1450: TypeError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,624 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
_______ test_collect_tests_with_cache_all_tests_decomposes_successfully ________

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_22')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b997c80>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13afa0c80>

    @pytest.mark.fast
    def test_collect_tests_with_cache_all_tests_decomposes_successfully(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RUN-TESTS-SEG-6  all-tests decomposes into dependent targets 
on miss."""
    
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", cache_dir)
        monkeypatch.setattr(
            rt, "TEST_COLLECTION_CACHE_FILE", cache_dir / 
"collection_cache.json"
        )
    
        tests_root = tmp_path / "tests"
        unit_dir = tests_root / "unit"
        integration_dir = tests_root / "integration"
        behavior_dir = tests_root / "behavior"
        for path in (unit_dir, integration_dir, behavior_dir):
            path.mkdir(parents=True)
            (path / "test_example.py").write_text(
                "def test_ok():\n    pass\n", encoding="utf-8"
            )
    
        monkeypatch.setattr(
            rt,
            "TARGET_PATHS",
            {
                "unit-tests": str(unit_dir),
                "integration-tests": str(integration_dir),
                "behavior-tests": str(behavior_dir),
                "all-tests": str(tests_root),
            },
        )
    
        calls: list[str] = []
    
        class DummyProcess:
            def __init__(self, stdout: str) -> None:
                self.stdout = stdout
                self.stderr = ""
                self.returncode = 0
    
        mapping = {
            str(unit_dir): "tests/unit/test_example.py::test_ok\n",
            str(integration_dir): 
"tests/integration/test_example.py::test_ok\n",
            str(behavior_dir): "tests/behavior/test_example.py::test_ok\n",
        }
    
        def fake_run(
            cmd,
            capture_output=True,
            text=True,
            timeout=None,
            **kwargs,
        ):  # noqa: ANN001
            test_path = cmd[3]
            calls.append(test_path)
            stdout = mapping.get(test_path)
            if stdout is None:
                raise AssertionError(f"Unexpected test path {test_path}")
            return DummyProcess(stdout)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        caplog.set_level(logging.INFO)
    
        result = rt.collect_tests_with_cache("all-tests", "fast")
    
>       assert result == [
            "tests/unit/test_example.py::test_ok",
            "tests/integration/test_example.py::test_ok",
            "tests/behavior/test_example.py::test_ok",
        ]
E       AssertionError: assert ['tests/unit/....py::test_ok'] == 
['tests/unit/....py::test_ok']
E         
E         Right contains one more item: 
'tests/behavior/test_example.py::test_ok'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmentation_helpers.py:341: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,673 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=all-tests (fast)  decomposing all-tests into dependent 
targets
2025-10-29 10:48:59,674 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,674 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=integration-tests (fast)  collecting via pytest
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=all-tests (fast)  decomposing all-tests into dependent targets
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=integration-tests (fast)  collecting via pytest
____ test_collect_tests_with_cache_timeout_falls_back_to_direct_collection _____

tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_collect_tests_with_cache_23')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13afa3320>
caplog = <_pytest.logging.LogCaptureFixture object at 0x13c026c30>

    @pytest.mark.fast
    def test_collect_tests_with_cache_timeout_falls_back_to_direct_collection(
        tmp_path: Path, monkeypatch: pytest.MonkeyPatch, caplog: 
pytest.LogCaptureFixture
    ) -> None:
        """ReqID: RUN-TESTS-SEG-7  retries smaller batches before broad 
fallback."""
    
        cache_dir = tmp_path / ".cache"
        monkeypatch.setattr(rt, "COLLECTION_CACHE_DIR", cache_dir)
        monkeypatch.setattr(
            rt, "TEST_COLLECTION_CACHE_FILE", cache_dir / 
"collection_cache.json"
        )
    
        tests_root = tmp_path / "tests"
        unit_dir = tests_root / "unit"
        integration_dir = tests_root / "integration"
        behavior_dir = tests_root / "behavior"
        for path in (unit_dir, integration_dir, behavior_dir):
            path.mkdir(parents=True)
            (path / "test_example.py").write_text(
                "def test_ok():\n    pass\n", encoding="utf-8"
            )
    
        monkeypatch.setattr(
            rt,
            "TARGET_PATHS",
            {
                "unit-tests": str(unit_dir),
                "integration-tests": str(integration_dir),
                "behavior-tests": str(behavior_dir),
                "all-tests": str(tests_root),
            },
        )
    
        calls: list[str] = []
    
        class DummyProcess:
            def __init__(self, stdout: str) -> None:
                self.stdout = stdout
                self.stderr = ""
                self.returncode = 0
    
        fallback_output = (
            "tests/unit/test_example.py::test_ok\n"
            "tests/integration/test_example.py::test_ok\n"
            "tests/behavior/test_example.py::test_ok\n"
        )
    
        timeout_targets = {str(unit_dir), str(integration_dir), 
str(behavior_dir)}
    
        def fake_run(
            cmd,
            capture_output=True,
            text=True,
            timeout=None,
            **kwargs,
        ):  # noqa: ANN001
            test_path = cmd[3]
            calls.append(test_path)
            if test_path in timeout_targets:
                raise subprocess.TimeoutExpired(cmd, timeout)
            if test_path == str(tests_root):
                return DummyProcess(fallback_output)
            raise AssertionError(f"Unexpected test path {test_path}")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        caplog.set_level(logging.INFO)
    
        result = rt.collect_tests_with_cache("all-tests", "fast")
    
        assert result == [
            "tests/unit/test_example.py::test_ok",
            "tests/integration/test_example.py::test_ok",
            "tests/behavior/test_example.py::test_ok",
        ]
        assert calls[-1] == str(tests_root)
        timeout_events = {
            getattr(record, "target", None)
            for record in caplog.records
            if getattr(record, "event", "") == "test_collection_timeout"
        }
>       assert timeout_events == {"unit-tests", "integration-tests", 
"behavior-tests"}
E       AssertionError: assert {'integration... 'unit-tests'} == 
{'behavior-te... 'unit-tests'}
E         
E         Extra items in the right set:
E         'behavior-tests'
E         Use -v to get more diff

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmentation_helpers.py:433: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,687 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=all-tests (fast)  decomposing all-tests into dependent 
targets
2025-10-29 10:48:59,687 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,687 - devsynth.testing.run_tests - WARNING - Test collection
timeout for target=unit-tests (fast); falling back to path
2025-10-29 10:48:59,687 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=integration-tests (fast)  collecting via pytest
2025-10-29 10:48:59,687 - devsynth.testing.run_tests - WARNING - Test collection
timeout for target=integration-tests (fast); falling back to path
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=all-tests (fast)  decomposing all-tests into dependent targets
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout
for target=unit-tests (fast); falling back to path
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=integration-tests (fast)  collecting via pytest
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Test collection timeout
for target=integration-tests (fast); falling back to path
______________ test_segmented_failure_appends_aggregate_tips_once ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13c253590>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_segmented_failure_appends0')

    @pytest.mark.fast
    def test_segmented_failure_appends_aggregate_tips_once(monkeypatch, 
tmp_path):
        """
        ReqID: RT-11  Aggregated troubleshooting tips appended once after 
segments.
        """
        # Arrange a fake tests directory and map unit-tests to it
        tests_dir = tmp_path / "tests" / "unit"
        tests_dir.mkdir(parents=True)
        monkeypatch.chdir(tmp_path)
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
    
        # Collected node ids => 2 batches with size=2
        collected = [
            "tests/unit/test_a.py::test_1",
            "tests/unit/test_a.py::test_2",
            "tests/unit/test_b.py::test_3",
            "tests/unit/test_b.py::test_4",
        ]
    
        def fake_run(
            cmd,
            check=False,
            capture_output=False,
            text=False,
            timeout=None,
            cwd=None,
            env=None,
        ):
            if "--collect-only" in cmd:
                return _DummyCompleted(stdout="\n".join(collected), stderr="", 
returncode=0)
            return _DummyCompleted(stdout="", stderr="", returncode=0)
    
        # First batch fails (rc=1), second succeeds (rc=0)
        dummy_popen = _DummyPopen(rc_sequence=[1, 0])
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", dummy_popen)
    
        # Act
        ok, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=2,
            maxfail=1,
            extra_marker=None,
        )
    
        # Assert: overall not ok due to failed batch
        assert ok is False
        # The troubleshooting tips block should appear once per failed batch (1)
plus
        # one aggregated block at the end => total 2 occurrences.
>       assert output.count("Troubleshooting tips:") == 2
E       AssertionError: assert 1 == 2
E        +  where 1 = <built-in method count of str object at 
0x13f5fb600>('Troubleshooting tips:')
E        +    where <built-in method count of str object at 0x13f5fb600> = 
'\nPytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m 
pytest...HTML report for context (saved under test_reports/):\n  devsynth 
run-tests --target unit-tests --speed=fast --report\n'.count

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_aggregate_fail_tips_once.py:104: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,758 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,759 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,759 - devsynth.testing.run_tests - INFO - Running 4 tests in
2 segments of size 2 for target=unit-tests
2025-10-29 10:48:59,759 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (2 tests)
2025-10-29 10:48:59,759 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 4 tests in 2 
segments of size 2 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (2 
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
____________ test_segmented_aggregate_tips_command_includes_maxfail ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b452fc0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segmented_aggregate_tips_command_includes_maxfail(monkeypatch) -> 
None:
        """
        ReqID: RT-11  When segmented mode runs and any batch fails, the 
aggregated
        troubleshooting tips are generated using a command that includes 
--maxfail
        if maxfail was provided.
        """
    
        collected_ids = "tests/unit/sample_test.py::test_a\n"
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            # collection phase returns one node id (ensures one batch)
            return SimpleNamespace(returncode=0, stdout=collected_ids, 
stderr="")
    
        class FailingBatch:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                # Simulate a failing batch
                self.returncode = 1
    
            def communicate(self) -> tuple[str, str]:
                return ("", "boom")
    
        captured = {}
    
        def fake_failure_tips(returncode, cmd):  # noqa: ANN001
            # Capture the command used to generate tips for later assertion
            captured["cmd"] = cmd
            return "\nTroubleshooting tips: ...\n"
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", FailingBatch)
        monkeypatch.setattr(rt, "_failure_tips", fake_failure_tips)
    
        success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=5,
            maxfail=2,
            extra_marker=None,
        )
    
        assert success is False
        assert "Troubleshooting tips" in output
        # Ensure --maxfail=2 was included in the aggregate cmd, not just batch 
cmd
>       assert any(
            isinstance(arg, str) and arg.startswith("--maxfail=") and 
arg.endswith("2")
            for arg in captured.get("cmd", [])
        ), f"--maxfail not propagated in aggregate cmd: {captured}"
E       AssertionError: --maxfail not propagated in aggregate cmd: {'cmd': 
['/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python', '-m', 
'pytest', 'tests/unit/sample_test.py::test_a', '-m', 'not memory_intensive and 
fast and not gui', '--cov=src/devsynth', 
'--cov-report=json:test_reports/coverage.json', '--cov-report=html:htmlcov', 
'--cov-append', '--maxfail', '2']}
E       assert False
E        +  where False = any(<generator object 
test_segmented_aggregate_tips_command_includes_maxfail.<locals>.<genexpr> at 
0x13d8eac00>)

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_aggregate_maxfail.py:68: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,768 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,769 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,769 - devsynth.testing.run_tests - INFO - Running 1 tests in
1 segments of size 5 for target=unit-tests
2025-10-29 10:48:59,769 - devsynth.testing.run_tests - INFO - Running segment 
1/1 (1 tests)
2025-10-29 10:48:59,769 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 1 tests in 1 
segments of size 5 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/1 (1 
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
___________ test_run_tests_segmented_falls_back_on_empty_collection ____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b453680>
tmp_path = 
PosixPath('/private/var/folders/2v/lbss3by10y51bg7c07nd23rh0000gn/T/pytest-of-ca
itlyn/pytest-57/test_run_tests_segmented_falls0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x13bbcd0a0>

    @pytest.mark.fast
    def test_run_tests_segmented_falls_back_on_empty_collection(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        """ReqID: RUN-TESTS-SEGMENTED-5  Fallback run executes when no node ids
exist."""
    
        tests_dir = tmp_path / "segmented-empty"
        tests_dir.mkdir()
        monkeypatch.setitem(rt.TARGET_PATHS, "unit-tests", str(tests_dir))
        monkeypatch.setattr(rt, "_reset_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "_ensure_coverage_artifacts", lambda: None)
        monkeypatch.setattr(rt, "ensure_pytest_cov_plugin_env", lambda _env: 
False)
        monkeypatch.setattr(rt, "ensure_pytest_bdd_plugin_env", lambda _env: 
False)
    
        def fake_collect(cmd, check=False, capture_output=True, text=True):  # 
noqa: ANN001
            assert "--collect-only" in cmd
            return SimpleNamespace(returncode=0, stdout="", stderr="")
    
        monkeypatch.setattr(rt.subprocess, "run", fake_collect)
    
        popen_calls: list[list[str]] = []
    
        class FakePopen:
            def __init__(
                self,
                cmd,
                stdout=None,
                stderr=None,
                text=True,
                env=None,
            ):  # noqa: ANN001
                popen_calls.append(cmd[:])
                self.returncode = 0
                self._stdout = "ok\n"
                self._stderr = ""
    
            def communicate(self):  # noqa: D401 - simple stub
                return self._stdout, self._stderr
    
        monkeypatch.setattr(rt.subprocess, "Popen", FakePopen)
    
        caplog.set_level(logging.WARNING)
    
>       success, output = rt.run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=10,
            maxfail=None,
            extra_marker=None,
        )

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_empty_node_ids.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1744: in run_tests
    nodes = collect_callable(
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1549: in collect_tests_with_cache
    node_ids = _collect_via_pytest(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _collect_via_pytest(
        *,
        target: str,
        test_path: str,
        category_expr: str,
        normalized_filter: str | None,
        timeout_seconds: float,
    ) -> list[str]:
        """Execute ``pytest --collect-only`` and return node identifiers."""
    
        # Use the same Python executable that poetry is using
        # Check for poetry's active python first, then fallback to virtual env 
python
        python_exe = os.environ.get("POETRY_ACTIVE_PYTHON")
        if not python_exe:
            # Try to find the virtual environment python
            venv_python = Path.cwd() / ".venv" / "bin" / "python3"
            if venv_python.exists():
                python_exe = str(venv_python)
            else:
                python_exe = sys.executable
        collect_cmd = [
            python_exe,
            "-m",
            "pytest",
            test_path,
            "--collect-only",
            "-q",
            "-o",
            "addopts=",
            "-m",
            category_expr,
        ]
    
        if normalized_filter:
            collect_cmd.extend(["-k", normalized_filter])
    
        # Get the project root (where pyproject.toml is located)
        project_root = Path(__file__).parent.parent.parent.parent
    
        # Inherit the full environment but override specific variables for 
collection
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root / "src")
        env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"
        env["PYTEST_PLUGINS"] = "pytest_bdd.plugin,pytest_cov.plugin"
        env["PYTEST_ADDOPTS"] = "-p pytest_bdd.plugin -p pytest_cov.plugin"
    
>       result = subprocess.run(
            collect_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=str(project_root),  # Ensure subprocess runs from project root
            env=env,  # Inherit environment but override key variables
        )
E       TypeError: 
test_run_tests_segmented_falls_back_on_empty_collection.<locals>.fake_collect() 
got an unexpected keyword argument 'timeout'

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/testing/run_te
sts.py:1337: TypeError
_____________ test_segment_batch_benchmark_warning_forces_success ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x13b9db6e0>

    @pytest.mark.fast
    @pytest.mark.requires_resource("codebase")
    def test_segment_batch_benchmark_warning_forces_success(monkeypatch) -> 
None:
        """ReqID: RT-09  PytestBenchmarkWarning in stderr forces success for 
the batch."""
    
        collected_ids = (
            "tests/unit/mod_x_test.py::test_x1\n" 
"tests/unit/mod_y_test.py::test_y1\n"
        )
    
        def fake_run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=None,
            cwd=None,
            env=None,
        ):  # noqa: ANN001
            return SimpleNamespace(returncode=0, stdout=collected_ids, 
stderr="")
    
        class WarnBatch:
            def __init__(
                self, cmd, stdout=None, stderr=None, text=False, env=None
            ):  # noqa: ANN001
                self._stderr = "PytestBenchmarkWarning: calibration"
                self.returncode = 1  # would fail without the special-case 
handling
    
            def communicate(self) -> tuple[str, str]:
                return ("ok\n", self._stderr)
    
        monkeypatch.setattr(rt.subprocess, "run", fake_run)
        monkeypatch.setattr(rt.subprocess, "Popen", WarnBatch)
    
        success, output = run_tests(
            target="unit-tests",
            speed_categories=["fast"],
            verbose=False,
            report=False,
            parallel=False,
            segment=True,
            segment_size=1,
            maxfail=None,
            extra_marker=None,
        )
    
>       assert success is True
E       assert False is True

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/testing/test_run
_tests_segmented_failure_paths.py:105: AssertionError
----------------------------- Captured stdout call -----------------------------
2025-10-29 10:48:59,820 - devsynth.testing.run_tests - INFO - Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
2025-10-29 10:48:59,821 - devsynth.testing.run_tests - INFO - test collection 
cache miss for target=unit-tests (fast)  collecting via pytest
2025-10-29 10:48:59,821 - devsynth.testing.run_tests - INFO - Running 2 tests in
2 segments of size 1 for target=unit-tests
2025-10-29 10:48:59,821 - devsynth.testing.run_tests - INFO - Running segment 
1/2 (1 tests)
2025-10-29 10:48:59,821 - devsynth.testing.run_tests - INFO - Running segment 
2/2 (1 tests)
2025-10-29 10:48:59,821 - devsynth.testing.run_tests - WARNING - Coverage 
artifact generation skipped: data file missing
------------------------------ Captured log call -------------------------------
INFO     devsynth.testing.run_tests:logging_setup.py:615 Injected -p 
pytest_asyncio.plugin into PYTEST_ADDOPTS to preserve async test support
INFO     devsynth.testing.run_tests:logging_setup.py:615 test collection cache 
miss for target=unit-tests (fast)  collecting via pytest
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running 2 tests in 2 
segments of size 1 for target=unit-tests
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 1/2 (1 
tests)
INFO     devsynth.testing.run_tests:logging_setup.py:615 Running segment 2/2 (1 
tests)
WARNING  devsynth.testing.run_tests:logging_setup.py:615 Coverage artifact 
generation skipped: data file missing
__________ test_api_health_and_metrics_startup_without_binding_ports ___________

    @pytest.mark.no_network
    @pytest.mark.fast
    def test_api_health_and_metrics_startup_without_binding_ports():
        """Metrics endpoint returns consistent counters without binding ports. 
ReqID: N/A"""
        pytest.importorskip("fastapi")
>       TestClient = pytest.importorskip("fastapi.testclient").TestClient
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/api/test_
api_startup.py:9: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/fastapi/testclient.py:1: in <module>
    from starlette.testclient import TestClient as TestClient  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError
_________________ test_agent_openapi_documents_workflow_models _________________

    @pytest.mark.no_network
    @pytest.mark.fast
    def test_agent_openapi_documents_workflow_models():
        """OpenAPI schema surfaces shared workflow models and live endpoints."""
    
        pytest.importorskip("fastapi")
>       TestClient = pytest.importorskip("fastapi.testclient").TestClient
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/integration/api/test_
api_startup.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/fastapi/testclient.py:1: in <module>
    from starlette.testclient import TestClient as TestClient  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    from __future__ import annotations
    
    import contextlib
    import inspect
    import io
    import json
    import math
    import queue
    import sys
    import typing
    import warnings
    from concurrent.futures import Future
    from functools import cached_property
    from types import GeneratorType
    from urllib.parse import unquote, urljoin
    
    import anyio
    import anyio.abc
    import anyio.from_thread
    from anyio.abc import ObjectReceiveStream, ObjectSendStream
    from anyio.streams.stapled import StapledObjectStream
    
    from starlette._utils import is_async_callable
    from starlette.types import ASGIApp, Message, Receive, Scope, Send
    from starlette.websockets import WebSocketDisconnect
    
    if sys.version_info >= (3, 10):  # pragma: no cover
        from typing import TypeGuard
    else:  # pragma: no cover
        from typing_extensions import TypeGuard
    
    try:
        import httpx
    except ModuleNotFoundError:  # pragma: no cover
        raise RuntimeError(
            "The starlette.testclient module requires the httpx package to be 
installed.\n"
            "You can install this with:\n"
            "    $ pip install httpx\n"
        )
    _PortalFactoryType = typing.Callable[[], 
typing.ContextManager[anyio.abc.BlockingPortal]]
    
    ASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]
    ASGI2App = typing.Callable[[Scope], ASGIInstance]
    ASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]
    
    
    _RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str], 
bytes]]
    
    
    def _is_asgi3(app: ASGI2App | ASGI3App) -> TypeGuard[ASGI3App]:
        if inspect.isclass(app):
            return hasattr(app, "__await__")
        return is_async_callable(app)
    
    
    class _WrapASGI2:
        """
        Provide an ASGI3 interface onto an ASGI2 app.
        """
    
        def __init__(self, app: ASGI2App) -> None:
            self.app = app
    
        async def __call__(self, scope: Scope, receive: Receive, send: Send) -> 
None:
            instance = self.app(scope)
            await instance(receive, send)
    
    
    class _AsyncBackend(typing.TypedDict):
        backend: str
        backend_options: dict[str, typing.Any]
    
    
    class _Upgrade(Exception):
        def __init__(self, session: WebSocketTestSession) -> None:
            self.session = session
    
    
>   class WebSocketDenialResponse(  # type: ignore[misc]
        httpx.Response,
        WebSocketDisconnect,
    ):
E   TypeError: Cannot create a consistent method resolution
E   order (MRO) for bases object, WebSocketDisconnect

/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site-p
ackages/starlette/testclient.py:79: TypeError
=============================== warnings summary ===============================
.venv/lib/python3.12/site-packages/_pytest/config/__init__.py:833
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/_pytest/config/__init__.py:833: PytestAssertRewriteWarning: Module 
already imported so cannot be rewritten; tests.fixtures.optional_deps
    self.import_plugin(import_spec)

.venv/lib/python3.12/site-packages/astor/op_util.py:92
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/op_util.py:92: DeprecationWarning: ast.Num is deprecated and 
will be removed in Python 3.14; use ast.Constant instead
    precedence_data = dict((getattr(ast, x, None), z) for x, y, z in op_data)

.venv/lib/python3.12/site-packages/vbuild/__init__.py:33
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:33: DeprecationWarning: 'pkgutil.find_loader' is 
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasLess = bool(pkgutil.find_loader("lesscpy"))

.venv/lib/python3.12/site-packages/vbuild/__init__.py:34
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:34: DeprecationWarning: 'pkgutil.find_loader' is 
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasSass = bool(pkgutil.find_loader("scss"))

.venv/lib/python3.12/site-packages/vbuild/__init__.py:35
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/vbuild/__init__.py:35: DeprecationWarning: 'pkgutil.find_loader' is 
deprecated and slated for removal in Python 3.14; use importlib.util.find_spec()
instead
    hasClosure = bool(pkgutil.find_loader("closure"))

.venv/lib/python3.12/site-packages/pytest_bdd/plugin.py:137
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/pytest_bdd/plugin.py:137: PytestUnknownMarkWarning: Unknown 
pytest.mark.test-infrastructure - is this a typo?  You can register custom marks
to avoid this warning - for details, see 
https://docs.pytest.org/en/stable/how-to/mark.html
    mark = getattr(pytest.mark, tag)

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 2 
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 12990 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:270: DeprecationWarning: ast.Str is deprecated and 
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Str") and isinstance(node, getattr(ast, "Str")):

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 2 
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 12990 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:273: DeprecationWarning: ast.Num is deprecated and 
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Num") and isinstance(node, getattr(ast, "Num")):

tests/unit/application/code_analysis/test_ast_workflow_integration.py: 3 
warnings
tests/unit/application/code_analysis/test_self_analyzer.py: 11437 warnings
tests/unit/application/code_analysis/test_transformer.py: 6 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:289: DeprecationWarning: ast.NameConstant is 
deprecated and will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "NameConstant") and isinstance(

tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer
tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
  <frozen runpy>:128: RuntimeWarning: 'devsynth.cli' found in sys.modules after 
import of package 'devsynth', but prior to execution of 'devsynth.cli'; this may
result in unpredictable behaviour

tests/unit/application/code_analysis/test_self_analyzer.py: 1765 warnings
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/src/devsynth/application/
code_analysis/analyzer.py:236: DeprecationWarning: ast.Str is deprecated and 
will be removed in Python 3.14; use ast.Constant instead
    elif hasattr(ast, "Str") and isinstance(node, getattr(ast, "Str")):

tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/code_gen.py:599: DeprecationWarning: ast.Str is deprecated and 
will be removed in Python 3.14; use ast.Constant instead
    if isinstance(value, ast.Str):

tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/astor/code_gen.py:601: DeprecationWarning: Attribute s is deprecated 
and will be removed in Python 3.14; use value instead
    self.write(value.s.replace('{', '{{').replace('}', '}}'))

tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/core/test_mvu.
py:56: DeprecationWarning: datetime.datetime.utcnow() is deprecated and 
scheduled for removal in a future version. Use timezone-aware objects to 
represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    now = datetime.utcnow()

tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/tests/unit/interface/test
_api_endpoints.py:452: RuntimeWarning: coroutine 'init_endpoint' was never 
awaited
    enhanced_api.init_endpoint(request, init_request, token=None)
  Enable tracemalloc to get traceback where the object was allocated.
  See 
https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings
for more info.

tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_par
ser.py:484: DeprecationWarning: ast.Str is deprecated and will be removed in 
Python 3.14; use ast.Constant instead
    elif isinstance(node, ast.Str):

tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/enhanced_test_par
ser.py:487: DeprecationWarning: ast.Num is deprecated and will be removed in 
Python 3.14; use ast.Constant instead
    elif isinstance(node, ast.Num):

tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/scripts/run_all_tests.py:
24: DeprecationWarning: scripts/run_all_tests.py is deprecated; use 'devsynth 
run-tests' instead.
    warnings.warn(

tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled
  /Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/lib/python3.12/site
-packages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning: 
  The default value will be `edges="edges" in NetworkX 3.6.
  
  To make this warning go away, explicitly set the edges kwarg, e.g.:
  
    nx.node_link_data(G, edges="links") to preserve current behavior, or
    nx.node_link_data(G, edges="edges") for forward compatibility.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.12.12-final-0 _______________

Coverage HTML written to dir htmlcov
Coverage JSON written to file test_reports/coverage.json
========================= Test Categorization Summary ==========================
Test Type Distribution:
  Unit Tests: 2732
  Integration Tests: 31
  Behavior Tests: 12

Test Speed Distribution:
  Fast Tests (< 1s): 2766
  Medium Tests (1-5s): 5
  Slow Tests (> 5s): 4
============================= Top 10 Slowest Tests =============================
1. 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers: 30.03s
2. 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly: 6.88s
3. 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error: 5.51s
4. 
tests/integration/deployment/test_deployment_scripts.py::test_prometheus_exporte
r_refuses_root: 5.02s
5. 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests: 3.66s
6. 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure: 3.54s
7. 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds: 1.55s
8. tests/unit/general/test_speed_option.py::test_speed_option_recognized: 1.09s
9. 
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation: 
1.01s
10. 
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module: 
0.96s
=========================== short test summary info ============================
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error
FAILED 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum
FAILED 
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-bat
ch]
FAILED 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-b
atches]
FAILED 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env
FAILED 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset
FAILED 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers
FAILED 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message
FAILED 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message
FAILED 
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled
FAILED 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing
FAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment
FAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner
FAILED 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated
FAILED 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback
FAILED 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary
FAILED 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold
FAILED 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery
FAILED 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag
FAILED 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success
FAILED 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure
FAILED 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold
FAILED 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression
FAILED 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error
FAILED 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key
FAILED 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models
FAILED 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property
FAILED 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property
FAILED 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic
FAILED 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion
FAILED 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line
FAILED 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence
FAILED 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior
FAILED 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records
FAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task
FAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement
FAILED 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state
FAILED 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
FAILED 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record
FAILED 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple
FAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase
FAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments
FAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument
FAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path
FAILED 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path
FAILED 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory
FAILED 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation
FAILED 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate
FAILED 
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors
FAILED 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry
FAILED 
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g
FAILED 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
FAILED 
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug
FAILED 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess
FAILED 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests
FAILED 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env
FAILED 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
FAILED 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment
FAILED 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker
FAILED 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url
FAILED 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint
FAILED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds
FAILED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds
FAILED 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds
FAILED 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds
FAILED 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds
FAILED 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory
FAILED 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds
FAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue
FAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code
FAILED 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent
FAILED 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores
FAILED 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds
FAILED 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds
FAILED 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks
FAILED 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches
FAILED 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts
FAILED 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role
FAILED 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus
FAILED 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds
FAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds
FAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds
FAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds
FAILED 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds
FAILED 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds
FAILED tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds
FAILED tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted
FAILED 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie
FAILED tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote - Typ...
FAILED 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error
FAILED 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds
FAILED 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds
FAILED 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds
FAILED 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error
FAILED tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token
FAILED tests/unit/general/test_api_health.py::test_health_endpoint_succeeds
FAILED tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds
FAILED 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds
FAILED 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds
FAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds
FAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure
FAILED 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds
FAILED tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled - Attri...
FAILED tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency
FAILED 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid
FAILED 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected
FAILED 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds
FAILED 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds
FAILED 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction
FAILED 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization
FAILED 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds
FAILED tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success - A...
FAILED tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure - A...
FAILED 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds
FAILED 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds
FAILED 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds
FAILED 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds
FAILED 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds
FAILED 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds
FAILED 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds
FAILED 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds
FAILED 
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds
FAILED 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds
FAILED tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds
FAILED tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds
FAILED 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds
FAILED 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds
FAILED 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches
FAILED 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds
FAILED 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error
FAILED 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error
FAILED 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds
FAILED 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds
FAILED 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds
FAILED 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds
FAILED 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds
FAILED 
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request
FAILED 
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error
FAILED 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches
FAILED 
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds
FAILED tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True]
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False]
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False]
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[absent-3-False]
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance
FAILED 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run
FAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session
FAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets
FAILED 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors
FAILED 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy
FAILED 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants
FAILED 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents
FAILED 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status
FAILED 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks
FAILED 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks
FAILED 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations
FAILED 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle
FAILED 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec
FAILED 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide
FAILED 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub
FAILED 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state
FAILED 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute
FAILED 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds
FAILED 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[warning-warning]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[success-success]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[info-info]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[unexpected-write]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[# Overview-expected_calls0]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[## Section-expected_calls1]
FAILED 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[### Deep Dive-expected_calls2]
FAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once
FAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta
FAILED 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance
FAILED 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance
FAILED 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached
FAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute
FAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour
FAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes
FAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status
FAILED 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time
FAILED 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
FAILED 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
FAILED 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
FAILED 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance
FAILED 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
FAILED 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module
FAILED 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation
FAILED 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds
FAILED 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error
FAILED 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[error-kwargs0-error]
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[warning-kwargs1-warning]
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[success-kwargs2-success]
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[highlight-kwargs3-info]
FAILED 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats
FAILED 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance
FAILED 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance
FAILED 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance
FAILED 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance
FAILED 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance
FAILED 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling
FAILED 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling
FAILED 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling
FAILED 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling
FAILED 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once
FAILED 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle
FAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir]
FAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[home-absolute]
FAILED 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[non-home-absolute]
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception
FAILED 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type
FAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete
FAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure
FAILED 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations
FAILED 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget
FAILED 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase
FAILED 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
FAILED 
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
FAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete
FAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries
FAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called
FAILED 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step
FAILED 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline
FAILED 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted
FAILED 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages
FAILED 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success
FAILED 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure
FAILED 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity
FAILED 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid
FAILED 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors
FAILED 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation
FAILED 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics
FAILED 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error
FAILED tests/unit/security/test_policy_audit.py::test_audit_detects_violation
FAILED tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file
FAILED tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy
FAILED 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate
FAILED 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran
FAILED 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches
FAILED 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty
FAILED 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json
FAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change
FAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change
FAILED 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change
FAILED 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call
FAILED 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh
FAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry
FAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch
FAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty
FAILED 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids
FAILED 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty
FAILED 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids
FAILED 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming
FAILED 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow
FAILED 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches
FAILED 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes
FAILED 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout
FAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception
FAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error
FAILED 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker
FAILED 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html
FAILED 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context
FAILED 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success
FAILED 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches
FAILED 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache
FAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins
FAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates
FAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts
FAILED 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile
FAILED 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled
FAILED 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable
FAILED 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success
FAILED 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return
FAILED 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success
FAILED 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression
FAILED 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression
FAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests
FAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates
FAILED 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success
FAILED 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags
FAILED 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message
FAILED 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir
FAILED 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success
FAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache
FAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired
FAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss
FAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime
FAILED 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all
FAILED 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories
FAILED 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl
FAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes
FAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio
FAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters
FAILED 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path
FAILED 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return
FAILED 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips
FAILED 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions
FAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command
FAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command
FAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command
FAILED 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run
FAILED 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto
FAILED 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto
FAILED 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout
FAILED 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once
FAILED 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir
FAILED 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success
FAILED 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips
FAILED 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully
FAILED 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection
FAILED 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once
FAILED 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail
FAILED 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection
FAILED 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success
FAILED 
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_ports
FAILED 
tests/integration/api/test_api_startup.py::test_agent_openapi_documents_workflow
_models
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0
ERROR 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts
ERROR 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true
ERROR 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env
ERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env
ERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled
ERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient
ERROR 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect
ERROR 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights
ERROR 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics
ERROR 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers
ERROR 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results
ERROR 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds
ERROR 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes
ERROR 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key 'api'-key_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-]
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error
ERROR 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error
ERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown
ERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors
ERROR 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information
ERROR 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: File not found: config.yaml-Make sure the 
file exists]
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Permission denied: secrets.env-necessary 
permissions]
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Invalid value: bad input-Please check your
input]
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Missing key: 'api_key'-Verify that the 
referenced key exists]
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Type error: wrong type-Check that all 
inputs]
ERROR 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-]
ERROR 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown
ERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once
ERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status
ERROR 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions
ERROR 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router
ERROR 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors
ERROR 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout
ERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router
ERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints
ERROR 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render
ERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once
ERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output
ERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks
ERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection
ERROR 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors
ERROR 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router
ERROR 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results
ERROR 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure
ERROR 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results
ERROR 
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs
= 541 failed, 2149 passed, 85 skipped, 1 deselected, 39232 warnings, 193 errors 
in 103.63s (0:01:43) =

Pytest exited with code 1. Command: 
/Users/caitlyn/Projects/github.com/ravenoak/devsynth/.venv/bin/python -m pytest 
tests/unit/adapters/cli/test_typer_adapter.py::test_show_help_invalid_mode_raise
s 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_usage_hint 
tests/unit/adapters/cli/test_typer_adapter.py::test_format_cli_error_runtime_hin
t 
tests/unit/adapters/cli/test_typer_adapter.py::test_command_help_format_includes
_sections 
tests/unit/adapters/issues/test_github_adapter.py::test_fetch_github_issue 
tests/unit/adapters/issues/test_jira_adapter.py::test_fetch_jira_issue 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_normalizes
_mapping 
tests/unit/adapters/llm/test_llm_adapter.py::test_llm_provider_config_without_pa
rameters_returns_none 
tests/unit/adapters/llm/test_llm_adapter.py::test_default_factory_delegates_to_g
lobal_registry 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_uses_injected_
factory 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_emits_typed_er
ror_for_unknown_provider 
tests/unit/adapters/llm/test_llm_adapter.py::test_create_provider_maps_registere
d_message 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_propaga
tes_factory_rejection 
tests/unit/adapters/llm/test_llm_adapter.py::test_register_provider_type_success
tests/unit/adapters/llm/test_llm_adapter.py::test_unknown_llm_provider_error_pre
serves_cause 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_stream
_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_generate_with_c
ontext_stream_returns_chunks 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_chunk_response_
helper_respects_chunk_size 
tests/unit/adapters/llm/test_mock_llm_adapter_streaming.py::test_stream_chunks_y
ields_all_segments 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_mock_response_templa
te_serializes 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_round_trip_pr
eserves_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_matches_cus
tom_template 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_uses_defaul
t_when_no_template_matches 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
coerces_sequences 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_config_from_mapping_
falls_back_to_defaults 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_adapter_initialises_
from_mapping 
tests/unit/adapters/llm/test_mock_llm_adapter_sync.py::test_generate_stream_prop
agates_generate_failure 
tests/unit/adapters/test_agent_adapter.py::test_factory_initializes_agent_with_c
onfig_payload 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_builds_consensus_p
ayload_from_solutions 
tests/unit/adapters/test_agent_adapter.py::test_process_task_without_agents_rais
es_validation_error 
tests/unit/adapters/test_agent_adapter.py::test_coerce_task_solutions_filters_in
valid_entries 
tests/unit/adapters/test_agent_adapter.py::test_import_agent_falls_back_on_error
tests/unit/adapters/test_agent_adapter.py::test_import_agent_rejects_non_class 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_caches_result
s 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_processing
_failures 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_handles_critical_d
ecisions 
tests/unit/adapters/test_agent_adapter.py::test_delegate_task_requires_active_te
am 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_singl
e_agent_flow 
tests/unit/adapters/test_agent_adapter.py::test_agent_initialization_payload_han
dles_unknown_type 
tests/unit/adapters/test_agent_adapter.py::test_coerce_helpers_normalize_inputs 
tests/unit/adapters/test_agent_adapter.py::test_unified_agent_fallback_behaviour
tests/unit/adapters/test_agent_adapter.py::test_load_default_config_uses_yaml_lo
ader 
tests/unit/adapters/test_agent_adapter.py::test_lookup_agent_class_uses_future_s
pecs 
tests/unit/adapters/test_agent_adapter.py::test_create_team_uses_collaborative_w
hen_memory_manager 
tests/unit/adapters/test_agent_adapter.py::test_add_agent_creates_default_team 
tests/unit/adapters/test_agent_adapter.py::test_agent_adapter_process_task_multi
_agent_path 
tests/unit/adapters/test_backend_resource_gates.py::test_chromadb_adapter_import
s tests/unit/adapters/test_backend_resource_gates.py::test_kuzu_adapter_imports 
tests/unit/adapters/test_backend_resource_gates.py::test_faiss_store_imports_and
_minimal 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_transaction_commit_
and_delete 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_provider_fallback_u
ses_default_embedder 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_store_raises_after_
retries 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_search_handles_empt
y_results 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_commit_failure_mark
s_transaction 
tests/unit/adapters/test_chromadb_memory_store_unit.py::test_rollback_transactio
n_states 
tests/unit/adapters/test_fake_memory_store.py::test_fake_memory_store_store_retr
ieve_search_delete_and_txn 
tests/unit/adapters/test_fake_memory_store.py::test_fake_vector_store_similarity
_and_stats 
tests/unit/adapters/test_github_project_adapter.py::test_payload_serialization 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_request_payload
_and_helpers 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_creates_colu
mns_and_cards 
tests/unit/adapters/test_github_project_adapter.py::test_fetch_and_mutations_wit
h_stub_client 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_missing_data_ra
ises 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_skips_existi
ng_items 
tests/unit/adapters/test_github_project_adapter.py::test_sync_board_raises_on_gr
aphql_errors 
tests/unit/adapters/test_github_project_adapter.py::test_graphql_error_formattin
g_handles_missing_messages 
tests/unit/adapters/test_jira_adapter.py::test_create_issue_payload_serializatio
n tests/unit/adapters/test_jira_adapter.py::test_transition_issue_missing_status
tests/unit/adapters/test_jira_adapter.py::test_create_issue_http_error_surfaced 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_i
nit_creates_empty_adapter 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_l
oad_model_sets_session 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_without_loaded_model_raises_error 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_with_loaded_model_calls_session_run 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_multiple_outputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_handles_empty_inputs 
tests/unit/adapters/test_onnx_runtime_adapter.py::TestONNXRuntimeAdapter::test_r
un_propagates_onnx_exceptions 
tests/unit/adapters/test_provider_safe_defaults.py::test_default_safe_falls_back
_to_stub_without_keys_and_lmstudio 
tests/unit/adapters/test_provider_safe_defaults.py::test_openai_explicit_without
_key_raises 
tests/unit/adapters/test_provider_safe_defaults.py::test_anthropic_implicit_with
out_key_falls_back_safe_default_stub 
tests/unit/adapters/test_provider_safe_defaults.py::test_lmstudio_not_attempted_
without_availability_flag 
tests/unit/adapters/test_provider_safe_defaults.py::test_disable_providers_retur
ns_null 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_complete_and_embed
_are_deterministic 
tests/unit/adapters/test_provider_stub.py::test_stub_provider_async_matches_sync
tests/unit/adapters/test_provider_stub.py::test_provider_system_reload_preserves
_settings_import 
tests/unit/adapters/test_provider_system.py::test_embed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_embed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_aembed_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_complete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_success_succeeds 
tests/unit/adapters/test_provider_system.py::test_acomplete_error_succeeds 
tests/unit/adapters/test_provider_system.py::test_null_provider_complete_raises_
error 
tests/unit/adapters/test_provider_system.py::test_null_provider_acomplete_raises
_error 
tests/unit/adapters/test_provider_system.py::test_null_provider_embed_raises_err
or 
tests/unit/adapters/test_provider_system.py::test_null_provider_aembed_raises_er
ror 
tests/unit/adapters/test_provider_system.py::test_null_provider_initialization 
tests/unit/adapters/test_provider_system.py::test_provider_factory_create_provid
er_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_base_provider_methods_succeeds
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[OpenAIProvider-config0] 
tests/unit/adapters/test_provider_system.py::test_provider_initialization_succee
ds[LMStudioProvider-config1] 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_initializati
on_skips_health_check_when_network_guard_active 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_succeeds 
tests/unit/adapters/test_provider_system.py::test_load_env_file_populates_config
tests/unit/adapters/test_provider_system.py::test_create_tls_config_has_expected
tests/unit/adapters/test_provider_system.py::test_get_env_or_default_succeeds 
tests/unit/adapters/test_provider_system.py::test_get_provider_config_has_expect
ed 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_has_e
xpected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_error
_raises_error 
tests/unit/adapters/test_provider_system.py::test_openai_provider_complete_retry
_has_expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_acomplete_has_
expected 
tests/unit/adapters/test_provider_system.py::test_openai_provider_embed_has_expe
cted 
tests/unit/adapters/test_provider_system.py::test_lmstudio_provider_complete_has
_expected 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_async_method
s_has_expected 
tests/unit/adapters/test_provider_system.py::test_provider_with_empty_inputs_has
_expected 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_selects_provider 
tests/unit/adapters/test_provider_system.py::test_provider_factory_injected_conf
ig_survives_missing_settings 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_respects_ord
er 
tests/unit/adapters/test_provider_system.py::test_openai_provider_retries_after_
transient_failure 
tests/unit/adapters/test_provider_system.py::test_fallback_provider_circuit_brea
ker_blocks_after_failure 
tests/unit/adapters/test_provider_system.py::test_complete_falls_back_to_next_pr
ovider 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_re
spects_disable_flag 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_stub_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_of
fline_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_defaults_to_safe_provider_when_lmstudio_unavailable 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_mi
ssing_openai_key_falls_back_to_lmstudio_when_marked_available 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_lm
studio_instantiation_failure_uses_null_safe_default 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_op
enai_explicit_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_an
thropic_missing_key_surfaces_error 
tests/unit/adapters/test_provider_system_additional.py::test_provider_factory_ac
cepts_provider_type_enum 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_req
uires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_lmstudio_provider_r
equires_requests_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_openai_provider_asy
nc_requires_httpx_dependency 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_defaults
_when_settings_missing 
tests/unit/adapters/test_provider_system_additional.py::test_tls_config_uses_exp
licit_settings 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_wir
ing 
tests/unit/adapters/test_provider_system_additional.py::test_retry_decorator_emi
ts_metrics_on_retry 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_n
o_valid_providers 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
ync_uses_circuit_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_failure_opens_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_respects_open_breaker 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
sync_records_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_a
ll_failures_surface_last_error 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
hort_circuits_after_first_success 
tests/unit/adapters/test_provider_system_additional.py::test_fallback_provider_s
kips_providers_with_open_breakers 
tests/unit/adapters/test_provider_system_additional.py::test_complete_failure_in
crements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_embed_wraps_unexpec
ted_error 
tests/unit/adapters/test_provider_system_additional.py::test_acomplete_failure_i
ncrements_metrics 
tests/unit/adapters/test_provider_system_additional.py::test_aembed_wraps_unexpe
cted_error 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_uses_next_provider 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_fallback_provid
er_complete_raises_after_exhaustion 
tests/unit/adapters/test_provider_system_fallbacks_fast.py::test_embed_wraps_une
xpected_exceptions 
tests/unit/adapters/test_provider_system_resilience.py::test_base_provider_retry
_harness_records_jitter 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_a
sync_breaker_failure_emits_metrics 
tests/unit/adapters/test_provider_system_resilience.py::test_fallback_provider_s
ync_breaker_failure_emits_metrics 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_skips_by_def
ault 
tests/unit/adapters/test_resource_gating_seams.py::test_tinydb_seam_runs_when_en
abled 
tests/unit/adapters/test_storage_adapter_protocol.py::test_storage_adapter_proto
col_shape 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
turns_structure 
tests/unit/agents/test_alignment_metrics_tool.py::test_alignment_metrics_tool_re
gistered 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_returns_structure 
tests/unit/agents/test_doctor_tool.py::test_doctor_tool_registered 
tests/unit/agents/test_multi_agent_coordinator.py::test_reach_consensus_majority
_choice 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_returns_structure 
tests/unit/agents/test_run_tests_tool.py::test_run_tests_tool_registered 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_returns_
structure 
tests/unit/agents/test_security_audit_tool.py::test_security_audit_tool_register
ed 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
existing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
missing_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
empty_file 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_load_template_
with_whitespace 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_boundary_value
s_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_error_conditio
ns_prompt_loaded 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_with_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_without_templates 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_build_edge_cas
e_prompts_mixed_availability 
tests/unit/agents/test_test_generator.py::TestTestGenerator::test_template_direc
tory_path_construction 
tests/unit/agents/test_tool_sandbox.py::test_file_access_restricted 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_blocked 
tests/unit/agents/test_tool_sandbox.py::test_shell_commands_allowed 
tests/unit/agents/test_tool_sandbox.py::test_sandbox_context_restores_hooks 
tests/unit/agents/test_tools.py::test_register_and_get_tool 
tests/unit/agents/test_tools.py::test_unknown_tool_returns_none 
tests/unit/agents/test_tools.py::test_export_for_openai_formats_tools 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_r
ecords_summary_and_flushes_memory 
tests/unit/agents/test_wsde_team_coordinator_strict.py::test_run_retrospective_s
upports_primus_rotation_cycle 
tests/unit/api/test_fastapi_testclient_import.py::test_testclient_imports_withou
t_mro_conflict 
tests/unit/api/test_public_api_contract.py::test_public_api_imports 
tests/unit/api/test_public_api_contract.py::test_deprecated_wrapper_emits_warnin
g 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_initializa
tion_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_no_llm_port_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_process_ab
stract_method_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_create_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_update_wsd
e_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_get_role_p
rompt_succeeds 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_error_raises_error 
tests/unit/application/agents/test_base_agent.py::TestBaseAgent::test_generate_t
ext_with_context_error_raises_error 
tests/unit/application/agents/test_test_agent_integration.py::test_process_scaff
olds_tests_from_context 
tests/unit/application/agents/test_validation_agent.py::test_process_affirmative
_is_valid_true 
tests/unit/application/agents/test_validation_agent.py::test_process_failure_tok
ens_set_invalid 
tests/unit/application/agents/test_validation_agent.py::test_process_neutral_tex
t_is_valid 
tests/unit/application/agents/test_validation_agent.py::test_is_valid_word_bound
ary_only 
tests/unit/application/agents/test_validation_agent.py::test_wsde_contains_agent
_and_role 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[All checks passed; no issues.-True] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[An error occurred in module A.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Exception occurred during run.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Some tests fail on CI.-False] 
tests/unit/application/agents/test_validation_agent_decision.py::test_decision_t
okens[Clean run; everything looks good.-True] 
tests/unit/application/agents/test_wsde_memory_integration_fast.py::test_store_a
nd_retrieve_dialectical_process 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_displays
_all_config 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_update_k
ey_value_saves_and_reports 
tests/unit/application/cli/commands/test_config_cmd.py::test_config_cmd_list_mod
els_displays_models 
tests/unit/application/cli/commands/test_config_cmd.py::test_enable_feature_cmd_
updates_and_saves 
tests/unit/application/cli/commands/test_doctor_cmd_typed.py::test_doctor_cmd_ac
cepts_path_arguments 
tests/unit/application/cli/commands/test_doctor_no_ui_imports.py::test_doctor_cm
d_does_not_import_streamlit_or_nicegui 
tests/unit/application/cli/commands/test_ingest_cli_command.py::test_ingest_cli_
command_uses_typed_options 
tests/unit/application/cli/commands/test_inspect_code_cmd_sanitization.py::test_
inspect_code_cmd_sanitizes_dynamic_output 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_preserves_alias_after_subtask_rename 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_rebinds_alias_on_multiple_description_updates 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_reports_eta_strings_when_progress_advances 
tests/unit/application/cli/commands/test_long_running_progress_timeline_bridge.p
y::test_progress_timeline_records_failure_history_for_diagnostics 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.align_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.alignment_metrics_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.analyze_manifest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.atomic_rewrite_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.code_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.completion_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.config_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dbschema_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.diagnostics_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.doctor_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.documentation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.dpg_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.edrr_cycle_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.enhanced_analysis_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.extra_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.gather_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generate_docs_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.generation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.ingest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.init_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_code_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.inspect_config_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.interface_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.metrics_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_exec_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_init_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_lint_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_report_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvu_rewrite_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.mvuu_dashboard_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.pipeline_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.refactor_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.reprioritize_issues_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_pipeline_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.run_tests_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.security_audit_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.serve_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.spec_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.test_metrics_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.testing_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_manifest_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validate_metadata_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.validation_cmds] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_chunk_commit_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.vcs_fix_rebase_pr_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webapp_cmd] 
tests/unit/application/cli/commands/test_module_imports.py::test_command_module_
import[devsynth.application.cli.commands.webui_cmd] 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_empty_list_returns_empty_dict 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_single_name_defaults_true 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_false_variants 
tests/unit/application/cli/commands/test_parse_feature_options_unit.py::test_par
se_feature_options_name_equals_true_variants 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
returns_mapping 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
invalid_json_returns_none 
tests/unit/application/cli/commands/test_run_pipeline_cmd.py::test_parse_report_
non_mapping_returns_none 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_allows_requests_
env_default_for_unit 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_smoke_mode_sets_
env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_feature_flag_map
ping_sets_env 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_marker_passthrou
gh 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inventory_export
s_file 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_integration_targ
et_retains_cov_when_no_report 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_target_p
rints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_invalid_speed_pr
ints_error_and_exits 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_inner_test_env_d
isables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_verbose_and_fast
_timeout_env_behavior 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_report_mode_prin
ts_report_path_message 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_failed_run_surfa
ces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_pytest_cov_missing 
tests/unit/application/cli/commands/test_run_tests_cmd.py::test_run_tests_cmd_ex
its_when_autoload_blocks_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_ma
rker_passthrough 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fe
ature_flags_set_environment 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_se
gmentation_arguments_forwarded 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_in
ventory_mode_exports_json 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_focus.py::test_cli_fa
ilure_propagates_exit_code 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_target 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_rejects_invalid_speed 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_failed_run_surfaces_maxfail_guidance 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_inventory_write_failure_exits_nonzero 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_invalid_inputs
.py::test_cli_runner_maxfail_option_propagates_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_segmented_run_injects_plugins_and_emits_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_inventory_mode_exports_json_via_typer 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_dry_run_invokes_preview 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_enforces_coverage_threshold_via_cli_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_smoke_mode_reports_coverage_skip_and_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_autoload_disables_pytest_cov 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_paths.py::test
_cli_exits_when_pytest_cov_disabled_via_autoload 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_reports_coverage_artifacts_success 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_exits_when_coverage_artifacts_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_cli_runner_thresholds.py:
:test_cli_surfaces_threshold_runtime_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_generates_coverage_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_smoke_command_injects_pytest_bdd_plugin 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_generates_coverage_artifacts_with_autoload_disabled 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_preserves_existing_cov_fail_under 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_medium_command_handles_empty_collection 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_generates_coverage_and_exits_successfully 
tests/unit/application/cli/commands/test_run_tests_cmd_coverage_artifacts.py::te
st_fast_profile_missing_coverage_artifacts_returns_exit_code_one 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_inner_
test_env_tightening_forces_no_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_env_paths.py::test_unit_t
ests_sets_allow_requests_by_default_and_respects_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_feature
_flags_set_env_and_success_message 
tests/unit/application/cli/commands/test_run_tests_cmd_features.py::test_marker_
option_is_passed_as_extra_marker 
tests/unit/application/cli/commands/test_run_tests_cmd_inner_test.py::test_inner
_test_mode_disables_plugins_and_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_mode_writes_file_and_prints_message 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory.py::test_invent
ory_handles_collection_errors 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_exports_json_and_skips_run 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_inventory_mode_handles_collection_failures 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_invalid_target_exits_with_help_text 
tests/unit/application/cli/commands/test_run_tests_cmd_inventory_and_validation.
py::test_marker_option_is_forwarded_to_runner 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_marker_a
nding_passthrough_multiple_speeds 
tests/unit/application/cli/commands/test_run_tests_cmd_markers.py::test_invalid_
marker_expression_exits_cleanly 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_speed_and_m
arker_forwarding 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_report_true
_prints_output_and_success 
tests/unit/application/cli/commands/test_run_tests_cmd_more.py::test_observabili
ty_and_error_path 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_are_applied_when_unset 
tests/unit/application/cli/commands/test_run_tests_cmd_provider_defaults.py::tes
t_provider_defaults_do_not_override_existing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_report_flag_warns_when_directory_missing 
tests/unit/application/cli/commands/test_run_tests_cmd_report_guidance.py::test_
cli_segment_option_failure_surfaces_failure_tips 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_repo
rt_flag_with_missing_directory_prints_warning 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_smok
e_mode_sets_env_and_disables_parallel 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_no_p
arallel_maps_to_n0 
tests/unit/application/cli/commands/test_run_tests_cmd_report_path.py::test_emit
_coverage_messages_reports_artifacts 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_emits_tips_and_reinjection 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[single-bat
ch] 
tests/unit/application/cli/commands/test_run_tests_cmd_segmentation_regressions.
py::test_segmented_cli_failure_repeats_banner_per_batch_and_aggregate[multiple-b
atches] tests/unit/application/cli/commands/test_run_tests_dummy.py::test_dummy 
tests/unit/application/cli/commands/test_run_tests_features.py::test_run_tests_c
li_feature_flags_set_env 
tests/unit/application/cli/commands/test_run_tests_provider_defaults.py::test_ru
n_tests_cmd_applies_stub_offline_defaults_when_unset 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cli_report_option_forwards_true 
tests/unit/application/cli/commands/test_run_tests_reporting_and_env.py::test_ru
n_tests_cmd_respects_explicit_provider_env 
tests/unit/application/cli/commands/test_run_tests_subprocess.py::test_run_tests
_command_succeeds_without_optional_providers 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_t
arget_exits_with_helpful_message 
tests/unit/application/cli/commands/test_run_tests_validation.py::test_invalid_s
peed_exits_with_helpful_message 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_check_requi
red_env_raises_when_missing_env 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_cmd_happy_path_with_skips 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_security_au
dit_runs_when_not_skipped 
tests/unit/application/cli/commands/test_security_audit_cmd.py::test_run_secrets
_scan_detects_simple_pattern 
tests/unit/application/cli/commands/test_testing_cmd.py::testing_cmd 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_basic_functionality 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_shows_expected_content 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_uses_cli_bridge 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_logging_configuration 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_script_paths_checked 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_output_formatting 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_quick_actions_displayed 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_performance_achievements 
tests/unit/application/cli/commands/test_testing_cmd.py::TestTestingCommand::tes
t_testing_cmd_phase_tasks_completed 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_group_cha
nges_categorizes_and_orders 
tests/unit/application/cli/commands/test_vcs_chunk_commit_cmd.py::test_generate_
message_includes_rationale_and_files 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_minimal_returns_text 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_simple_highlight_false_returns_str 
tests/unit/application/cli/test_command_output_formatter.py::test_format_message
_standard_with_markup_returns_panel_passthrough 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_dict_and_list 
tests/unit/application/cli/test_command_output_formatter.py::test_format_table_w
ith_unsupported_type_falls_back 
tests/unit/application/cli/test_command_output_formatter.py::test_format_list_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_code_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_format_help_va
riants 
tests/unit/application/cli/test_command_output_formatter.py::test_display_does_n
ot_raise 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_defaults 
tests/unit/application/cli/test_ingest_cmd.py::test_load_manifest_reads_yaml 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_is_exported 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_base_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_protocol_alias_import_statement_works 
tests/unit/application/cli/test_long_running_progress.py::test_progress_indicato
r_aliases_listed_in_all 
tests/unit/application/cli/test_long_running_progress.py::test_update_adapts_int
erval_and_checkpoints 
tests/unit/application/cli/test_long_running_progress.py::test_status_history_tr
acks_unique_status_changes 
tests/unit/application/cli/test_long_running_progress.py::test_summary_reflects_
fake_timeline_and_sanitizes_descriptions 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_updates_r
emap_and_short_circuit 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_completio
n_rolls_up_and_freezes_summary 
tests/unit/application/cli/test_long_running_progress.py::test_subtask_checkpoin
t_spacing_respects_minimum 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_produces_deterministic_transcript 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_tracks_history_and_alias_renames 
tests/unit/application/cli/test_long_running_progress.py::test_simulation_timeli
ne_remains_deterministic_after_reload 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_stays_exported 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_pro
gress_indicator_base_alias_direct_import_succeeds 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_upd
ate_thresholds_with_deterministic_clock 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_sub
task_flow_preserves_mappings_and_progress 
tests/unit/application/cli/test_long_running_progress_deterministic.py::test_run
_with_progress_completes_after_exception 
tests/unit/application/cli/test_output.py::TestOutputType::test_output_type_valu
es 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
contains_all_types 
tests/unit/application/cli/test_output.py::TestOutputStyles::test_output_styles_
values 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_in
fo 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_su
ccess 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_colorize_er
ror 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_info 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_succe
ss 
tests/unit/application/cli/test_output.py::TestOutputFunctions::test_print_error
tests/unit/application/cli/test_progress.py::test_progress_manager_handles_lifec
ycle 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_is_concrete_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_available_at_import_time 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_protocol_exists 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_inherits_correctly 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_module_reload_preserves_base_class 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_import_from_module_works_after_reload 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_long_running_progress_indicator_instantiation 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_progress_indicator_base_has_expected_methods 
tests/unit/application/cli/test_progress_aliasing.py::TestProgressIndicatorAlias
ing::test_deterministic_tests_can_import_base 
tests/unit/application/cli/test_requirements_commands.py::test_wizard_cmd_back_n
avigation_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_gather_requiremen
ts_cmd_yaml_succeeds 
tests/unit/application/cli/test_requirements_commands.py::test_initialize_servic
es_configures_singletons 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_handles_empty_repository 
tests/unit/application/cli/test_requirements_commands.py::test_list_requirements
_renders_rich_table 
tests/unit/application/cli/test_requirements_commands.py::test_create_requiremen
t_invokes_service 
tests/unit/application/cli/test_requirements_gathering.py::test_gather_cmd_loggi
ng_exc_info_succeeds 
tests/unit/application/cli/test_run_tests_cmd.py::test_parse_feature_options 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_accepts_feature_flags
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_reports_coverage_perc
ent 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_plugins_d
isabled 
tests/unit/application/cli/test_run_tests_cmd.py::test_cli_errors_when_artifacts
_missing 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_feature_flags_set
_environment 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_no_parallel_flag_
is_passed_to_runner 
tests/unit/application/cli/test_run_tests_cmd_options.py::test_segment_options_a
re_propagated 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_sets_pyt
est_disable_plugin_autoload_env 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_cov_disabled 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_cli_impo
rts_fastapi_testclient 
tests/unit/application/cli/test_run_tests_cmd_smoke.py::test_smoke_mode_skips_co
verage_gate_when_instrumented 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_instantiation
_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_wizard_prompts_via_cli_bri
dge_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_run_succeeds 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_abort_succeed
s 
tests/unit/application/cli/test_setup_wizard.py::test_prompt_features_uses_promp
t_toolkit_multiselect 
tests/unit/application/cli/test_setup_wizard.py::test_setup_wizard_accepts_typed
_inputs 
tests/unit/application/cli/test_setup_wizard_textual.py::test_textual_and_cli_pa
yloads_match 
tests/unit/application/cli/test_setup_wizard_textual.py::test_requirements_wizar
d_supports_shortcut_navigation 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_planning_cmd_re
turns_structured_plan 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_defaults_when_missing 
tests/unit/application/cli/test_sprint_cmd_types.py::test_sprint_retrospective_c
md_handles_invalid_json 
tests/unit/application/code_analysis/test_analyzer.py::test_analyze_code_simple 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_add_docstring_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_complex_transformations_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_extract_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_remove_unused_imports_and_variables_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_function_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_identifier_no_change 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_parameter_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_rename_variable_succeeds 
tests/unit/application/code_analysis/test_ast_transformer.py::TestAstTransformer
::test_validate_syntax_is_valid 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_complexity_and_readability_metrics_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_differentiate_selects_best_option_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_expand_implementation_options_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_refine_implementation_succeeds 
tests/unit/application/code_analysis/test_ast_workflow_integration.py::TestAstWo
rkflowIntegration::test_retrospect_code_quality_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_initialization_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_index_files_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_detect_languages_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_categorize_file_assigns_lists 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_infer_architecture_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_identify_components_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_analyze_requirements_spec_alignment_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer.py::TestProject
StateAnalyzer::test_generate_health_report_succeeds 
tests/unit/application/code_analysis/test_project_state_analyzer_error_paths.py:
:test_project_state_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_analyze_maps_dependencies_and_structure 
tests/unit/application/code_analysis/test_repo_analyzer.py::TestRepoAnalyzer::te
st_cli_entry_invokes_repo_analyzer 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_initialization_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_architecture_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_detect_architecture_type_unknown 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_layers_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_layer_dependencies_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_check_architecture_violations_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_code_quality_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_analyze_test_coverage_succeeds 
tests/unit/application/code_analysis/test_self_analyzer.py::TestSelfAnalyzer::te
st_identify_improvement_opportunities_succeeds 
tests/unit/application/code_analysis/test_self_analyzer_error_paths.py::test_sel
f_analyzer_analyze_graceful_fallback 
tests/unit/application/code_analysis/test_transformer.py::TestAstTransformer::te
st_record_change_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedImportRemove
r::test_remove_unused_imports_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestRedundantAssignmen
tRemover::test_remove_redundant_assignments_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestUnusedVariableRemo
ver::test_remove_unused_variables_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestStringLiteralOptim
izer::test_optimize_string_literals_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeStyleTransform
er::test_improve_code_style_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_code_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_file_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_transform_directory_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestCodeTransformer::t
est_find_python_files_succeeds 
tests/unit/application/code_analysis/test_transformer.py::TestSymbolUsageCounter
::test_count_symbol_usage_succeeds 
tests/unit/application/code_analysis/test_transformer_basic.py::test_optimize_st
ring_literals_simple 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_apply_doc
string_spec_inserts_function_docstring 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_met
hod_from_function_respects_method_type 
tests/unit/application/code_analysis/test_transformer_helpers.py::test_build_cla
ss_from_functions_wraps_functions 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_to_dict 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_ag
ent_message_accepts_string_payload 
tests/unit/application/collaboration/test_agent_collaboration_system.py::test_cr
eate_team_stores_in_memory 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_consensus_outcome_normalizes
_participants_and_metadata 
tests/unit/application/collaboration/test_collaborative_wsde_team_task_managemen
t.py::TestCollaborativeWSDETeamTaskManagement::test_peer_review_consensus_error_
embeds_serialized_outcome 
tests/unit/application/collaboration/test_memory_utils_conversion.py::test_task_
round_trip_to_memory_item 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_colla
boration_payload_protocol_support 
tests/unit/application/collaboration/test_message_protocol.py::test_ensure_messa
ge_filter_rejects_invalid_input 
tests/unit/application/collaboration/test_message_protocol.py::test_message_filt
er_invalid_timestamp_raises 
tests/unit/application/collaboration/test_peer_review_store.py::test_store_in_me
mory_persists_peer_review_record 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_returns_review_decisions 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_failure_yields_error_decision 
tests/unit/application/collaboration/test_peer_review_store.py::test_collect_rev
iews_wraps_consensus_error_with_serialized_outcome 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_build_
consensus_stores_decision_and_summary 
tests/unit/application/collaboration/test_wsde_memory_sync_hooks.py::test_summar
ize_voting_result_persists_summary 
tests/unit/application/collaboration/test_wsde_team_consensus_conflict_detection
.py::test_identify_conflicts_detects_opposing_opinions 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_tie 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_voting_result_winner 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_s
ummarize_consensus_result_methods 
tests/unit/application/collaboration/test_wsde_team_consensus_summary.py::test_c
onsensus_outcome_round_trip_orders_conflicts 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_contradictions 
tests/unit/application/collaboration/test_wsde_team_consensus_utils.py::test_opi
nions_conflict_detects_different_approaches 
tests/unit/application/collaboration/test_wsde_team_extended_peer_review.py::tes
t_peer_review_solution_excludes_author 
tests/unit/application/collaboration/test_wsde_team_task_management_mixin.py::te
st_delegate_subtasks_assigns_best_agent 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_html_documentation_extracts_sections 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_parse_markdown_documentation_respects_heading_levels 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_convert_docstrings_to_chunks_builds_expected_metadata 
tests/unit/application/documentation/test_documentation_fetcher_parsing.py::test
_version_key_supports_numeric_sorting_and_literals 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_prefers_vector_results 
tests/unit/application/documentation/test_ingestion_search_variance.py::test_sea
rch_documentation_falls_back_to_metadata_items 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_basic_creation 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_phase_context 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorError::
test_error_with_details 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_defaults 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_initialization_custom_config 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorInitial
ization::test_coordinator_dependencies_initialization 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseEx
ecution::test_start_cycle_from_manifest 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_depth_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_granularity 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_cost_benefit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_terminate_recursion_resource_limit 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorRecursi
on::test_should_not_terminate_recursion_good_metrics 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_register_micro_cycle_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorMicroCy
cles::test_invoke_micro_cycle_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_sync_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_invoke_sync_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_register_recovery_hook 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorHooks::
test_execute_recovery_hooks 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_set_manual_phase_override 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorPhaseMa
nagement::test_get_phase_quality_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_positive_int 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorUtility
Methods::test_sanitize_threshold 
tests/unit/application/edrr/coordinator/test_core.py::TestEDRRCoordinatorIntegra
tion::test_edrr_cycle_error_recovery 
tests/unit/application/edrr/test_coordinator.py::test_run_micro_cycles_stops_aft
er_threshold 
tests/unit/application/edrr/test_coordinator_core.py::test_maybe_auto_progress_r
espects_flag 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_success 
tests/unit/application/edrr/test_coordinator_reasoning.py::test_apply_dialectica
l_reasoning_consensus_failure 
tests/unit/application/edrr/test_edrr_coordinator_enhanced.py::test_enhanced_dec
ide_next_phase_respects_auto_phase 
tests/unit/application/edrr/test_edrr_phase_transitions_fast.py::test_collect_ph
ase_metrics_uses_stubbed_helpers 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
missing_memory_manager 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flushes_
on_success 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_handles_
errors 
tests/unit/application/edrr/test_persistence_module.py::test_safe_store_flush_fa
ilure_does_not_raise 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_norma
lizes_outputs 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_missi
ng_manager_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_safe_retrieve_witho
ut_support_returns_empty 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_stores_context 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_uses_deep_copy 
tests/unit/application/edrr/test_persistence_module.py::test_persist_context_sna
pshot_ignores_empty 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_enforces_dependencies 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ph
ase_updates_state 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_respects_quality_threshold 
tests/unit/application/edrr/test_phase_management_module.py::test_maybe_auto_pro
gress_invokes_progression 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_consumes_manual_override 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_requires_auto_transitions 
tests/unit/application/edrr/test_phase_management_module.py::test_decide_next_ph
ase_returns_none_for_final_phase 
tests/unit/application/edrr/test_phase_management_module.py::test_progress_to_ne
xt_phase_rejects_final_phase 
tests/unit/application/edrr/test_reasoning_loop_retries.py::test_reasoning_loop_
retries_on_transient_error 
tests/unit/application/edrr/test_recursion_termination.py::test_micro_cycle_resp
ects_depth_bounds 
tests/unit/application/edrr/test_recursion_termination.py::test_complexity_thres
hold_triggers_termination 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_sp
rint_planning_phase_constant 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_basic 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_empty 
tests/unit/application/edrr/test_sprint_planning.py::TestSprintPlanning::test_ma
p_requirements_to_plan_partial 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_sprint_retrospective_phase_constant 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_basic 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_empty 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_none 
tests/unit/application/edrr/test_sprint_retrospective.py::TestSprintRetrospectiv
e::test_map_retrospective_to_summary_partial 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_positive_in
t_handles_out_of_range 
tests/unit/application/edrr/test_threshold_helpers.py::test_sanitize_threshold_c
lamps_invalid_values 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_respects_config 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_phase_quality_th
reshold_returns_none_when_missing 
tests/unit/application/edrr/test_threshold_helpers.py::test_get_micro_cycle_conf
ig_sanitizes_values 
tests/unit/application/ingestion/test_ingestion_pure.py::test_is_artifact_change
d_respects_metadata_differences 
tests/unit/application/ingestion/test_ingestion_pure.py::test_identify_improveme
nt_areas_flags_missing_manifest_information 
tests/unit/application/ingestion/test_ingestion_pure.py::test_generate_recommend
ations_reflects_project_context 
tests/unit/application/ingestion/test_phases.py::test_run_expand_phase_populates
_artifacts 
tests/unit/application/ingestion/test_phases.py::test_run_differentiate_phase_us
es_structure 
tests/unit/application/llm/test_import_without_openai.py::test_import_openai_pro
vider_without_openai_succeeds 
tests/unit/application/llm/test_import_without_openai.py::test_openai_provider_r
equires_api_key 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_succ
eeds_when_sync_api_lists_models 
tests/unit/application/llm/test_lmstudio_health_check.py::test_health_check_boun
ded_retry_and_returns_false_on_failure 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_ti
meout_raises_connection_error_quickly 
tests/unit/application/llm/test_lmstudio_offline_resilience.py::test_generate_in
valid_response_raises_model_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_lazy_import 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_ensure_caching 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProxy::test_pr
oxy_getattr_delegation 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestAttrForwarder::test_at
tr_forwarder_call 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_initialization 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_getattr 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_list_downloaded_models 
tests/unit/application/llm/test_lmstudio_provider.py::TestNamespaceForwarder::te
st_namespace_forwarder_configure_default_client 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestRequireLMStudio::test_
require_lmstudio_import_error 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_default_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_initialization_custom_config 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_complete_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_embed_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_success 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_health_check_failure 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_get_client_method 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_model_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioProvider::test
_provider_available_models_property 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_inheritance 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_connection_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestLMStudioExceptions::te
st_model_error_message 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_exists 
tests/unit/application/llm/test_lmstudio_provider.py::TestModuleLevelProxy::test
_module_level_proxy_has_expected_attributes 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_prefixes_with_offline 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
enerate_with_context_concatenates 
tests/unit/application/llm/test_offline_provider.py::TestOfflineProvider::test_g
et_embedding_is_deterministic 
tests/unit/application/llm/test_openai_env_key_mock.py::test_openai_provider_use
s_mocked_env_key_without_network 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_succ
ess_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_time
out_retries_and_raises_connection_error 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_stre
am_yields_tokens_offline 
tests/unit/application/llm/test_openai_offline_resilience.py::test_generate_inva
lid_response_raises_model_error 
tests/unit/application/llm/test_provider_factory.py::test_default_selection_is_d
eterministic 
tests/unit/application/llm/test_provider_factory.py::test_case_insensitive_selec
tion 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_not_selected_when_flag_false 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_lmstud
io_selected_when_flag_true 
tests/unit/application/llm/test_provider_factory_lmstudio_gating.py::test_offlin
e_killswitch_overrides_explicit_selection 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_off
line 
tests/unit/application/llm/test_provider_selection.py::test_get_llm_provider_def
ault 
tests/unit/application/memory/test_chromadb_store.py::test_store_and_retrieve_wi
th_fallback 
tests/unit/application/memory/test_chromadb_store_typed.py::test_search_normaliz
es_serialized_rows 
tests/unit/application/memory/test_chromadb_store_typed.py::test_fallback_retrie
ve_uses_serialization_helpers 
tests/unit/application/memory/test_circuit_breaker.py::test_circuit_breaker_open
s_after_failures 
tests/unit/application/memory/test_circuit_breaker.py::test_registry_returns_sam
e_instance 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_without_vector_extension_falls_back 
tests/unit/application/memory/test_duckdb_store_schema_flags.py::test_initialize
_schema_configures_hnsw_when_enabled 
tests/unit/application/memory/test_error_logger.py::test_log_error_enforces_max_
errors 
tests/unit/application/memory/test_error_logger.py::test_log_error_accepts_neste
d_context 
tests/unit/application/memory/test_error_logger.py::test_persist_errors_respects
_toggle 
tests/unit/application/memory/test_error_logger.py::test_get_recent_errors_and_s
ummary 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_learn_from_code_execution 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_enhance_code_understanding 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_semantic_robustness_testing 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_get_learning_statistics 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_validate_against_research_benchmarks 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionLearningIntegration::test_export_import_learning_state 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_initialization 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_analyze_code_structure 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_extract_execution_patterns 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_create_memetic_units_from_trajectories 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_get_execution_insights 
tests/unit/application/memory/test_execution_learning_integration.py::TestExecut
ionTrajectoryCollector::test_validate_trajectory_quality 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_extract_semantic_components 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_analyze_behavioral_intent 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_detect_semantic_equivalence 
tests/unit/application/memory/test_execution_learning_integration.py::TestSemant
icUnderstandingEngine::test_predict_execution_behavior 
tests/unit/application/memory/test_faiss_store.py::test_store_and_retrieve_round
_trip_preserves_metadata 
tests/unit/application/memory/test_faiss_store.py::test_transaction_commit_persi
sts_changes 
tests/unit/application/memory/test_faiss_store.py::test_transaction_rollback_res
tores_snapshot 
tests/unit/application/memory/test_faiss_store.py::test_similarity_search_and_st
ats_ignore_deleted_vectors 
tests/unit/application/memory/test_fast_in_memory_components.py::test_graph_memo
ry_adapter_in_memory_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_enhanced_g
raph_memory_adapter_edrr_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_man
ager_sync_hooks_fire 
tests/unit/application/memory/test_fast_in_memory_components.py::test_dummy_tran
saction_context_commit_and_rollback 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sys
tem_adapter_in_memory_components 
tests/unit/application/memory/test_fast_in_memory_components.py::test_fallback_s
tore_falls_back_on_failure 
tests/unit/application/memory/test_fast_in_memory_components.py::test_json_file_
store_round_trip 
tests/unit/application/memory/test_fast_in_memory_components.py::test_memory_sna
pshot_save_and_load 
tests/unit/application/memory/test_graph_memory_adapter.py::TestGraphMemoryAdapt
er::test_traverse_graph_depth_and_missing_nodes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_begin_tran
saction_tracks_and_cleans_up 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_commit_tra
nsaction_persists_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_rollback_t
ransaction_discards_explicit_changes 
tests/unit/application/memory/test_lmdb_store.py::TestLMDBStore::test_get_all_it
ems_returns_everything 
tests/unit/application/memory/test_memory_manager.py::TestRouteQuery::test_route
_query_normalizes_context_mapping 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_regist
er_and_notify_sync_hook_succeeds 
tests/unit/application/memory/test_memory_manager.py::TestSyncHooks::test_sync_h
ook_errors_are_logged 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
disabled_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_chromadb_
enabled_uses_adapter_and_store 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_various_backends 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_kuzu_init
ialization_and_fallback 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_lmdb_miss
ing_falls_back_to_memory 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_initializ
e_memory_system_branches_execution 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_cache_and
_transaction_workflow 
tests/unit/application/memory/test_memory_system_adapter_unit.py::test_transacti
on_wrappers_raise_without_support 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_round_trip_preserves_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_recor
d_from_row_handles_stringified_metadata 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_query
_results_from_rows_shapes_records 
tests/unit/application/memory/test_metadata_serialization_helpers.py::test_build
_memory_record_coerces_legacy_mapping 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_process_advanced_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_analyze_and_segment_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_multi_hop_reasoning 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execute_hybrid_llm_processing 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_apply_metacognitive_enhancement 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_contextual_prompts 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_integrate_and_validate_results 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_get_system_status 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_benchmark_against_research 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_export_import_system_state 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_validate_system_integrity 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_optimize_system_performance 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_memory_graph_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_execution_learning_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestPhase3Integ
rationSystem::test_automata_metacognitive_integration_check 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_process_complex_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_parse_query_intent 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_extract_relationships 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_calculate_required_hops 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_resolve_entities 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_plan_multi_hop_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestEnhancedGra
phRAGQueryEngine::test_execute_semantic_traversal 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_synthesize_automata_from_exploration 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_generate_task_segmentation 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_validate_automata_quality 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_create_memetic_units_from_automata 
tests/unit/application/memory/test_phase3_integration_system.py::TestAutomataSyn
thesisEngine::test_get_task_segmentation_for_query 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_process_complex_reasoning_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_optimal_provider_for_task 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_benchmark_hybrid_vs_individual 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_add_provider 
tests/unit/application/memory/test_phase3_integration_system.py::TestHybridLLMAr
chitecture::test_get_architecture_statistics 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_start_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_record_verbalization 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_end_think_aloud_session 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_get_metacognitive_insights 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_apply_metacognitive_improvements 
tests/unit/application/memory/test_phase3_integration_system.py::TestMetacogniti
veTrainingSystem::test_generate_self_monitoring_report 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_initialization 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_engineer_contextual_prompt 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_behavioral_directive 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_add_environmental_constraint 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_get_prompt_performance_analytics 
tests/unit/application/memory/test_phase3_integration_system.py::TestContextualP
romptingSystem::test_create_agent_specific_prompt 
tests/unit/application/memory/test_query_router.py::test_direct_query_and_vector
_branch 
tests/unit/application/memory/test_query_router.py::test_cross_store_query_group
s_results 
tests/unit/application/memory/test_query_router.py::test_cascading_and_federated
tests/unit/application/memory/test_query_router.py::test_context_aware_and_route
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_returns_existing_identifier 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_begin_tran
saction_generates_uuid 
tests/unit/application/memory/test_rdflib_store_transactions.py::test_transactio
n_methods_are_noops 
tests/unit/application/memory/test_search_memory_fallback.py::test_search_memory
_fallback_without_vector_adapter_returns_results 
tests/unit/application/memory/test_sync_manager_transactions.py::test_queue_upda
te_enqueues_memory_record 
tests/unit/application/memory/test_sync_manager_transactions.py::test_transactio
n_rollback_uses_normalized_snapshots 
tests/unit/application/memory/test_tiered_cache_termination.py::test_eviction_lo
op_terminates 
tests/unit/application/memory/test_tiered_cache_termination.py::test_preserves_t
yped_values 
tests/unit/application/memory/test_tinydb_adapter_bytes_tuple.py::test_tinydb_ad
apter_serializes_bytes_and_tuple 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_default_
provider_registration 
tests/unit/application/memory/test_vector_memory_adapter_extra.py::test_optional
_provider_guard 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_edrr_coo
rdinator_delegates_to_helper 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_returns_result 
tests/unit/application/orchestration/test_dialectical_reasoner.py::test_dialecti
cal_reasoner_logs_consensus_failure 
tests/unit/application/promises/test_agent_create_promise.py::test_create_promis
e_sets_metadata_and_parent_relationship 
tests/unit/application/promises/test_interface_not_implemented.py::test_promise_
interface_id_not_implemented 
tests/unit/application/promises/test_interface_pure.py::test_basic_promise_metad
ata_round_trip 
tests/unit/application/promises/test_interface_pure.py::test_then_on_fulfilled_p
romise_invokes_callback_immediately 
tests/unit/application/promises/test_interface_pure.py::test_catch_on_rejected_p
romise_yields_handler_result 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_success_rate_and_a
verage_feedback_are_computed_from_state 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_performance_score_
combines_success_and_feedback 
tests/unit/application/prompts/test_auto_tuning_pure.py::test_round_trip_seriali
sation_preserves_variant_fields 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_reaches_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_logs_consensus_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_failure_stores_retrospect 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_receives_consensus 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluatio
n_hook_runs_on_failure 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_non_text_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_evaluate_
change_invalid_response_errors 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_assess_im
pact_stores_with_phase 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_parses_counterarguments 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_generate_
arguments_handles_missing_counterargument 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_positive_path 
tests/unit/application/requirements/test_dialectical_reasoner.py::test_wsde_team
_hook_negative_path 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_argument_parsing_consensus_failure_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_parsing_payloads.p
y::test_assess_impact_recommendations_payload_preserved 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_requirements_collects_dependencies 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_iden
tify_affected_components_merges_sources 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_asse
ss_risk_level_accounts_for_priority 
tests/unit/application/requirements/test_dialectical_reasoner_pure.py::test_esti
mate_effort_scales_with_affected_entities 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_writes_json 
tests/unit/application/requirements/test_interactions.py::test_requirements_coll
ector_cancelled 
tests/unit/application/requirements/test_interactions.py::test_gather_requiremen
ts_supports_backtracking 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_updat
e_requirement_uses_typed_dto_and_dialectical_hooks 
tests/unit/application/requirements/test_requirement_service_dtos.py::test_delet
e_requirement_emits_retrospect_phase 
tests/unit/application/requirements/test_wizard.py::test_priority_and_constraint
s_persist_after_navigation 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_each_step 
tests/unit/application/requirements/test_wizard.py::test_requirements_wizard_log
s_exc_info 
tests/unit/application/sprint/test_planning.py::test_map_requirements_to_plan_ex
tracts_fields 
tests/unit/application/test_documentation_fetcher.py::test_download_success_retu
rns_manifest 
tests/unit/application/test_documentation_fetcher.py::test_download_failure_retu
rns_false_manifest 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_initialization_with_memory_port 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_unit 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_behavior 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_by_category_nonexistent 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_collect_tests_all_categories 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_get_tests_with_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_caching_functionality 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_force_refresh_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_info 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_clear_cache 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_memory_integration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_is_valid_test_file 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_contains_test_code 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_test_has_marker 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_analyze_markers 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_operations 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_expiration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_store_collection_results 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_nonexistent_directory 
tests/unit/application/testing/test_enhanced_test_collector.py::TestEnhancedTest
Collector::test_cache_file_corruption 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestCollecti
onResult::test_as_dict 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestTestInfo::te
st_with_docstring 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_directory_creation 
tests/unit/application/testing/test_enhanced_test_collector.py::TestCacheOperati
ons::test_cache_ttl_configuration 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_unicode_decode_error 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_os_error_handling 
tests/unit/application/testing/test_enhanced_test_collector.py::TestErrorHandlin
g::test_memory_storage_failure 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_with_extra 
tests/unit/application/utils/test_extras_helper.py::test_suggest_install_message
_without_extra 
tests/unit/application/utils/test_extras_helper.py::test_require_optional_packag
e_wraps_importerror 
tests/unit/behavior/test_alignment_metrics_steps_unit.py::test_metrics_fail_patc
hes_calculate 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_code 
tests/unit/behavior/test_analyze_commands_steps_unit.py::test_run_command_inspec
t_config_update tests/unit/cli/test_cli_entry.py::test_cli_entry_invokes_run_cli
tests/unit/cli/test_cli_error_handling.py::test_main_handles_run_cli_errors 
tests/unit/cli/test_cli_help.py::test_cli_help_exits_zero_and_shows_summary 
tests/unit/cli/test_command_module_loading.py::test_command_modules_register_com
mands_and_build_app 
tests/unit/cli/test_command_registry.py::test_build_app_registers_commands_from_
registry 
tests/unit/cli/test_command_registry.py::test_enable_feature_not_top_level 
tests/unit/cli/test_completion_progress.py::test_completion_cmd_outputs_script_a
nd_progress 
tests/unit/cli/test_entry_points_help.py::test_devsynth_help_module_invocation 
tests/unit/cli/test_entry_points_help.py::test_console_scripts_declared 
tests/unit/cli/test_entry_points_help.py::test_mvuu_dashboard_help_via_module 
tests/unit/cli/test_help_examples.py::test_get_command_help_includes_examples 
tests/unit/cli/test_help_examples.py::test_get_command_help_unknown_command 
tests/unit/cli/test_import_gating.py::test_import_devsynth_does_not_import_heavy
_optionals 
tests/unit/cli/test_import_gating.py::test_cli_entrypoint_lazy_imports 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_list 
tests/unit/cli/test_init_features_option.py::test_init_cmd_accepts_feature_json 
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv0]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv1]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv2]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv3]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv4]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv5]
tests/unit/cli/test_key_commands_help.py::test_key_commands_help_succeeds[argv6]
tests/unit/cli/test_logging_flags.py::test_global_debug_flag_sets_log_level_debu
g 
tests/unit/cli/test_logging_flags.py::test_env_debug_sets_log_level_when_no_flag
tests/unit/cli/test_logging_flags.py::test_log_level_option_overrides_env_debug 
tests/unit/cli/test_mvu_commands.py::test_mvu_help_lists_subcommands 
tests/unit/cli/test_mvu_commands.py::test_mvu_init_creates_config_and_matches_sc
hema 
tests/unit/cli/test_mvuu_command_registration.py::test_mvuu_dashboard_command_re
gistered 
tests/unit/cli/test_mvuu_dashboard_smoke.py::test_mvuu_dashboard_module_no_run_a
voids_subprocess 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_generat
es_signed_telemetry 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_uses_li
ve_connectors 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_falls_b
ack_on_connector_error 
tests/unit/cli/test_mvuu_dashboard_telemetry.py::test_mvuu_dashboard_cli_force_l
ocal_mode 
tests/unit/cli/test_run_tests_regression.py::test_cli_run_tests_unit_fast_comple
tes_with_non_zero_tests 
tests/unit/cli/test_version.py::test_cli_version_option_prints_version_and_exits
_zero 
tests/unit/config/test_config_llm_env.py::test_configure_llm_settings_reads_env 
tests/unit/config/test_exception_handling.py::test_is_devsynth_managed_project_i
nvalid_toml_returns_false 
tests/unit/config/test_exception_handling.py::test_unified_config_exists_returns
_false_on_invalid_toml 
tests/unit/config/test_exception_handling.py::test_load_config_malformed_toml_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_load_config_invalid_values_ra
ises_configuration_error 
tests/unit/config/test_exception_handling.py::test_set_default_memory_dir_handle
s_configuration_error 
tests/unit/config/test_feature_flag_defaults.py::test_feature_flags_default_off 
tests/unit/config/test_feature_flag_defaults.py::test_can_enable_known_feature_f
lag 
tests/unit/config/test_provider_env.py::test_parse_bool_truthy_and_falsy_cases 
tests/unit/config/test_provider_env.py::test_from_env_defaults_and_with_test_def
aults_sets_stub_and_offline 
tests/unit/config/test_provider_env.py::test_apply_to_env_respects_existing_lmst
udio_flag 
tests/unit/config/test_provider_env.py::test_as_dict_roundtrip_and_types 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_sets_e
xpected_vars 
tests/unit/config/test_provider_env_apply_and_parse.py::test_apply_to_env_does_n
ot_override_explicit_lmstudio_flag 
tests/unit/config/test_provider_env_apply_and_parse.py::test_from_env_reads_curr
ent_environment 
tests/unit/config/test_provider_env_behavior.py::test_from_env_defaults_when_uns
et 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_overrid
es_to_safe_when_unset 
tests/unit/config/test_provider_env_behavior.py::test_with_test_defaults_respect
s_explicit_provider 
tests/unit/config/test_provider_env_behavior.py::test_apply_to_env_and_as_dict_r
oundtrip 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_parses_
true_and_false_variants 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_from_env_unrecog
nized_values_fall_back_to_defaults 
tests/unit/config/test_provider_env_bool_parsing_edges.py::test_as_dict_reflects
_values_and_with_test_defaults_sets_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_sets_offline_stub_and_openai_key 
tests/unit/config/test_provider_env_with_test_defaults.py::test_with_test_defaul
ts_respects_explicit_provider 
tests/unit/config/test_unified_loader.py::test_loads_from_pyproject_succeeds 
tests/unit/core/mvu/test_api.py::test_get_by_trace_id 
tests/unit/core/mvu/test_api.py::test_get_by_affected_path 
tests/unit/core/mvu/test_atomic_rewrite.py::test_cluster_commits_by_file 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_valid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_block 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_bad_traceid 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_issue 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_mvuu_false 
tests/unit/core/mvu/test_linter.py::test_lint_commit_message_missing_mvuu 
tests/unit/core/mvu/test_mvuu_schema_validation.py::test_mvuu_example_conforms_t
o_schema tests/unit/core/mvu/test_report.py::test_generate_report_markdown 
tests/unit/core/mvu/test_report.py::test_generate_report_html 
tests/unit/core/mvu/test_storage.py::test_format_mvuu_footer_contains_json 
tests/unit/core/mvu/test_storage.py::test_append_mvuu_footer_appends_block 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_accepts_vali
d 
tests/unit/core/mvu/test_validator.py::test_validate_commit_message_rejects_bad_
header 
tests/unit/core/mvu/test_validator.py::test_validate_affected_files_reports_mism
atches 
tests/unit/core/test_config_loader.py::test_core_config_normalizes_mvuu_invalid_
entries 
tests/unit/core/test_config_loader_json_types.py::test_load_config_supports_nest
ed_json_resources 
tests/unit/core/test_config_loader_json_types.py::test_environment_override_pres
erves_resources 
tests/unit/core/test_config_loader_json_types.py::test_core_config_rejects_exces
sively_deep_resources 
tests/unit/core/test_config_loader_mvu.py::test_load_config_merges_mvuu_settings
tests/unit/core/test_config_loader_optional_deps.py::test_load_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_dump_toml_mapping_requ
ires_optional_dependency 
tests/unit/core/test_config_loader_optional_deps.py::test_save_global_config_han
dles_missing_yaml 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw2-expected2] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[raw3-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_issue_provider_con
fig_filters_payloads[not-a-mapping-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[payload2-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_issues_only_a
ccepts_known_providers[not-a-mapping-None] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload0-expected0] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload1-expected1] 
tests/unit/core/test_config_loader_validation.py::test_coerce_mvuu_config_collap
ses_invalid_sections[payload2-expected2] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories0-True-expected0] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories1-False-expected1] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[directories2-False-expected2] 
tests/unit/core/test_config_loader_validation.py::test_directory_map_validation_
and_coercion[not-a-mapping-False-expected3] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[15-True] 
tests/unit/core/test_config_loader_validation.py::test_coerce_json_object_enforc
es_depth_limit[16-False] 
tests/unit/core/test_config_loader_validation.py::test_load_yaml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_load_toml_returns_coerced
_core_config_data 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[single_override] 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[multiple_fields] 
tests/unit/core/test_config_loader_validation.py::test_parse_env_extracts_known_
overrides[ignores_irrelevant_keys] 
tests/unit/core/test_config_loader_validation.py::test_load_config_merges_source
s_without_mutating_resources 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[github_only] 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[jira_only] 
tests/unit/core/test_config_loader_validation.py::test_load_config_normalizes_mv
uu_with_env_overrides[both_providers] 
tests/unit/core/test_deterministic_fixtures.py::test_deterministic_seed_sets_env
_and_random_sequence 
tests/unit/core/test_deterministic_fixtures.py::test_mock_datetime_fixture_freez
es_time 
tests/unit/core/test_deterministic_fixtures.py::test_mock_uuid_fixture_returns_f
ixed_uuid tests/unit/core/test_mvu.py::test_schema_has_required_fields 
tests/unit/core/test_mvu.py::test_end_to_end_mvu_flow 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_rejects_in
valid_environment 
tests/unit/deployment/test_bootstrap_script.py::test_bootstrap_script_requires_d
ocker 
tests/unit/deployment/test_bootstrap_script.py::test_install_dev_installs_task 
tests/unit/deployment/test_deployment_scripts.py::test_bootstrap_script_exists 
tests/unit/deployment/test_deployment_scripts.py::test_health_check_script_exist
s 
tests/unit/deployment/test_enforcement.py::test_shell_scripts_enforce_non_root_a
nd_env_validation 
tests/unit/deployment/test_enforcement.py::test_docker_compose_enforces_user_and
_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_repor
ts_healthy 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_root_user 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_env_file 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_requi
res_strict_permissions 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_rejec
ts_invalid_url 
tests/unit/deployment/test_health_check_smoke.py::test_health_check_script_fails
_on_unhealthy_endpoint 
tests/unit/deployment/test_scripts_dir.py::test_scripts_bootstrap_exists 
tests/unit/deployment/test_scripts_dir.py::test_scripts_health_check_exists 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_noo
p_without_flag 
tests/unit/deployment/test_security_hardening.py::test_require_non_root_user_rai
ses_for_root 
tests/unit/deployment/test_security_hardening.py::test_check_required_env_vars 
tests/unit/deployment/test_security_hardening.py::test_apply_secure_umask 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_invokes_he
lpers 
tests/unit/deployment/test_security_hardening.py::test_harden_runtime_raises_whe
n_env_missing 
tests/unit/devsynth/test_consensus.py::test_build_consensus_majority 
tests/unit/devsynth/test_consensus.py::test_build_consensus_no_consensus 
tests/unit/devsynth/test_consensus.py::test_build_consensus_tracks_unique_dissen
ting_options 
tests/unit/devsynth/test_consensus.py::test_build_consensus_invalid_threshold 
tests/unit/devsynth/test_consensus.py::test_build_consensus_empty_votes 
tests/unit/devsynth/test_fallback_reliability.py::test_named_condition_callbacks
_record_metrics 
tests/unit/devsynth/test_fallback_reliability.py::test_circuit_breaker_open_hook
_and_metrics 
tests/unit/devsynth/test_logger.py::test_log_exception_object_normalized 
tests/unit/devsynth/test_logger.py::test_log_true_uses_current_exception 
tests/unit/devsynth/test_logger.py::test_log_invalid_exc_info_dropped 
tests/unit/devsynth/test_metrics.py::test_memory_metrics_increment_and_reset 
tests/unit/devsynth/test_metrics.py::test_provider_and_retry_metrics 
tests/unit/devsynth/test_metrics.py::test_dashboard_metrics 
tests/unit/devsynth/test_metrics.py::test_inc_memory_unhashable_raises_type_erro
r tests/unit/devsynth/test_simple_addition.py::test_add_returns_sum 
tests/unit/devsynth/test_simple_addition.py::test_add_raises_type_error_on_non_n
umeric 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_tests_but_
not_docs 
tests/unit/docs/test_dialectical_audit.py::test_fails_when_feature_in_docs_but_n
ot_tests 
tests/unit/domain/interfaces/test_interfaces.py::test_cli_interface_raises_not_i
mplemented 
tests/unit/domain/interfaces/test_interfaces.py::test_file_analysis_result_raise
s_not_implemented 
tests/unit/domain/interfaces/test_interfaces.py::test_onnx_runtime_raises_not_im
plemented 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_none_values 
tests/unit/domain/models/test_agent_coverage.py::test_agent_config_post_init_wit
h_existing_values 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_initial
ization 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticMetadata::test_seriali
zation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_creation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_content_has
h_generation 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_serializati
on_roundtrip 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_link_manage
ment 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_salience_up
date 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_lifecycle_m
anagement 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticUnit::test_cognitive_t
ype_properties 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_creati
on 
tests/unit/domain/models/test_memetic_unit.py::TestMemeticLink::test_link_serial
ization 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_defa
ult_standard 
tests/unit/domain/models/test_project.py::test_project_model_structure_type_mono
repo 
tests/unit/domain/models/test_project.py::test_artifact_metadata_defaults_to_sep
arate_dicts 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_init
ialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_str_
representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestArtifact::test_artifact_repr
_representation_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_project_m
odel_initialization_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_structure_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_sta
ndard_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_build_mon
orepo_model_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
act_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_artif
acts_by_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_get_relat
ed_artifacts_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_determine
_artifact_type_succeeds 
tests/unit/domain/models/test_project_model.py::TestProjectModel::test_to_dict_s
ucceeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_add_agent_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_dialectical_hook_invok
ed_on_add_solution_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_rotate_primus_succeeds
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_primus_empty_team_
succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_get_agent_by_role_succ
eeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_assign_roles_with_rota
tion_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDETeam::test_apply_dialectical_reas
oning_with_knowledge_graph_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_succeeds 
tests/unit/domain/models/test_wsde.py::TestWSDE::test_initialization_with_metada
ta_succeeds 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_ws
de_dataclass_initialises_timestamps 
tests/unit/domain/models/test_wsde_base_methods.py::TestWSDEBaseMethods::test_te
am_post_init_restores_missing_attributes 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_inserts_validation 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_credential
s_noop_when_already_secure 
tests/unit/domain/models/test_wsde_code_improvements.py::test_improve_error_hand
ling_wraps_body 
tests/unit/domain/models/test_wsde_decision_making.py::test_calculate_idea_simil
arity_overlap 
tests/unit/domain/models/test_wsde_decision_making.py::test_evaluate_options_ran
ks_by_weighted_score 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_similar_entries 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_handles_agent_failures 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_limits_count 
tests/unit/domain/models/test_wsde_decision_making.py::test_generate_diverse_ide
as_filters_duplicates_with_strict_threshold 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_antithe
sis_returns_typed_draft 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_categorize_criti
ques_by_domain_returns_tuples 
tests/unit/domain/models/test_wsde_dialectical_helpers.py::test_generate_synthes
is_returns_resolution_plan 
tests/unit/domain/models/test_wsde_dialectical_typing.py::test_dialectical_seque
nce_round_trip 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_apply_dialectic
al_reasoning_invokes_hooks_and_memory 
tests/unit/domain/models/test_wsde_dialectical_workflow.py::test_dialectical_tas
k_serialization_round_trip 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_dynamic_role_reassignment_selects_expert_primus_succeeds 
tests/unit/domain/models/test_wsde_dynamic_workflows.py::TestWSDERoleReassignmen
t::test_build_consensus_multiple_solutions_succeeds 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_categorize_crit
iques_by_domain_groups_terms 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_identify_domain
_conflicts_finds_performance_security 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_generates_synthesis 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_requires_solution 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_combines_solutions 
tests/unit/domain/models/test_wsde_enhanced_dialectical.py::test_apply_enhanced_
dialectical_reasoning_multi_requires_solutions 
tests/unit/domain/models/test_wsde_knowledge.py::test_get_task_id_uses_existing_
id 
tests/unit/domain/models/test_wsde_knowledge.py::test_identify_relevant_knowledg
e_matches_keywords 
tests/unit/domain/models/test_wsde_knowledge.py::test_knowledge_graph_insights_p
arses_payload 
tests/unit/domain/models/test_wsde_knowledge.py::test_integrate_knowledge_builds
_summary 
tests/unit/domain/models/test_wsde_knowledge.py::test_generate_improvement_sugge
stions_deduplicates_entries 
tests/unit/domain/models/test_wsde_roles_personas.py::test_enumerate_research_pe
rsonas_includes_overlays 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Synthesizer] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Contrarian] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Fact Checker] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Planner] 
tests/unit/domain/models/test_wsde_roles_personas.py::test_persona_payload_expos
es_overlay_metadata[Moderator] 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_detects_issue 
tests/unit/domain/models/test_wsde_security_checks.py::test_check_security_best_
practices_accepts_clean_code 
tests/unit/domain/models/test_wsde_security_checks.py::test_balance_security_and
_performance_idempotent 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_s
cores_requirements 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_analyze_solution_h
ighlights_gaps 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_identifies_best_solution 
tests/unit/domain/models/test_wsde_solution_analysis.py::test_generate_comparati
ve_analysis_handles_empty 
tests/unit/domain/models/test_wsde_strategies.py::test_weighted_voting_prefers_d
omain_expertise 
tests/unit/domain/models/test_wsde_strategies.py::test_role_assignment_uses_expe
rtise_scores 
tests/unit/domain/models/test_wsde_strategies.py::test_multidisciplinary_analysi
s_structures_results 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_add_agent_succeed
s 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_rotate_primus_suc
ceeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_succee
ds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_get_primus_empty_
team_succeeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_assign_roles_succ
eeds 
tests/unit/domain/models/test_wsde_team.py::TestWSDETeam::test_analyze_trade_off
s_detects_conflicts_succeeds 
tests/unit/domain/models/test_wsde_utils.py::test_send_message_invokes_protocol 
tests/unit/domain/models/test_wsde_utils.py::test_broadcast_message_excludes_sen
der tests/unit/domain/models/test_wsde_utils.py::test_get_messages_uses_protocol
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_creates_cy
cle 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_collects_f
eedback 
tests/unit/domain/models/test_wsde_utils.py::test_conduct_peer_review_handles_mi
ssing_peer_review 
tests/unit/domain/models/test_wsde_utils.py::test_add_solution_appends_and_trigg
ers_hooks 
tests/unit/domain/models/test_wsde_utils.py::test_request_peer_review_logs_warni
ng_on_failure 
tests/unit/domain/models/test_wsde_voting_logic.py::test_deterministic_voting_wi
th_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_determi
nistic_with_seed 
tests/unit/domain/models/test_wsde_voting_logic.py::test_weighted_voting_tie_is_
fair 
tests/unit/domain/models/test_wsde_voting_logic.py::test_handle_tied_vote_produc
es_consensus_result 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_analyzer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_noop_transformer 
tests/unit/domain/test_code_analysis_interfaces.py::TestCodeAnalysisInterfaces::
test_simple_file_analysis 
tests/unit/domain/test_wsde_expertise_score.py::test_calculate_expertise_score_m
ultiple_matches 
tests/unit/domain/test_wsde_facade.py::test_summarize_consensus_result_outputs_e
xpected_sections 
tests/unit/domain/test_wsde_facade.py::test_summarize_voting_result_reports_winn
er_and_counts 
tests/unit/domain/test_wsde_facade_roles.py::test_select_primus_updates_index_an
d_role 
tests/unit/domain/test_wsde_facade_roles.py::test_dynamic_role_reassignment_rota
tes_primus 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_peer_review_cross_stor
e_sync_succeeds 
tests/unit/domain/test_wsde_peer_review_workflow.py::test_mvu_helpers_cover_modu
le 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_initial_selection_prefe
rs_unused_agent_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_documentation_tasks_pic
k_documentation_experts_succeeds 
tests/unit/domain/test_wsde_phase_role_rotation.py::test_assign_roles_for_phase_
rotates_after_all_primus_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_first_time_selection_prior
itizes_unused_agents_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_resets_after_all_
have_served_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_current_primus_considered_
in_selection_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_documentation_tasks_prefer
_doc_experts_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_nested_task_metadata_is_fl
attened_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_rotation_when_all_agents_u
sed_resets_flags_succeeds 
tests/unit/domain/test_wsde_primus_selection.py::test_select_primus_by_expertise
_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_by_expertise_prefers_doc
umentation_agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_tie_triggers
_consensus_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_weighted_vot
ing_succeeds 
tests/unit/domain/test_wsde_team.py::test_build_consensus_multiple_and_single_su
cceeds 
tests/unit/domain/test_wsde_team.py::test_documentation_task_selects_unused_doc_
agent_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotation_resets_after_all_have_served_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_prefers_doc_expertise_vi
a_config_succeeds 
tests/unit/domain/test_wsde_team.py::test_rotate_primus_resets_usage_flags_and_r
ole_map_succeeds 
tests/unit/domain/test_wsde_team.py::test_multiple_task_cycles_reset_primus_flag
s_succeeds 
tests/unit/domain/test_wsde_team.py::test_vote_on_critical_decision_coverage_suc
ceeds tests/unit/domain/test_wsde_team.py::test_force_wsde_coverage_succeeds 
tests/unit/domain/test_wsde_team.py::test_expertise_selection_and_flag_rotation_
succeeds 
tests/unit/domain/test_wsde_team.py::test_select_primus_coverage_succeeds 
tests/unit/domain/test_wsde_voting_logic.py::test_majority_voting_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_handle_tied_vote_primus_breaks
tests/unit/domain/test_wsde_voting_logic.py::test_weighted_voting_tie_primus_res
olution 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_majo
rity 
tests/unit/domain/test_wsde_voting_logic.py::test_vote_on_critical_decision_weig
hted 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_majority_voting_no_tie 
tests/unit/domain/test_wsde_voting_logic.py::test_consensus_vote 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_simple 
tests/unit/domain/test_wsde_voting_logic.py::test_build_consensus_rounds 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_primus_t
ie 
tests/unit/domain/test_wsde_voting_logic.py::test_apply_weighted_voting_random 
tests/unit/fallback/test_retry_counts.py::test_retry_count_metrics 
tests/unit/fallback/test_retry_counts.py::test_retry_only_network_errors 
tests/unit/fallback/test_retry_predicates.py::test_retry_predicate_triggers_retr
y 
tests/unit/fallback/test_retry_predicates.py::test_integer_predicate_records_met
rics 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_add
_agent_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_agent_type_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_to_team_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_missing_parameters_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_no_agents_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_type_not_found_succeeds 
tests/unit/general/test_agent_coordinator.py::TestAgentCoordinatorImpl::test_del
egate_task_agent_execution_error_raises_error 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_type_enum_s
ucceeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_init
ialization_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_agent_config_with
_parameters_succeeds 
tests/unit/general/test_agent_models.py::TestAgentModels::test_mvp_capabilities_
succeeds 
tests/unit/general/test_agent_system.py::test_agent_state_keys_has_expected 
tests/unit/general/test_agent_system.py::test_process_input_node_success_is_vali
d 
tests/unit/general/test_agent_system.py::test_process_input_node_empty_input_suc
ceeds 
tests/unit/general/test_agent_system.py::test_process_input_node_adds_tool_list 
tests/unit/general/test_agent_system.py::test_llm_call_node_success_succeeds 
tests/unit/general/test_agent_system.py::test_llm_call_node_llm_failure_fails 
tests/unit/general/test_agent_system.py::test_llm_call_node_skip_on_prior_error_
raises_error 
tests/unit/general/test_agent_system.py::test_llm_call_node_missing_processed_in
put_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_success_is_valid
tests/unit/general/test_agent_system.py::test_parse_output_node_missing_llm_resp
onse_succeeds 
tests/unit/general/test_agent_system.py::test_parse_output_node_skip_on_prior_er
ror_raises_error 
tests/unit/general/test_agent_system.py::test_base_agent_graph_compiles_raises_e
rror 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
connection_error_raises_error 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
generate_with_context_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
get_embedding_succeeds 
tests/unit/general/test_anthropic_provider_unit.py::TestAnthropicProvider::test_
model_error_raises_error 
tests/unit/general/test_api.py::test_verify_token_rejects_invalid_token 
tests/unit/general/test_api.py::test_health_endpoint_accepts_valid_token 
tests/unit/general/test_api_health.py::test_health_endpoint_succeeds 
tests/unit/general/test_api_health.py::test_metrics_endpoint_succeeds 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_help_shows_co
mmand 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_disabled_exit
s_with_guidance 
tests/unit/general/test_atomic_rewrite_cli.py::test_atomic_rewrite_enabled_dry_r
un_succeeds 
tests/unit/general/test_backend_resource_flags.py::test_backend_flag_mapping_res
pects_env_vars 
tests/unit/general/test_backend_resource_flags.py::test_rdflib_env_mapping_disab
les_rdflib 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
handles_partial_spec 
tests/unit/general/test_backend_resource_flags.py::test_skip_if_missing_backend_
converts_find_spec_value_error 
tests/unit/general/test_base.py::test_dummy_adapter_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_initiali
zation_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_an
d_retrieve_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_store_ve
ctor_without_id_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_similari
ty_search_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_v
ector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_delete_n
onexistent_vector_succeeds 
tests/unit/general/test_chroma_db_adapter.py::TestChromaDBAdapter::test_get_coll
ection_stats_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_delete_succee
ds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_persistence_s
ucceeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_exact_
match_matches_expected 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_search_semant
ic_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_store_and_ret
rieve_succeeds 
tests/unit/general/test_chromadb_store.py::TestChromaDBStore::test_token_usage_s
ucceeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_lists_comm
ands_succeeds 
tests/unit/general/test_cli_commands.py::TestCLIHelpOutput::test_help_omits_depr
ecated_aliases_succeeds 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_provider_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_code_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_interface.py::TestCodeAnalysisInterface::t
est_file_analysis_result_interface_has_expected 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_co
de_analysis_implementation_succeeds 
tests/unit/general/test_code_analysis_models.py::TestCodeAnalysisModels::test_fi
le_analysis_implementation_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_code_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_directo
ry_succeeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_analyze_file_su
cceeds 
tests/unit/general/test_code_analyzer.py::TestCodeAnalyzer::test_project_structu
re_metrics_succeeds 
tests/unit/general/test_config_loader.py::test_load_yaml_config_succeeds 
tests/unit/general/test_config_loader.py::test_load_pyproject_toml_succeeds 
tests/unit/general/test_config_loader.py::test_autocomplete_succeeds 
tests/unit/general/test_config_loader.py::test_save_persists_version_succeeds 
tests/unit/general/test_config_loader.py::test_version_mismatch_logs_warning_mat
ches_expected 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_default_values_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_from_environment_variables_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_llm_set
tings_returns_expected_result 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[true-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[True-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[TRUE-True] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[false-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[False-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_boolean_env
ironment_variables_succeeds[FALSE-False] 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_load_dotenv
_file_not_found_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_get_setting
s_with_dotenv_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_invalid_sec
urity_boolean_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_empty_opena
i_api_key_raises 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_defaults_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_settin
gs_from_env_succeeds 
tests/unit/general/test_config_settings.py::TestConfigSettings::test_kuzu_embedd
ed_attribute_lookup_succeeds 
tests/unit/general/test_core_config_loader.py::test_precedence_env_over_project_
over_global_succeeds 
tests/unit/general/test_core_config_loader.py::test_load_toml_project_succeeds 
tests/unit/general/test_core_config_loader.py::test_save_global_config_yaml_succ
eeds tests/unit/general/test_core_values.py::test_load_core_values_succeeds 
tests/unit/general/test_core_values.py::test_find_value_conflicts_succeeds 
tests/unit/general/test_core_values.py::test_check_report_for_value_conflicts_su
cceeds 
tests/unit/general/test_core_workflows.py::test_filter_args_removes_none_values_
succeeds 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[init_project-init-kwargs0-expected0] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_specs-spec-kwargs1-expected1] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_tests-test-kwargs2-expected2] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[generate_code-code-kwargs3-expected3] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[run_pipeline-run-pipeline-kwargs4-expected4] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs5-expected5] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[update_config-config-kwargs6-expected6] 
tests/unit/general/test_core_workflows.py::test_wrappers_call_execute_command_su
cceeds[inspect_requirements-inspect-kwargs7-expected7] 
tests/unit/general/test_core_workflows.py::test_gather_requirements_creates_file
_succeeds 
tests/unit/general/test_core_workflows.py::test_workflow_manager_singleton_succe
eds 
tests/unit/general/test_delegate_task_disabled.py::test_delegate_task_collaborat
ion_disabled_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_a
ssess_impact_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_c
reate_session_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_consensus_failure 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_e
valuate_change_succeeds 
tests/unit/general/test_dialectical_reasoner.py::TestDialecticalReasoner::test_p
rocess_message_succeeds 
tests/unit/general/test_documentation_fetcher.py::test_fetcher_initialization_su
cceeds tests/unit/general/test_dpg_flag.py::test_dpg_command_disabled 
tests/unit/general/test_dpg_flag.py::test_dpg_command_missing_dependency 
tests/unit/general/test_dpg_flag.py::test_dpg_command_enabled 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_no_input_raises_e
rror 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_missing_
raises_error 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manifest_success_
succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_prompt_success_su
cceeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_manual_succeeds 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_custom_bridge_has
_expected 
tests/unit/general/test_edrr_cycle_cmd.py::test_edrr_cycle_cmd_error_handling_ra
ises_error 
tests/unit/general/test_edrr_manifest_string.py::test_start_cycle_from_manifest_
string_succeeds 
tests/unit/general/test_exception_logging.py::test_log_exception_emits_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_message
_only_succeeds 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_error_c
ode_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_init_with_details
_raises_error 
tests/unit/general/test_exceptions.py::TestDevSynthError::test_to_dict_succeeds 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_validation_erro
r_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_configuration_e
rror_raises_error 
tests/unit/general/test_exceptions.py::TestUserInputErrors::test_command_error_r
aises_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_internal_error_rai
ses_error 
tests/unit/general/test_exceptions.py::TestSystemErrors::test_resource_exhausted
_error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_provider_timeout_
error_raises_error 
tests/unit/general/test_exceptions.py::TestAdapterErrors::test_memory_adapter_er
ror_raises_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_agent_error_raises
_error 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_workflow_error_suc
ceeds 
tests/unit/general/test_exceptions.py::TestDomainErrors::test_dialectical_reason
ing_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_error
_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_promise_state
_error_raises_error 
tests/unit/general/test_exceptions.py::TestApplicationErrors::test_ingestion_err
or_raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_memory_port_error_ra
ises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_provider_port_error_
raises_error 
tests/unit/general/test_exceptions.py::TestPortErrors::test_agent_port_error_rai
ses_error 
tests/unit/general/test_fallback_utils.py::test_bulkhead_limits_concurrency 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_defau
lts_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_with_custo
m_manifest_succeeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_dry_run_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_validate_o
nly_is_valid 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_verbose_su
cceeds 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_forwards_a
uto_phase_flag 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_non_intera
ctive_flag_sets_env 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_env_var_en
ables_non_interactive 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_priority_u
pdates_config 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_manifest_e
rror_raises_error 
tests/unit/general/test_ingest_cmd.py::TestIngestCmd::test_ingest_cmd_ingestion_
error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_success_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_file_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_schema_not_found_is_valid 
tests/unit/general/test_ingest_cmd.py::TestValidateManifest::test_validate_manif
est_validation_failed_fails 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_succ
ess_is_valid 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_yaml
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestLoadManifest::test_load_manifest_file
_error_raises_error 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_expand_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_differentiate_phase_has_
expected 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_refine_phase_has_expecte
d 
tests/unit/general/test_ingest_cmd.py::TestPhases::test_retrospect_phase_has_exp
ected 
tests/unit/general/test_ingestion_edrr_integration.py::test_run_ingestion_invoke
s_edrr_phases_succeeds 
tests/unit/general/test_ingestion_type_hints.py::test_ingestion_type_hints_raise
s_error 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_update_succee
ds 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_prune_succeed
s 
tests/unit/general/test_inspect_config_cmd.py::test_inspect_config_no_config_suc
ceeds 
tests/unit/general/test_inspect_config_cmd.py::test_analyze_project_structure_re
turns_directories 
tests/unit/general/test_inspect_config_cmd.py::test_compare_with_manifest_return
s_differences 
tests/unit/general/test_inspect_config_cmd.py::test_update_manifest_adds_directo
ry 
tests/unit/general/test_isolation.py::TestIsolation::test_devsynth_dir_isolation
_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_global_config_isolatio
n_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_memory_path_isolation_
succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_no_file_logging_preven
ts_directory_creation_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_path_redirection_in_te
st_environment_succeeds 
tests/unit/general/test_isolation.py::TestIsolation::test_comprehensive_isolatio
n_succeeds 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_tmp_p
ath_fixture 
tests/unit/general/test_isolation_auto_marking.py::test_auto_isolation_for_netwo
rk_keyword 
tests/unit/general/test_kuzu_adapter.py::test_store_and_retrieve_vector_succeeds
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_succeeds 
tests/unit/general/test_kuzu_adapter.py::test_persistence_between_instances_succ
eeds 
tests/unit/general/test_kuzu_adapter.py::test_similarity_search_without_numpy_su
cceeds 
tests/unit/general/test_kuzu_embedded_missing.py::test_ephemeral_kuzu_store_init
ialises_without_kuzu_embedded 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_creation_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_to_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestWorkflowState::test_workflow_s
tate_from_dict_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_checkpoint_path_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_get_checkpoint_not_exists_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemCheckpointSaver::tes
t_put_checkpoint_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
create_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
add_step_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestLangGraphWorkflowEngine::test_
execute_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_save_and_get_workflow_succeeds 
tests/unit/general/test_langgraph_adapter.py::TestFileSystemWorkflowRepository::
test_list_workflows_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_offline_mode_selects_off
line_provider_succeeds 
tests/unit/general/test_llm_provider_selection.py::test_online_mode_uses_configu
red_provider_succeeds 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_registration 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_configuration_loading 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_settings_extraction 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_initialization_with_defaults 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_provider_mock_initialization 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_environment_variable_handling 
tests/unit/general/test_lmstudio_integration_regression.py::TestLMStudioIntegrat
ionRegression::test_lmstudio_config_file_integration 
tests/unit/general/test_lmstudio_service.py::test_lmstudio_mock_fixture_returns_
base_url 
tests/unit/general/test_logger.py::test_configure_logging_creates_rotating_handl
er tests/unit/general/test_logger.py::test_dev_synth_logger_normalizes_exc_info 
tests/unit/general/test_logging_setup.py::test_log_records_include_request_conte
xt_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_passes_through_succeeds 
tests/unit/general/test_logging_setup.py::test_exc_info_true_uses_current_except
ion 
tests/unit/general/test_logging_setup.py::test_extra_kwargs_and_reserved_keys_sa
fely_handled 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_idem
potent_no_duplicate_handlers 
tests/unit/general/test_logging_setup_idempotent.py::test_configure_logging_thre
ad_safe 
tests/unit/general/test_logging_setup_idempotent.py::test_no_file_logging_toggle
_prevents_file_handler 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_enu
m_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_ini
tialization_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_wit
h_metadata_succeeds 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_type_ali
ases 
tests/unit/general/test_memory_models.py::TestMemoryModels::test_memory_item_typ
e_alias 
tests/unit/general/test_memory_store.py::test_memory_store_abstract_methods_succ
eeds 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestInMemoryStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_delete_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_persistence_su
cceeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_search_succeed
s 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_store_and_retr
ieve_succeeds 
tests/unit/general/test_memory_system.py::TestJSONFileStore::test_token_usage_su
cceeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_add_and
_get_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_clear_c
ontext_succeeds 
tests/unit/general/test_memory_system.py::TestSimpleContextManager::test_get_ful
l_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_add
_and_get_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_cle
ar_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_full_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_get
_relevant_context_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_per
sistence_succeeds 
tests/unit/general/test_memory_system.py::TestPersistentContextManager::test_tok
en_usage_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_file_bas
ed_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_in_memor
y_adapter_succeeds 
tests/unit/general/test_memory_system.py::TestMemorySystemAdapter::test_token_us
age_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_with_chromadb_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_initialization_without_vector_store_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_memory_and_vector_store_integration_succeeds 
tests/unit/general/test_memory_system_with_chromadb.py::TestMemorySystemWithChro
maDB::test_context_manager_with_chromadb_succeeds 
tests/unit/general/test_methodology_logging.py::test_phase_timeout_logs_warning_
succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_multi_agent_consensus_and_primus_selection_succeeds 
tests/unit/general/test_multi_agent_adapter_workflow.py::TestMultiAgentAdapterWo
rkflow::test_bulk_add_agents_succeeds 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_success 
tests/unit/general/test_mvu_exec_cli.py::test_mvu_exec_cli_failure 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_combines_streams 
tests/unit/general/test_mvu_exec_cmd.py::test_mvu_exec_cmd_returns_exit_code 
tests/unit/general/test_mvu_init_cmd.py::test_mvu_init_cmd_creates_file 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_success 
tests/unit/general/test_mvu_lint_cli.py::test_mvu_lint_cli_failure 
tests/unit/general/test_mvuu_dashboard_cli.py::test_mvuu_dashboard_help_succeeds
tests/unit/general/test_mypy_config.py::test_mypy_configuration_raises_error 
tests/unit/general/test_mypy_config.py::test_mypy_project_configuration_raises_e
rror 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_ensure_path_exists_respects_no_file_logging_succeeds 
tests/unit/general/test_no_devsynth_dir_creation.py::TestNoDevSynthDirCreation::
test_settings_respects_no_file_logging_succeeds 
tests/unit/general/test_onnx_port.py::test_onnx_port_load_and_run_succeeds 
tests/unit/general/test_path_restrictions.py::test_ensure_path_exists_within_pro
ject_dir_succeeds 
tests/unit/general/test_path_restrictions.py::test_configure_logging_within_proj
ect_dir_succeeds 
tests/unit/general/test_ports_with_fixtures.py::test_ports_fixtures_succeeds 
tests/unit/general/test_primus_selection.py::test_highest_expertise_score_become
s_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_prioritizes_agents_who_have_no
t_served_as_primus_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prefer_doc
umentation_experts_succeeds 
tests/unit/general/test_primus_selection.py::test_weighted_expertise_prefers_spe
cialist_succeeds 
tests/unit/general/test_primus_selection.py::test_rotation_resets_after_all_agen
ts_served_succeeds 
tests/unit/general/test_primus_selection.py::test_documentation_tasks_prioritize
_best_doc_expert_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_success_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_load_proje
ct_yaml_fallback_to_legacy_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_project_ya
ml_path_preference_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_manifest_v
ersion_locking_succeeds 
tests/unit/general/test_project_yaml.py::TestProjectYamlLoading::test_default_ma
nifest_returned_when_missing_returns_expected_result 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_in
itialization_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_di
rect_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_execution_succeeds 
tests/unit/general/test_promise_agent.py::TestCapabilityHandler::test_handler_pr
omise_error_raises_error 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_agent_initializ
ation_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_regi
stration_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_requ
est_and_fulfillment_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_unauthorized_ac
cess_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_capability_not_
found_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgent::test_get_available_c
apabilities_succeeds 
tests/unit/general/test_promise_agent.py::TestPromiseAgentMixin::test_mixin_with
_custom_agent_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_initial_state_succe
eds 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_fulfilled_succ
eeds 
tests/unit/general/test_promise_system.py::TestPromise::test_then_rejected_succe
eds tests/unit/general/test_promise_system.py::TestPromise::test_catch_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_chaining_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_error_propagation_r
aises_error 
tests/unit/general/test_promise_system.py::TestPromise::test_resolve_value_stati
c_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_reject_with_static_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_all_with_rejection_
succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_race_succeeds 
tests/unit/general/test_promise_system.py::TestPromise::test_metadata_succeeds 
tests/unit/general/test_provider_logging.py::test_provider_logging_cleanup 
tests/unit/general/test_provider_logging.py::test_lmstudio_retry_metrics_and_cir
cuit_breaker 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_chat_
models_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_diale
ctical_reasoning_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_impac
t_assessment_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_change_model_succeeds 
tests/unit/general/test_requirement_models.py::TestRequirementModels::test_requi
rement_model_succeeds 
tests/unit/general/test_requirement_repository_interface.py::test_requirement_re
pository_interface_crud 
tests/unit/general/test_requirement_repository_port_interface.py::test_requireme
nt_repository_port_is_abstract 
tests/unit/general/test_requirement_repository_port_interface.py::test_dummy_req
uirement_port_methods_raise_not_implemented 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_app
rove_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_cre
ate_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_del
ete_requirement_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_rej
ect_change_succeeds 
tests/unit/general/test_requirement_service.py::TestRequirementService::test_upd
ate_requirement_succeeds 
tests/unit/general/test_resource_markers.py::test_is_lmstudio_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_codebase_available_succeeds
tests/unit/general/test_resource_markers.py::test_is_cli_available_succeeds 
tests/unit/general/test_resource_markers.py::test_is_resource_available_succeeds
tests/unit/general/test_resource_markers.py::test_with_resource_marker_succeeds 
tests/unit/general/test_resource_markers.py::test_pytest_collection_modifyitems_
succeeds 
tests/unit/general/test_retry_failure_scenarios.py::test_named_retry_condition_a
borts_and_records_metrics 
tests/unit/general/test_retry_failure_scenarios.py::test_circuit_breaker_open_re
cords_abort_metrics 
tests/unit/general/test_speed_option.py::test_speed_option_recognized 
tests/unit/general/test_sync_manager_persistence.py::test_sync_manager_persists_
to_all_stores 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_templat
es_exist_in_temp_location_succeeds 
tests/unit/general/test_template_location.py::TestTemplateLocation::test_can_use
_template_to_create_test_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_analyz
e_commit_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_calcul
ate_metrics_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_genera
te_metrics_report_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_get_co
mmit_history_succeeds 
tests/unit/general/test_test_first_metrics.py::TestTestFirstMetrics::test_main_s
ucceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_conversat
ion_tokens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_message_t
okens_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_count_tokens_su
cceeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_ensure_token_li
mit_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_fallback_tokeni
zer_succeeds 
tests/unit/general/test_token_tracker.py::TestTokenTracker::test_prune_conversat
ion_succeeds 
tests/unit/general/test_unified_agent_code_prompt.py::test_process_code_task_inc
ludes_language_and_paradigm_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_yaml_succeeds 
tests/unit/general/test_unified_config_loader.py::test_load_from_pyproject_succe
eds 
tests/unit/general/test_unified_config_loader.py::test_save_and_exists_succeeds 
tests/unit/general/test_unified_config_loader.py::test_missing_files_succeeds 
tests/unit/general/test_unified_config_loader.py::test_version_mismatch_warning_
succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_yaml
_succeeds 
tests/unit/general/test_unified_config_loader.py::test_loader_save_function_pypr
oject_succeeds tests/unit/general/test_unit_cli_commands.py::test_cmd 
tests/unit/general/test_ux_bridge.py::test_cli_bridge_methods_succeeds 
tests/unit/general/test_ux_bridge.py::test_webui_bridge_methods_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_handle_human_inte
rvention_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_create_workflow_f
or_command_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_add_init_workflow
_steps_succeeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_s
ucceeds 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_f
ailure_fails 
tests/unit/general/test_workflow.py::TestWorkflowManager::test_execute_command_h
uman_intervention_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
atus_enum_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_st
ep_initialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_in
itialization_succeeds 
tests/unit/general/test_workflow_models.py::TestWorkflowModels::test_workflow_wi
th_steps_succeeds 
tests/unit/general/test_wsde_dynamic_roles.py::test_assign_roles_for_phase_selec
ts_primus_by_expertise_has_expected 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_initialization_s
ucceeds 
tests/unit/general/test_wsde_model.py::TestWSDEModel::test_wsde_with_custom_valu
es_succeeds 
tests/unit/general/test_wsde_role_mapping.py::test_assign_roles_with_explicit_ma
pping_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_wsde_team_init
ialization_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_add_agent_succ
eeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_rotate_primus_
succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_primus_suc
ceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_s
ucceeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_get_role_speci
fic_agents_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
by_expertise_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_based_str
ucture_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_autonomous_col
laboration_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_consensus_base
d_decision_making_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
view_process_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_acceptance_criteria_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_revision_cycle_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_peer_review_wi
th_dialectical_analysis_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_contextdriven_
leadership_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_dialectical_re
asoning_with_external_knowledge_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_multi_discipli
nary_dialectical_reasoning_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_assign_roles_f
or_phase_varied_contexts_has_expected 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_majority_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_vote_on_critic
al_decision_weighted_path_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
task_selects_doc_agent_and_updates_role_assignments_succeeds 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_select_primus_
fallback_when_no_expertise_matches 
tests/unit/general/test_wsde_team_extended.py::TestWSDETeam::test_documentation_
expert_becomes_primus_succeeds 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_not_critical_raises_error 
tests/unit/general/test_wsde_team_voting_invalid.py::test_vote_on_critical_decis
ion_no_options_raises_error 
tests/unit/general/test_wsde_voting.py::test_majority_vote_with_three_unique_cho
ices_succeeds 
tests/unit/general/test_wsde_voting.py::test_tie_triggers_handle_tied_vote_succe
eds 
tests/unit/general/test_wsde_voting.py::test_weighted_voting_prefers_expert_vote
_succeeds 
tests/unit/general/test_wsde_voting.py::test_vote_on_critical_decision_no_votes_
succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_initiates_voting_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_majority_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_tied_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_weighted_vote_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_records_results_succeeds 
tests/unit/general/test_wsde_voting_mechanisms.py::TestWSDEVotingMechanisms::tes
t_vote_on_critical_decision_updates_history_succeeds 
tests/unit/infrastructure/test_test_infrastructure_sanity.py::test_global_test_i
solation_sets_env_and_dirs 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_and_query_su
ccess 
tests/unit/integrations/test_autoresearch_client.py::test_handshake_disabled_by_
flag 
tests/unit/integrations/test_autoresearch_client.py::test_query_failure_falls_ba
ck 
tests/unit/interface/test_agent_api_fastapi_guard.py::test_fastapi_testclient_gu
ard_allows_minimal_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_initialization 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_record_request 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_within_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_count_exceeds_limit 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_prune_old_requests 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimiter::test_rate_limit
er_multiple_clients 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_health_en
dpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_metrics_e
ndpoint_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_init_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_gather_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_synthesiz
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_spec_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_code_requ
est_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_doctor_re
quest_model 
tests/unit/interface/test_agentapi_enhanced.py::TestAPIEndpoints::test_edrr_cycl
e_request_model 
tests/unit/interface/test_agentapi_enhanced.py::TestRouter::test_router_exists 
tests/unit/interface/test_agentapi_enhanced.py::TestRateLimitingIntegration::tes
t_rate_limiting_logic_integration 
tests/unit/interface/test_agentapi_enhanced.py::TestErrorHandling::test_error_re
sponse_structure 
tests/unit/interface/test_agentapi_enhanced.py::TestEndpointIntegration::test_re
quest_models_validation 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_answers_a
nd_defaults 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_api_bridge_confirm_c
hoice_coerces_booleans 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_progress_tr
acks_subtasks 
tests/unit/interface/test_agentapi_enhanced_bridge.py::test_enhanced_rate_limit_
blocks_abusive_clients 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_allow
s_after_window 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_rate_limit_raise
s_when_exceeded 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_records_subtasks 
tests/unit/interface/test_agentapi_rate_limit_progress.py::test_api_bridge_progr
ess_normalizes_string_advances 
tests/unit/interface/test_api_endpoints.py::test_enhanced_rate_limit_state_track
s_buckets 
tests/unit/interface/test_api_endpoints.py::test_enhanced_metrics_snapshot_typed
tests/unit/interface/test_api_endpoints.py::test_enhanced_init_endpoint_returns_
typed_error 
tests/unit/interface/test_cli_components.py::test_cliprogressindicator_sanitize_
output_succeeds 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_ini
t_with_bad_description_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_upd
ate_with_bad_inputs_uses_fallback 
tests/unit/interface/test_cli_progress_indicator.py::test_progress_indicator_sub
tasks_with_bad_inputs_use_fallbacks 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_ask_question_us
es_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_confirm_choice_
uses_prompt_toolkit 
tests/unit/interface/test_cli_prompt_toolkit_bridge.py::test_cli_prompt_fallback
_to_rich 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_noninteractive_re
turns_defaults_and_logs 
tests/unit/interface/test_cli_uxbridge_noninteractive.py::test_display_result_lo
gging_branches 
tests/unit/interface/test_command_output.py::test_format_and_display_message 
tests/unit/interface/test_command_output.py::test_format_error_suggestions 
tests/unit/interface/test_command_output.py::test_list_and_structured_outputs 
tests/unit/interface/test_command_output.py::test_set_console 
tests/unit/interface/test_dpg_ui.py::test_all_buttons_trigger_callbacks_and_prog
ress tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog 
tests/unit/interface/test_dpg_ui.py::test_requirements_wizard_dialog_error 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_actionable_error_suggestion_str_includes_details 
tests/unit/interface/test_enhanced_error_handler.py::TestEnhancedErrorHandler::t
est_format_error_wraps_with_footer 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_defaul
t_file 
tests/unit/interface/test_mvuu_dashboard.py::test_load_traceability_reads_specif
ied_file 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_invokes_strea
mlit tests/unit/interface/test_mvuu_dashboard.py::test_require_streamlit_raises 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_snaps
hot 
tests/unit/interface/test_mvuu_dashboard.py::test_render_research_overlays_witho
ut_optional_sections 
tests/unit/interface/test_mvuu_dashboard.py::test_render_dashboard_with_overlays
_loads_telemetry 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_pointer_legacy_env 
tests/unit/interface/test_mvuu_dashboard.py::test_signature_secret_falls_back_to
_legacy 
tests/unit/interface/test_mvuu_dashboard.py::test_resolve_telemetry_path_prefers
_legacy 
tests/unit/interface/test_nicegui_bridge.py::test_session_storage_roundtrip 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_notifies_and_re
cords 
tests/unit/interface/test_nicegui_bridge.py::test_progress_indicator_updates_and
_completes 
tests/unit/interface/test_nicegui_bridge.py::test_display_result_falls_back_with
out_nicegui 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_update 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_progr
ess_indicator_complete 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_initialization 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_nicegui_bridg
e_create_progress 
tests/unit/interface/test_nicegui_webui.py::TestNiceGUIWebUI::test_main_function
_exists 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_json_yaml_with_and_without_console 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_table_fallback_and_empty_list 
tests/unit/interface/test_output_formatter_command_options_fast.py::test_format_
command_output_rich_renderables 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_sanitize_outp
ut_delegates_and_handles_edge_cases 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[ERROR: Disk failure-error] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[warning: Low memory-warning] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Task completed successfully-success] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[INFO: FYI-info] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[# Heading-heading] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_detect_messag
e_type_covers_known_patterns[Routine update-normal] 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_format_messag
e_applies_status_styles 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_highl
ight_branch_emits_panel 
tests/unit/interface/test_output_formatter_core_behaviors.py::test_display_witho
ut_console_raises_value_error 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_format_
message_error_styles_and_escapes_markup 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_markdow
n_branch_sanitizes_hyperlinks 
tests/unit/interface/test_output_formatter_error_rendering_fast.py::test_table_b
ranch_sanitizes_script_links 
tests/unit/interface/test_output_formatter_fallbacks.py::test_table_format_falls
_back_to_text_for_nontabular_inputs 
tests/unit/interface/test_output_formatter_fallbacks.py::test_rich_format_select
s_renderables_for_data_shapes 
tests/unit/interface/test_output_formatter_fallbacks.py::test_list_of_dicts_tabl
e_renders_missing_and_complex_values 
tests/unit/interface/test_output_formatter_fallbacks.py::test_set_format_options
_and_command_output_overrides 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-syntax] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[json-plain] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-syntax] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[yaml-plain] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-dict] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[markdown-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-empty-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[table-heterogeneous] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-dict] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-list-of-dicts] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-bullet-panel] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[rich-falsy-list] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_struc
tured_exercises_all_branches[text-scalar] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_mark
down_handles_nested_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_to_mark
down_handles_mixed_items 
tests/unit/interface/test_output_formatter_structured_fast.py::test_dict_to_tabl
e_serializes_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_list_of_dict
s_to_table_handles_missing_keys 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-nested] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[markdown-list-item] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-dict-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-key] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[table-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_structured_h
elpers_raise_on_exploding_strings[list-table-value] 
tests/unit/interface/test_output_formatter_structured_fast.py::test_format_table
_and_list_preserve_sanitized_complex_values 
tests/unit/interface/test_output_formatter_structured_fast.py::test_command_outp
ut_unknown_extension_and_highlight_panel 
tests/unit/interface/test_output_sanitization.py::test_cliuxbridge_sanitizes_scr
ipt_tag_succeeds 
tests/unit/interface/test_progress_helpers.py::test_dummy_progress_supports_nest
ed_protocol 
tests/unit/interface/test_progress_helpers.py::test_subtask_snapshot_typed_struc
ture 
tests/unit/interface/test_progress_utils.py::test_progress_manager_create_get_co
mplete_and_context_manager 
tests/unit/interface/test_progress_utils.py::test_progress_manager_track_updates
_on_item_and_slice 
tests/unit/interface/test_progress_utils.py::test_progress_indicator_context_man
ager_completes 
tests/unit/interface/test_progress_utils.py::test_step_progress_sequencing_and_c
omplete 
tests/unit/interface/test_progress_utils.py::test_create_and_track_progress_help
ers_use_manager 
tests/unit/interface/test_progress_utils.py::test_progress_tracker_forced_update
_and_complete 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_prefers_di
alog_selection 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_text_validates_
input 
tests/unit/interface/test_prompt_toolkit_adapter.py::test_prompt_multi_select_re
turns_checkbox_choices 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_produces_timeline_snapshot 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_merges_extended_metadata 
tests/unit/interface/test_research_telemetry.py::test_merge_extended_metadata_in
to_payload_appends_sections 
tests/unit/interface/test_research_telemetry.py::test_build_research_telemetry_p
ayload_invokes_connectors 
tests/unit/interface/test_research_telemetry.py::test_signature_roundtrip_valida
tes 
tests/unit/interface/test_research_telemetry.py::test_signature_failure_with_wro
ng_secret 
tests/unit/interface/test_textual_ux_bridge.py::test_question_and_display_intera
ctions_are_recorded 
tests/unit/interface/test_textual_ux_bridge.py::test_confirm_choice_falls_back_t
o_default 
tests/unit/interface/test_textual_ux_bridge.py::test_progress_updates_capture_ne
sted_subtasks 
tests/unit/interface/test_textual_ux_bridge.py::test_capabilities_reflect_textua
l_availability 
tests/unit/interface/test_textual_ux_bridge.py::test_require_textual_guard 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_enabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_with_sanit
ization_disabled 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_backward_compatib
ility_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_uxbridge_handle_error_defa
ult_implementation 
tests/unit/interface/test_ux_bridge_coverage.py::test_progress_indicator_context
_manager 
tests/unit/interface/test_ux_bridge_coverage.py::test_dummy_progress_methods 
tests/unit/interface/test_ux_bridge_coverage.py::test_sanitize_output_fallback_i
mport tests/unit/interface/test_uxbridge_aliases.py::test_function 
tests/unit/interface/test_uxbridge_aliases.py::test_print_alias_delegates 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_lazy_streamlit_
forwards_attributes 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_require_streaml
it_guidance_and_cache 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ask_question_an
d_confirm_choice_respects_defaults 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
routes_error_and_highlight_paths 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
handles_multiple_message_types 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
info_and_error_fallbacks_sanitize 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
markup_fallback_uses_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
error_prefix_triggers_guidance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_display_result_
covers_all_message_channels 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_render_tracebac
k_captures_output 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_error_mapping_h
elpers_cover_cases 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_est
imates_and_subtasks 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_com
plete_cascades_and_falls_back_to_write 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_formats_hours 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_sta
tus_transitions_cover_all_thresholds 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ui_progress_eta
_minutes_branch 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[500-1-True] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[800-2-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[1300-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_get_layout_conf
ig_breakpoints[absent-3-False] 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_responsive_
layout_and_router_invocation 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_htm
l_failure 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_handles_pag
e_config_error 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_without_com
ponents_invokes_router 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_ensure_router_c
aches_router_instance 
tests/unit/interface/test_webui_behavior_checklist_fast.py::test_run_module_entr
ypoint_invokes_webui_run 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_run_registers_rout
er_and_hydrates_session 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_i
nvokes_cli_targets 
tests/unit/interface/test_webui_bootstrap_fast.py::test_webui_command_dispatch_r
eports_value_errors 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_progress_indicator
_extensive_paths_cover_hierarchy 
tests/unit/interface/test_webui_bridge_aa_coverage.py::test_z_bridge_accessors_a
nd_wizard_paths_cover_invariants 
tests/unit/interface/test_webui_bridge_cli_parity.py::test_webui_bridge_matches_
cli_prompt_defaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_handle
s_fallbacks_and_missing_parents 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_nested_subtask_status
_progression_without_explicit_status 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_helpers_normal
ize_mixed_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_prompt_helpers_echo_d
efaults 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_append
s_documentation_links 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_require_streamlit_cac
hes_and_guides 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_wizard_clamps_handle_
invalid_inputs 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_and_pr
ogress_use_formatter 
tests/unit/interface/test_webui_bridge_fast_suite.py::test_display_result_highli
ght_falls_back_to_write 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_uses
_cached_stub 
tests/unit/interface/test_webui_bridge_handshake.py::test_require_streamlit_impo
rts_when_missing 
tests/unit/interface/test_webui_bridge_handshake.py::test_adjust_wizard_step_han
dles_invalid_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_nes
ted_tasks_cover_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_progress_indicator_sta
tus_defaults_and_fallbacks 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_routes_
messages_and_sanitizes 
tests/unit/interface/test_webui_bridge_handshake.py::test_display_result_error_b
ranch_records_message 
tests/unit/interface/test_webui_bridge_handshake.py::test_bridge_prompt_helpers_
return_defaults 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
handles_varied_inputs 
tests/unit/interface/test_webui_bridge_normalize.py::test_normalize_wizard_step_
invalid_total_defaults_to_zero 
tests/unit/interface/test_webui_bridge_normalize.py::test_progress_indicator_rej
ects_missing_parent 
tests/unit/interface/test_webui_bridge_normalize.py::test_display_result_routes_
messages_to_streamlit 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_upda
te_paths 
tests/unit/interface/test_webui_bridge_progress.py::test_progress_indicator_subt
asks_and_nested_operations 
tests/unit/interface/test_webui_bridge_progress.py::test_require_streamlit_failu
re 
tests/unit/interface/test_webui_bridge_progress.py::test_adjust_wizard_step_edge
s 
tests/unit/interface/test_webui_bridge_progress.py::test_nested_subtask_default_
status_cycle 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_display_re
sult_routes_and_sanitizes 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_session_ac
cess_wrappers 
tests/unit/interface/test_webui_bridge_progress.py::test_webui_bridge_prompt_ali
ases_and_progress 
tests/unit/interface/test_webui_bridge_progress.py::test_normalize_wizard_step_v
aried_inputs 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_require_stream
lit_raises 
tests/unit/interface/test_webui_bridge_require_streamlit.py::test_progress_indic
ator_status_transitions 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_handshake
_routes_to_streamlit 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_error_rou
te_sanitizes_output 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_respects_
sanitization_flag 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_highlight
_routes_to_info 
tests/unit/interface/test_webui_bridge_routing.py::test_display_result_success_r
outes_to_success 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_require_streamlit
_missing_dependency_surfaces_install_guidance 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_nested_progress_s
tatus_defaults_follow_spec 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_navigation
_normalization_matches_state_invariants 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_wizard_manager_ac
cessors_follow_integration_guide 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_prompt_defaults_a
lign_with_uxbridge_contract 
tests/unit/interface/test_webui_bridge_spec_alignment.py::test_display_result_ch
annels_respect_output_formatter_contract 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_get_wiza
rd_manager_uses_session_state 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_create_w
izard_manager_instantiates_stub 
tests/unit/interface/test_webui_bridge_state_fast.py::test_webui_bridge_session_
helpers_delegate 
tests/unit/interface/test_webui_bridge_targeted.py::test_adjust_wizard_step_inva
lid_direction_keeps_bounds 
tests/unit/interface/test_webui_bridge_targeted.py::test_normalize_wizard_step_h
andles_strings 
tests/unit/interface/test_webui_bridge_targeted.py::test_question_and_confirmati
on_defaults 
tests/unit/interface/test_webui_bridge_targeted.py::test_display_result_highligh
t_routes_to_info 
tests/unit/interface/test_webui_bridge_targeted.py::test_create_progress_cycles_
statuses 
tests/unit/interface/test_webui_bridge_targeted.py::test_session_helpers_delegat
e_to_state_access 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_pers
ists_state 
tests/unit/interface/test_webui_bridge_targeted.py::test_get_wizard_manager_requ
ires_session_state 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_nested_completion_and_sanitization 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_wizard_na
vigation_and_display_fallback 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_default_s
tatus_thresholds 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_progress_
indicator_updates_and_completion 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_nested_su
btask_lifecycle 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_display_r
esult_routes_by_type 
tests/unit/interface/test_webui_bridge_wizard_navigation_fast.py::test_get_wizar
d_manager_and_create 
tests/unit/interface/test_webui_commands.py::test_cli_returns_module_attribute 
tests/unit/interface/test_webui_commands.py::test_cli_returns_none_when_missing 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_pass_thr
ough 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-File not found] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Permission denied] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Invalid value] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Missing key] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_specific
_exceptions[<lambda>-Type error] 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_generic_
exception 
tests/unit/interface/test_webui_commands.py::test_cli_uses_cli_module_when_avail
able 
tests/unit/interface/test_webui_commands.py::test_handle_command_errors_reraises
_devsynth_error 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_layout_bre
akpoints_toggle_between_modes 
tests/unit/interface/test_webui_dashboard_toggles_fast.py::test_webui_error_guid
ance_surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_require_streamlit_la
zy_loader 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[500-expected0] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[800-expected1] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[1200-expected2] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_layout_config_br
eakpoints[None-expected3] 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_rende
rs_markup_and_sanitizes 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_highl
ight_uses_info 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_route
s_message_types_and_plain_write 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_suggestions_and_docs 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_error
_prefix_without_message_type 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_headi
ng_routes_to_header 
tests/unit/interface/test_webui_display_and_layout.py::test_display_result_addit
ional_headings 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[File not found: missing.yaml-file_not_found] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Permission denied when opening-permission_denied] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid parameter --foo-invalid_parameter] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Invalid format provided-invalid_format] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Missing key 'api'-key_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Type error while casting-type_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Configuration error detected-config_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Connection error occurred-connection_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[API error status-api_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Validation error raised-validation_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Syntax error unexpected token-syntax_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Import error for module-import_error] 
tests/unit/interface/test_webui_display_and_layout.py::test_get_error_type_mappi
ngs[Unrelated message-] 
tests/unit/interface/test_webui_display_and_layout.py::test_error_helper_default
s 
tests/unit/interface/test_webui_display_and_layout.py::test_render_traceback_use
s_expander 
tests/unit/interface/test_webui_display_and_layout.py::test_format_error_message
tests/unit/interface/test_webui_display_and_layout.py::test_ensure_router_caches
_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_run_configures_strea
mlit_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_page_con
fig_error 
tests/unit/interface/test_webui_display_and_layout.py::test_run_handles_componen
ts_error 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_updates_
emit_eta 
tests/unit/interface/test_webui_display_and_layout.py::test_ui_progress_subtask_
flow 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_ensure_router_
caches_instance 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_configures
_layout_and_router 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_pa
ge_config_error 
tests/unit/interface/test_webui_display_and_layout.py::test_webui_run_handles_co
mponent_error 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_transla
tes_markup_to_markdown 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_surface
s_guidance_for_file_errors 
tests/unit/interface/test_webui_display_guidance.py::test_display_result_highlig
hts_information 
tests/unit/interface/test_webui_display_guidance.py::test_ui_progress_tracks_sta
tus_and_subtasks 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_highlight
_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_error_rai
ses_error 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_warning_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_success_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_heading_s
ucceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_subheadin
g_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_rich_mark
up_succeeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_display_result_normal_su
cceeds 
tests/unit/interface/test_webui_enhanced.py::test_webui_progress_indicator_succe
eds 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_passthrough 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: File not found: config.yaml-Make sure the 
file exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Permission denied: secrets.env-necessary 
permissions] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Invalid value: bad input-Please check your
input] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Missing key: 'api_key'-Verify that the 
referenced key exists] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_known_exceptions[<lambda>-ERROR: Type error: wrong type-Check that all 
inputs] 
tests/unit/interface/test_webui_handle_command_errors.py::test_handle_command_er
rors_generic_exception 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[640-expected0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[820-expected1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_get_layout
_config_breakpoints[1200-expected2] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_rich_markup_uses_markdown 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_error_type_renders_context 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[warning-warning] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[success-success] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[info-info] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_message_types[unexpected-write] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_highlight_uses_info 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_defaults_to_write 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[# Overview-expected_calls0] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[## Section-expected_calls1] 
tests/unit/interface/test_webui_layout_and_display_branching.py::test_display_re
sult_renders_headings[### Deep Dive-expected_calls2] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_layout_config_
respects_breakpoints 
tests/unit/interface/test_webui_layout_and_messaging.py::test_ask_question_and_c
onfirm_choice_use_streamlit_controls 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mes
sage_types_provide_guidance 
tests/unit/interface/test_webui_layout_and_messaging.py::test_display_result_mar
kup_and_keyword_routing 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[File not found-file_not_found] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Permission denied-permission_denied] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid parameter-invalid_parameter] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Invalid format-invalid_format] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Missing key-key_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Type error-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[TypeError-type_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Configuration error-config_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Connection error-connection_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[API error-api_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Validation error-validation_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Syntax error-syntax_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Import error-import_error] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_get_error_type_mat
ches_keywords[Completely different-] 
tests/unit/interface/test_webui_layout_and_messaging.py::test_error_suggestions_
and_docs_cover_known_and_unknown 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_lazy_streamlit_proxy_i
mports_once 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ui_progress_tracks_sta
tus_and_eta 
tests/unit/interface/test_webui_lazy_loader_fast.py::test_ensure_router_creates_
single_instance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_lazy_str
eamlit_proxy_imports_once 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_missing_
streamlit_surfaces_install_guidance 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_progress
_indicator_emits_eta_and_sanitized_status 
tests/unit/interface/test_webui_lazy_progress_suggestions_fast.py::test_permissi
on_denied_error_renders_suggestions 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_lazy_streamli
t_import_is_cached 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_display_resul
t_translates_markup_to_html 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_normalize_ste
p_logs_warning_on_invalid_value 
tests/unit/interface/test_webui_lazy_streamlit_and_wizard.py::test_adjust_step_w
arns_on_invalid_direction 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_secon
ds_when_under_minute 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_minut
es_when_under_hour 
tests/unit/interface/test_webui_progress.py::test_ui_progress_eta_displays_hours
_and_minutes 
tests/unit/interface/test_webui_progress.py::test_ui_progress_status_transitions
_without_explicit_status 
tests/unit/interface/test_webui_progress.py::test_ui_progress_subtasks_update_wi
th_frozen_time 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_progress_complete
_cascades_with_sanitized_fallback 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_layout_and_
display_behaviors 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ui_progress_statu
s_transitions_and_eta 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_ensure_router_cac
hes_instance 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_configu
res_layout_and_router 
tests/unit/interface/test_webui_progress_cascade_fast.py::test_webui_run_handles
_streamlit_errors 
tests/unit/interface/test_webui_progress_time.py::test_update_records_time 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/test_webui_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/test_webui_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/test_webui_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/test_webui_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/test_webui_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/interface/test_webui_rendering_module.py::test_validate_requirements_
step_requires_fields 
tests/unit/interface/test_webui_rendering_module.py::test_handle_requirements_na
vigation_cancel_clears_state 
tests/unit/interface/test_webui_rendering_module.py::test_save_requirements_clea
rs_temporary_keys 
tests/unit/interface/test_webui_rendering_progress.py::test_gather_wizard_render
s_cli_summary 
tests/unit/interface/test_webui_rendering_progress.py::test_render_progress_summ
ary_prefers_checkpoint_eta_strings 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_ret
urns_module 
tests/unit/interface/test_webui_require_streamlit.py::test_require_streamlit_rai
ses 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_initialization 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_step_navigation_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_requirements_wizard
_save_requirements_succeeds 
tests/unit/interface/test_webui_requirements_wizard.py::test_validate_requiremen
ts_step 
tests/unit/interface/test_webui_requirements_wizard.py::test_handle_requirements
_navigation_next 
tests/unit/interface/test_webui_requirements_wizard.py::test_save_requirements_w
rites_file 
tests/unit/interface/test_webui_requirements_wizard.py::test_priority_persists_t
hrough_navigation 
tests/unit/interface/test_webui_requirements_wizard.py::test_title_and_descripti
on_persist 
tests/unit/interface/test_webui_routing.py::test_router_uses_session_state 
tests/unit/interface/test_webui_routing.py::test_router_resets_invalid_selection
tests/unit/interface/test_webui_routing.py::test_router_handles_sidebar_exceptio
n 
tests/unit/interface/test_webui_routing.py::test_router_surfaces_page_exception 
tests/unit/interface/test_webui_routing.py::test_router_requires_pages 
tests/unit/interface/test_webui_routing.py::test_router_honors_explicit_default 
tests/unit/interface/test_webui_routing.py::test_router_reports_missing_page_han
dler 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_invalid_
navigation_option 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_page_exc
eption_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_streamli
t_exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_sidebar_
exception_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_method_with_multiple
_exceptions_raises_error 
tests/unit/interface/test_webui_run_edge_cases.py::test_standalone_run_function_
succeeds 
tests/unit/interface/test_webui_run_edge_cases.py::test_run_webui_alias_succeeds
tests/unit/interface/test_webui_run_edge_cases.py::test_main_block_succeeds 
tests/unit/interface/test_webui_run_fast.py::test_webui_run_injects_resize_scrip
t_and_configures_layout 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_r
ecords_summary_and_errors 
tests/unit/interface/test_webui_simulations_fast.py::test_rendering_simulation_h
andles_nested_summary_and_clock 
tests/unit/interface/test_webui_simulations_fast.py::test_ui_progress_simulation
_drives_eta_and_completion 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_display_result_s
anitises_error 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_simulatio
n_sanitises_nested_tasks 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_require_streamli
t_cache 
tests/unit/interface/test_webui_simulations_fast.py::test_webui_bridge_require_s
treamlit_guidance 
tests/unit/interface/test_webui_state_errors.py::test_clear_reraises_after_loggi
ng 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_webui_run_
configures_dashboard_and_invokes_router 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_progress_u
pdates_emit_telemetry_and_sanitize_checkpoints 
tests/unit/interface/test_webui_streamlit_free_progress_fast.py::test_display_re
sult_sanitizes_message_before_render 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_requir
e_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_require_streamlit_reports_install_guidance 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[error-kwargs0-error] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[warning-kwargs1-warning] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[success-kwargs2-success] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_displa
y_result_sanitizes_without_streamlit[highlight-kwargs3-info] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[error-kwargs0-error] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[warning-kwargs1-warning] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[success-kwargs2-success] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_bridge
_display_result_sanitizes_without_streamlit[highlight-kwargs3-info] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_progre
ss_indicator_nested_lifecycle_and_statuses 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[0-0-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[10-100-Starting...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[25-100-Processing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[50-100-Halfway there...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[80-100-Almost done...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[99-100-Finalizing...] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_default_stat
us_thresholds[100-100-Complete] 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_webui_ui_pro
gress_eta_formats 
tests/unit/interface/test_webui_streamlit_free_regressions.py::test_wizard_helpe
rs_clamp_malformed_inputs 
tests/unit/interface/test_webui_streamlit_stub.py::test_lazy_loader_imports_stre
amlit_stub_once 
tests/unit/interface/test_webui_streamlit_stub.py::test_missing_streamlit_surfac
es_install_guidance 
tests/unit/interface/test_webui_streamlit_stub.py::test_display_result_sanitizes
_error_output 
tests/unit/interface/test_webui_streamlit_stub.py::test_ui_progress_tracks_statu
s_and_subtasks 
tests/unit/interface/test_webui_streamlit_stub.py::test_router_run_uses_default_
and_persists_selection 
tests/unit/interface/test_webui_streamlit_stub.py::test_webui_run_configures_rou
ter_and_layout 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_selectbo
x_indexes_default 
tests/unit/interface/test_webui_targeted_branches.py::test_ask_question_text_inp
ut_when_no_choices 
tests/unit/interface/test_webui_targeted_branches.py::test_confirm_choice_return
s_checkbox_value 
tests/unit/interface/test_webui_targeted_branches.py::test_display_result_error_
surfaces_suggestions_and_docs 
tests/unit/interface/test_webui_targeted_branches.py::test_render_traceback_expa
nder_renders_code 
tests/unit/interface/test_webui_targeted_branches.py::test_ui_progress_sanitizes
_updates 
tests/unit/interface/test_webui_targeted_branches.py::test_ensure_router_memoize
s_instance 
tests/unit/interface/test_webui_targeted_branches.py::test_run_handles_page_conf
ig_errors 
tests/unit/interface/test_webui_targeted_branches.py::test_run_renders_layout_an
d_router 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_basic 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_errors 
tests/unit/interface/webui/test_rendering.py::TestSimulateProgressRendering::tes
t_simulate_progress_rendering_with_clock 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestProjectSetupPages::test_projec
t_setup_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestLifecyclePages::test_lifecycle
_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_initialization 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_inheritance 
tests/unit/interface/webui/test_rendering.py::TestOperationsPages::test_operatio
ns_pages_method_existence 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_initialization 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_inheritance 
tests/unit/interface/webui/test_rendering.py::TestSupportPages::test_support_pag
es_method_existence 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_initialization 
tests/unit/interface/webui/test_rendering.py::TestPageRenderer::test_page_render
er_method_existence 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_page_rendering_with_different_page_types 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_with_mock_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingIntegration::tes
t_rendering_error_handling 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
progress_simulation_utility 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingUtilities::test_
rendering_import_dependencies 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_loading 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingConfiguration::t
est_rendering_with_config_saving 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_page_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingPerformance::tes
t_renderer_initialization_performance 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_with_invalid_bridge 
tests/unit/interface/webui/test_rendering.py::TestWebUIRenderingErrorHandling::t
est_rendering_method_error_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_valid_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_default_config 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_auto_model_selection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_with_custom_port 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderInitialization::te
st_initialization_lmstudio_unavailable 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_availability_detection 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_server_unavailable_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderAvailabilityProbin
g::test_model_list_retrieval 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_with_defaults 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderConfiguration::tes
t_configuration_precedence 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_counting_integration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderTokenTracking::tes
t_token_limit_validation 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_c
ircuit_breaker_initialization 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderResilience::test_r
etry_logic_configuration 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_temperature_range 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderErrorHandling::tes
t_invalid_max_tokens 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_em
pty_model_list_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_ti
meout_handling 
tests/unit/llm/test_lmstudio_provider.py::TestLMStudioProviderEdgeCases::test_un
icode_content_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_valid_config 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_environment_variable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_without_api_key_raises_error 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_default_model 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_custom_base_url 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderInitialization::test_i
nitialization_with_openai_client_unavailable 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_temperature_range 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderErrorHandling::test_in
valid_max_tokens 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_with_defaults 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderConfiguration::test_co
nfiguration_precedence 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_counting_integration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderTokenTracking::test_to
ken_limit_validation 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_circu
it_breaker_initialization 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderResilience::test_retry
_logic_configuration 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_metrics_
collection_setup 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderMetrics::test_telemetr
y_emission 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_correct_
headers_set 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderHeaders::test_custom_a
pi_key_header 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_empty_
response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_malfor
med_response_handling 
tests/unit/llm/test_openai_provider.py::TestOpenAIProviderEdgeCases::test_unicod
e_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_valid_config 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_environment_variable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_without_api_key_raises_error 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_default_free_tier_model 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_httpx_unavailable 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderInitialization
::test_initialization_with_custom_base_url 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_temperature_range 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderErrorHandling:
:test_invalid_max_tokens 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_with_defaults 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderConfiguration:
:test_configuration_precedence 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_counting_integration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderTokenTracking:
:test_token_limit_validation 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_circuit_breaker_initialization 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderResilience::te
st_retry_logic_configuration 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
metrics_collection_setup 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderMetrics::test_
telemetry_emission 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
correct_headers_set 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderHeaders::test_
custom_referer_header 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_empty_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_malformed_response_handling 
tests/unit/llm/test_openrouter_provider.py::TestOpenRouterProviderEdgeCases::tes
t_unicode_handling 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_message_args_
and_mappings 
tests/unit/logging/test_logging_setup.py::test_redact_filter_property_loop_prese
rves_inputs 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_attaches_c
ontext 
tests/unit/logging/test_logging_setup.py::test_json_formatter_serializes_request
_context 
tests/unit/logging/test_logging_setup.py::test_redaction_in_message_and_payload 
tests/unit/logging/test_logging_setup.py::test_request_context_filter_injects_fi
elds_and_clears 
tests/unit/logging/test_logging_setup.py::test_jsonformatter_includes_exception_
block 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_no_file_l
ogging 
tests/unit/logging/test_logging_setup.py::test_get_log_dir_and_file_use_env_over
rides 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_uses_project_dir_f
or_relative_path 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_under_te
st_project_dir 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_redirects_absolute
_outside_home 
tests/unit/logging/test_logging_setup.py::test_ensure_log_dir_respects_project_d
ir_when_file_logging_disabled 
tests/unit/logging/test_logging_setup.py::test_configure_logging_redirects_home_
and_disables_file_handler 
tests/unit/logging/test_logging_setup.py::test_short_secret_not_redacted 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_merges_and_fi
lters_kwargs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_table_normali
zation 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_does_not_muta
te_extra_inputs 
tests/unit/logging/test_logging_setup.py::test_devsynth_logger_log_normalizes_tr
uthy_exc_info 
tests/unit/logging/test_logging_setup.py::test_configure_logging_console_only_us
es_caplog 
tests/unit/logging/test_logging_setup.py::test_redact_filter_masks_secret_tokens
_via_caplog 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_handles_missing_
log_file_path 
tests/unit/logging/test_logging_setup.py::test_dev_synth_logger_emits_structured
_extras_with_context 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_secrets_f
ilter_masks_values 
tests/unit/logging/test_logging_setup_additional_paths.py::test_json_formatter_i
ncludes_context_and_extras 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_respects_project_dir 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_skips_creation_when_disabled 
tests/unit/logging/test_logging_setup_additional_paths.py::test_ensure_log_dir_e
xists_warns_when_creation_fails 
tests/unit/logging/test_logging_setup_additional_paths.py::test_devsynth_logger_
filters_reserved_extra_keys 
tests/unit/logging/test_logging_setup_additional_paths.py::test_redact_filter_ma
sks_args_and_payload 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_provis
ions_json_file_handler 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_consol
e_only_mode 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[permission-error] 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_handle
r_parity_when_file_handler_fails[file-not-found] 
tests/unit/logging/test_logging_setup_branches.py::test_configure_logging_idempo
tent_with_identical_configuration 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_e
xplicit_level_overrides_env 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_j
son_handler_writes_structured_output 
tests/unit/logging/test_logging_setup_configuration.py::test_configure_logging_r
econfigures_console_only_toggle 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_file-logging] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[project-dir_console-only] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_file-logging] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_resolves_paths[no-project_console-custom] 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_idempotent_with_identical_settings 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_invokes_directory_creation_once 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_preserves_filters_on_reconfigure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_falls_back_to_console_on_file_handler_failure 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_create_dir_guard_preserves_console_only_mode 
tests/unit/logging/test_logging_setup_configure_logging.py::test_configure_loggi
ng_reenables_file_handler_after_console_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_cli_context_wires_consol
e_and_json_file_handlers 
tests/unit/logging/test_logging_setup_contexts.py::test_test_context_redirects_a
nd_supports_console_only_toggle 
tests/unit/logging/test_logging_setup_contexts.py::test_create_dir_toggle_disabl
es_json_file_handler 
tests/unit/logging/test_logging_setup_contexts.py::test_console_and_json_handler
s_report_consistent_payloads 
tests/unit/logging/test_logging_setup_invariants.py::test_configure_logging_is_i
dempotent_for_handlers 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
masks_known_tokens 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
redacts_payload_and_details 
tests/unit/logging/test_logging_setup_invariants.py::test_redact_secrets_filter_
survives_mapping_errors 
tests/unit/logging/test_logging_setup_invariants.py::test_cli_to_test_context_sw
itch_updates_log_destination 
tests/unit/logging/test_logging_setup_invariants.py::test_json_formatter_include
s_structured_extras 
tests/unit/logging/test_logging_setup_levels.py::test_configure_logging_honors_e
nv_log_level 
tests/unit/logging/test_logging_setup_levels.py::test_json_formatter_captures_re
quest_context 
tests/unit/logging/test_logging_setup_levels.py::test_dev_logger_attaches_filter
s_and_handlers 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[create-dir-disabled] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reten
tion_matrix[no-file-env-create-dir-disabled] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[home-absolute] 
tests/unit/logging/test_logging_setup_retention.py::test_configure_logging_reloc
ates_absolute_paths[non-home-absolute] 
tests/unit/memory/test_issue3_regression_guard.py::test_issue3_findings_persist 
tests/unit/memory/test_layered_cache.py::test_promotes_value_to_higher_layer 
tests/unit/memory/test_layered_cache.py::test_hit_ratio_tracking 
tests/unit/memory/test_layered_cache.py::test_read_and_write_alias_methods 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_rel
oad_exposes_runtime_protocol 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_protocol_runtime_
checks_accept_custom_layers 
tests/unit/memory/test_layered_cache_runtime_protocol.py::test_layered_cache_pro
tocol_remains_runtime_checkable 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_initialization 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_missing_required_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_write_to_all_stores 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_from_first_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_fallback_to_second_store 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_read_raises_keyerror_if_not_found 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_commit 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_transaction_rollback_on_exception 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_m
emory_store_protocol_runtime_check 
tests/unit/memory/test_sync_manager_protocol.py::TestSyncManagerProtocol::test_s
ync_manager_with_generic_type 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_impor
t_and_construction_succeeds 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_accep
ts_optional_backends 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_still
_requires_primary_backend 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_rejec
ts_unknown_backend_names 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_stub_store_matches
_protocol_runtime 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_memory_store_param
eters_are_runtime_typevars 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_parameterised_memo
ry_store_runtime_is_safe 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_snapshot_alias_pre
serves_runtime_origin 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_value_typevar_iden
tity_is_preserved 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_sync_manager_and_s
napshot_share_runtime_typevar 
tests/unit/memory/test_sync_manager_protocol_runtime.py::test_transaction_rolls_
back_typed_stores 
tests/unit/memory/test_sync_manager_transaction_failure.py::test_transaction_rol
ls_back_all_stores 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_commit_unknown_tr
ansaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_rollback_unknown_
transaction_returns_false 
tests/unit/memory/test_transaction_lifecycle_failures.py::test_double_commit_fai
ls_and_state_persists 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_complete
s_with_deterministic_seed 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_tr
ansitions_and_memory_integration 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_time_bud
get_exceeded 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_consensu
s_error_handling 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_transien
t_error_retry 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_result_t
ype_handling 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_phase_ov
erride_from_result 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_invalid_
result_type_raises 
tests/unit/methodology/edrr/test_reasoning_loop.py::test_reasoning_loop_determin
istic_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_seeds_random_and_numpy_modules 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_logs_backoff_and_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_coordinator_records_each_phase 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_exits_when_total_budget_elapsed 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_accepts_dialectical_sequence_payload 
tests/unit/methodology/edrr/test_reasoning_loop_additional_branches.py::test_rea
soning_loop_records_unknown_phase_and_next_phase_fallbacks 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_returns_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_imp
ort_accessor_default_path_executes 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_dia
lectical_sequence_records_with_coordinator_fallback 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_tolerates_seed_failures 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_trace_complete 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_configures_seed_providers 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_budget_precheck 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_retries_then_succeeds 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_retry_exhaustion_sets_stop 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_copies_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_handles_dialectical_sequence 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_raises_for_non_mapping_payload 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_halts_when_result_missing 
tests/unit/methodology/edrr/test_reasoning_loop_branch_completeness.py::test_rea
soning_loop_branch_matrix 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_exhausts_retry_budget_and_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_retries_clamp_sleep_to_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_stops_retry_when_total_budget_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_coordinator_records_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_records_dialectical_sequences_for_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_fallbacks_for_invalid_phase_and_next_phase 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_honors_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_control_flow.py::test_reasoning_
loop_seeds_random_sources 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_preserves_nonstandard_phase_without_hints 
tests/unit/methodology/edrr/test_reasoning_loop_extended_phases.py::test_reasoni
ng_loop_handles_extended_phase_transitions 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_enforces_total_time_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retries_until_success 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_fallback_transitions_and_propagation 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_respects_max_iterations_limit 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_retry_backoff_respects_remaining_budget 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_honors_phase_and_next_phase_fields 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_clamps_retry_when_budget_consumed 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_invariants.py::test_reasoning_lo
op_logs_retry_exhaustion_telemetry 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_exits_when_budget_elapsed_before_iteration 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_retry_sequence_updates_phase_and_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_regressions.py::test_reasoning_l
oop_records_results_before_consensus_failure 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
tries_on_transient 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_emits_debug_and_clamps_sleep 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_without_budget_uses_base_backoff 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_clamps_backoff_and_respects_budget 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_remaining_budget_spent 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
cords_consensus_failure_via_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_lo
gs_consensus_failure_without_coordinator 
tests/unit/methodology/edrr/test_reasoning_loop_retry.py::test_reasoning_loop_re
try_stops_when_budget_already_exhausted 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_invalid_next
_phase_falls_back_to_transition_map 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_missing_stat
us_relies_on_max_iterations 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_raises_for_non_mapping_results 
tests/unit/methodology/edrr/test_reasoning_loop_safeguards.py::test_reasoning_lo
op_rejects_non_mapping_task_payload 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_handles_seed_failures_gracefully 
tests/unit/methodology/edrr/test_reasoning_loop_seed_fallbacks.py::test_reasonin
g_loop_logs_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_import_he
lper_exposes_typed_apply 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_immediate_timeout_skips_apply_invocation 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_respects_total_budget_and_emits_debug 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_uses_fallback_after_invalid_phase 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_stops_after_retry_exhaustion 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_seeds_random_and_numpy 
tests/unit/methodology/edrr/test_reasoning_loop_timeouts_fast.py::test_reasoning
_loop_applies_synthesis_to_task 
tests/unit/methodology/test_adhoc_adapter.py::test_should_start_cycle_true 
tests/unit/methodology/test_adhoc_adapter.py::test_should_progress_to_next_phase
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_evaluation
_terminates_with_many_hooks 
tests/unit/methodology/test_dialectical_reasoner_termination.py::test_hooks_cont
inue_after_exception 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_record
s_results 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_logs_c
onsensus_failure 
tests/unit/methodology/test_dialectical_reasoning.py::test_reasoning_loop_persis
ts_phase_results 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
uns_until_complete 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_l
ogs_consensus_failure 
tests/unit/methodology/test_dialectical_reasoning_loop.py::test_reasoning_loop_r
espects_max_iterations 
tests/unit/methodology/test_edrr_coordinator.py::test_automate_retrospective_rev
iew_summarizes_results 
tests/unit/methodology/test_edrr_coordinator.py::test_record_consensus_failure_l
ogs tests/unit/methodology/test_kanban_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_kanban_adapter.py::test_progress_respects_wip_limit 
tests/unit/methodology/test_milestone_adapter.py::test_should_start_cycle 
tests/unit/methodology/test_milestone_adapter.py::test_progress_requires_approva
l_when_configured 
tests/unit/methodology/test_reasoning_loop_time_budget.py::test_reasoning_loop_r
espects_total_time_budget 
tests/unit/methodology/test_sprint_adapter.py::test_calculate_phase_end_time 
tests/unit/methodology/test_sprint_adapter.py::test_is_phase_time_exceeded_false
tests/unit/methodology/test_sprint_adapter.py::test_should_progress_when_time_ex
ceeded 
tests/unit/methodology/test_sprint_adapter.py::test_ceremony_mapping_to_phase 
tests/unit/methodology/test_sprint_adapter.py::test_before_cycle_provides_contex
t 
tests/unit/methodology/test_sprint_adapter.py::test_before_expand_sets_phase_sta
rt_time 
tests/unit/methodology/test_sprint_adapter.py::test_after_retrospect_captures_sp
rint_plan 
tests/unit/methodology/test_sprint_hooks.py::test_map_ceremony_to_phase_defaults
tests/unit/methodology/test_sprint_hooks.py::test_adapter_uses_ceremony_defaults
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_graph_tran
sitions_complete 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_failure_br
anch_sets_failed 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_retry_bran
ch_succeeds_with_max_retries 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_streaming_
callback_called 
tests/unit/orchestration/test_graph_transitions_and_controls.py::test_cancellati
on_pauses_before_first_step 
tests/unit/policies/test_verify_security_policy.py::test_passes_when_all_variabl
es_set 
tests/unit/policies/test_verify_security_policy.py::test_fails_when_variable_mis
sing 
tests/unit/providers/test_provider_contract.py::test_stub_provider_offline_defau
lts_to_stub 
tests/unit/providers/test_provider_stub_offline.py::test_adapter_openai_provider
_stub_offline 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_uses_
safe_provider 
tests/unit/providers/test_provider_system_additional.py::test_offline_mode_null_
provider 
tests/unit/providers/test_provider_system_additional.py::test_unknown_provider_f
alls_back 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_us
es_provider_config 
tests/unit/providers/test_provider_system_additional.py::test_retry_decorator_re
spects_track_metrics_flag 
tests/unit/providers/test_provider_system_additional.py::test_stub_provider_dete
rministic_embeddings 
tests/unit/providers/test_provider_system_additional.py::test_create_tls_config_
uses_settings 
tests/unit/providers/test_provider_system_additional.py::test_provider_factory_p
refers_explicit_tls_config 
tests/unit/providers/test_provider_system_additional.py::test_fallback_async_ski
ps_open_circuit 
tests/unit/providers/test_provider_system_branches.py::test_factory_honors_disab
le_flag 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_s
tub_safe_default 
tests/unit/providers/test_provider_system_branches.py::test_offline_guard_uses_n
ull_when_requested 
tests/unit/providers/test_provider_system_branches.py::test_explicit_openai_with
out_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_availabilit
y_guard_returns_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_fallback_fa
ilure_promotes_safe_provider 
tests/unit/providers/test_provider_system_branches.py::test_explicit_anthropic_w
ithout_key_returns_null 
tests/unit/providers/test_provider_system_branches.py::test_anthropic_unsupporte
d_error 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_us
es_next_provider_on_failure 
tests/unit/providers/test_provider_system_branches.py::test_fallback_provider_pr
opagates_failure_when_all_fail 
tests/unit/providers/test_provider_system_branches.py::test_fallback_disabled_tr
ies_only_first_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_initializat
ion_orders_providers_and_records_circuit_results 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_skips
_open_circuit_breaker 
tests/unit/providers/test_provider_system_branches.py::test_openai_async_retry_e
mits_telemetry 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_circu
it_breaker_recovery 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[primary-success] 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[secondary-success] 
tests/unit/providers/test_provider_system_branches.py::test_async_fallback_metri
cs_permutations[all-fail] 
tests/unit/providers/test_provider_system_branches.py::test_factory_applies_tls_
and_retry_settings 
tests/unit/providers/test_provider_system_branches.py::test_emit_retry_telemetry
_logs_and_counts 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_comp
lete_builds_payload 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_reje
cts_invalid_temperature 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_embe
d_returns_embeddings 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_co
mplete_uses_custom_messages 
tests/unit/providers/test_provider_system_branches.py::test_fallback_embed_moves
_to_next_provider 
tests/unit/providers/test_provider_system_branches.py::test_fallback_aembed_reco
vers_from_failure 
tests/unit/providers/test_provider_system_branches.py::test_get_provider_config_
reads_env_file 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ope
nai_success_path 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_ant
hropic_requires_key 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_unk
nown_provider_uses_null 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_lms
tudio_fallback_when_openai_missing 
tests/unit/providers/test_provider_system_branches.py::test_openai_provider_asyn
c_paths 
tests/unit/providers/test_provider_system_branches.py::test_provider_factory_rea
l_module_branches 
tests/unit/providers/test_provider_system_branches.py::test_lmstudio_provider_as
ync_paths 
tests/unit/providers/test_provider_system_branches.py::test_complete_helper_incr
ements_metrics_and_propagates_error 
tests/unit/providers/test_provider_system_branches.py::test_embed_helper_wraps_n
on_provider_errors 
tests/unit/providers/test_provider_system_branches.py::test_acomplete_helper_inc
rements_metrics 
tests/unit/providers/test_provider_system_branches.py::test_aembed_helper_promot
es_unexpected_errors 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_skip
_by_default 
tests/unit/providers/test_resource_gating_meta.py::test_openai_marked_tests_run_
when_enabled 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_components_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_identify_
affected_requirements_deterministic 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_generate_
arguments_sorted 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_edrr_phas
e_mapping_on_persist 
tests/unit/requirements/test_dialectical_reasoner_determinism.py::test_evaluatio
n_hook_invoked_on_consensus_true 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[chromadb] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[faiss] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[kuzu] 
tests/unit/retrieval/test_backend_gating_smoke.py::test_backend_importable_when_
enabled[tinydb] 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_file_operations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_network_calls 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_global_state 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestDependencyAnalyzer
::test_detects_fixture_usage 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_simple_test_file 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_analyzes_file_with_isolation_marker 
tests/unit/scripts/test_analyze_test_dependencies.py::TestTestFileAnalyzer::test
_handles_syntax_errors 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_generates_recommendations 
tests/unit/scripts/test_analyze_test_dependencies.py::TestRecommendationGenerati
on::test_calculates_percentages 
tests/unit/scripts/test_analyze_test_dependencies.py::TestIntegration::test_end_
to_end_analysis 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_help 
tests/unit/scripts/test_analyze_test_dependencies.py::test_main_function_missing
_test_dir 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_test_execution_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_coverage_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_categ
orizes_validation_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_shell_script 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAnalyzer::test_handl
es_syntax_errors 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_finds_
testing_scripts 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_analyz
es_overlaps 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_git_us
age_frequency 
tests/unit/scripts/test_audit_testing_scripts.py::TestScriptAuditor::test_genera
tes_consolidation_recommendations 
tests/unit/scripts/test_audit_testing_scripts.py::TestMarkdownGeneration::test_g
enerates_markdown_report 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_help 
tests/unit/scripts/test_audit_testing_scripts.py::test_main_function_missing_scr
ipts_dir 
tests/unit/scripts/test_audit_testing_scripts.py::test_integration_audit_workflo
w 
tests/unit/scripts/test_auto_issue_comment.py::test_parse_issue_numbers_extracts
_ids 
tests/unit/scripts/test_auto_issue_comment.py::test_dry_run_when_env_missing 
tests/unit/scripts/test_auto_issue_comment.py::test_posts_comment_when_env_prese
nt 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_initialization 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_success 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_timeout 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_run_benchmark_failure 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_empty 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_analyze_results_with_data 
tests/unit/scripts/test_benchmark_test_execution.py::TestTestExecutionBenchmark:
:test_generates_recommendations 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_help 
tests/unit/scripts/test_benchmark_test_execution.py::test_main_function_invalid_
workers 
tests/unit/scripts/test_benchmark_test_execution.py::test_integration_benchmark_
workflow 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
valid_anchor 
tests/unit/scripts/test_check_internal_links.py::test_check_internal_links_with_
missing_anchor 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_integratio
n_missing_component 
tests/unit/scripts/test_enhanced_test_parser.py::test_build_test_path_unit 
tests/unit/scripts/test_enhanced_test_parser_marker_parity.py::test_parametrize_
speed_marker_parity 
tests/unit/scripts/test_examples_smoke_script.py::test_main_default_examples_suc
ceeds 
tests/unit/scripts/test_examples_smoke_script.py::test_main_reports_failure_when
_analyze_raises 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_error_when_syntax_is
_invalid 
tests/unit/scripts/test_find_syntax_errors.py::test_returns_zero_with_no_errors 
tests/unit/scripts/test_gen_ref_pages.py::test_gen_ref_pages_matches_examples 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_with_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_coverage_metrics_without_file 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_get_property_test_metrics 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_calculate_overall_quality_score 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_generate_quality_recommendations 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_quality_score_with_missing_mutation 
tests/unit/scripts/test_generate_quality_report.py::TestQualityReportGenerator::
test_recommendations_for_good_metrics 
tests/unit/scripts/test_generate_quality_report.py::test_html_generation 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_invokes_cli 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_translates_featur
es 
tests/unit/scripts/test_run_all_tests_wrapper.py::test_wrapper_returns_error_for
_failures 
tests/unit/scripts/test_security_ops.py::test_collect_logs_missing_directory 
tests/unit/scripts/test_security_ops.py::test_run_audit_calls_security_audit 
tests/unit/scripts/test_security_ops.py::test_list_outdated_runs_poetry 
tests/unit/scripts/test_security_ops.py::test_apply_updates_runs_poetry 
tests/unit/scripts/test_security_scan_script.py::test_main_non_strict_no_tools_r
eturns_ok 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_passes_when_above 
tests/unit/scripts/test_verify_coverage_threshold.py::test_verify_coverage_thres
hold_fails_when_below 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_valid 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_issue 
tests/unit/scripts/test_verify_mvuu_references.py::test_verify_mvuu_affected_fil
es_missing_mvuu 
tests/unit/scripts/test_verify_release_state.py::test_draft_status_missing_tag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_without_t
ag 
tests/unit/scripts/test_verify_release_state.py::test_published_status_with_tag 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_returns
_fields 
tests/unit/scripts/test_verify_release_state.py::test_parse_front_matter_without
_header 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_missing 
tests/unit/scripts/test_verify_release_state.py::test_tag_exists_when_present 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_when_log_mi
ssing 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_unreso
lved_questions 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_only_r
esolved 
tests/unit/scripts/test_verify_release_state.py::test_audit_is_clean_with_invali
d_json 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_collect
ion_error 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_cache_i
nvalidation 
tests/unit/scripts/test_verify_test_markers.py::test_verify_test_markers_path_fi
lter 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_f
lags_missing_docs 
tests/unit/scripts/test_verify_test_markers.py::test_find_undocumented_markers_p
asses_when_documented 
tests/unit/scripts/test_verify_test_markers_cli.py::test_argparser_includes_chan
ged_flag 
tests/unit/scripts/test_verify_test_markers_cli.py::test_verify_files_with_temp_
test 
tests/unit/scripts/test_verify_test_markers_cross_check.py::test_argparser_inclu
des_cross_check_flag 
tests/unit/scripts/test_wsde_edrr_simulation.py::test_simulation_converges 
tests/unit/security/test_api_authentication.py::test_verify_token_valid_is_valid
tests/unit/security/test_api_authentication.py::test_verify_token_invalid_is_val
id 
tests/unit/security/test_api_authentication.py::test_verify_token_missing_succee
ds 
tests/unit/security/test_api_authentication.py::test_verify_token_wrong_format_s
ucceeds 
tests/unit/security/test_api_authentication.py::test_verify_token_access_control
_disabled_succeeds 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_password_hasher_parameters_safe_defaults 
tests/unit/security/test_auth_and_encryption_defaults.py::TestArgon2Defaults::te
st_hash_and_verify_roundtrip 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_generate_key_validates_and_encrypts 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_invalid_key_rejected 
tests/unit/security/test_auth_and_encryption_defaults.py::TestFernetKeyValidatio
n::test_missing_key_env_raises 
tests/unit/security/test_authentication_optional_dependency.py::test_authenticat
ion_handles_missing_argon2 
tests/unit/security/test_authorization_checks.py::test_require_authorization_all
ows_authorized_action 
tests/unit/security/test_authorization_checks.py::test_require_authorization_rai
ses_forbidden 
tests/unit/security/test_deployment_coverage.py::test_require_non_root_user_when
_not_required 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_missing_vars 
tests/unit/security/test_deployment_coverage.py::test_check_required_env_vars_wi
th_all_present 
tests/unit/security/test_deployment_coverage.py::test_apply_secure_umask 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_with_requir
ed_env 
tests/unit/security/test_deployment_coverage.py::test_harden_runtime_without_req
uired_env 
tests/unit/security/test_encryption.py::test_generate_key_returns_expected_resul
t 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_roundtrip_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_string_key_succeeds
tests/unit/security/test_encryption.py::test_get_fernet_with_bytes_key_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_with_env_var_succeeds 
tests/unit/security/test_encryption.py::test_get_fernet_no_key_raises_error 
tests/unit/security/test_encryption.py::test_encrypt_decrypt_with_env_var_succee
ds 
tests/unit/security/test_encryption.py::test_decrypt_invalid_token_raises_error 
tests/unit/security/test_encryption.py::test_decrypt_with_wrong_key_raises_error
tests/unit/security/test_logging_redaction.py::test_logging_redacts_openai_api_k
ey 
tests/unit/security/test_logging_redaction.py::test_logging_redacts_in_extra_det
ails 
tests/unit/security/test_memory_encryption.py::test_json_file_store_encryption_s
ucceeds 
tests/unit/security/test_memory_encryption.py::test_lmdb_store_encryption_succee
ds 
tests/unit/security/test_memory_encryption.py::test_tinydb_store_encryption_succ
eeds tests/unit/security/test_policy_audit.py::test_audit_detects_violation 
tests/unit/security/test_policy_audit.py::test_audit_passes_clean_file 
tests/unit/security/test_review.py::test_review_due_when_interval_elapsed 
tests/unit/security/test_review.py::test_review_not_due_before_interval 
tests/unit/security/test_review.py::test_next_review_date_calculation 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_script_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_control_ch
ars_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_removes_both_succe
eds 
tests/unit/security/test_sanitization.py::test_sanitize_input_strips_whitespace_
succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_script_tags_suc
ceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_no_control_chars_s
ucceeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_complex_script_tag
s_succeeds 
tests/unit/security/test_sanitization.py::test_sanitize_input_multiple_script_ta
gs_succeeds 
tests/unit/security/test_sanitization.py::test_validate_safe_input_with_safe_inp
ut_returns_expected_result 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_s
cript_raises_error 
tests/unit/security/test_sanitization.py::test_validate_safe_input_raises_with_c
ontrol_chars_raises_error 
tests/unit/security/test_security_audit.py::test_run_executes_checks 
tests/unit/security/test_security_audit.py::test_run_raises_on_policy_failure 
tests/unit/security/test_security_audit.py::test_report_writes_results 
tests/unit/security/test_security_audit.py::test_report_records_failure 
tests/unit/security/test_security_audit.py::test_run_requires_pre_deploy 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_runs_che
cks 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_respects
_skip_flags 
tests/unit/security/test_security_audit_cmd.py::test_security_audit_cmd_register
ed 
tests/unit/security/test_security_flags_env.py::test_authentication_disabled_all
ows_any_credentials 
tests/unit/security/test_security_flags_env.py::test_authentication_enabled_enfo
rces 
tests/unit/security/test_security_flags_env.py::test_authorization_disabled_allo
ws 
tests/unit/security/test_security_flags_env.py::test_authorization_enabled_enfor
ces 
tests/unit/security/test_security_flags_env.py::test_sanitization_disabled_no_er
ror 
tests/unit/security/test_security_flags_env.py::test_sanitization_enabled_raises
tests/unit/security/test_tls_config.py::test_tls_config_timeout_env_override 
tests/unit/security/test_tls_config.py::test_tls_config_timeout_explicit_overrid
e 
tests/unit/security/test_tls_config.py::test_tls_config_validation_raises_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_partial_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_validation_key_only_succ
eeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_cert_only_suc
ceeds 
tests/unit/security/test_tls_config.py::test_tls_config_validation_missing_raise
s_error 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_defau
lt_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_verif
y_false_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
ca_file_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_and_key_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_with_
cert_only_has_expected 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_ca_fi
le_precedence_succeeds 
tests/unit/security/test_tls_config.py::test_tls_config_as_requests_kwargs_all_p
arams_succeeds 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_valid_string_
is_valid 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[   ] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_invalid_strin
g_is_valid[None] 
tests/unit/security/test_validation.py::TestValidateNonEmpty::test_non_string_va
lue_succeeds 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[5-5_1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[-5--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_is_
valid[0-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[5-1-10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[1-1-10-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[10-1-10-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[-5--10-0--5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_valid_int_wit
h_range_is_valid[0--10-10-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[abc] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[1.5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[None] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value4] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_invalid_int_i
s_valid[value5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[0-1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[-5-0] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_below_min_val
ue_succeeds[5-10] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[10-5] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[0--1] 
tests/unit/security/test_validation.py::TestValidateIntRange::test_above_max_val
ue_succeeds[100-99] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[a-choices0] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[1-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[True-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[None-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[value-choices4] 
tests/unit/security/test_validation.py::TestValidateChoice::test_valid_choice_is
_valid[5-choices5] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[d-choices0] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[4-choices1] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[None-choices2] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[missing-choices3] 
tests/unit/security/test_validation.py::TestValidateChoice::test_invalid_choice_
is_valid[20-choices4] 
tests/unit/specifications/test_mvuu_config_schema_validation.py::test_mvuu_confi
g_schema_and_sample_validate 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_option 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_analyze_repo_with_no_path 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_run_tests_command 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_standard_cli_fallback 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_missing_run_tests_m
odule 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_cli_import_errors 
tests/unit/test_cli.py::TestCLIEntryPoint::test_main_handles_runtime_errors 
tests/unit/test_sentinel_speed_markers.py::test_sentinel_fast_bucket_present 
tests/unit/test_simple_addition.py::test_add_returns_sum_for_integers 
tests/unit/test_simple_addition.py::test_add_accepts_floats_and_mixed_numeric_ty
pes 
tests/unit/test_simple_addition.py::test_add_raises_type_error_for_non_numeric_i
nputs 
tests/unit/test_verify_test_organization_sentinel.py::test_verify_test_organizat
ion_returns_zero 
tests/unit/testing/test_collect_behavior_fallback.py::test_collect_behavior_test
s_fallback_when_no_tests_ran 
tests/unit/testing/test_collect_cache_sanitize.py::test_sanitize_node_ids_strips
_line_numbers_only_when_no_function_delimiter 
tests/unit/testing/test_collect_cache_sanitize.py::test_collect_tests_with_cache
_prunes_nonexistent_and_caches 
tests/unit/testing/test_collect_synthesize_on_empty.py::test_collect_tests_with_
cache_synthesizes_when_empty 
tests/unit/testing/test_collect_tests_cache_bad_json.py::test_collect_tests_with
_cache_bad_json 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_file_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_marker_change 
tests/unit/testing/test_collect_tests_cache_invalidation.py::test_cache_invalida
tion_on_target_path_change 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_uses_fresh_cache_
without_subprocess_call 
tests/unit/testing/test_collect_tests_cache_ttl.py::test_cache_ttl_expired_trigg
ers_subprocess_and_refresh 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_respects_ttl_expiry 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_regenerates_on_fingerprint_mismatch 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_falls_back_to_cache_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_additional_paths.py::test_colle
ct_tests_with_cache_synthesizes_and_caches_node_ids 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_uses_
cached_and_prunes_when_collection_empty 
tests/unit/testing/test_collect_tests_with_cache_fallback.py::test_collect_falls
_back_to_unfiltered_and_returns_sanitized_ids 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_union_
reaches_threshold_with_overlap 
tests/unit/testing/test_coverage_segmentation_simulation.py::test_segment_thresh
old_detection_matches_cli_expectations 
tests/unit/testing/test_deterministic_seed_fixture.py::test_deterministic_seed_f
ixture_sets_env_vars 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_bad_ttl_env_falls_back_to_
default 
tests/unit/testing/test_env_ttl_and_sanitize.py::test_sanitize_node_ids_preserve
s_function_qualifier_and_strips_line_numbers 
tests/unit/testing/test_failure_tips.py::test_failure_tips_contains_core_guidanc
e 
tests/unit/testing/test_html_report_artifacts.py::test_html_report_artifacts_cre
ated_with_stable_naming 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_can_mutate_addition 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_mutates_addition_to_subtraction 
tests/unit/testing/test_mutation_testing.py::TestArithmeticOperatorMutator::test
_cannot_mutate_non_arithmetic 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_can_mutate_equality 
tests/unit/testing/test_mutation_testing.py::TestComparisonOperatorMutator::test
_mutates_equality_to_inequality 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_ca
n_mutate_and_operation 
tests/unit/testing/test_mutation_testing.py::TestBooleanOperatorMutator::test_mu
tates_and_to_or 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_can_
mutate_not_operation 
tests/unit/testing/test_mutation_testing.py::TestUnaryOperatorMutator::test_muta
tes_not_by_removal 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_can_mutat
e_boolean_constant 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_t
rue_to_false 
tests/unit/testing/test_mutation_testing.py::TestConstantMutator::test_mutates_n
umber_to_zero_and_one 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_mutations_for_simple_code 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_handles
_syntax_errors 
tests/unit/testing/test_mutation_testing.py::TestMutationGenerator::test_generat
es_different_mutation_types 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_initializa
tion 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_killed 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_run_single
_mutation_survived 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
esult_dataclass 
tests/unit/testing/test_mutation_testing.py::TestMutationTester::test_mutation_r
eport_dataclass 
tests/unit/testing/test_mutation_testing.py::test_integration_mutation_workflow 
tests/unit/testing/test_run_tests.py::test_sanitize_node_ids_strips_line_numbers
_without_function_delimiter 
tests/unit/testing/test_run_tests.py::test_failure_tips_contains_key_guidance_li
nes 
tests/unit/testing/test_run_tests.py::test_run_tests_keyword_filter_no_matches 
tests/unit/testing/test_run_tests.py::test_run_tests_segment_batches 
tests/unit/testing/test_run_tests.py::test_collect_tests_with_cache_writes_cache
_and_sanitizes 
tests/unit/testing/test_run_tests_additional_coverage.py::test_failure_tips_ment
ions_core_troubleshooting_flags 
tests/unit/testing/test_run_tests_additional_coverage.py::test_ensure_pytest_cov
_plugin_env_injects_and_skips 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_coverage_artifact
s_status_missing_json 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_success 
tests/unit/testing/test_run_tests_additional_coverage.py::test_enforce_coverage_
threshold_errors 
tests/unit/testing/test_run_tests_additional_coverage.py::test_sanitize_node_ids
_removes_line_numbers 
tests/unit/testing/test_run_tests_additional_coverage.py::test_collect_tests_wit
h_cache_handles_timeout 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_collect_tests_
with_cache_handles_subprocess_exception 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_hand
les_unexpected_execution_error 
tests/unit/testing/test_run_tests_additional_error_paths.py::test_run_tests_segm
ent_merges_extra_marker 
tests/unit/testing/test_run_tests_artifacts.py::test_reset_coverage_artifacts_re
moves_stale_files 
tests/unit/testing/test_run_tests_artifacts.py::test_ensure_coverage_artifacts_g
enerates_reports 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_fails_when_pytest
_cov_missing 
tests/unit/testing/test_run_tests_artifacts.py::test_run_tests_successful_single
_batch 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_h
andles_missing_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_r
ejects_invalid_json 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_missing_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_d
etects_empty_html 
tests/unit/testing/test_run_tests_artifacts.py::test_coverage_artifacts_status_s
uccess 
tests/unit/testing/test_run_tests_artifacts.py::test_failure_tips_includes_comma
nd_context 
tests/unit/testing/test_run_tests_benchmark_warning.py::test_segmented_run_treat
s_benchmark_warning_as_success 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_failure_tips_con
tains_suggestions 
tests/unit/testing/test_run_tests_cache_prune_and_tips.py::test_collect_tests_wi
th_cache_prunes_nonexistent_and_caches 
tests/unit/testing/test_run_tests_cache_pruning.py::test_prunes_nonexistent_path
s_and_uses_cache 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_i
nject_plugins_and_emit_tips 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batch_exc
eption_emits_tips_and_plugins 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_segmented_batches_r
einject_when_env_mutates 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_env_var_p
ropagation_retains_existing_addopts 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_run_tests_option_wi
ring_includes_expected_flags 
tests/unit/testing/test_run_tests_cli_helpers_focus.py::test_failure_tips_surfac
e_cli_remediations 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_expression_
includes_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_failure_surfaces_a
ctionable_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_batches_fo
llow_segment_size 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_segment_failure_em
its_aggregate_tips 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_han
dles_resource_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_marker_filters_mer
ge_extra_marker 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_report_mode_adds_h
tml_argument 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_co
verage_totals 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_skips_placeh
older_artifacts 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_env_passthrough_an
d_coverage_lifecycle 
tests/unit/testing/test_run_tests_cli_invocation.py::test_cli_keyword_filter_ret
urns_success_when_no_matches 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_for_normal_profile 
tests/unit/testing/test_run_tests_cli_invocation.py::test_run_tests_generates_ar
tifacts_with_autoload_disabled 
tests/unit/testing/test_run_tests_collection_cache.py::test_sanitize_node_ids_st
rips_trailing_line_without_function_delimiter 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_reset_coverage_art
ifacts_removes_files_and_directories 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_data_missing 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_warns_when_no_measured_files 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_generates_reports_and_syncs_legacy 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_skips_when_module_unavailable 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_ensure_coverage_ar
tifacts_html_failure_still_writes_json 
tests/unit/testing/test_run_tests_coverage_artifacts.py::test_run_tests_writes_m
anifest_with_coverage_reference 
tests/unit/testing/test_run_tests_coverage_artifacts_fragments.py::test_ensure_c
overage_artifacts_combines_fragment_files 
tests/unit/testing/test_run_tests_coverage_short_circuit.py::test_ensure_coverag
e_artifacts_short_circuits_without_measured_files 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_repor
ts_missing_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_flags
_invalid_json 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_totals 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_requi
res_html_index 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_rejec
ts_empty_html 
tests/unit/testing/test_run_tests_coverage_status.py::test_coverage_status_succe
ss_path 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_failure_tips_formats_
return_code_and_cmd 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_reset_coverage_artifa
cts_handles_oserror 
tests/unit/testing/test_run_tests_coverage_uplift.py::test_ensure_coverage_artif
acts_handles_unreadable_html 
tests/unit/testing/test_run_tests_extra.py::test_keyword_filter_no_matches_retur
ns_success 
tests/unit/testing/test_run_tests_extra.py::test_failure_tips_appended_on_nonzer
o_return 
tests/unit/testing/test_run_tests_extra_marker.py::test_keyword_filter_lmstudio_
no_matches_returns_success 
tests/unit/testing/test_run_tests_extra_marker.py::test_extra_marker_merges_into
_m_expression 
tests/unit/testing/test_run_tests_extra_marker_passthrough.py::test_run_tests_me
rges_extra_marker_into_category_expression 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_fallback_on_behav
ior_speed_no_tests 
tests/unit/testing/test_run_tests_extra_paths.py::test_collect_malformed_cache_r
egenerates 
tests/unit/testing/test_run_tests_extra_paths.py::test_run_tests_lmstudio_extra_
marker_keyword_early_success 
tests/unit/testing/test_run_tests_failure_tips.py::test_failure_tips_include_com
mon_flags 
tests/unit/testing/test_run_tests_keyword_exec.py::test_keyword_marker_executes_
matching_node_ids 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_no_matc
hes_returns_success_message 
tests/unit/testing/test_run_tests_keyword_filter.py::test_keyword_filter_honors_
report_flag_and_creates_report_dir 
tests/unit/testing/test_run_tests_keyword_filter_empty.py::test_run_tests_lmstud
io_keyword_filter_with_no_matches_returns_success 
tests/unit/testing/test_run_tests_logic.py::test_sanitize_node_ids_strips_line_n
umbers_without_function_delimiter 
tests/unit/testing/test_run_tests_logic.py::test_failure_tips_contains_key_guida
nce_lines 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_uses_c
ache 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_regene
rates_when_expired 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_miss 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_mtime 
tests/unit/testing/test_run_tests_logic.py::test_collect_tests_with_cache_invali
dated_by_marker 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_success 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_single_execut
ion_failure 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_segmented_exe
cution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_marker_expres
sion_building 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_env_defaults 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exception_han
dling 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_exit_code_5_s
uccess 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_dry_run_skips
_execution 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_keyword_filte
r 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_maxfail_optio
n 
tests/unit/testing/test_run_tests_main_function.py::test_run_tests_smoke_mode_pl
ugin_injection 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_s
uccess 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_f
rom_existing_cache 
tests/unit/testing/test_run_tests_main_logic.py::test_collect_tests_with_cache_c
ollection_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_basic_execution 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_verbose_and_repo
rt 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_markers_and
_keyword_filter 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_maxfail 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_custom_env 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_collection_failu
re_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_no_tests_collect
ed_returns_true_with_message 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_execution_failur
e_returns_false 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_segmented_execut
ion_with_failure 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_parallel_executi
on_disabled_by_segment 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_env_var_pro
pagation 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_no_target_p
ath_raises_error 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_empty_speed
_categories_uses_all 
tests/unit/testing/test_run_tests_main_logic.py::test_run_tests_with_specific_sp
eed_categories 
tests/unit/testing/test_run_tests_marker_fallback.py::test_run_tests_marker_fall
back_skips_segmentation 
tests/unit/testing/test_run_tests_marker_fallback.py::test_build_segment_metadat
a_uses_typed_sequences 
tests/unit/testing/test_run_tests_marker_merge.py::test_speed_marker_merged_with
_lmstudio_keyword_filter 
tests/unit/testing/test_run_tests_marker_merge.py::test_global_marker_with_lmstu
dio_keyword_filter 
tests/unit/testing/test_run_tests_module.py::test_sanitize_node_ids_dedup_and_st
rip_line_numbers 
tests/unit/testing/test_run_tests_module.py::test_collect_tests_with_cache_uses_
cache_and_respects_ttl 
tests/unit/testing/test_run_tests_module.py::test_run_tests_translates_args_and_
handles_return_codes 
tests/unit/testing/test_run_tests_module.py::test_run_tests_keyword_filter_for_e
xtra_marker_lmstudio 
tests/unit/testing/test_run_tests_module.py::test_run_tests_handles_popen_except
ion_without_speed_filters 
tests/unit/testing/test_run_tests_module.py::test_collect_unknown_target_uses_al
l_tests_path 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_exi
t_and_return 
tests/unit/testing/test_run_tests_module.py::test_failure_tips_includes_segmenta
tion_guidance 
tests/unit/testing/test_run_tests_module.py::test_run_tests_segment_appends_aggr
egation_tips 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_missing_file 
tests/unit/testing/test_run_tests_module.py::test_enforce_coverage_threshold_err
ors_on_invalid_json 
tests/unit/testing/test_run_tests_no_xdist_assertions.py::test_run_tests_complet
es_without_xdist_assertions 
tests/unit/testing/test_run_tests_option_parsing.py::test_parse_pytest_addopts_h
andles_balanced_and_unbalanced_quotes 
tests/unit/testing/test_run_tests_option_parsing.py::test_addopts_has_plugin_det
ects_split_and_concatenated_forms 
tests/unit/testing/test_run_tests_option_parsing.py::test_coverage_plugin_disabl
ed_detects_common_overrides 
tests/unit/testing/test_run_tests_orchestration.py::test_verbose_flag_adds_v_to_
pytest_command 
tests/unit/testing/test_run_tests_orchestration.py::test_report_flag_adds_html_r
eport_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_no_parallel_flag_adds_n
0_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_maxfail_flag_adds_maxfa
il_to_command 
tests/unit/testing/test_run_tests_orchestration.py::test_segment_flags_trigger_s
egmented_run 
tests/unit/testing/test_run_tests_orchestration.py::test_pytest_addopts_are_pres
erved 
tests/unit/testing/test_run_tests_orchestration.py::test_extra_marker_adds_m_fla
g_to_command 
tests/unit/testing/test_run_tests_parallel_flags.py::test_run_tests_parallel_inc
ludes_cov_and_n_auto 
tests/unit/testing/test_run_tests_parallel_no_cov.py::test_parallel_injects_cov_
reports_and_xdist_auto 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env0-False---no-cov -s] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_cov_plugin_env-initial_env1-True--k smoke -p 
pytest_cov] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env2-False--p 
no:pytest_bdd -s] 
tests/unit/testing/test_run_tests_plugin_env.py::test_ensure_pytest_plugin_env_a
ddopts_overrides[ensure_pytest_bdd_plugin_env-initial_env3-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_handles_subprocess_timeout 
tests/unit/testing/test_run_tests_plugin_timeouts.py::test_collect_tests_with_ca
che_honors_env_timeout 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_respects_explicit_disables 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_detects_inline_plugin_token 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[--no-cov -s-False---no-cov -s] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_ensure_pytest_cov_p
lugin_env_handles_explicit_optouts[-k smoke-True--k smoke -p pytest_cov] 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_pytest_cov_support_
status_missing_plugin 
tests/unit/testing/test_run_tests_pytest_cov_plugin.py::test_run_tests_aborts_wh
en_pytest_cov_missing 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_adds_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_requires_autoload_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_detects_existing_plugin 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_respects_explicit_disable 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-p no:pytest_bdd -s-False--p no:pytest_bdd 
-s] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_ensure_pytest_bdd_
plugin_env_handles_explicit_optouts[-k feature-True--k feature -p 
pytest_bdd.plugin] 
tests/unit/testing/test_run_tests_pytest_plugins_bdd.py::test_pytest_plugins_reg
isters_pytest_bdd_once 
tests/unit/testing/test_run_tests_report.py::test_run_tests_report_injects_html_
args_and_creates_dir 
tests/unit/testing/test_run_tests_returncode5_success.py::test_single_pass_non_k
eyword_returncode_5_is_success 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_strips_tra
iling_line_numbers_without_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_keeps_ids_
with_function_sep 
tests/unit/testing/test_run_tests_sanitize_node_ids.py::test_sanitize_deduplicat
es_preserving_order 
tests/unit/testing/test_run_tests_segmentation.py::test_segmented_batches_surfac
e_plugin_fallbacks_and_failure_tips 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_single_speed 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_multiple_speeds 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_no_tests_found 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_failure_with_maxfail 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_all_tests_decomposes_successfully 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_timeout_falls_back_to_direct_collection 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_collect_tests_wi
th_cache_reuses_cache_and_preserves_environment 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_segmented_te
sts_dry_run_batches_use_typed_requests 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_command_building 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_multiple_node_ids 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_smoke_mode_env 
tests/unit/testing/test_run_tests_segmentation_helpers.py::test_run_single_test_
batch_no_parallel 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_batches
_execute 
tests/unit/testing/test_run_tests_segmented.py::test_run_tests_segmented_honors_
keyword_filter 
tests/unit/testing/test_run_tests_segmented.py::test_run_segmented_tests_stop_af
ter_maxfail 
tests/unit/testing/test_run_tests_segmented_aggregate_fail_tips_once.py::test_se
gmented_failure_appends_aggregate_tips_once 
tests/unit/testing/test_run_tests_segmented_aggregate_maxfail.py::test_segmented
_aggregate_tips_command_includes_maxfail 
tests/unit/testing/test_run_tests_segmented_empty_node_ids.py::test_run_tests_se
gmented_falls_back_on_empty_collection 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_failure_appends_tips 
tests/unit/testing/test_run_tests_segmented_failure_paths.py::test_segment_batch
_benchmark_warning_forces_success 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_segmente
d_failure_surfaces_remediation 
tests/unit/testing/test_run_tests_segmented_failures.py::test__run_segmented_tes
ts_aggregates_outputs 
tests/unit/testing/test_run_tests_segmented_failures.py::test_segmented_runs_rei
nject_plugins_without_clobbering_addopts 
tests/unit/testing/test_run_tests_segmented_failures.py::test_run_tests_single_b
atch_uses_request_object 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_success_invokes_publish 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_failure_skips_graph 
tests/unit/testing/test_run_tests_segmented_orchestration.py::test_run_tests_seg
mented_reports_append_graph 
tests/unit/testing/test_run_tests_segmented_report_flag.py::test_run_segmented_t
ests_reports_only_last_segment 
tests/unit/testing/test_run_tests_speed_keyword_loop.py::test_speed_loop_uses_ke
yword_filter_and_executes_node_ids 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_merges_fast
_and_medium_collections 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_defaults_to
_fast_and_medium_when_unspecified 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_excludes_gu
i_by_default 
tests/unit/testing/test_run_tests_speed_selection.py::test_run_tests_allows_gui_
when_requested 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_strips_line
_numbers_without_function_selector 
tests/unit/testing/test_sanitize_node_ids.py::test_sanitize_node_ids_preserves_o
rder 
tests/unit/testing/test_sanitize_node_ids_minimal.py::test_sanitize_node_ids_str
ips_line_when_no_function 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_tuple_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_invalid
_exc_info 
tests/unit/utils/test_logging_coverage.py::test_get_logger_returns_correct_insta
nce 
tests/unit/utils/test_logging_coverage.py::test_setup_logging_with_different_log
_levels 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_false_e
xc_info 
tests/unit/utils/test_logging_coverage.py::test_dev_synth_logger_handles_none_ex
c_info 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
baseexception_direct 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
true_with_active_exception 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_exc_info_
none_and_false_explicit 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_invalid_e
xc_info_to_hit_line_48 
tests/unit/utils/test_logging_final_coverage.py::test_get_logger_function_direct
_call 
tests/unit/utils/test_logging_final_coverage.py::test_setup_logging_function_dir
ect_calls 
tests/unit/utils/test_logging_final_coverage.py::test_dev_synth_logger_with_kwar
gs 
tests/unit/utils/test_logging_utils.py::test_dev_synth_logger_normalizes_exc_inf
o_tuple_and_exception 
tests/unit/utils/test_logging_utils.py::test_setup_logging_calls_configure_loggi
ng 
tests/unit/utils/test_logging_utils.py::test_get_logger_returns_dev_synth_logger
_instance 
tests/unit/utils/test_serialization.py::test_dumps_deterministic_round_trip_simp
le tests/unit/utils/test_serialization.py::test_dump_and_load_file_round_trip 
tests/unit/utils/test_serialization.py::test_provider_env_as_dict_deterministic_
serialization 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_with_s
tring_already_having_newline 
tests/unit/utils/test_serialization_coverage.py::test_dumps_deterministic_ensure
s_single_newline 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_no_trailing_new
line 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_trailing_newlin
e 
tests/unit/utils/test_serialization_coverage.py::test_dump_to_file_complete_cove
rage 
tests/unit/utils/test_serialization_coverage.py::test_load_from_file_complete_co
verage 
tests/unit/utils/test_serialization_coverage.py::test_loads_with_multiple_traili
ng_newlines 
tests/unit/utils/test_serialization_coverage.py::test_serialization_with_unicode
_characters 
tests/unit/utils/test_serialization_coverage.py::test_serialization_edge_cases 
tests/unit/utils/test_serialization_coverage.py::test_file_operations_with_speci
al_paths 
tests/unit/utils/test_serialization_edges.py::test_loads_tolerates_missing_and_s
ingle_trailing_newline 
tests/unit/utils/test_serialization_edges.py::test_dump_to_file_overwrites_and_k
eeps_single_newline 
tests/unit/utils/test_serialization_extra.py::test_dumps_and_loads_deterministic
_round_trip_unicode_and_newline 
tests/unit/utils/test_serialization_extra.py::test_dump_and_load_file_round_trip
_handles_utf8 
tests/unit/utils/test_serialization_extra.py::test_loads_accepts_without_trailin
g_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
direct_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
with_string_that_might_have_newline 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_direct_line_co
verage 
tests/unit/utils/test_serialization_final_coverage.py::test_dump_to_file_direct_
line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_load_from_file_direc
t_line_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_serialization_functi
ons_with_mock_to_ensure_coverage 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_with_various_n
ewline_scenarios 
tests/unit/utils/test_serialization_final_coverage.py::test_file_operations_with
_explicit_paths 
tests/unit/utils/test_serialization_final_coverage.py::test_dumps_deterministic_
return_path 
tests/unit/utils/test_serialization_final_coverage.py::test_loads_return_path 
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_pass 
tests/integration/agents/test_generation/test_run_generated_tests.py::test_run_g
enerated_tests_failure 
tests/integration/agents/test_generation/test_scaffold_generation.py::test_scaff
old_hook_creates_placeholder 
tests/integration/agents/test_generation/test_scaffold_generation.py::test_proce
ss_generates_tests_and_scaffolds 
tests/integration/api/test_api_startup.py::test_api_health_and_metrics_startup_w
ithout_binding_ports 
tests/integration/api/test_api_startup.py::test_agent_openapi_documents_workflow
_models 
tests/integration/deployment/test_compose_workflow.py::test_setup_env_refuses_ro
ot 
tests/integration/deployment/test_compose_workflow.py::test_check_health_env_per
missions 
tests/integration/deployment/test_compose_workflow.py::test_rollback_requires_ta
g 
tests/integration/deployment/test_deployment_scripts.py::test_bootstrap_env_refu
ses_root 
tests/integration/deployment/test_deployment_scripts.py::test_health_check_valid
ates_url 
tests/integration/deployment/test_deployment_scripts.py::test_prometheus_exporte
r_refuses_root 
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions[start_stack.sh] 
tests/integration/deployment/test_deployment_scripts.py::test_stack_scripts_env_
permissions[stop_stack.sh] 
tests/integration/general/test_complex_workflow.py::test_cmd 
tests/integration/general/test_end_to_end_workflow.py::test_cmd 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_registration 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_configuration_loading 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_settings_extraction 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_initialization_with_defaults 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_provider_mock_initialization 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_environment_variable_handling 
tests/integration/general/test_lmstudio_integration_regression.py::TestLMStudioI
ntegrationRegression::test_lmstudio_config_file_integration 
tests/integration/generated/test_generated_module.py::test_generated_module_work
flow 
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_success 
tests/integration/generated/test_run_generated_tests.py::test_run_generated_test
s_failure 
tests/integration/llm/test_lmstudio_timing_baseline.py::test_timeout_configurati
on_sanity 
tests/integration/mvu/test_command_execution.py::test_mvu_exec_runs_command 
tests/integration/mvu/test_command_execution.py::test_mvu_exec_propagates_error 
tests/integration/utils/test_logging_integration.py::test_setup_logging_returns_
project_logger 
tests/integration/utils/test_logging_integration.py::test_log_normalizes_excepti
on 
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_results_mu
ltiple_categories 
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::tests_with_impr
ovements 
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_collection
_performance 
tests/behavior/steps/test_enhanced_test_infrastructure_steps.py::test_analysis_p
erformance 
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed 
tests/behavior/steps/test_webui_synthesis_steps.py::test_generation_executed_cus
tom tests/behavior/test_documentation_generation.py::test_docs_build 
tests/behavior/test_marker_auto_injection_guardrail.py::test_behavior_requires_e
xplicit_speed_marker 
tests/behavior/test_progress_failover_and_recursion.py::test_long_running_progre
ss_records_telemetry 
tests/behavior/test_progress_failover_and_recursion.py::test_run_with_long_runni
ng_progress_completes_on_exception 
tests/behavior/test_progress_failover_and_recursion.py::test_provider_factory_fa
lls_back_to_stub_offline 
tests/behavior/test_progress_failover_and_recursion.py::test_recursion_terminati
on_at_max_depth 
tests/behavior/test_webui_smoke.py::test_webui_layout_config_smoke -m not 
memory_intensive and fast and not gui --cov=src/devsynth 
--cov-report=json:test_reports/coverage.json --cov-report=html:htmlcov 
--cov-append
Troubleshooting tips:
- Smoke mode: reduce third-party plugin surface to isolate issues:
  poetry run devsynth run-tests --smoke --speed=fast --no-parallel --maxfail=1
- Marker discipline: default is '-m not memory_intensive'.
  Ensure exactly ONE of @pytest.mark.fast|medium|slow per test.
- Plugin autoload: avoid PYTEST_DISABLE_PLUGIN_AUTOLOAD unless using --smoke; 
plugin options may fail otherwise.
- Diagnostics: run 'poetry run devsynth doctor' for a quick environment check.
- Narrow scope: use '-k <expr>' and '-vv' to focus a failure.
- Segment large suites to localize failures and flakes:
  devsynth run-tests --target unit-tests --speed=fast --segment 
--segment-size=50
- Limit failures early to speed iteration:
  poetry run devsynth run-tests --target unit-tests --speed=fast --maxfail=1
- Disable parallelism if xdist interaction is suspected:
  devsynth run-tests --target unit-tests --speed=fast --no-parallel
- Generate an HTML report for context (saved under test_reports/):
  devsynth run-tests --target unit-tests --speed=fast --report

Tests failed
